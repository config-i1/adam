<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 A short introduction to main statistical ideas | Time Series Analysis and Forecasting with ADAM</title>
  <meta name="description" content="This textbook explains how to do time series analysis and forecasting using Advanced Dynamic Adaptive Model, implemented in smooth package for R." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 A short introduction to main statistical ideas | Time Series Analysis and Forecasting with ADAM" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This textbook explains how to do time series analysis and forecasting using Advanced Dynamic Adaptive Model, implemented in smooth package for R." />
  <meta name="github-repo" content="config-i1/adam" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 A short introduction to main statistical ideas | Time Series Analysis and Forecasting with ADAM" />
  
  <meta name="twitter:description" content="This textbook explains how to do time series analysis and forecasting using Advanced Dynamic Adaptive Model, implemented in smooth package for R." />
  

<meta name="author" content="Ivan Svetunkov" />


<meta name="date" content="2020-07-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="methods.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Time Series Analysis and Forecasting with ADAM</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#forecastingProcess"><i class="fa fa-check"></i><b>1.1</b> Forecasting process and forecasts evaluation</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#errorMeasures"><i class="fa fa-check"></i><b>1.1.1</b> Measuring accuracy of point forecasts</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#measuring-uncertainty"><i class="fa fa-check"></i><b>1.1.2</b> Measuring uncertainty</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#rolling-origin"><i class="fa fa-check"></i><b>1.1.3</b> Rolling origin</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="a-short-introduction-to-main-statistical-ideas.html"><a href="a-short-introduction-to-main-statistical-ideas.html"><i class="fa fa-check"></i><b>2</b> A short introduction to main statistical ideas</a><ul>
<li class="chapter" data-level="2.1" data-path="a-short-introduction-to-main-statistical-ideas.html"><a href="a-short-introduction-to-main-statistical-ideas.html#distributions"><i class="fa fa-check"></i><b>2.1</b> Theory of distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="a-short-introduction-to-main-statistical-ideas.html"><a href="a-short-introduction-to-main-statistical-ideas.html#normal-distribution"><i class="fa fa-check"></i><b>2.1.1</b> Normal distribution</a></li>
<li class="chapter" data-level="2.1.2" data-path="a-short-introduction-to-main-statistical-ideas.html"><a href="a-short-introduction-to-main-statistical-ideas.html#laplace-distribution"><i class="fa fa-check"></i><b>2.1.2</b> Laplace distribution</a></li>
<li class="chapter" data-level="2.1.3" data-path="a-short-introduction-to-main-statistical-ideas.html"><a href="a-short-introduction-to-main-statistical-ideas.html#s-distribution"><i class="fa fa-check"></i><b>2.1.3</b> S distribution</a></li>
<li class="chapter" data-level="2.1.4" data-path="a-short-introduction-to-main-statistical-ideas.html"><a href="a-short-introduction-to-main-statistical-ideas.html#generalised-normal-distribution"><i class="fa fa-check"></i><b>2.1.4</b> Generalised Normal distribution</a></li>
<li class="chapter" data-level="2.1.5" data-path="a-short-introduction-to-main-statistical-ideas.html"><a href="a-short-introduction-to-main-statistical-ideas.html#asymmetric-laplace-distribution"><i class="fa fa-check"></i><b>2.1.5</b> Asymmetric Laplace distribution</a></li>
<li class="chapter" data-level="2.1.6" data-path="a-short-introduction-to-main-statistical-ideas.html"><a href="a-short-introduction-to-main-statistical-ideas.html#log-normal-log-laplace-log-s-and-log-gn-distributions"><i class="fa fa-check"></i><b>2.1.6</b> Log Normal, Log Laplace, Log S and Log GN distributions</a></li>
<li class="chapter" data-level="2.1.7" data-path="a-short-introduction-to-main-statistical-ideas.html"><a href="a-short-introduction-to-main-statistical-ideas.html#inverse-gaussian-distribution"><i class="fa fa-check"></i><b>2.1.7</b> Inverse Gaussian distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>3</b> Methods</a></li>
<li class="chapter" data-level="4" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>4</b> Applications</a><ul>
<li class="chapter" data-level="4.1" data-path="applications.html"><a href="applications.html#example-one"><i class="fa fa-check"></i><b>4.1</b> Example one</a></li>
<li class="chapter" data-level="4.2" data-path="applications.html"><a href="applications.html#example-two"><i class="fa fa-check"></i><b>4.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>5</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Time Series Analysis and Forecasting with ADAM</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="a-short-introduction-to-main-statistical-ideas" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> A short introduction to main statistical ideas</h1>
<p>Before moving forward and discussing distributions and models, it is also quite important to make sure that we understand what <strong>bias</strong>, <strong>efficiency</strong> and <strong>consistency</strong> of estimates of parameters mean. Although there are strict statistical definitions of the aforementioned terms (you can easily find them in Wikipedia or anywhere else), I do not want to copy-paste them here, because there are only a couple of important points worth mentioning in our context.</p>
<p><strong>Bias</strong> refers to the expected difference between the estimated value of parameter (on a specific sample) and the “true” one (in the true model). Having unbiased estimates of parameters is important because they should lead to more accurate forecasts (at least in theory). For example, if the estimated parameter is equal to zero, while in fact it should be 0.5, then the model would not take the provided information into account correctly and as a result will produce less accurate point forecasts and incorrect prediction intervals. In inventory context this may mean that we constantly order 100 units less than needed only because the parameter is lower than it should be.</p>
<p><strong>Efficiency</strong> means, if the sample size increases, then the estimated parameters will not change substantially, they will vary in a narrow range (variance of estimates will be small). In the case with inefficient estimates the increase of sample size from 50 to 51 observations may lead to the change of a parameter from 0.1 to, let’s say, 10. This is bad because the values of parameters usually influence both point forecasts and prediction intervals. As a result the inventory decision may differ radically from day to day. For example, we may decide that we urgently need 1000 units of product on Monday, and order it just to realise on Tuesday that we only need 100. Obviously this is an exaggeration, but no one wants to deal with such an erratically behaving model, so we need to have efficient estimates of parameters.</p>
<p><strong>Consistency</strong> means that our estimates of parameters will get closer to the stable values (true value in the population) with the increase of the sample size. This is important because in the opposite case estimates of parameters will diverge and become less and less realistic. This once again influences both point forecasts and prediction intervals, which will be less meaningful than they should have been. In a way consistency means that with the increase of the sample size the parameters will become more efficient and less biased. This in turn means that the more observations we have, the better.</p>
<div class="remark">
<p>
There is a prejudice in the world of practitioners that the situation in the market changes so fast that the old observations become useless very fast. As a result many companies just through away the old data. Although, in general the statement about the market changes is true, the forecasters tend to work with the models that take this into account (e.g. Exponential smoothing, ARIMA, discussed in this book). These models adapt to the potential changes. So, we may benefit from the old data because it allows us getting more consistent estimates of parameters. Just keep in mind, that you can always remove the annoying bits of data but you can never un-throw away the data.
</p>
</div>
<p>Another important aspect to cover is what the term <strong>asymptotic</strong> means in our context. Here and after in this book, when this word is used, we refer to an unrealistic hypothetical situation of having all the data in the multiverse, where the time index <span class="math inline">\(t \rightarrow \infty\)</span>. While this is impossible, the idea is useful, because asymptotic behaviour of estimators and models is helpful on large samples of data.</p>
<p>Finally, we will use different estimation techniques throughout this book, one of the main of which is <strong>Maximum Likelihood Estimate</strong> (MLE). We will not go into explanation of what specifically this is at this stage, but a rough understanding should suffice. In case of MLE, we assume that a variable follows a parametric distribution and that the parameters of the model that we use can be optimised in order to maximise the respective probability density function. The main advantages of MLE is that it gives consistent, asymptotically efficient and normal estimates of parameters.</p>
<p>Now that we have a basic understanding of these statistical terms, we can move to the next topic, distributions.</p>
<div id="distributions" class="section level2">
<h2><span class="header-section-number">2.1</span> Theory of distributions</h2>
<p>There are several probability distributions that will be helpful in the further chapters of this textbook. Here, I want to briefly discuss those of them that will be useful.</p>
<div id="normal-distribution" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Normal distribution</h3>
<p>Every statistical textbook has normal distribution. It is that one famous bell-curved distribution that every statistician likes because it is easy to work with and it is an asymptotic distribution for many other well-behaved distributions in some conditions (so called “Central Limit Theorem”). Here is the probability density function (PDF) of this distribution:
<span class="math display" id="eq:Normal">\[\begin{equation}
    f(y_t) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{\left(y_t - \mu_t \right)^2}{2 \sigma^2} \right) ,
    \tag{2.1}
\end{equation}\]</span>
where <span class="math inline">\(y_t\)</span> is the value of the response variable, <span class="math inline">\(\mu_t\)</span> is the mean on observation <span class="math inline">\(t\)</span> and <span class="math inline">\(\sigma^2\)</span> is the variance of the error term. The maximum likelihood estimate of <span class="math inline">\(\sigma^2\)</span> is:
<span class="math display" id="eq:sigmaNormal">\[\begin{equation}
    \hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^T \left(y_t - \mu_t \right)^2 ,
    \tag{2.2}
\end{equation}\]</span>
which coincides with Mean Squared Error (MSE), discussed in the <a href="intro.html#errorMeasures">section 1</a>.</p>
<p>And here how this distribution looks:</p>
<p><img src="adam_files/figure-html/dnormPlot-1.png" width="672" /></p>
<p>What we typically assume in the basic time series models is that a variable is random and follows normal distribution, meaning that there is a central tendency (in our case - the mean <span class="math inline">\(mu\)</span>), around which the concentration of values is the highest and there are other potential cases, but their probability of appearance reduces proportionally to the distance from the centre.</p>
<p>The normal distribution has skewness of zero and kurtosis of 3 (and excess kurtosis, being kurtosis minus three, of 0).</p>
<p>Additionally, if normal distribution is used for the maximum likelihood estimation of a model, it gives the same parameters as the minimisation of MSE would give.</p>
</div>
<div id="laplace-distribution" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Laplace distribution</h3>
<p>A more exotic distribution is Laplace, which has some similarities with Normal, but has higher excess. It has the following PDF:</p>
<p><span class="math display" id="eq:Laplace">\[\begin{equation}
    f(y_t) = \frac{1}{2 s} \exp \left( -\frac{\left| y_t - \mu_t \right|}{s} \right) ,
    \tag{2.3}
\end{equation}\]</span>
where <span class="math inline">\(s\)</span> is the scale parameter, which, when estimated using likelihood, is equal to the Mean Absolute Error (MAE):
<span class="math display" id="eq:sLaplace">\[\begin{equation}
    \hat{s} = \frac{1}{T} \sum_{t=1}^T \left| y_t - \mu_t \right| .
    \tag{2.4}
\end{equation}\]</span></p>
<p>It has the following shape:</p>
<p><img src="adam_files/figure-html/dlaplacePlot-1.png" width="672" /></p>
<p>Similar to the normal distribution, the skewness of Laplace is equal to zero. However, it has fatter tails - its kurtosis is equal to 6 instead of 3.</p>
</div>
<div id="s-distribution" class="section level3">
<h3><span class="header-section-number">2.1.3</span> S distribution</h3>
<p>This is something relatively new, but not ground braking. I have derived S distribution few years ago, but have never written a paper on that. It has the following density function:
<span class="math display" id="eq:S">\[\begin{equation}
    f(y_t) = \frac{1}{4 s^2} \exp \left( -\frac{\sqrt{|y_t - \mu_t|}}{s} \right) ,
    \tag{2.5}
\end{equation}\]</span>
where <span class="math inline">\(s\)</span> is the scale parameter. If estimated via maximum likelihood, the scale parameter is equal to:
<span class="math display" id="eq:sS">\[\begin{equation}
    \hat{s} = \frac{1}{2T} \sum_{t=1}^T \sqrt{\left| y_t - \mu_t \right|} ,
    \tag{2.6}
\end{equation}\]</span>
which corresponds to the minimisation of a half of “Mean Root Absolute Error” or “Half Absolute Moment” (HAM). This is a more exotic type of scale, but the main benefit of this distribution is sever heavy tails - it has kurtosis of 25.2. It might be useful in cases of randomly occurring incidents and extreme values (Black Swans?).</p>
<p><img src="adam_files/figure-html/dsPlot-1.png" width="672" /></p>
</div>
<div id="generalised-normal-distribution" class="section level3">
<h3><span class="header-section-number">2.1.4</span> Generalised Normal distribution</h3>
<p>Generalised Normal distribution (as the name says) is a generalisation for normal distribution, which also includes Laplace and S as special cases <span class="citation">(Nadarajah <a href="#ref-Nadarajah2005">2005</a>)</span>. There are two versions of this distribution: one with a shape and another with a skewness parameter. We are mainly interested in the first one, which has the following PDF:
<span class="math display" id="eq:GND">\[\begin{equation}
    f(y_t) = \frac{\beta}{2 s \Gamma(\beta^{-1})} \exp \left( -\left(\frac{|y_t - \mu_t|}{s}\right)^{\beta} \right),
    \tag{2.7}
\end{equation}\]</span>
where <span class="math inline">\(\beta\)</span> is the shape parameter, and <span class="math inline">\(s\)</span> is the scale of the distribution, which, when estimated via MLE, is equal to:
<span class="math display" id="eq:sGND">\[\begin{equation}
    \hat{s} = \sqrt[^beta]{\frac{\beta}{T} \sum_{t=1}^T\left| y_t - \mu_t \right|^{\beta}},
    \tag{2.8}
\end{equation}\]</span>
which has MSE, MAE and HAM as special cases, when <span class="math inline">\(\beta\)</span> is equal to 2, 1 and 0.5 respectively. The parameter <span class="math inline">\(\beta\)</span> influences the kurtosis directly, it can be calculated for each special case as <span class="math inline">\(\frac{\Gamma(5/\beta)\Gamma(1/\beta)}{\Gamma(3/\beta)^2}\)</span>. The higher <span class="math inline">\(\beta\)</span> is, the lower the kurtosis is.</p>
<p>The advantage of GN distribution is it’s flexibility. In theory, it is possible to model extremely rare events with this distribution, if the shape parameter <span class="math inline">\(\beta\)</span> is fractional and close to zero. Alternatively, when <span class="math inline">\(\beta \rightarrow \infty\)</span>, the distribution converges point-wise to the uniform distribution on <span class="math inline">\((\mu_t - s, \mu_t + s)\)</span>.</p>
<p>Note that the estimation of <span class="math inline">\(\beta\)</span> is a difficult task, especially, when it is less than 2 - the MLE of it looses properties of consistency and asymptotic normality.</p>
<p>Dending on the value of <span class="math inline">\(\beta\)</span>, the distribution can have different shapes:</p>
<p><img src="adam_files/figure-html/dgnormPlot-1.png" width="672" /></p>
</div>
<div id="asymmetric-laplace-distribution" class="section level3">
<h3><span class="header-section-number">2.1.5</span> Asymmetric Laplace distribution</h3>
</div>
<div id="log-normal-log-laplace-log-s-and-log-gn-distributions" class="section level3">
<h3><span class="header-section-number">2.1.6</span> Log Normal, Log Laplace, Log S and Log GN distributions</h3>
</div>
<div id="inverse-gaussian-distribution" class="section level3">
<h3><span class="header-section-number">2.1.7</span> Inverse Gaussian distribution</h3>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Nadarajah2005">
<p>Nadarajah, Saralees. 2005. “A generalized normal distribution.” <em>Journal of Applied Statistics</em> 32 (7): 685–94. <a href="https://doi.org/10.1080/02664760500079464">https://doi.org/10.1080/02664760500079464</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/Chapters//02-statistics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["adam.pdf", "adam.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
