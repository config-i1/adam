[["index.html", "Forecasting and Analytics with ADAM Preface", " Forecasting and Analytics with ADAM Ivan Svetunkov 2021-07-02 Preface This textbook uses two packages from R, namely greybox, which focuses on forecasting using regression models, and smooth, which implements Single Source of Error (SSOE) state space models for purposes of time series analysis and forecasting. The textbook focuses on explaining how ADAM (“ADAM is Dynamic Adaptive Model” - recursive acronym), one of the smooth functions (introduced in v3.0.0) works, also showing how it can be used in practice with examples from R. ADAM is a state space model based on exponential smoothing in ETS form and ARIMA. It encompasses both models and is expanded by introducing: Explanatory variables (including time varying parameters); Multiple frequencies; Handling intermittent data (data with natural zeroes); Handling missing data; Variables and components selection and combination; Analysis of parameters of the model; And other minor features. All these extentions are needed in order to solve specific real life problems, so we will have examples and case studies later in the book, in order to see how all of this can be used. If you want to run exampels from the textbook, two packages are needed (Svetunkov, 2021a, 2021b): install.packages(&quot;greybox&quot;) install.packages(&quot;smooth&quot;) Some explanations of functions from the packages are given in my blog: Package greybox for R, Package smooth for R. A very important thing to note is that this textbook does not use tidyverse packages. I like base R, and, to be honest, I am sure that tidyverse packages are great, but I have never needed them in my research. So, I will not use pipeline operators, tibble or tsibble objects and ggplot2. It is assumed throughout the textbook that you can do all those nice tricks on your own if you want to. If you want to get in touch with me, there are lots of ways to do that: comments section on any page of my website, my Russian website, vk.com, Facebook, Linkedin, Twitter. You can also find me on ResearchGate, StackExchange and StackOverflow, although I’m not really active there. Finally, I also have GitHub account. You can use the following to cite the online version of this book: Svetunkov, I. (2021) Forecasting and Analytics with ADAM: Lancaster, UK. openforecast.org/adam. Accessed on [current date]. If you use LaTeX, the following can be used instead: @MISC{SvetunkovAdam, title = {Forecasting and Analytics with ADAM}, author = {Ivan Svetunkov}, howpublished = {OpenForecast}, note = {(version: [current date])}, url = {https://openforecast.org/adam/}, year = {2021} } License This textbook is licensed under Creative Common License by-nc-sa 4.0, which means that you can share, copy, redistribute and remix the content of the textbook for non-commercial purposes as long as you give appropriate credit to the author and provide the link to the original license. If you remix, transform, or build upon the material, you must distribute your contributions under the same CC-BY-NC-SA 4.0 license. See the explanation on the Creative Commons website. What is ADAM? ADAM, as stated earlier, stands for “Augmented Dynamic Adaptive Model.” It is “Augmented,” because it is not just ETS or ARIMA, it is a combination of the two with additional features. It is “Dynamic,” because it includes ETS and ARIMA components (dynamic models). It is “Adaptive,” because it has mechanism of update of parameters of explanatory variables. It is one framework for constructing ETS / ARIMA / Regression, based on more advanced statistical instruments. For example, classical ARIMA is built on the assumption of normality of the error term, but ADAM lifts this assumption and allows using other distributions as well (e.g. Generalised Normal, Inverse Gaussian etc). Another example, the conventional models are estimated either via the maximisation of the likelihood function or using basic losses like MSE or MAE, but ADAM has a wider spectrum of losses and allows using custom ones. There is much more, and different aspects of ADAM will be discussed in detail later in this textbook. For now, here is a brief list of features available in ADAM: ETS; ARIMA; Regression; TVP regression; Combination of (1), (2) and either (3), or (4); Automatic selection / combination of states for ETS; Automatic orders selection for ARIMA; Variables selection for regression part; Normal and non-normal distributions; Automatic selection of most suitable distributions; Advanced and custom loss functions; Multiple seasonality; Occurrence part of the model to handle zeroes in data (intermittent demand); Model diagnostics using plot() and other methods; Confidence intervals for parameters of models; Automatic outliers detection; Handling missing data; Fine tuning of persistence vector (smoothing parameters); Fine tuning of initial values of the state vector (e.g. level / trend / seasonality); Two initialisation options (optimal / backcasting); Provided ARMA parameters; Fine tuning of optimiser (select algorithm and convergence criteria); In this textbook we will discuss the model underlying ADAM and how different features are built in the function. Acknowledgments I would like to thank Tobias Schmidt for his help in refining earlier parts of the textbook and correcting grammatical mistakes. References "],["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction I started writing this book in 2020 during the COVID-19 pandemic, having figured out that it had been more than 10 years since the publishing of the fundamental textbook of (Hyndman et al., 2008), who discuss ETS (Error-Trend-Seasonality) framework in the Single Source of Error (SSOE) form and that the topic has not been updated substantially since then. If you are interested in learning more about exponential smoothing, then this is must-read material on the topic. However, there has been some progress in the area since 2008, and I have developed some models and functions based on SSOE, making the framework more flexible and general. Given that publication of all aspects of these models in peer-reviewed journals would be very time consuming, I have decided to summarise all progress in this book, showing what happens inside the models and how to use the functions in different cases, so that there is a source to refer to. References "],["modelsMethods.html", "1.1 Models, methods et al. ", " 1.1 Models, methods et al.  Before we move to nitty gritty details of the models, it is important to agree what we are talking about. So, here is a couple of definitions: Statistical model (or ‘stochastic model,’ or just ‘model’ in this textbook) is a ‘mathematical representation of a real phenomenon with a complete specification of distribution and parameters’ (Svetunkov and Boylan, 2019). Very roughly, the statistical model is something that contains a structure (defined by its parameters) and a noise that follows some distribution. True model is the idealistic statistical model that is correctly specified (has all the necessary components in correct form), and applied to the data in population. By this definition, true model is never reachable in reality, but it is achievable in theory if for some reason we know what components and variables and in what form should be in the model, and have all the data in the world. The notion itself is important when discussing how far the model that we use is from the true one. Estimated model (aka ‘applied model’ or ‘used model’) is the statistical model that was constructed and estimated on the available sample of data. This typically differs from the true model, because the latter is not known. Even if the specification of the true model is known for some reason, the parameters of the estimated model will differ from the true parameters due to sampling randomness, but will hopefully converge to the true ones if the sample size increases. Data generating process (DGP) is an artificial statistical model, showing how the data could be generated in theory. This notion is utopic and can be used in simulation experiments in order to check, how the selected model with the specific estimator behave in a specific setting. In real life, the data is not generated from any process, but is usually based on complex interactions between different agents in a dynamic environment. Note that I make a distinction between DGP and true model, because I do not think that the idea of something being generated using a mathematical formula is helpful. Many statisticians will not agree with me on this distinction. Forecasting method is a mathematical procedure that generates point and / or interval forecasts, with or without a statistical model (Svetunkov and Boylan, 2019). Very roughly, forecasting method is just a way of producing forecasts that does not explain how the components of time series interact with each other. It might be needed in order to filter out the noise and extrapolate the structure. Mathematically in the simplest case the true model can be presented in the form: \\[\\begin{equation} y_t = \\mu_{y,t} + \\epsilon_t, \\tag{1.1} \\end{equation}\\] where \\(y_t\\) is the actual observation, \\(\\mu_{y,t}\\) is the structure in the data and \\(\\epsilon_t\\) is the noise with zero mean, unpredictable element, which arises because of the effect of a lot of small factors. An example would be the daily sales of beer in a pub, which has some seasonality (we see growth in sales every weekends), some other elements of structure plus the white noise (I might go to a different pub, reducing the sales of beer by one pint). So what we typically want to do in forecasting is to capture the structure and also represnt the noise with a distribution with some parameters. When it comes to applying the chosen model to the data, it can be presented as: \\[\\begin{equation} y_t = \\hat{\\mu}_{y,t} + e_t, \\tag{1.2} \\end{equation}\\] where \\(\\hat{\\mu}_{y,t}\\) is the estimate of the structure and \\(e_t\\) is the estimate of the white noise (also known as “residuals”). As you see even if the structure is correctly captured, the main difference between (1.1) and (1.2) is that the latter is estimated on a sample, so we can only approximate the true structure with some degree of precision. If we generate the data from the model (1.1), then we can talk about the DGP, keeping in mind that we are talking about an artificial experiment, for which we know the true model and the parameters. This can be useful if we want to see how different models and estimators behave in different conditions. The simplest forecasting method can be represented with the equation: \\[\\begin{equation} \\hat{y}_t = \\hat{\\mu}_{y,t}, \\tag{1.3} \\end{equation}\\] where \\(\\hat{y}_t\\) is the point forecast. This equation does not explain where the structure and the noise come from, it just shows the way of producing point forecasts. In addition, we will discuss in this textbook two types of models: Additive, where (most) components are added to one another; Multiplicative, where the components are multiplied. (1.1) is an example of additive error model. A general example of multiplicative error model is: \\[\\begin{equation} y_t = \\mu_{y,t} \\varepsilon_t, \\tag{1.4} \\end{equation}\\] where \\(\\varepsilon_t\\) is some noise again, which in the reasonable cases should take only positive values and have mean of one. We will discuss this type of model later in the textbook. We will also see several examples of statistical models, forecasting methods, DGPs and other notions and discuss how they relate to each other. Furthermore, we will be talking about forecasting in this textbook, so it is also important to introduce the notation \\(\\hat{y}_{t+h}\\), which corresponds to the \\(h\\) steps ahead point forecast produced from the observation \\(t\\). Typically, this corresponds to the conditional expectation of the model \\(\\mu_{t+h|t}\\), given all the information on observation \\(t\\), although this does not hold all the time for the classical ETS models. When we use notation \\(\\mu_{y,t}\\), then this means one step ahead conditional expectation on observation \\(t\\), given the information on the previous observation \\(t-1\\). We will also discuss prediction interval, the term which means specific bounds that should include \\((1-\\alpha)\\times100\\)% of observations in the holdout sample (for example, 95% of observations). Finally, we will also use sometimes the term confidence interval usually refers to similar bounds constructed either for parameters of models or for the specific statistics (e.g. conditional mean or variance). Note that this textbook assumes that the reader is familiar with introductory statistics and knows basic forecasting principles. Hanck et al. (2020) is a good textbook on econometrics and statistics. Hyndman and Athanasopoulos (2018) can be a good start if you do not know forecasting principles. We will also use elements of linear algebra to explain some modelling parts, but this will not be the main focus of the textbook and you will be able to skip the more challenging parts without jeopardising the main understanding of the topic. References "],["scales.html", "1.2 Scales of information", " 1.2 Scales of information Whenever we work with information, we need to understand how to measure it properly. If we cannot do that, then we cannot make proper decisions, supported by some evidence. For example, if a person feels ill but we cannot say what the temperature of their body is, then we cannot decide, whether anything needs to be done to reduce it. If we can measure something then we can model it and produce forecasts. Continuing our example, if the temperature is 39°C, then we can conclude that the person is sick and needs to take Paracetamol or some other pills that would reduce the temperature. So, whenever we collect some sort of information about a system’s behaviour or about a process, we will inevitably deal with scales of information and it is important to understand what we are dealing with in order to process that information correctly. There are four fundamental scales: Nominal, Ordinal, Interval, Ratio. The first two form the so called “categorical” scale, while the latter two are typically united in the “numerical” one. Each one of these scales can have one of the following characteristics: Description, Order, Distance, Natural zero, Natural unit. The last characteristics is typically ignored analytics and forecasting as it does not provide any useful information. But as for the other four, they provide important properties to the scales of information, giving them more flexibility. Here we discuss the scales in detail. 1.2.1 Nominal scale This is the scale that only has “description” characteristics. It does not have an order, a distance or a natural zero. There is only one operation that can be done in this scale, and it is comparison, whether the value is “equal” or “not equal” to something. An example of data measured in such scale is the following question in a survey: What is your nationality? Russian, English, Greek, Swiss, Belgian, Lebanese, Indonesian, Other. In this case after collecting the data we can only say whether each respondent is Russian or not, English or not etc. So, the only thing that can be done with the data measured in this scale is to produce a basic summary, showing how many people selected one option or another. Among the statistical instruments, only the mode is useful, as it shows which of the options was selected the most. If there are several variables measured in nominal scale, we can calculate some measures of association to see if there are any patterns in respondents behaviour (e.g. those who select “Russian” would prefer Vodka, while those who selected “Belgian” will tend to drink “Beer”). When it comes to constructing models, the nominal scale is typically transformed in a set of dummy variables, which will be discussed later in regression analysis of this textbook. If you are not sure, whether your data is measured in nominal or another scale, you can do a simple test: if changing the places of two values does not break the scale, then this is the nominal one. For example, in the question above, moving “Greek” to the first place will not change anything, so this is indeed the nominal scale. Another example of nominal scale is the number on the T-shirt of football players. They are only descriptive, and if two players change numbers, this will not change anything (although it might confuse football fans). 1.2.2 Ordinal scale In addition to description, the ordinal scale has the “order.” It is possible to say that one value can be placed higher or lower than the other on a scale (thus, permitting operations “greater” and “smaller” in addition to the “equal” and “not equal”) However, it is not possible to say how far the elements are placed from each other, so the number of operations in the scale is still limited. Here is an example of a survey question with such scale: How old are you? Too young, Young, Not too young, Not too old Old, Too old. In this scale above we have a natural order, and when collecting the data in this scale we can conclude, whether a respondent is older than another one or not. Sometimes ordinal scales look confusing and seem to be of a higher level than they are, here is an example: How old are you? Younger than 16, 16 - 25, 26 - 40, 41 - 60 Older than 60. This is still an ordinal scale, because it has the natural order, and because we cannot measure the distance between the value: if, for example we subtract “16 - 25” from “26 - 40,” we will not get anything meaningful. The ordinal scale, being more complex than the nominal one, allows using some additional statistical instruments (besides the mode), such as quantiles of distribution, including median. Unfortunately, the arithmetic mean is not applicable to the data in ordinal scale, because of the absence of distance. Even if you encode every answer in numbers, the resulting average will not be meaningful. Indeed, if in the question above with the five options, we use the numbers (“1” for the first option, “2” for the second one, etc.) and take average, the resulting number of, for example, 3.75 will not mean anything, as there is no element in the scale that would correspond to that number. When it comes to measuring relations between two variables in ordinal scale, we can use Kendall’s \\(\\tau\\) correlation coefficient, Goodman-Kruskal’s \\(\\gamma\\) and Yule’s Q. These are discussed in detail in Section 2.6. As for using the variables in ordinal scale in modelling, the typical thing to do would be to create a set of dummy variables, similarly to how it is done for variables in nominals scale. As for the identification of scale, if in doubt, you can do any transformation of elements of scale without the loss of its meaning. For example, if we assign numbers from 1 to 5 to the responses above, we can square each one of them and get 1, 4, 9, 16 and 25, which would not change the original scale, but only encode the answers differently (select “16” for the option “41 - 60”). 1.2.3 Interval scale This scale is even more complex than the previous two, as in addition to description and order it also has a distance. This permits doing addition and subtraction to the elements of scale, which are meaningful operations in this case. Arithmetic mean and standard deviation become available in this scale in addition to all those used in lower level scales discussed above. The classical example of a variable measured in this scale is the temperature. Indeed, we can not only say if the temperature of one person is higher than the temperature of the other one, but also by how much: 39°C - 37°C = 2°C, which is a meaningful number in the scale. The only limitation in this scale is that there is no natural zero. 0°C does not mean the absence of temperature, but rather means the point at which water starts freezing. If we switch to Fahrenheit (although why would anyone do that?!), then the 0°F would correspond to the point, where the mixture of ice, water, and ammonium chloride used to stabilise back in 1724, when Fahrenheit proposed the scale. The relation between two variables in interval scale can be measured by Pearson’s correlation coefficient. The scale can be used in the model as is, although some error metrics cannot be used for the measurement of accuracy of models for this scale (for example, MAPE cannot be used as it assumes meaningful zero). Finally, when it comes to the identification of scale, only linear transformations are permitted for the variables without the loss of its properties. This means that if we measure temperature of two respondents and then do their linear transformations via \\(y=a+bx\\), then the characteristics of scale will not be broken: it will still have description, order and distance with the same meaning as prior to the transformation. In the example of temperature, this is how you switch, for example, from Celsius to Fahrenheit (\\(y=a+bx\\)). 1.2.4 Ratio scale The most complex of the four, this ratio has a natural zero (in addition to all the other characteristics). It permits any operations to the values of scale, including product, division, and non-linear transformations. Coefficient of variation can be used together in addition to all the previous instruments. An example of the information measured in this scale is the height of respondents in meters. You can compare two respondents via their height and not only say that one is higher than the other, but also by how much and how many times. All these operations will be meaningful in this scale. If you need to check, whether the variables is indeed in ratio scale, note that only the transformation via multiplication would maintain the meaning of the scale. For example, height measured in meters can be transformed into height in feet via the multiplication by approximately 3.28. If you add a constant to the values of scale, it will break it. Being the most complex, this scale permits usage of all correlation coefficients and all error metrics. Finally, the variables measured in this scale can be either integer or continuous. This might cause some confusions, because the integer numbers sometimes look suspiciously similar to the values of ordinal scale, but the tools of identification discussed above might help. If a company needs to buy 7 planes, then this is an integer variable measured in ratio scale: 7 planes is more than 6 planes by one plane, and zero planes means that there are no planes (all the characteristics of ratio scale). Furthermore, squaring the number of planes breaks the distance between them (\\(7^2 - 6^2 \\neq 1^2\\)), while linear transformation breaks the scale (\\(7\\times 2 + 3\\) has a completely different meaning in the scale than just 7). "],["statistics.html", "Chapter 2 Introduction to statistics", " Chapter 2 Introduction to statistics Before we move to the discussion of ETS, ARIMA and ADAM, it makes sense to discuss some of the basics statistical terms and what they mean in the context of forecasting. Although, many of them originate from statistics and econometrics, we need to look at them from a different perspective: we are more interested in the forecasting process rather than in the estimation of parameters. Still, if you do not know statistics and econometrics well and want to have a good source on the topic, I would recommend reading an online book by Hanck et al. (2020). Before we move further, we need to agree what the term “estimator” means, which will be used several times in this chapter: Estimate of a parameter is an in sample result of application of a statistical procedure to the data for obtaining some coefficients of a model. The value calculated using the arithmetic mean would be an estimate of the population mean; Estimator is the rule for calculating estimates of parameters based on a sample of data. For example, arithmetic mean is an estimator of the population mean. Another example would be method of Ordinary Least Squares, which is a rule for producing estimates of parameters of a regression model and thus an estimator. References "],["dataAnalysis.html", "2.1 Preliminary data analysis", " 2.1 Preliminary data analysis One of the basic thing that is worth doing before starting any modelling is the preliminary data analysis. This can be done either using numerical or graphical analysis. The former is useful when you want to have a summary information about the data without trying to find detailed information about it. The latter is useful when you can spend more time, investigating relations and issues in the data. In many cases, they compliment each other. 2.1.1 Numerical analysis In this section we will use the classical mtcars dataset from datasets package for R. It contains 32 observations with 11 variables. While all the variables are numerical, some of them are in fact categorical variables encoded as binary ones. We can check the description of the dataset in R: ?mtcars Judging by the explanation in the R documentation, the following variables are categorical: vs - Engine (0 = V-shaped, 1 = straight), am - Transmission (0 = automatic, 1 = manual). In addition, the following variables are integer numeric ones: cyl - Number of cylinders, hp - Gross horsepower, gear - Number of forward gears, carb - Number of carburetors. All the other variables are continuous numeric. Takign this into account, we will create a data frame, encoding the categorical variables as factors for further analysis: mtcarsData &lt;- data.frame(mtcars) mtcarsData$vs &lt;- factor(mtcarsData$vs,levels=c(0,1),labels=c(&quot;V-shaped&quot;,&quot;Straight&quot;)) mtcarsData$am &lt;- factor(mtcarsData$am,levels=c(0,1),labels=c(&quot;automatic&quot;,&quot;manual&quot;)) Given that we only have two options in those variables, it is not compulsory to do this encoding, but it will help us in the further analysis. We can start with the basic summary statistics. We remember from the scales of information (Section 1.2) that the nominal variables can be analysed only via frequencies, so this is what we can produce for them: table(mtcarsData$vs) ## ## V-shaped Straight ## 18 14 table(mtcarsData$am) ## ## automatic manual ## 19 13 These tables are called contingency tables, they show the frequency of appearance of values of variables. Based on this, we can conclude that the cars with V-shaped engine are met more often in the dataset than the cars with the Straight one. In addition, the automatic transmission prevails in the data. The related statistics which is useful for analysis of categorical variables is called mode. It shows which of the values happens most often in the data. Judging by the frequencies above, we can conclude that the mode for the first variable is the value “V-shaped.” All of this is purely descriptive information, which does not provide us much. We could probably get more information if we analysed the contingency table based on these two variables: table(mtcarsData$vs,mtcarsData$am) ## ## automatic manual ## V-shaped 12 6 ## Straight 7 7 For now, we can only conclude that the cars with V-shaped engine and automatic transmission are met more often than the other cars in the dataset. Next, we can look at the numerical variables. As we recall from Section 1.2, this scale supports all operations, so we can use quantiles, mean, standard deviation etc. Here how we can analyse, for example, the variable mpg: setNames(mean(mtcarsData$mpg),&quot;mean&quot;) ## mean ## 20.09062 quantile(mtcarsData$mpg) ## 0% 25% 50% 75% 100% ## 10.400 15.425 19.200 22.800 33.900 setNames(median(mtcarsData$mpg),&quot;median&quot;) ## median ## 19.2 The output above produces: Mean - the average value of mpg in the dataset, \\(\\bar{y}=\\frac{1}{n}\\sum_{j=1}^n y_j\\). Quantiles - the values that show, below which values the respective proportions of the dataset lie. For example, 25% of observations have mpg less than 15.425. The 25%, 50% and 75% quantiles are also called 1st, 2nd and 3rd quartiles respectively. Median, which splits the sample in two halves. It corresponds to the 50% quantile. If median is greater than mean, then this typically means that the distribution of the variable is skewed (it has some rare observations that have large values). This is the case in our case, we can investigate it further using skewness and kurtosis from timeDate package: timeDate::skewness(mtcarsData$mpg) ## [1] 0.610655 ## attr(,&quot;method&quot;) ## [1] &quot;moment&quot; timeDate::kurtosis(mtcarsData$mpg) ## [1] -0.372766 ## attr(,&quot;method&quot;) ## [1] &quot;excess&quot; Skewness shows the asymmetry of distribution. If it is greater than zero, then the distribution has the long right tail. If it is equal to zero, then it is symmetric. Kurtosis shows the excess of distribution (fatness of tails) in comparison with the normal distribution. If it is equal to zero, then it is the same as for normal distribution. Based on all of this, we can conclude that the distribution of mpg is skewed and has the longer right tail. This is expected for such variable, because the cars that have higher mileage are not common in this dataset. All the conventional statistics discussed above can be produced using the following summary for all variables in the dataset: summary(mtcarsData) ## mpg cyl disp hp ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 ## drat wt qsec vs am ## Min. :2.760 Min. :1.513 Min. :14.50 V-shaped:18 automatic:19 ## 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 Straight:14 manual :13 ## Median :3.695 Median :3.325 Median :17.71 ## Mean :3.597 Mean :3.217 Mean :17.85 ## 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 ## Max. :4.930 Max. :5.424 Max. :22.90 ## gear carb ## Min. :3.000 Min. :1.000 ## 1st Qu.:3.000 1st Qu.:2.000 ## Median :4.000 Median :2.000 ## Mean :3.688 Mean :2.812 ## 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :5.000 Max. :8.000 2.1.2 Graphical analysis Continuing our example with mtcars dataset, we now investigate what plots can be used for different types of data. As discussed earlier, we have two categorical variables: vs and am - and they need to be treated differently than the numerical ones. We can start by producing their barplots: barplotVS &lt;- barplot(table(mtcarsData$vs), xlab=&quot;Type of engine&quot;) text(barplotVS,table(mtcarsData$vs)/2,table(mtcarsData$vs),cex=1.25) Figure 2.1: Barplot for the engine type. This is just a graphical presentation of the contingency table we have already discussed earlier. Note that histograms do not make sense in case of categorical variables, because they assume that variables are numerical and continuous. Barplots are useful when you deal with either categorical variables or integer numerical ones. Here is what we can produce in case of the integer variable cyl: barplotCYL &lt;- barplot(table(mtcarsData$cyl), xlab=&quot;Number of cylinders&quot;) text(barplotCYL,table(mtcarsData$cyl)/2,table(mtcarsData$cyl),cex=1.25) Figure 2.2: Barplot for the number of cylinders. Figure 2.2 shows that there are three types of cars in the data: with 4, 6 and 8 cylinders. The most frequently met is the car with 8 cylinders. Judging by the plot, half of cars have not more than 6 cylinders (median is equal to 6). All of this can be deducted from the barplot. And here how the histogram would look like for cylinders: hist(mtcarsData$cyl) Figure 2.3: Histogram for the number of cylinders. Do not do this! Figure 2.3 is difficult to read, because on histogram, the bars show frequency at which continuous variable appears in pre-specified bins. In our case we would conclude that the most frequently cars in the dataset are those that have 7.5 - 8 cylinders, which is wrong and misleading. In addition, this basic plot does not have a readable label for x-axis and a meaningful title (in fact, we do not need one, given that we have caption). So, always label your axis and make sure that the text on plots is easy to understand for those people who do not work with the data. Coming back to categorical variables, we can construct two-dimensional plots to investigate potential relations between variables. We will first try the same barplot as above, but with vs and am variables: barplot(table(mtcarsData$vs,mtcarsData$am), xlab=&quot;Type of transmission&quot;, legend.text=levels(mtcarsData$vs)) Figure 2.4: Barplot for the type of engine and transmission. Figure 2.4 provides some information about the distribution of type of engine and transmission. For example, we can say that the most often met car in the dataset is the one with automatic transmission and V-shaped engine. However, it is not possible to say much about the relation between the two variables based on this plot. So, there is an alternative presentation, which uses the heat map (tableplot() from greybox): tableplot(mtcarsData$vs,mtcarsData$am, xlab=&quot;Type of engine&quot;, ylab=&quot;Type of transmission&quot;) Figure 2.5: Heat map for the type of engine and transmission. The idea of this plot is that the darkness of areas shows the frequency of occurrence of each specific value. The numbers inside the box show the proportions for each answer. So, we can conclude (again), that automatic transmission with V-shaped engine is met in 37.5% of cases. On the other hand, the least frequent type of car is the one with V-shaped engine and manual transmission. There might be some tendency in the dataset: the engine and transmission might be related (v-shaped with automatic vs Straigh with manual) - but it is not very well pronounced. The same table plot can be used for the analysis of relations between integer variables (and categorical). Here, for example, the plot between the number of cylinders and the type of engine: tableplot(mtcarsData$cyl,mtcarsData$vs, xlab=&quot;Number of cylinders&quot;, ylab=&quot;Type of engine&quot;) Figure 2.6: Heat map for the number of cylinders and the type of engine. Figure 2.6 allows making more solid conclusions about the relation between the two variables: we see that with the increase of the number of cylinders, the cars tend to switch from Straight to the V-shaped engines. This has an explanation: the engines with more cylinders need to have a different geometry to fit them all, and the V shape is more suitable for them. The table plot shows clearly this relation between the two variables. Next, we can analyse the numerical variables. We can start with the basic histogram: hist(mtcarsData$wt, xlab=&quot;Weight&quot;, main=&quot;&quot;, probability=TRUE) lines(density(mtcarsData$wt), col=&quot;red&quot;) Figure 2.7: Distribution of the weights of cars. The histogram 2.7 shows that there is a slight skewness in the data: the cars with weight from 3 to 4 thousands pounds are met more often than the cars with more than 5. The left tail of this distribution is slightly longer than the right one. Note that I have produced the probabilities on the y-axis of the plot in order to add the density curve, which smooths out the frequencies and shows how the distribution looks like. An alternative presentation of the histogram is the boxplot, which graphically presents quantiles of distribution: boxplot(mtcarsData$wt, ylab=&quot;Weight&quot;) points(mean(mtcarsData$wt),col=&quot;red&quot;, pch=16) Figure 2.8: Boxplot of the variable weight. This plot has the box in the middle, the whiskers on the sides, points at the top and the red point at the centre. The box shows 1st, 2nd and 3rd quartiles of distribution, thus the black line in the middle is the median. The distance between the 1st and the 3rd quartiles is called “Interquartile range” (IQR) and is used for the calculation of the interval (1st / 3rd quartile \\(\\pm 1.5 \\times\\)IQR), which corresponds roughly to the 99.3% interval (read more about this in Section 2.4) from Normal distribution and is graphically drawn as the furthest observation in the interval. So, the lower whisker on our plot corresponds to the minimum value in the data, which is still in the interval, while the upper whisker corresponds to the bound of the interval. All the observations that lie beyond the interval are marked as potential outliers. Note that this does not mean that the values are indeed outliers, they just lie outside the 99.3% interval of Normal distribution. Finally, the red dot was added by me to show where the mean is. It is lower than median, this implies that there is a slight skewness in the distribution of weight. There is also a way for producing the plots that would combine elements of histogram, density curve and boxplot. There is a plot called “violin plot.” We will use vioplot() function from vioplot package in order to produce them: vioplot(mtcarsData$wt, ylab=&quot;Weight&quot;) points(mean(mtcarsData$wt),col=&quot;red&quot;, pch=16) Figure 2.9: Violin plot together with boxplot of the variable weight. Figure 2.9 unites the boxplot and the density curve from the plots above, providing not only information about the quantiles, but also about the shape of the distribution. Finally, if we want to compare the distribution of a variable with a known theoretical distribution, we can produce the QQ-plot. Here how it looks for Normal distribution: qqnorm(mtcarsData$wt) qqline(mtcarsData$wt) Figure 2.10: QQ plot of Normal distribution for variable weight. The idea of the plot on Figure 2.10 is to compare theoretical quantiles with the empirical ones. If the variable would follow the specific distribution, then all the points would lie on the solid line. In our case, they do not: there are points in the right tail that are very far from the line - so we would conclude that the distribution of weight does not look Normal. So far, we have discussed the univariate analysis of numerical variables, but we can also produce plots showing potential relations between them. We start with the classical scatterplot: plot(mtcarsData$wt, mtcarsData$mpg, xlab=&quot;Weight&quot;, ylab=&quot;Mileage&quot;) lines(lowess(mtcarsData$wt, mtcarsData$mpg), col=&quot;red&quot;) Figure 2.11: Scatterplot diagram between weight and mileage. The plot on Figure 2.11 shows the observations that have specific weight and mileage. Based on this, we can see if there is a relation between variables or not and what sort of relation this is. In order to simplify analysis, I have added the lowess line to the plot. It smooths the relation between variables, drawing the smooth line through the points and helps in understanding the existing relations in the data. Judging by Figure 2.11, there is a negative, slightly non-linear relation between the variables: the mileage decreases with reduced speed, when weight of a car increases. This relation makes sense, because heavier cars will consume more fuel and thus drive less on a gallon of petrol. We could construct similar plots for all the other numerical variables, but not all plots would be helpful. For example, a plot of mileage versus number of forward gears would be very difficult to read (see Figure 2.12). plot(mtcarsData$gear, mtcarsData$mpg, xlab=&quot;Number of gears&quot;, ylab=&quot;Mileage&quot;) Figure 2.12: Scatterplot diagram between weight and mileage. This is because one of the variables is integer and takes only a handful of values. In this case, a boxplot or a violin plot would be more useful: boxplot(mpg~gear, mtcarsData, xlab=&quot;Number of gears&quot;, ylab=&quot;Mileage&quot;) points(tapply(mtcarsData$mpg, mtcarsData$gear, mean), col=&quot;red&quot;, pch=16) Figure 2.13: Boxplot of mileage vs number of gears. The plot on Figure 2.13 is more informative than the one on Figure 2.12: it shows how the distribution of mileage changes with the increase of the numeric variable number of gears. We can also see that the mean value first increases and then goes down slightly. I do not have any good explanation of this phenomenon, but it might be related with how efficient the cars become with the increase fo the number of gears, or this could happen due to some latent, unobserved factors. So, the data tells us that there is a non-linear relation between number of gears and mileage. Similarly, we can produce violin plots for the same data using the following code: vioplot(mpg~gear, mtcarsData, xlab=&quot;Number of gears&quot;, ylab=&quot;Mileage&quot;) points(tapply(mtcarsData$mpg, mtcarsData$gear, mean), col=&quot;red&quot;, pch=16) Finally, using exactly the same idea with boxplots / violin plots, we can analyse relations between categorical and numerical variables. Figure 2.14 shows the relation between transmission type and mileage. We can conclude that the cars with manual transmission tend to have a higher mileage than the ones with the automatic one in our dataset. vioplot(mpg~am, mtcarsData, xlab=&quot;Transmission type&quot;, ylab=&quot;Mileage&quot;) points(tapply(mtcarsData$mpg, mtcarsData$am, mean), col=&quot;red&quot;, pch=16) Figure 2.14: Violin plot of mileage vs transmission type. Finally, producing plots one by one might be a tedious and challenging task, so it is good to have some instruments for producing several of them together. The plot() method will produce scatterplot matrix for numerical variables, but does not deal well with integer and categorical variables: plot(mtcars) Figure 2.15: Scatterplot matrix for the mtcars dataset. Figure 2.15 is informative for the variables mpg, cyl, disp, hp, drat, qsec and carb, but is difficult to read for the others. In order to address this issue, we can use the spread() function from greybox, which will detect types of variables and produce the necessary plots automatically: spread(mtcarsData, lowess=TRUE) Figure 2.16: Spread plot for the mtcars dataset. The plot on Figure 2.16 is the collection of the plots discussed above, so I will not stop on explaining what it shows. As a final word for this section, when analysing data, it is critically important not to just describe what we see, but also explain why a result or a relationship is meaningful, otherwise this becomes an exercise of stating the obvious which does not have any value. So, for example, concluding based on Figure 2.16 that the mileage has a negative relation with displacement is not enough. If you want to analyse the data properly, you need to explain that this relation is meaningful, because with the increase of the size of engine, the fuel consumption will increase as well, and as a result the mileage will go down. Furthermore, the relation is non-linear because the change in decrease will slow down with cars with bigger engines. Inevitably, the car with a gigantic engine will be able to travel a short distance on a gallon of fuel - the mileage will not become negative, so the non-linearity is not an artefact of the data, but an existing phenomenon. "],["LLNandCLT.html", "2.2 Law of Large Numbers and Central Limit Theorem", " 2.2 Law of Large Numbers and Central Limit Theorem Consider a case, when you want to understand what is the average height of teenagers living in your town. It is very expensive and time consuming to go from one house to another and ask every single teenager (if you find one), what their hight is. If we could do that, we would get the true mean, true average height of teenagers living in the town. But in reality, it is more practical to ask a sample of teenagers and make conclusions about the “population” (all teenagers in the town) based on this sample. Indeed, you will spend much less time collecting the information about the height of 100 people rather than 100,000. However, when we take a sample of something, the statistics we work with will always differ from the truth: sample mean will never be equal to the true mean, but it can be shown mathematically that it will converge to the truth, when some specific conditions are met and when the sample size increases. If we set up the experiment correctly, then we can expect our statistics to follow some laws. 2.2.1 Law of Large Numbers The first law is called the Law of Large Numbers (LLN). It is the theorem saying that (under wide conditions) the average of a variable obtained over the large number of trials will be close to its expected value and will get closer to it with the increase of the sample size. This can be demonstrated with the following example: obs &lt;- 10000 # Generate data from normal distribution y &lt;- rnorm(obs,100,100) # Create sub-samples of 50 and 100 observations y30 &lt;- sample(y, 30) y1000 &lt;- sample(y, 1000) par(mfcol=c(1,2)) hist(y30, xlab=&quot;y&quot;) abline(v=mean(y30), col=&quot;red&quot;) hist(y1000, xlab=&quot;y&quot;) abline(v=mean(y1000), col=&quot;red&quot;) Figure 2.17: Histograms of samples of data from variable y. What we will typically see on the plots above is that the mean (red line) on the left plot will be further away from the true mean of 100 than in the case of the right plot. Given that this is randomly generated, the situation might differ, but the idea would be that with the increase of the sample size the estimated sample mean will converge to the true one. We can even produce a plot showing how this happens: yMean &lt;- vector(&quot;numeric&quot;,obs) for(i in 1:obs){ yMean[i] &lt;- mean(sample(y,i)) } plot(yMean, type=&quot;l&quot;, xlab=&quot;Sample size&quot;, ylab=&quot;Sample mean&quot;) Figure 2.18: Demonstration of Law of Large Numbers. We can see from the plot above that with the increase of the sample size the sample mean reaches the true value of 100. This is a graphical demonstration of the Law of Large Numbers: it only tells us about what will happen when the sample size increases. But it is still useful, because it used for many statistical inferences and if it does not work, then the estimate of mean would be incorrect, meaning that we cannot make conclusions about the behaviour in population. In order for LLN to work, the distribution of variable needs to have finite mean and variance. This is discussed in some detail in the next subsection. In summary, what LLN tells us is that if we average things out over a large number of observations, then that average starts looking very similar to the population value. However, this does not say anything about the performance of estimators on small samples. 2.2.2 Central Limit Theorem As we have already seen on Figure 2.18, the sample mean is not exactly equal to the population mean even when the sample size is very large (thousands of observations). There is always some sort of variability around the population mean. In order to understand how this variability looks like, we could conduct a simple experiment. We could take a random sample of, for instance, 1000 observations several times and record each of the obtained means. We then can see how the variable will be distributed to see if there are any patterns in the behaviour of the estimator: nIterations &lt;- 1000 yMean &lt;- vector(&quot;numeric&quot;,nIterations) for(i in 1:nIterations){ yMean[i] &lt;- mean(sample(y,1000)) } hist(yMean, xlab=&quot;Sample mean&quot;, main=&quot;&quot;) Figure 2.19: Histogram of the mean of the variable y. There is a theorem that says that the distribution of mean in the experiment above will follow normal distribution under several conditions (discussed later in this section). It is called Central Limit Theorem (CLT) and very roughly it says that when independent random variables are added, their normalised sum will asymptotically follow normal distribution, even if the original variables do not follow it. Note that this is the theorem about what happens with the estimate (sum in this case), not with individual observations. This means that the error term might follow, for example, Inverse Gaussian distribution, but the estimate of its mean (under some conditions) will follow normal distribution. There are different versions of this theorem, built with different assumptions with respect to the random variable and the estimation procedure, but we do not dicuss these details in this textbook. In order for CLT to hold, the following important assumptions need to be satisfied: The true value of parameter is not near the bound. e.g. if the variable follows uniform distribution on (0, \\(a\\)) and we want to estimate \\(a\\), then its distribution will not be Normal (because in this case the true value is always approached from below). This assumption is important in our context, because ETS and ARIMA typically have restrictions on their parameters. The random variables are identically independent distributed (i.i.d.). If they are not, then their average might not follow normal distribution (in some conditions it still might). The mean and variance of the distribution are finite. This might seem as a weird issue, but some distributions do not have finite moments, so the CLT will not hold if a variable follows them, just because the sample mean will be all over the plane due to randomness and will not converge to the “true” value. Cauchy distribution is one of such examples. If these assumptions hold, then CLT will work for the estimate of a parameter, no matter what the distribution of the random variable is. This becomes especially useful, when we want to test a hypothesis or construct a confidence interval for an estimate of a parameter. "],["estimatesProperties.html", "2.3 Properties of estimators", " 2.3 Properties of estimators Before we move further, it is important to understand the terms bias, efficiency and consistency of estimates of parameters, which are directly related to LLN and CLT. Although there are strict statistical definitions of the aforementioned terms (you can easily find them in Wikipedia or anywhere else), I do not want to copy-paste them here, because there are only a couple of important points worth mentioning in our context. Note that all the discussions in this chapter relate to the estimates of parameters, not to the distribution of a random variable itself. A common mistake that students make when studying statistics, is that they think that the properties apply to the variable \\(y_t\\) instead of the estimate of its parameters (e.g. mean of \\(y_t\\)). 2.3.1 Bias Bias refers to the expected difference between the estimated value of parameter (on a specific sample) and the “true” one (in the true model). Having unbiased estimates of parameters is important because they should lead to more accurate forecasts (at least in theory). For example, if the estimated parameter is equal to zero, while in fact it should be 0.5, then the model would not take the provided information into account correctly and as a result will produce less accurate point forecasts and incorrect prediction intervals. In inventory context this may mean that we constantly order 100 units less than needed only because the parameter is lower than it should be. The classical example of bias in statistics is the estimation of variance in sample. The following formula gives biased estimate of variance in sample: \\[\\begin{equation} \\mathrm{V}(y) = \\frac{1}{T} \\sum_{j=1}^T \\left( y_t - \\bar{y} \\right)^2, \\tag{2.1} \\end{equation}\\] where \\(T\\) is the sample size and \\(\\bar{y} = \\frac{1}{T} \\sum_{j=1}^T y_t\\) is the mean of the data. There is a lot of proofs in the literature of this issue (even Wikipedia (2020a) has one), we will not spend time on that. Instead, we will see this effect in the following simple simulation experiment: mu &lt;- 100 sigma &lt;- 10 nIterations &lt;- 1000 # Generate data from normal distribution, 10,000 observations y &lt;- rnorm(10000,mu,sigma) # This is the function, which will calculate the two variances varFunction &lt;- function(y){ return(c(var(y), mean((y-mean(y))^2))) } # Calculate biased and unbiased variances for the sample of 30 observations, # repeat nIterations times varValues &lt;- replicate(nIterations, varFunction(sample(y,30))) This way we have generated 1000 samples with 30 observations and calculated variances using the formulae (2.1) and the corrected one for each step. Now we can plot it in order to see how it worked out: par(mfcol=c(1,2)) # Histogram of the biased estimate hist(varValues[2,], xlab=&quot;V(y)&quot;, ylab=&quot;y&quot;, main=&quot;Biased estimate of V(y)&quot;) abline(v=mean(varValues[2,]), col=&quot;red&quot;) legend(&quot;topright&quot;,legend=TeX(paste0(&quot;E$\\\\left(V(y)\\\\right)$=&quot;,round(mean(varValues[2,]),2))),lwd=1,col=&quot;red&quot;) # Histogram of unbiased estimate hist(varValues[1,], xlab=&quot;V(y)&quot;, ylab=&quot;y&quot;, main=&quot;Unbiased estimate of V(y)&quot;) abline(v=mean(varValues[1,]), col=&quot;red&quot;) legend(&quot;topright&quot;,legend=TeX(paste0(&quot;E$\\\\left(V(y)\\\\right)$=&quot;,round(mean(varValues[1,]),2))),lwd=1,col=&quot;red&quot;) Figure 2.20: Histograms for biased and unbiased estimates of variance. Every run of this experiment will produce different plots, but typically what we will see is that, the biased estimate of variance (the histogram on the right hand side of the plot) will have lower mean than the unbiased one. This is the graphical example of the effect of not taking the number of estimated parameters into account. The correct formula for the unbiased estimate of variance is: \\[\\begin{equation} s^2 = \\frac{1}{T-k} \\sum_{j=1}^T \\left( y_t - \\bar{y} \\right)^2, \\tag{2.2} \\end{equation}\\] where \\(k\\) is the number of all independent estimated parameters. In this simple example \\(k=1\\), because we only estimate mean (the variance is based on it). Analysing the formulae (2.1) and (2.2), we can say that with the increase of the sample size, the bias will disappear and the two formulae will give almost the same results: when the sample size \\(T\\) becomes big enough, the difference between the two becomes negligible. This is the graphical presentation of the bias in the estimator. 2.3.2 Efficiency Efficiency means, if the sample size increases, then the estimated parameters will not change substantially, they will vary in a narrow range (variance of estimates will be small). In the case with inefficient estimates the increase of sample size from 50 to 51 observations may lead to the change of a parameter from 0.1 to, let’s say, 10. This is bad because the values of parameters usually influence both point forecasts and prediction intervals. As a result the inventory decision may differ radically from day to day. For example, we may decide that we urgently need 1000 units of product on Monday, and order it just to realise on Tuesday that we only need 100. Obviously this is an exaggeration, but no one wants to deal with such an erratically behaving model, so we need to have efficient estimates of parameters. Another classical example of not efficient estimator is the median, when used on the data that follows Normal distribution. Here is a simple experiment demonstrating the idea: mu &lt;- 100 sigma &lt;- 10 nIterations &lt;- 500 obs &lt;- 100 varMeanValues &lt;- vector(&quot;numeric&quot;,obs) varMedianValues &lt;- vector(&quot;numeric&quot;,obs) y &lt;- rnorm(100000,mu,sigma) for(i in 1:obs){ ySample &lt;- replicate(nIterations,sample(y,i*100)) varMeanValues[i] &lt;- var(apply(ySample,2,mean)) varMedianValues[i] &lt;- var(apply(ySample,2,median)) } In order to establish the efficiency of the estimators, we will take their variances and look at the ratio of mean over median. If both are equally efficient, then this ratio will be equal to one. If the mean is more efficient than the median, then the ratio will be less than one: options(scipen=6) plot(1:100*100,varMeanValues/varMedianValues, type=&quot;l&quot;, xlab=&quot;Sample size&quot;,ylab=&quot;Relative efficiency&quot;) abline(h=1, col=&quot;red&quot;) Figure 2.21: An example of a relatively inefficient estimator. What we should typically see on this graph, is that the black line should be below the red one, indicating that the variance of mean is lower than the variance of the median. This means that mean is more efficient estimator of the true location of the distribution \\(\\mu\\) than the median. In fact, it is easy to proove that asymptotically the mean will be 1.57 times more efficient than median (Wikipedia, 2020b) (so, the line should converge approximately to the value of 0.64). 2.3.3 Consistency Consistency means that our estimates of parameters will get closer to the stable values (true value in the population) with the increase of the sample size. This follows directly from LLN and is important because in the opposite case estimates of parameters will diverge and become less and less realistic. This once again influences both point forecasts and prediction intervals, which will be less meaningful than they should have been. In a way consistency means that with the increase of the sample size the parameters will become more efficient and less biased. This in turn means that the more observations we have, the better. An example of inconsistent estimator is Chebyshev (or max norm) metric. It is formulated the following way: \\[\\begin{equation} \\mathrm{LMax} = \\max \\left(|y_1-\\hat{y}|, |y_2-\\hat{y}|, \\dots, |y_T-\\hat{y}| \\right). \\tag{2.3} \\end{equation}\\] Minimising this norm, we can get an estimate \\(\\hat{y}\\) of the location parameter \\(\\mu\\). The simulation experiment becomes a bit more tricky in this situation, but here is the code to generate the estimates of the location parameter: LMax &lt;- function(y){ estimator &lt;- function(par){ return(max(abs(y-par))); } return(optim(mean(y), fn=estimator, method=&quot;Brent&quot;, lower=min(y), upper=max(y))); } mu &lt;- 100 sigma &lt;- 10 nIterations &lt;- 1000 y &lt;- rnorm(10000, mu, sigma) LMaxEstimates &lt;- vector(&quot;numeric&quot;, nIterations) for(i in 1:nIterations){ LMaxEstimates[i] &lt;- LMax(y[1:(i*10)])$par; } And here how the estimate looks with the increase of sample size: plot(1:nIterations*10, LMaxEstimates, type=&quot;l&quot;, xlab=&quot;Sample size&quot;,ylab=TeX(&quot;Estimate of $\\\\mu$&quot;)) abline(h=mu, col=&quot;red&quot;) Figure 2.22: An example of inconsistent estimator. While in the example with bias we could see that the lines converge to the red line (the true value) with the increase of the sample size, the Chebyshev metric example shows that the line does not approach the true one, even when the sample size is 10000 observations. The conclusion is that when Chebyshev metric is used, it produces inconsistent estimates of parameters. Remark. There is a prejudice in the world of practitioners that the situation in the market changes so fast that the old observations become useless very fast. As a result many companies just through away the old data. Although, in general the statement about the market changes is true, the forecasters tend to work with the models that take this into account (e.g. Exponential smoothing, ARIMA, discussed in this book). These models adapt to the potential changes. So, we may benefit from the old data because it allows us getting more consistent estimates of parameters. Just keep in mind, that you can always remove the annoying bits of data but you can never un-throw away the data. 2.3.4 Asymptotic normality Finally, asymptotic normality is not critical, but in many cases is a desired, useful property of estimates. What it tells us is that the distribution of the estimate of parameter will be well behaved with a specific mean (typically equal to \\(\\mu\\)) and a fixed variance. This follows directly from CLT. Some of the statistical tests and mathematical derivations rely on this assumption. For example, when one conducts a significance test for parameters of model, this assumption is implied in the process. If the distribution is not Normal, then the confidence intervals constructed for the parameters will be wrong together with the respective t- and p- values. Another important aspect to cover is what the term asymptotic, which we have already used, means in our context. Here and after in this book, when this word is used, we refer to an unrealistic hypothetical situation of having all the data in the multiverse, where the time index \\(t \\rightarrow \\infty\\). While this is impossible in practice, the idea is useful, because asymptotic behaviour of estimators and models is helpful on large samples of data. Besides, even if we deal with small samples, it is good to know what to expect to happen if the sample size increases. References "],["confidenceIntervals.html", "2.4 Confidence and prediction intervals", " 2.4 Confidence and prediction intervals As mentioned in the previous section, we always work with samples and inevitably we deal with randomness just because of that even, when there are no other sources of uncertainty in the data. For example, if we want to estimate the mean of a variable based on the observed data, the value we get will differ from one sample to another. This should have become apparent from the examples we discussed earlier. And, if the LLN and CLT hold, then we know that the estimate of our parameter will have its own distribution and will converge to the population value with the increase of the sample size. This is the basis for the confidence and prediction interval construction, discussed in this section. Depending on our needs, we can focus on the uncertainty of either the estimate of a parameter, or the random variable \\(y\\) itself. When dealing with the former, we typically work with the confidence interval - the interval constructed for the estimate of a parameter, while in the latter case we are interested in the prediction interval - the interval constructed for the random variable \\(y\\). In order to simplify further discussion in this section, we will take the population mean and its in-sample estimate as an example. In this case we have: A random variable \\(y\\), which is assumed to follow some distribution with finite mean \\(\\mu\\) and variance \\(\\sigma^2\\); A sample of size \\(T\\) from the population of \\(y\\); Estimates of mean \\(\\hat{\\mu}=\\bar{y}\\) and variance \\(\\hat{\\sigma}^2 = s^2\\), obtained based on the sample of size \\(T\\). 2.4.1 Confidence interval What we want to get by doing this is an idea about the population mean \\(\\mu\\). The value \\(\\bar{y}\\) does not tell us much on its own due to randomness and if we do not capture its uncertainty, we will not know, where the true value \\(\\mu\\) can be. But using LLN and CLT, we know that the sample mean should converge to the true one and should follow normal distribution. So, the distribution of the sample mean would look like this (Figure 2.23). Figure 2.23: Distribution of the sample mean. On its own, this distribution just tells us that the variable is random around the true mean \\(\\mu\\) and that its density function has a bell-like shape. In order to make this more useful, we can construct the confidence interval for it, which would tell us where the true parameter is most likely to lie. We can cut the tails of this distribution to determine the width of the interval, expecting it to cover \\((1-\\alpha)\\times 100\\)% of cases. In the ideal world, asymptotically, the confidence interval will be constructed based on the true value, like this: Figure 2.24: Distribution of the sample mean and the confidence interval based on the population data. Figure 2.24 shows the classical normal distribution curve around the population mean \\(\\mu\\), confidence interval of the level \\(1-\\alpha\\) and the cut off tails, the overall surface of which corresponds to \\(\\alpha\\). The value \\(1-\\alpha\\) is called confidence level, while \\(\\alpha\\) is the significance level. By constructing the interval this way, we expect that in the \\((1-\\alpha)\\times 100\\)% of cases the value will be inside the bounds, and in \\(\\alpha\\times 100\\)% it will not. In reality we do not know the true mean \\(\\mu\\), so we do a slightly different thing: we construct a confidence interval based on the sample mean \\(\\bar{y}\\) and sample variance \\(s^2\\), hoping that due to LLN they will converge to the true values. We use Normal distribution, because we expect CLT to work. This process looks something like in Figure 2.25, with the bell curve in the background representing the true distribution for the sample mean and the curve on the foreground representing the assumed distribution based on our sample: Figure 2.25: Distribution of the sample mean and the confidence interval based on a sample. So, what the confidence interval does in reality is tries to cover the unknown population mean, based on the sample values of \\(\\bar{y}\\) and \\(s^2\\). If we construct the confidence interval of the width \\(1-\\alpha\\) (e.g. 0.95) for thousands of random samples (thousands of trials), then in \\((1-\\alpha)\\times 100\\)% of cases (e.g. 95%) the true mean will be covered by the interval, while in \\(\\alpha \\times 100\\)% cases it will not be. The interval itself is random, and we rely on LLN and CLT, when constructing it, expecting for it to work asymptotically, with the increase of the number of trials. Mathematically the red bounds in Figure 2.25 are represented using the following well-known formula for the confidence interval: \\[\\begin{equation} \\mu \\in (\\bar{y} + t_{\\alpha/2}(df) s_{\\bar{y}}, \\bar{y} + t_{1-\\alpha/2}(df) s_{\\bar{y}}), \\tag{2.4} \\end{equation}\\] where \\(t_{\\alpha/2}(df)\\) is Student’s t-statistics for \\(df=T-k\\) degrees of freedom (\\(T\\) is the sample size and \\(k\\) is the number of estimated parameters, e.g. \\(k=1\\) in our case) and level \\(\\alpha/2\\), and \\(s_{\\bar{y}}=\\frac{1}{\\sqrt{T}}s\\) is the estimate of the standard deviation of the sample mean. If we knew for some reason the true variance \\(\\sigma^2\\), then we could use z-statistics instead of t, but we typically do not, so we need to take the uncertainty about the variance into account as well, thus the use of t-statistics. Note, that in order to construct confidence interval, we do not care what distribution \\(y\\) follows, as long as LLN and CLT hold. 2.4.2 Prediction interval If we are interested in capturing the uncertainty about the random variable \\(y\\), then we should refer to prediction interval. In this case, we typically rely on LLN and the assumed distribution for the random variable \\(y\\). For example, if we know that \\(y \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then based on our sample we can construct a prediction interval of the width \\(1-\\alpha\\): \\[\\begin{equation} y \\in (\\bar{y} + z_{\\alpha/2} s, \\bar{y} + z_{1-\\alpha/2} s), \\tag{2.5} \\end{equation}\\] where \\(z_{\\alpha/2}\\) is the z-statistics (quantile of standard normal distribution) for the level \\(\\alpha/2\\) and \\(\\bar{y}\\) is the sample estimate of \\(\\mu\\) and \\(s\\) is the sample estimate of \\(\\sigma\\). If we assume some other distributions for the random variable, the formula would change. In a way, the prediction interval just comes to getting the quantiles of the assumed distribution based on estimated parameters. In some cases, when some of the assumptions do not hold, we might switch to more advanced methods for prediction interval construction. "],["hypothesisTesting.html", "2.5 Hypothesis testing", " 2.5 Hypothesis testing While we will not need hypothesis testing for ADAM itself, we might face it in some parts of the textbook, so it is worth discussing it briefly. Hypothesis testing arises naturally from the idea of confidence intervals: instead of constructing the interval and getting the idea about the uncertainty of the parameter, we could check, whether the sample agrees with our expectations or not. For example, we could test, whether the population mean is equal to zero based on our sample. We could either construct a confidence interval for the sample mean and see if zero is included in it (in which case it might indicate that zero is one of the possible values of the population mean), or we could reformulate the problem and compare some calculated value with the theoretical threshold. The latter approach is in the nutshell what hypothesis testing does. Fundamentally, the hypothesis testing process relies on the ideas of induction and dichotomy: we have a null (H\\(_0\\)) and alternative (H\\(_1\\)) hypotheses about the process or a property in the population, and we want to find some evidence to reject the H\\(_0\\). Rejecting a hypothesis is actually more useful than not rejecting it, because in the former case we know what not to expect from the data, while in the latter we just might not have enough evidence to make any solid conclusion. For example, we could formulate H\\(_0\\) that all cats are white. Failing to reject this hypothesis based on the data that we have (e.g. a dataset of white cats) does not mean that they are all (in the universe) indeed white, it just means that we have not observed the non-white ones. If we collect enough evidence to reject H\\(_0\\) (i.e. encountered a black cat), then we can conclude that not all cats are white. This is a more solid conclusion than the one in the previous case. So, if you are interested in a specific outcome, then it makes sense to put this in the alternative hypothesis and see if the data allows to reject the null. For example, if we want to see if the average salary of professors in the UK is higher than £100k per year we would formulate the hypotheses in the following way: \\[\\begin{equation*} \\mathrm{H}_0: \\mu \\leq 100, \\mathrm{H}_1: \\mu &gt; 100. \\end{equation*}\\] Having formulated hypotheses, we can check them, but in order to do that, we need to follow a proper procedure, which can be summarised in the six steps: Formulate null and alternative hypotheses (H\\(_0\\) and H\\(_1\\)) based on your understanding of the problem; Select the significance level \\(\\alpha\\) on which the hypothesis will be tested; Select the test appropriate for the formulated hypotheses (1); Conduct the test (3) and get the calculated value; Compare the value in (4) with the threshold one; Make a conclusion based on (5) on the selected level (2). Note that the order of some elements might change depending on the circumstances, but (2) should always happen before (4), otherwise we might be dealing with so called “p-hacking,” trying to make results look nicer than they really are. Consider an example, where we want to check, whether the population mean \\(\\mu\\) is equal to zero, based on a sample of 36 observations, where \\(\\bar{y}=-0.5\\) and \\(s^2=1\\). In this case, we formulate the null and alternative hypotheses: \\[\\begin{equation*} \\mathrm{H}_0: \\mu=0, \\mathrm{H}_1: \\mu \\neq 0. \\end{equation*}\\] We then select the significance level \\(\\alpha=0.05\\) (just as an example) and select the test. Based on the description of the task, this can be either a t-test, or a z-test, depending on whether the variance of the variable is known or not. Usually it is not, so we tend to use t-test. We then conduct the test using the formula: \\[\\begin{equation} t = \\frac{\\bar{y} - \\mu}{s_{\\bar{y}}} = \\frac{-0.5 - 0}{\\frac{1}{\\sqrt{36}}} = -3 . \\tag{2.6} \\end{equation}\\] After that we get the critical value of t with \\(df=36-1=35\\) degrees of freedom and significance level \\(\\alpha/2=0.025\\), which is approximately equal to -2.03. We compare this value with the (2.6) by absolute and reject H\\(_0\\) if the calculated value is higher than the critical one. In our case it is, so it appears that we have enough evidence to say that the population mean is not equal to 0, on the 5% significance level. Visually, the whole process of hypothesis testing explained above can be represented in the following way: Figure 2.26: The process of hypothesis testing with t value. If the blue line on Figure 2.26 would lie inside the red bounds (i.e. the calculated value is less than the critical value by absolute), then we would fail to reject H\\(_0\\). But in our example it is outside the bounds, so we have enough evidence to conclude that the population mean is not equal to zero on 5% significance level. Notice, how similar the mechanisms of confidence interval construction and hypothesis testing are. This is because they are one and the same thing, presented differently. In fact, we could test the same hypothesis by constructing the 95% confidence interval using (2.4) and checking, whether the interval covers the \\(\\mu=0\\): \\[\\begin{equation*} \\begin{aligned} &amp; \\mu \\in \\left(-0.50 -2.03 \\frac{1}{\\sqrt{36}}, -0.50 + 2.03 \\frac{1}{\\sqrt{36}} \\right), \\\\ &amp; \\mu \\in (-0.84, -0.16). \\end{aligned} \\end{equation*}\\] In our case it does not, so we conclude that we reject H\\(_0\\) on 5% significance level. This can be roughly represented by the graph on Figure 2.27: Figure 2.27: The process of hypothesis testing with t value. Note that the positioning of the blue line has changed in the case of confidence interval, which happens because of the transition from (2.6) to (2.4). The idea and the message, however, stay the same: if the value is not inside the light grey area, then we reject H\\(_0\\) on the selected significance level. Also note that we never say that we accept H\\(_0\\), because this is not what we do in hypothesis testing: if the value would lie inside the interval, then this would only mean that our sample shows that the tested value is covered by the region - the true value can be any of the numbers between the bounds. Finally, there is a third way to test the hypothesis. We could calculate how much surface is left in the tails with the cut off of the assumed distribution by the blue line on Figure 2.26 (calculated value). In R this can be done using the pt() function: pt(-3, 36-1) ## [1] 0.002474416 Given that we had the inequality in the alternative hypothesis, we need to consider both tails, multiplying the value by 2 to get approximately 0.0049. This is the significance level, for which the switch from “reject” to “do not reject” happens. We could compare this value with the pre-selected significance level directly, rejecting H\\(_0\\) if it is lower than \\(\\alpha\\). This value is called “p-value” and simplifies the hypothesis testing, because we do not need to look at critical values or construct the confidence interval. There are different definitions of what it is, I personally find the following easier to comprehend: p-value is the smallest significance level at which a null hypothesis can be rejected. Despite this simplification, we still need to follow the procedure and select \\(\\alpha\\) before conducting the test! We should not change the significance level after observing the p-values, otherwise we might end up bending reality for our needs. Also note that if in one case p-value is 0.2, while in the other it is 0.3, it does not mean that the the first case is more significant than the second! P-values are not comparable with each other and they do not tell you about the size of significance. This is still a binary process: we either reject, or fail to reject H\\(_0\\), depending on whether p-value is smaller or greater than the selected significance level. While p-value is a comfortable instrument, I personally prefer using confidence intervals, because they show the uncertainty clearer and are less confusing. Consider the following cases to see what I mean: We reject H\\(_0\\) because t-value is -3, which is smaller than the critical value of -2.03 (or equivalently the absolute of t-value is 3, while the critical is 2.03); We reject H\\(_0\\) because p-value is 0.0049, which is smaller than the significance level \\(\\alpha=0.05\\); The confidence interval for the mean is \\(\\mu \\in (-0.84, -0.16)\\). It does not include zero, so we reject \\(\\mathrm{H}_0\\). In case of (3), we not only get the same message as in (1) and (2), but we also see how far the bound is from the tested value. In addition, in the situation, when we fail to reject H\\(_0\\), the approach (3) gives more appropriate information. Consider the case, when we test, whether \\(\\mu=-0.6\\) in the example above. We then have the following three approaches to the problem: We fail to reject H\\(_0\\) because t-value is 0.245, which is smaller than the critical value of 2.03; We fail to reject H\\(_0\\) because p-value is 0.808, which is greater than the significance level \\(\\alpha=0.05\\); The confidence interval for the mean is \\(\\mu \\in (-0.84, -0.16)\\). It includes -0.6, so we fail to reject H\\(_0\\). This does not mean that the true mean is indeed equal to -0.6, but it means that the region will cover this number in 95% of cases if we do resampling many times. In my opinion, the third approach is more informative and saves from making wrong conclusions about the tested hypothesis, making you work a bit more (you cannot change the confidence level on the fly, you would need to reconstruct the interval). Having said that, either of the three is fine, as long as you understand what they really imply. Furthermore, if you do hypothesis testing and use p-values, it is worth mentioning the statement of American Statistical Association about p-values (Wasserstein and Lazar, 2016). Among the different aspects discussed in this statement, there is a list of principles related to p-values, which I cite below: P-values can indicate how incompatible the data are with a specified statistical model; P-values do not measure: the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone; Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold; Proper inference requires full reporting and transparency; A p-value, or statistical significance, does not measure: the size of an effect or the importance of a result; By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis. The statement provides more details about that, but summarising, whatever hypothesis you test and however you test it, you should have apriori understanding of the problem. Diving in the data and trying to see what floats (i.e. which of the p-values is higher than \\(\\alpha\\)) is not a good idea (Wasserstein and Lazar, 2016). Follow the proper procedure if you want to test the hypothesis. Finally, the hypothesis testing mechanism has been criticised by many scientists over the years. For example, Cohen (1994) discussed issues with the procedure, making several important points, some of which are outlined above. He also points out at the fundamental problem with hypothesis testing, which is typically neglected by proponents of the procedure: in practice, null hypothesis is always wrong. In reality, it is not possible for a value to be equal, for example, to zero. Even an unimportant effect of one variable on another would be close to zero, but not equal to it. This means that with the increase of the sample size, H\\(_0\\) will inevitably be rejected. Furthermore, our mind operates with binary constructs: true / not true - while the hypothesis testing works in the dichotomy “I know / I don’t know,” with the latter appearing when there is not enough evidence to reject H\\(_0\\). Summarising all of this, in my opinion, it makes sense to move away from hypothesis testing if possible and switch to other instruments for uncertainty measurement, such as confidence intervals. 2.5.1 Common mistakes related to hypothesis testing Over the years of teaching statistics, I have seen many different mistakes, related to hypothesis testing. No wonder, this is a difficult topic to grasp. Here, I have decided to summarise several typical erroneous statements, providing explanations why they are wrong. They partially duplicate the 6 principles from ASA discussed above, but they are formulated slightly differently. “Calculated value is lower than the critical one, so the null hypothesis is true.” This is wrong on so many level, that I do not even know where to start. We can never know if the hypothesis is true or wrong. All the evidence might point towards the H\\(_0\\) being correct, but it still can be wrong and at some point in future one observation might reject it. The classical example is the “Black swan in Australia.” Up until the discover of Australia, the Europeans thought that there only exist white swans. This was supported by all the observations they had. Wise people would say that “We fail to reject H\\(_0\\) that all swans are white.” Uneducated people would be tempted to say that \" H\\(_0\\): All swans are white\" is true. After discovering Australia in 1606, Europeans have collected evidence of existence of black swans, thus rejecting H\\(_0\\) and showing that “not all swans are white,” which implies that actually the alternative hypothesis is true. This is the essence of scientific method: we always try rejecting H\\(_0\\), collecting some evidence. If we fail to reject it, it might just mean that we have not collected enough evidence or have not modelled it correctly. “Calculated value is lower than the critical one, so we accept the null hypothesis.” We never accept null hypothesis. Even if your house is on fire or there is a tsunami coming, you should not “accept H\\(_0\\).” This is a fundamental statistical principle. We collect evidence to reject the null hypothesis. If we do not have enough evidence, then we just fail to reject it, but we can never accept it, because failing to reject just means that we need to collect more data. As mentioned earlier, we focus on rejecting hypothesis, because this at least tells us, what the phenomenon is not (e.g. that not all swans are white). “The parameter in the model is significant, so we can conclude that…” You cannot conclude if something is significant or not without specifying the significance level. Things are only significant if they pass specific test on a specified level \\(\\alpha\\). The correct sentence would be “The parameter in the model is significant on 3%, so we can conclude that…,” where 3% is the selected significance level \\(\\alpha\\). “The parameter in the model is significant because p-value&lt;0.0000” Indeed, some statistical software will tell you that p-value&lt;0.0000, but this just says that the value is very small and cannot be printed. Even if it is that small, you need to state your significance level and compare it with the p-value. You might wonder, “why bother if it is that low?” Well, if you change the sample size or change model specification, your p-value will change as well, and in some cases it might all of a sudden become higher than your significance level. So, you always need to keep it in mind and make conclusions based on the significance level, not just based on what software tells you. “The parameter is not significant, so we remove the variable from the model.” This is one of the worst motivations for removing variables that there is (statistical blasphemy!). There are thousands of reasons, why you might get p-value greater than your significance level (assumptions do not hold, sample is too small, the test is too weak, the true value is small etc) and only one of them is that the explanatory variable does not impact the response variable and thus you fail to reject H\\(_0\\). Are you sure that you face exactly this one special case? If yes, then you already have some other (better) reasons to remove the variable. This means that you should not make decisions just based on the results of a statistical test. You always need to have some fundamental reason to include or remove variables in the model. Hypothesis testing just gives you additional information that can be helpful for decision making. “The parameter in the new model is more significant than in the old one.” There is no such thing as “more significant” or “less significant.” Significance is binary and depends on the selected level. The only thing you can conclude is whether the parameter is significant on the chosen level \\(\\alpha\\) or not. “The p-value of one variables is higher than the p-value of another, so….” p-values are not comparable between variables. They only show on what level the hypothesis is rejected and only work together with the chosen significance level. (6) is similar to this mistake. Remember that the p-value itself is random and will change if you change the sample size or the model specification. Always keep this in mind, when conducting statistical tests. All these mistakes typically arise because of the misuse of p-values and hypothesis testing mechanism. This is one of the reasons, why I prefer confidence intervals, when possible (as discussed above). Finally, a related question to all of this, is how to select the significance level. Dave Worthington, a colleague of mine and a Statistics mentor at Lancaster University, has proposed an interesting motivation for that. If you do not have a level, driven by the problem (e.g. we need to satisfy 99% of demand, thus the significance level is 1%), then select the one for your life time. In how many cases in your life would you be ready to make a mistake? Would it be 5%? 3%? 1%? Select something and stick with it. Then over the years you will know that you have made the selected proportion of mistakes, when conducting different statistical test in variaous circumstances. References "],["correlations.html", "2.6 Correlation and measures of association", " 2.6 Correlation and measures of association Now that we have discussed confidence intervals and hypothesis testing, we can move towards the analysis of relations between variables, in a way continuing the preliminary data analysis that we finished in Section 2.1. We continue using the same dataset mtcarsData with the two categorical variables, am and vs. 2.6.1 Nominal scale As discussed in Section 1.2, not all scales support the more advanced operations (such as taking mean in ordinal scale). This means that if we want to analyse relations between variables, we need to use appropriate instrument. The coefficients that show relations between variables are called “measures of association.” We start their discussions with the simplest scale - nominal. There are several measures of association for the variables in nominal scale. They are all based on calculating the number of specific values of variables, but use different formulae. The first one is called contingency coefficient \\(\\phi\\) and can only be calculated between variables that have only two values. As the name says, this measure is based on the contingency table. Here is an example: table(mtcarsData$vs,mtcarsData$am) ## ## automatic manual ## V-shaped 12 6 ## Straight 7 7 The \\(\\phi\\) coefficient is calculated as: \\[\\begin{equation} \\phi = \\frac{n_{1,1} n_{2,2} - n_{1,2} n_{2,1}}{\\sqrt{n_{1,\\cdot}\\times n_{2,\\cdot}\\times n_{\\cdot,1}\\times n_{\\cdot,2}}} , \\tag{2.7} \\end{equation}\\] where \\(n_{i,j}\\) is the element of the table on row \\(i\\) and column \\(j\\), \\(n_{i,\\cdot}=\\sum_{j}n_{i,j}\\) - is the sum in row \\(i\\) and \\(n_{\\cdot,j}=\\sum_{i} n_{i,j}\\) - is the sum in column \\(j\\). This coefficient lies between -1 and 1 and has a simple interpretation: if will be close to 1, when the elements on diagonal are greater than the off-diagonal ones, implying that there is a relation between variables. The value of -1 can only be obtained, when off-diagonal elements are non-zero, while the diagonal ones are zero. Finally, if the values in the contingency table are distributed evenly, the coefficient will be equal to zero. In our case the value of \\(\\phi\\) is: (12*7 - 6*7)/sqrt(19*13*14*18) ## [1] 0.1683451 This is a very low value, so even if the two variables are related, the relation is not well pronounced. In order to see, whether this value is statistically significantly different from zero, we could test a statistical hypothesis (hypothesis testing was discussed in Section 2.5): \\(H_0\\): there is no relation between variables \\(H_1\\): there is some relation between variables This can be done using \\(\\chi^2\\) test, the statistics for which is calculated via: \\[\\begin{equation} \\chi^2 = \\sum_{i,j} \\frac{n \\times n_{i,j} - n_{i,\\cdot} \\times n_{\\cdot,j}}{n \\times n_{i,\\cdot} \\times n_{\\cdot,j}} , \\tag{2.8} \\end{equation}\\] where \\(n\\) is the sum of elements in the contingency table. The value calculated based on (2.8) will follow \\(\\chi^2\\) distribution with \\((r-1)(c-1)\\) degrees of freedom, where \\(r\\) is the number of rows and \\(c\\) is the number of columns in contingency table. This is a proper statistical test, so it should be treated as one. We select my favourite significance level, 1% and can now conduct the test: chisq.test(table(mtcarsData$vs,mtcarsData$am)) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: table(mtcarsData$vs, mtcarsData$am) ## X-squared = 0.34754, df = 1, p-value = 0.5555 Given that p-value is greater than 1%, we fail to reject the null hypothesis and can conclude that the relation does not seem to be different from zero - we do not find a relation between the variables in our data. The main limitation of the coefficient \\(\\phi\\) is that it only works for the \\(2\\times 2\\) tables. In reality we can have variables in nominal scale that take several values and it might be useful to know relations between them. For example, we can have a variable colour, which takes values red, green and blue and we would want to know if it is related to the transmission type. We do not have this variable in the data, so just for this example, we will create one (using multinomial distribution): colour &lt;- c(1:3) %*% rmultinom(nrow(mtcars), 1, c(0.4,0.5,0.6)) colour &lt;- factor(colour, levels=c(1:3), labels=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;)) barplot(table(colour), xlab=&quot;Colour&quot;) In order to measure relation between the new variable and the am, we can use Cramer’s V coefficient, which relies on the formula of \\(\\chi^2\\) test (2.8): \\[\\begin{equation} V = \\sqrt{\\frac{\\chi^2}{n\\times \\min(r-1, c-1)}} . \\tag{2.9} \\end{equation}\\] Cramer’s V always lies between 0 and 1, becoming close to one only if there is some relation between the two categorical variables. greybox package implements this coefficient in cramer() function: cramer(mtcarsData$am,colour) ## Cramer&#39;s V: 0.1003 ## Chi^2 statistics = 0.3222, df: 2, p-value: 0.8512 The output above shows that the value of the coefficient is approximately 0.1, which is low, implying that the relation between the two variables is very weak. In addition, the p-value tells us that we fail to reject the null hypothesis on 1% level in the \\(\\chi^2\\) test (2.8), and the relation does not look statistically significant. So we can conclude that according to our data, the two variables are not related (no wonder, we have generated one of them). The main limitation of Cramer’s V is that it is difficult to interpret beyond “there is a relation.” Imagine a situation, where the colour would be related to the variable “class” of a car, that can take 5 values. What could we say more than to state the fact that the two are related? After all, in that case you end up with a contingency table of \\(3\\times 5\\), and it might not be possible to say how specifically one variable changes with the change of another one. Still, Cramer’s V at least provides some information about the relation of two categorical variables. 2.6.2 Ordinal scale As discussed in Section 1.2, ordinal scale has more flexibility than the nominal one - its values have natural ordering, which can be used, when we want to measure relations between several variables in ordinal scale. Yes, we can use Cramer’s V and \\(\\chi^2\\) test, but this way we would not be using all advantages of the scale. So, what can we use in this case? There are three popular measures of association for variables in ordinal scale: Goodman-Kruskal’s \\(\\gamma\\), Yule’s Q, Kendall’s \\(\\tau\\). Given that the ordinal scale does not have distances, the only thing we can do is to compare values of variables between different observations and say, whether one is greater than, less than or equal to another. What can be done with two variables in ordinal scale is the comparison of the values of those variables for a couple respondents. Based on that the pairs of the observations can be called: Concordant if both \\(x_1 &lt; x_2\\) and \\(y_1 &lt; y_2\\) or \\(x_1 &gt; x_2\\) and \\(y_1 &gt; y_2\\) - implying that there is an agreement in order between the two variables (e.g. with a switch from a younger age group to the older one, the size of the T-shirt will switch from S to M); Discordant if for \\(x_1 &lt; x_2\\) and \\(y_1 &gt; y_2\\) or for \\(x_1 &gt; x_2\\) and \\(y_1 &lt; y_2\\) - implying that there is a disagreement in the order of the two variables (e.g. with a switch from a younger age group to the older one, the satisfaction from drinking Coca-Cola will switch to the lower level); Ties if both \\(x_1 = x_2\\) and \\(y_1 = y_2\\); Neither otherwise (e.g. when \\(x_1 = x_2\\) and \\(y_1 &lt; y_2\\)). All the measures of association for the variables in ordinal scale rely on the number of concordant, discordant variables and number of ties. All of these measures lie in the region of [-1, 1]. Goodman-Kruskal’s \\(\\gamma\\) is calculated using the following formula: \\[\\begin{equation} \\gamma = \\frac{n_c - n_d}{n_c + n_d}, \\tag{2.10} \\end{equation}\\] where \\(n_c\\) is the number of concordant pairs, \\(n_d\\) is the number of discordant pairs. This is a very simple measure of association, but it only works with scales of the same size (e.g. 5 options in one variable and 5 options in the other one) and ignores the ties. In order to demonstrate this measure in action, we will create two artificial variables in ordinal scale: Age of a person: young, adult and elderly; Size of t-shirt they wear: S, M or L. Here how we can do that in R: age &lt;- c(1:3) %*% rmultinom(nrow(mtcars), 1, c(0.4,0.5,0.6)) age &lt;- factor(age, levels=c(1:3), labels=c(&quot;young&quot;,&quot;adult&quot;,&quot;elderly&quot;)) size &lt;- c(1:3) %*% rmultinom(nrow(mtcars), 1, c(0.3,0.5,0.7)) size &lt;- factor(size, levels=c(1:3), labels=c(&quot;S&quot;,&quot;M&quot;,&quot;L&quot;)) And here is how the relation between these two artificial variables looks: tableplot(age,size,xlab=&quot;Age&quot;,ylab=&quot;T-shirt size&quot;) Figure 2.28: Heat map for age of a respondent and the size of their t-shirt. The graphical analysis based on Figure 2.28 does not provide a clear information about the relation between the two variables. But this is where the Goodman-Kruskal’s \\(\\gamma\\) becomes useful. We will use GoodmanKruskalGamma() function from DescTools package for R for this: DescTools::GoodmanKruskalGamma(age,size,conf.level=0.95) ## gamma lwr.ci upr.ci ## -0.03846154 -0.51302449 0.43610141 This function returns three values: the \\(\\gamma\\), which is close to zero in our case, implying that there is no relation between the variables, lower and upper bounds of the 95% confidence interval. Note that the interval shows us how big the uncertainty about the parameter is: the true value in the population can be anywhere between -0.51 and 0.44. But based on all these values we can conclude that we do not see any relation between the variables in our sample. The next measure is called Yule’s Q and is considered as a special case of Goodman-Kruskal’s \\(\\gamma\\) for the variables that only have 2 options. It is calculated based on the resulting contingency \\(2\\times 2\\) table and has some similarities with the contingency coefficient \\(\\phi\\): \\[\\begin{equation} \\mathrm{Q} = \\frac{n_{1,1} n_{2,2} - n_{1,2} n_{2,1}}{n_{1,1} n_{2,2} + n_{1,2} n_{2,1}} . \\tag{2.11} \\end{equation}\\] The main difference from the contingency coefficient is that it assumes that the data has ordering, it implicitly relies on the number of concordant (on the diagonal) and discordant (on the off diagonal) pairs. In our case we could calculate it if we had two simplified variables based on age and size (in real life we would need to recode them to “young,” “older” and “S,” “Bigger than S” respectively): table(age,size)[1:2,1:2] ## size ## age S M ## young 2 4 ## adult 2 2 (2*2-4*2)/(2*2+4*2) ## [1] -0.3333333 In our toy example, the measure shows that there is a weak negative relation between the trimmed age and size variables. We do not make any conclusions based on this, because this is not meaningful and is shown here just for purposes of demonstration. Finally, there is Kendall’s \\(\\tau\\). In fact, there are three different coefficients, which have the same name, so in the literature they are known as \\(\\tau_a\\), \\(\\tau_b\\) and \\(\\tau_c\\). \\(\\tau_a\\) coefficient is calculated using the formula: \\[\\begin{equation} \\tau_a = \\frac{n_c - n_d}{\\frac{T (T-1)}{2}}, \\tag{2.12} \\end{equation}\\] where \\(T\\) is the number of observations, and thus in the denominator, we have the number of all the pairs in the data. In theory this coefficient should lie between -1 and 1, but it does not solve the problem with ties, so typically it will not reach the boundary values and will say that the relation is weaker than it really is. Similar to Goodman-Kruskal’s \\(\\gamma\\), it can only be applied to the variables that have the same number of levels (same sizes of scales). In order to resolve some of these issues, \\(\\tau_b\\) was developed: \\[\\begin{equation} \\tau_b = \\frac{n_c - n_d}{\\sqrt{\\left(\\frac{T (T-1)}{2} - n_x\\right)\\left(\\frac{T (T-1)}{2} - n_y\\right)}}, \\tag{2.13} \\end{equation}\\] where \\(n_x\\) and \\(n_y\\) are the number of ties calculated for both variables. This coefficient resolves the problem with ties and can now reach the boundary values in practice. However, this coefficient does not resolve the issue with different scale sizes. And in order to address this problem, we have \\(\\tau_c\\) (Stuart-Kendall’s \\(\\tau_c\\)): \\[\\begin{equation} \\tau_c = \\frac{n_c - n_d}{\\frac{n^2}{2}\\frac{\\min(r, c)-1}{\\min(r, c)}}, \\tag{2.13} \\end{equation}\\] where \\(r\\) is the number of rows and \\(c\\) is the number of columns. This coefficient works for variables with different lengths of scales (e.g. age with 5 options and t-shirt size with 7 options). But now we are back to the problem with the ties… In R, the cor() function implements Kendall’s \\(\\tau_a\\) and \\(\\tau_b\\) (the function will select automatically based on the presence of ties). There are also functions KendallTauA(), KendallTauB() and StuartTauC() in DescTools package that implement the three respective measures of association. The main limitation of cor() function is that it only works with numerical variables, so we would need to transform variables before applying the function. The functions from DescTools package, on the other hand, work with factors. Here are the values of the three coefficients for our case: DescTools::KendallTauA(age,size,conf.level=0.95) ## tau_a lwr.ci upr.ci ## -0.01612903 -0.15347726 0.12121920 DescTools::KendallTauB(age,size,conf.level=0.95) ## tau_b lwr.ci upr.ci ## -0.02469136 -0.32938991 0.28000720 DescTools::StuartTauC(age,size,conf.level=0.95) ## tauc lwr.ci upr.ci ## -0.0234375 -0.3126014 0.2657264 Given that both variables have the same scale sizes, we should use either \\(\\tau_a\\) or \\(\\tau_b\\) for the analysis. However, we do not know if there are any ties in the data, so the safer option would be to use \\(\\tau_b\\) coefficient. The value of the coefficient and its confidence interval tell us that there is no obvious association between the two variables in our sample. This is expected, because the two variables were generated independently of each other. 2.6.3 Numerical scale Finally we come to the discussion of relations between variables measured in numerical scales. The most famous measure in this category is the Pearson’s correlation coefficient, which population value is: \\[\\begin{equation} \\rho_{x,y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\sigma_y}, \\tag{2.14} \\end{equation}\\] where \\(\\sigma_{x,y}\\) is the covariance between variables \\(x\\) and \\(y\\), while \\(\\sigma_x\\) and \\(\\sigma_y\\) are standard deviations of these variables. Typically, we do not know the population values, so this coefficient can be estimated in sample via: \\[\\begin{equation} r_{x,y} = \\frac{\\mathrm{cov}(x,y)}{\\sqrt{V(x)V(y)}}, \\tag{2.15} \\end{equation}\\] where all the values from (2.14) are substituted by their in-sample estimates. This coefficient measures the strength of linear relation between variables and lies between -1 and 1, where the boundary values correspond to perfect linear relation and 0 implies that there is no linear relation between the variables. In some textbooks the authors claim that this coefficient relies on Normal distribution of variables, but nothing in the formula assumes that. It was originally derived based on the simple linear regression (see Section 3.1) and its rough idea is to get information about the angle of the straight line drawn on the scatterplot. It might be easier to explain this on an example: plot(mtcarsData$disp,mtcarsData$mpg, xlab=&quot;Displacement&quot;,ylab=&quot;Mileage&quot;) abline(lm(mpg~disp,mtcarsData),col=&quot;red&quot;) Figure 2.29: Scatterplot for dispalcement vs mileage variables in mtcars dataset Figure 2.29 shows the scatterplot between the two variables and also has the straight line, going through the cloud of points. The closer the points are to the line, the stronger the linear relation between the two variables is. The line corresponds to the formula \\(\\hat{y}=a_0+a_1 x\\), where \\(x\\) is the displacement and \\(\\hat{y}\\) is the line value for the Mileage. The same relation can be presented if we swap the axes and draw the line \\(\\hat{x}=b_0+b_1 y\\): plot(mtcarsData$mpg,mtcarsData$disp, xlab=&quot;Displacement&quot;,ylab=&quot;Mileage&quot;) abline(lm(disp~mpg,mtcarsData),col=&quot;red&quot;) Figure 2.30: Scatterplot for mileage vs dispalcement The slopes for the two lines will in general differ, and will only coincide if the two variables have functional relations (all the point lie on the line). Based on this property, the correlation coefficient was originally constructed, as a geometric mean of the two parameters of slopes: \\(r_{x,y}=\\sqrt{a_1 b_1}\\). We will come back to this specific formula later in Section 3.1. But this idea provides an explanation why the correlation coefficient measures the strength of linear relation. For the two variables of interest it will be: cor(mtcarsData$mpg,mtcarsData$disp) ## [1] -0.8475514 Which shows strong negative linear relation between the displacement and mileage. This makes sense, because in general the cars with bigger engines will have bigger consumption and thus will make less miles per gallon of fuel. The more detailed information about the correlation is provided by the cor.test() function: cor.test(mtcarsData$mpg,mtcarsData$disp) ## ## Pearson&#39;s product-moment correlation ## ## data: mtcarsData$mpg and mtcarsData$disp ## t = -8.7472, df = 30, p-value = 9.38e-10 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9233594 -0.7081376 ## sample estimates: ## cor ## -0.8475514 In addition to the value, we now have results of the hypothesis testing (where null hypothesis is \\(\\rho_{x,y}=0\\)) and the confidence interval for the parameter. Given that the value of the parameter is close to its bound, we could conclude that the linear relation between the two variables is strong and statistically significant on 1% level. Note that the value of correlation coefficient only depends on the distance of points from the straight line, it does not depend on the slope (excluding case, when slope is equal to zero and thus the coefficient is equal to zero as well). So the following two cases will have exactly the same correlation coefficients: error &lt;- rnorm(100,0,10) x &lt;- c(1:100) y1 &lt;- 10+0.5*x+0.5*error y2 &lt;- 2+1.5*x+1.5*error # Produce the plots par(mfcol=c(1,2)) plot(x,y1,ylim=c(0,200)) abline(lm(y1~x),col=&quot;red&quot;) text(30,150,paste0(&quot;r=&quot;,round(cor(x,y1),5))) plot(x,y2,ylim=c(0,200)) abline(lm(y2~x),col=&quot;red&quot;) text(30,150,paste0(&quot;r=&quot;,round(cor(x,y2),5))) Figure 2.31: Example of relations with exactly the same correlations, but different slopes. There are other examples of cases, when correlation coefficient would be misleading or not provide the necessary information. One of the canonical examples is the Anscombe’s quartet (Wikipedia, 2021a), which shows very different types of relations, for which the Pearson’s correlation coefficient would be exactly the same. An important lesson from this is to always do graphical analysis (see Section 2.1.2) of your data, when possible - this way misleading situations can be avoided. Coming back to the scatterplot in Figure 2.29, it demonstrates some non-linearity in the relation between the two variables. So, it would make sense to have a different measure that could take it into account. This is where Spearman’s correlation coefficient becomes useful. It is calculated using exactly the same formula (2.15), but applied to the data in ranks. By using ranks, we loose information about the natural zero and distances between values of the variable, but at the same time we linearise possible non-linear relations. So, Spearman’s coefficient shows the strength of monotonic relation between the two variables: cor.test(mtcarsData$mpg,mtcarsData$disp, method=&quot;spearman&quot;) ## Warning in cor.test.default(mtcarsData$mpg, mtcarsData$disp, method = ## &quot;spearman&quot;): Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: mtcarsData$mpg and mtcarsData$disp ## S = 10415, p-value = 6.37e-13 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.9088824 We can notice that the value of the Spearman’s coefficient in our case is higher than the value of the Pearson’s correlation, which implies that there is indeed non-linear relation between variables. The two variables have a strong monotonic relation, which makes sense for the reasons discussed earlier. The non-linearity makes sense as well because the car with super powerful engines would still be able to do several miles on a gallon of fuel, no matter what. The relation will never be zero or even negative. Note that while Spearman’s correlation will tell you something about monotonic relations, it will fail to capture all other non-linear relations between variables. For example, in the following case the true relation is trigonometric: x &lt;- c(1:100) y &lt;- sin(x) plot(x,y,type=&quot;l&quot;) But neither Pearson’s nor Spearman’s coefficients will be able to capture it: cor(x,y) ## [1] -0.04806497 cor(x,y,method=&quot;spearman&quot;) ## [1] -0.04649265 In order to correctly diagnose such non-linear relation, either one or both variables need to be transformed to linearise the relation. In our case this implies measuring the relation between \\(y\\) and \\(\\sin(x)\\) instead of \\(y\\) and \\(x\\): cor(sin(x),y) ## [1] 1 2.6.4 Mixed scales Finally, when we have two variables measured in different scales, the general recommendation is to use the measure of association for the lower scale. For example, if we have the nominal variable colour and the ordinal variable size (both related to T-shirts people prefer), we should use Cramer’s V in order to measure the relation between them: cramer(colour,size) ## Cramer&#39;s V: 0.2991 ## Chi^2 statistics = 5.7241, df: 4, p-value: 0.2207 Similarly, if we have a numerical and ordinal variables, we should use one of the measures for ordinal scales. However, in some cases we might be able to use a different measure of association. One of those is called multiple correlation coefficient and can be calculated for variables in numerical vs categorical scales. This coefficient can be calculated using different principles, the simplest of which is constructing a regression model (discussed later in Section 3.2) of numerical variable from the dummy variables (see Section ??) created from the categorical one and then extracting the square root of coefficient of determination (discussed in Section 3.2.2). The resulting coefficient lies between 0 and 1, where 1 implies perfect linear relation between the two variables and 0 implies no linear relation between them. mcor() function from greybox implements this: mcor(mtcars$am, mtcars$mpg) ## Multiple correlations value: 0.5998 ## F-statistics = 16.8603, df: 1, df resid: 30, p-value: 3e-04 Based on the value above, we can conclude that the type of transmission has a linear relation with the mileage. This aligns with what we have already discovered earlier, in preliminary analysis section (Section 2.1.2) in Figure 2.14. References "],["distributions.html", "2.7 Theory of distributions", " 2.7 Theory of distributions There are several probability distributions that will be helpful in the further chapters of this textbook. Here, I want to briefly discuss those of them that will be used. While this might not seem important at this point, we will refer to this section of the textbook in the next chapters. 2.7.1 Normal distribution Every statistical textbook has Normal distribution. It is that one famous bell-curved distribution that every statistician likes because it is easy to work with and it is an asymptotic distribution for many other well-behaved distributions in some conditions (see discussion of “Central Limit Theorem” in Section 2.2.2). Here is the probability density function (PDF) of this distribution: \\[\\begin{equation} f(y_t) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{\\left(y_t - \\mu_{y,t} \\right)^2}{2 \\sigma^2} \\right) , \\tag{2.16} \\end{equation}\\] where \\(y_t\\) is the value of the response variable, \\(\\mu_{y,t}=\\mu_{y,t|t-1}\\) is the one step ahead conditional expectation on observation \\(t\\), given the information on \\(t-1\\) and \\(\\sigma^2\\) is the variance of the error term. The maximum likelihood estimate of \\(\\sigma^2\\) is: \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T \\left(y_t - \\hat{\\mu}_{y,t} \\right)^2 , \\tag{2.17} \\end{equation}\\] where \\(T\\) is the sample size and \\(\\hat{\\mu}_{y,t}\\) is the estimate of the conditional expecatation \\(\\mu_{y,t}\\). (2.17) coincides with Mean Squared Error (MSE), discussed in the section 1. And here how this distribution looks (Figure 2.32). Figure 2.32: Probability Density Function of Normal distribution What we typically assume in the basic time series models is that a variable is random and follows Normal distribution, meaning that there is a central tendency (in our case - the mean \\(mu\\)), around which the density of values is the highest and there are other potential cases, but their probability of appearance reduces proportionally to the distance from the centre. The Normal distribution has skewness of zero and kurtosis of 3 (and excess kurtosis, being kurtosis minus three, of 0). Additionally, if Normal distribution is used for the maximum likelihood estimation of a model, it gives the same parameters as the minimisation of MSE would give. The log-likelihood based on the Normal distribution is derived by taking the sum of logarithms of the PDF of Normal distribution (2.16): \\[\\begin{equation} \\ell(\\mathbf{Y}| \\theta, \\sigma^2) = - \\frac{T}{2} \\log \\left(2 \\pi \\sigma^2\\right) - \\sum_{t=1}^T \\frac{\\left(y_t - \\mu_{y,t} \\right)^2}{2 \\sigma^2} , \\tag{2.18} \\end{equation}\\] where \\(\\theta\\) is the vector of all the estimated parameters in the model, and \\(\\log\\) is the natural logarithm. If one takes the derivative of (2.18) with respect to \\(\\sigma^2\\), then the formula (2.17) is obtained. Another useful thing to note is the concentrated log-likelihood, which is obtained by inserting the estimated variance (2.17) in (2.18): \\[\\begin{equation} \\ell(\\mathbf{Y}| \\theta, \\hat{\\sigma}^2) = - \\frac{T}{2} \\log \\left( 2 \\pi e \\hat{\\sigma}^2 \\right) , \\tag{2.19} \\end{equation}\\] where \\(e\\) is the Euler’s constant. The concentrated log-likelihood is handy, when estimating the model and calculating information criteria. Sometimes, statisticians drop the \\(2 \\pi e\\) part from the (2.19), because it does not affect any inferences, as long as one works only with Normal distribution (for example, this is what is done in ets() function from forecast package in R). However, it is not recommended to do (Burnham and Anderson, 2004), because this makes the comparison with other distributions impossible. Normal distribution is available in stats package with dnorm(), qnorm(), pnorm() and rnorm() functions. 2.7.2 Laplace distribution A more exotic distribution is Laplace, which has some similarities with Normal, but has higher excess. It has the following PDF: \\[\\begin{equation} f(y_t) = \\frac{1}{2 s} \\exp \\left( -\\frac{\\left| y_t - \\mu_{y,t} \\right|}{s} \\right) , \\tag{2.20} \\end{equation}\\] where \\(s\\) is the scale parameter, which, when estimated using likelihood, is equal to the Mean Absolute Error (MAE): \\[\\begin{equation} \\hat{s} = \\frac{1}{T} \\sum_{t=1}^T \\left| y_t - \\hat{\\mu}_{y,t} \\right| . \\tag{2.21} \\end{equation}\\] It has the shape shown on Figure 2.33. Figure 2.33: Probability Density Function of Laplace distribution Similar to the Normal distribution, the skewness of Laplace is equal to zero. However, it has fatter tails - its kurtosis is equal to 6 instead of 3. The variance of the random variable following Laplace distribution is equal to: \\[\\begin{equation} \\sigma^2 = 2 s^2. \\tag{2.22} \\end{equation}\\] The dlaplace, qlaplace, plaplace and rlaplace functions from greybox package implement different sides of Laplace distribution in R. 2.7.3 S distribution This is something relatively new, but not ground braking. I have derived S distribution few years ago, but have never written a paper on that. It has the following density function (it is as a special case of Generalised Normal distribution, when \\(\\beta=0.5\\)): \\[\\begin{equation} f(y_t) = \\frac{1}{4 s^2} \\exp \\left( -\\frac{\\sqrt{|y_t - \\mu_{y,t}|}}{s} \\right) , \\tag{2.23} \\end{equation}\\] where \\(s\\) is the scale parameter. If estimated via maximum likelihood, the scale parameter is equal to: \\[\\begin{equation} \\hat{s} = \\frac{1}{2T} \\sum_{t=1}^T \\sqrt{\\left| y_t - \\hat{\\mu}_{y,t} \\right|} , \\tag{2.24} \\end{equation}\\] which corresponds to the minimisation of a half of “Mean Root Absolute Error” or “Half Absolute Moment” (HAM). This is a more exotic type of scale, but the main benefit of this distribution is sever heavy tails - it has kurtosis of 25.2. It might be useful in cases of randomly occurring incidents and extreme values (Black Swans?). Figure 2.34: Probability Density Function of S distribution The variance of the random variable following S distribution is equal to: \\[\\begin{equation} \\sigma^2 = 120 s^4. \\tag{2.25} \\end{equation}\\] The ds, qs, ps and rs from greybox package implement the density, quantile, cumulative and random generation functions. 2.7.4 Generalised Normal distribution Generalised Normal (\\(\\mathcal{GN}\\)) distribution (as the name says) is a generalisation for Normal distribution, which also includes Laplace and S as special cases (Nadarajah, 2005). There are two versions of this distribution: one with a shape and another with a skewness parameter. We are mainly interested in the first one, which has the following PDF: \\[\\begin{equation} f(y_t) = \\frac{\\beta}{2 s \\Gamma(\\beta^{-1})} \\exp \\left( -\\left(\\frac{|y_t - \\mu_{y,t}|}{s}\\right)^{\\beta} \\right), \\tag{2.26} \\end{equation}\\] where \\(\\beta\\) is the shape parameter, and \\(s\\) is the scale of the distribution, which, when estimated via MLE, is equal to: \\[\\begin{equation} \\hat{s} = \\sqrt[^{\\beta}]{\\frac{\\beta}{T} \\sum_{t=1}^T\\left| y_t - \\hat{\\mu}_{y,t} \\right|^{\\beta}}, \\tag{2.27} \\end{equation}\\] which has MSE, MAE and HAM as special cases, when \\(\\beta\\) is equal to 2, 1 and 0.5 respectively. The parameter \\(\\beta\\) influences the kurtosis directly, it can be calculated for each special case as \\(\\frac{\\Gamma(5/\\beta)\\Gamma(1/\\beta)}{\\Gamma(3/\\beta)^2}\\). The higher \\(\\beta\\) is, the lower the kurtosis is. The advantage of \\(\\mathcal{GN}\\) distribution is its flexibility. In theory, it is possible to model extremely rare events with this distribution, if the shape parameter \\(\\beta\\) is fractional and close to zero. Alternatively, when \\(\\beta \\rightarrow \\infty\\), the distribution converges point-wise to the uniform distribution on \\((\\mu_{y,t} - s, \\mu_{y,t} + s)\\). Note that the estimation of \\(\\beta\\) is a difficult task, especially, when it is less than 2 - the MLE of it looses properties of consistency and asymptotic normality. Depending on the value of \\(\\beta\\), the distribution can have different shapes shown in Figure 2.35 Figure 2.35: Probability Density Functions of Generalised Normal distribution Typically, estimating \\(\\beta\\) consistently is a tricky thing to do, especially if it is less than one. Still, it is possible to do that by maximising the likelihood function (2.26). The variance of the random variable following Generalised Normal distribution is equal to: \\[\\begin{equation} \\sigma^2 = s^2\\frac{\\Gamma(3/\\beta)}{\\Gamma(1/\\beta)}. \\tag{2.28} \\end{equation}\\] The working functions for the Generalised Normal distribution are implemented in the greybox package for R. 2.7.5 Asymmetric Laplace distribution Asymmetric Laplace distribution (\\(\\mathcal{AL}\\)) can be considered as a two Laplace distributions with different parameters \\(s\\) for left and right sides from the location \\(\\mu_{y,t}\\). There are several ways to summarise the probability density function, the neater one relies on the asymmetry parameter \\(\\alpha\\) (Yu and Zhang, 2005): \\[\\begin{equation} f(y_t) = \\frac{\\alpha (1- \\alpha)}{s} \\exp \\left( -\\frac{y_t - \\mu_{y,t}}{s} (\\alpha - I(y_t \\leq \\mu_{y,t})) \\right) , \\tag{2.29} \\end{equation}\\] where \\(s\\) is the scale parameter, \\(\\alpha\\) is the skewness parameter and \\(I(y_t \\leq \\mu_{y,t})\\) is the indicator function, which is equal to one, when the condition is satisfied and to zero otherwise. The scale parameter \\(s\\) estimated using likelihood is equal to the quantile loss: \\[\\begin{equation} \\hat{s} = \\frac{1}{T} \\sum_{t=1}^T \\left(y_t - \\hat{\\mu}_{y,t} \\right)(\\alpha - I(y_t \\leq \\hat{\\mu}_{y,t})) . \\tag{2.30} \\end{equation}\\] Thus maximising the likelihood (2.29) is equivalent to estimating the model via the minimisation of \\(\\alpha\\) quantile, making this equivalent to quantile regression approach. So quantile regression models assume indirectly that the error term in the model is \\(\\epsilon_t \\sim \\mathcal{AL}(0, s, \\alpha)\\) (Geraci and Bottai, 2007). Depending on the value of \\(\\alpha\\), the distribution can have different shapes, shown in Figure 2.36. Figure 2.36: Probability Density Functions of Asymmetric Laplace distribution Similarly to \\(\\mathcal{GN}\\) distribution, the parameter \\(\\alpha\\) can be estimated during the maximisation of the likelihood, although it makes more sense to set it to some specific values in order to obtain the desired quantile of distribution. The variance of the random variable following Asymmetric Laplace distribution is equal to: \\[\\begin{equation} \\sigma^2 = s^2\\frac{(1-\\alpha)^2+\\alpha^2}{\\alpha^2(1-\\alpha)^2}. \\tag{2.31} \\end{equation}\\] Functions dalaplace, qalaplace, palaplace and ralaplace from greybox package implement the Asymmetric Laplace distribution. 2.7.6 Log Normal, Log Laplace, Log S and Log GN distributions In addition, it is possible to derive the log-versions of the Normal, \\(\\mathcal{Laplace}\\), \\(\\mathcal{S}\\), and \\(\\mathcal{GN}\\) distributions. The main differences between the original and the log-versions of density functions for these distributions can be summarised as follows: \\[\\begin{equation} f_{log}(\\log(y_t)) = \\frac{1}{y_t} f(\\log y_t). \\tag{2.32} \\end{equation}\\] They are defined for positive values only and will have different right tail, depending on the location, scale and shape parameters. \\(\\exp(\\mu_{\\log y,t})\\) in this case represents the geometric mean (and median) of distribution rather than the arithmetic one. The conditional expectation in these distributions is typically higher than \\(\\exp(\\mu_{\\log y,t})\\) and depends on the value of the scale parameter. It is known for log\\(\\mathcal{N}\\) and is equal to: \\[\\begin{equation} \\mathrm{E}(y_t) = \\mathrm{exp}\\left(\\mu_{\\log y,t} + \\frac{\\sigma^2}{2} \\right). \\tag{2.33} \\end{equation}\\] However, it does not have a simple form for the other distributions. 2.7.7 Inverse Gaussian distribution An exotic distribution that will be useful for what comes in this textbook is the Inverse Gaussian (\\(\\mathcal{IG}\\)), which is parameterised using mean value \\(\\mu_{y,t}\\) and either the dispersion parameter \\(s\\) or the scale \\(\\lambda\\) and is defined for positive values only. This distribution is useful because it is scalable and has some similarities with the Normal one. In our case, the important property is the following: \\[\\begin{equation} \\text{if } (1+\\epsilon_t) \\sim \\mathcal{IG}(1, s) \\text{, then } y_t = \\mu_{y,t} \\times (1+\\epsilon_t) \\sim \\mathcal{IG}\\left(\\mu_{y,t}, \\frac{s}{\\mu_{y,t}} \\right), \\tag{2.34} \\end{equation}\\] implying that the dispersion of the model changes together with the expectation. The PDF of the distribution of \\(1+\\epsilon_t\\) is: \\[\\begin{equation} f(1+\\epsilon_t) = \\frac{1}{\\sqrt{2 \\pi s (1+\\epsilon_t)^3}} \\exp \\left( -\\frac{\\epsilon_t^2}{2 s (1+\\epsilon_t)} \\right) , \\tag{2.35} \\end{equation}\\] where the dispersion parameter can be estimated via maximising the likelihood and is calculated using: \\[\\begin{equation} \\hat{s} = \\frac{1}{T} \\sum_{t=1}^T \\frac{e_t^2}{1+e_t} , \\tag{2.36} \\end{equation}\\] where \\(e_t\\) is the estimate of \\(\\epsilon_t\\). This distribution becomes very useful for multiplicative models, where it is expected that the data can only be positive. Figure 2.37 shows how the PDF of \\(\\mathcal{IG}(1,s)\\) looks for different values of the dispersion \\(s\\) Figure 2.37: Probability Density Functions of Inverse Gaussian distribution statmod package implements density, quantile, cumulative and random number generator functions for the \\(\\mathcal{IG}\\). 2.7.8 Gamma distribution Finally, another distribution that will be useful for ETS and ARIMA is Gamma (\\(\\mathcal{\\Gamma}\\)), which is parameterised using shape \\(\\xi\\) and scale \\(s\\), and is defined for positive values only. This distribution is useful because it is scalable and is as flexible as (\\(\\mathcal{IG}\\)) in terms of possible shapes. It also has an important scalability property (simila to \\(\\mathcal{IG}\\)), but the shape needs to be restricted in order to make sense in ETS model: \\[\\begin{equation} \\text{if } (1+\\epsilon_t) \\sim \\mathcal{\\Gamma}(s^{-1}, s) \\text{, then } y_t = \\mu_{y,t} \\times (1+\\epsilon_t) \\sim \\mathcal{\\Gamma}\\left(s^{-1}, s \\mu_{y,t} \\right), \\tag{2.37} \\end{equation}\\] implying that the scale of the model changes together with the expectation. The restriction on the shape parameters is needed in order to make the expectation of \\((1+\\epsilon_t)\\) equal to one. The PDF of the distribution of \\(1+\\epsilon_t\\) is: \\[\\begin{equation} f(1+\\epsilon_t) = \\frac{1}{\\Gamma(s^{-1}) (s)^{s^{-1}}} (1+\\epsilon_t)^{s^{-1}-1}\\exp \\left(-\\frac{1+\\epsilon_t}{s}\\right) . \\tag{2.38} \\end{equation}\\] However, the scale \\(s\\) cannot be estimated via the maximisation of likelihood analytically due to the restriction (2.37). Luckliy, the method of moments can be used instead, where based on the expectation and variance we get: \\[\\begin{equation} \\hat{s} = \\frac{1}{T} \\sum_{t=1}^T e_t^2 , \\tag{2.39} \\end{equation}\\] where \\(e_t\\) is the estimate of \\(\\epsilon_t\\). So, imposing the restrictions (2.37) implies that the scale of \\(\\mathcal{\\Gamma}\\) is equal to the variance of the error term. Figure 2.38 demonstrates how the PDF of \\(\\mathcal{\\Gamma}(s^{-1},s)\\) looks for different values of \\(s\\): Figure 2.38: Probability Density Functions of Gamma distribution With the increase of the shape \\(\\xi=s^{-1}\\) (in our case this implies the decrease of variance \\(s\\)), \\(\\mathcal{\\Gamma}\\) distribution converges to the normal one with \\(\\mu=\\xi s=1\\) and variance \\(\\sigma^2=s\\). This demonstrates indirectly that the estimate of the scale (2.39) maximises the likelihood of the function (2.38), although I do not have any proper proof of this. References "],["regression.html", "Chapter 3 Regression analysis", " Chapter 3 Regression analysis While we do not expect to cover the regression analysis in its fullness, I think that it is important to have some basic understanding of what regression model is, how it can be estimated and used for forecasting. This chapter introduces the regression starting from the simple linear model and then moving to more advanced topics of multiple linear regression, model estimation, regression assumptions, dummy variables, variables transformations, ARDL and model for scale of distribution. "],["simpleLinearRegression.html", "3.1 Simple Linear Regression", " 3.1 Simple Linear Regression When we want to analyse some relations between variables, we can do graphical and correlations analysis. But this will not provide us sufficient information about what happens with the response variable with the change of explanatory variable. So it makes sense to consider the possible relations between variables, and the basis for this is Simple Linear Regression, which can be represented in the form: \\[\\begin{equation} y_t = a_0 + a_1 x_t + \\epsilon_t , \\tag{3.1} \\end{equation}\\] where \\(a_0\\) is the intercept (constant term), \\(a_1\\) is the coefficient for the slope parameter and \\(\\epsilon_t\\) is the error term. The regression model is a basic statistical model that captures the relation between an explanatory variable \\(x_t\\) and the response variable \\(y_t\\). The parameters of the models are typically denoted as \\(\\beta_0\\) and \\(\\beta_1\\) in econometrics literature, but we use \\(a_0\\) and \\(a_1\\) because we will use \\(\\beta\\) for other purposes later in this textbook. In order to better understand what simple linear regression implies, consider the scatterplot (we discussed it earlier in Section 2.1.2) shown in Figure 3.1. slmMPGWt &lt;- lm(mpg~wt,mtcarsData) plot(mtcarsData$wt, mtcarsData$mpg, xlab=&quot;Weight&quot;, ylab=&quot;Mileage&quot;, xlim=c(0,6), ylim=c(0,40)) abline(h=0, col=&quot;grey&quot;) abline(v=0, col=&quot;grey&quot;) abline(slmMPGWt,col=&quot;red&quot;) text(4,35,paste0(c(&quot;mpg=&quot;,round(coef(slmMPGWt),2),&quot;wt+et&quot;),collapse=&quot;&quot;)) Figure 3.1: Scatterplot diagram between weight and mileage. The line drawn on the plot is the regression line, parameters of which were estimated based on the available data. In this case the intercept \\(\\hat{a}_0\\)=37.29, meaning that this is where the red line crosses the y-axis, while the parameter of slope \\(\\hat{a}_1\\)=-5.34 shows how fast the values change (how steep the line is). I’ve added hat symbols on the parameters to point out that they were estimated based on a sample of data. If we had all the data in the universe (population) and estimated a correct model on it, we would not need the hats. In simple linear regression, the re line will always go through the cloud of points, showing the averaged out tendencies. The one that we observe above can be summarise as “with the increase of weight, on average the mileage of cars goes down.” Note that we might find some specific points, where the increase of weight would not decrease mileage (e.g. the two furthest left points show this), but this can be considered as a random fluctuation, so overall, the average tendency is as described above. 3.1.1 Ordinary Least Squares (OLS) For obvious reasons, we do not have the values of parameters from the population. This means that we will never know what the true intercept and slope are. Luckily, we can estimate them based on the sample of data. There are different ways of doing that, and the most popular one is called “Ordinary Least Squares” method. This is the method that was used in the estimation of the model in Figure 3.1. So, how does it work? Figure 3.2: Scatterplot diagram between weight and mileage. When we estimate the simple linear regression model, the model (3.1) transforms into: \\[\\begin{equation} y_t = \\hat{a}_0 + \\hat{a}_1 x_t + e_t . \\tag{3.2} \\end{equation}\\] This is because we do not know the true values of parameters and thus they are substituted by their estimates. This also applies to the error term for which in general \\(e_t \\neq \\epsilon_t\\) because of the sample estimation. Now consider the same situation with weight vs mileage in Figure 3.2 but with some arbitrary line with unknown parameters. Each point on the plot will typically lie above or below the line, and we would be able to calculate the distances from those points to the line. They would correspond to \\(e_t = y_t - \\hat{y}_t\\), where \\(\\hat{y}_t\\) is the value of the regression line (aka “fitted” value) for each specific value of explanatory variable. For example, for the weight of car of 1.835 tones, the actual mileage is 33.9, while the fitted value is 27.478. The resulting error (or residual of model) is 6.422. We could collect all these errors of the model for all available cars based on their weights and this would result in a vector of positive and negative values like this: ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## -2.2826106 -0.9197704 -2.0859521 1.2973499 ## Hornet Sportabout Valiant Duster 360 Merc 240D ## -0.2001440 -0.6932545 -3.9053627 4.1637381 ## Merc 230 Merc 280 Merc 280C Merc 450SE ## 2.3499593 0.2998560 -1.1001440 0.8668731 ## Merc 450SL Merc 450SLC Cadillac Fleetwood Lincoln Continental ## -0.0502472 -1.8830236 1.1733496 2.1032876 ## Chrysler Imperial Fiat 128 Honda Civic Toyota Corolla ## 5.9810744 6.8727113 1.7461954 6.4219792 ## Toyota Corona Dodge Challenger AMC Javelin Camaro Z28 ## -2.6110037 -2.9725862 -3.7268663 -3.4623553 ## Pontiac Firebird Fiat X1-9 Porsche 914-2 Lotus Europa ## 2.4643670 0.3564263 0.1520430 1.2010593 ## Ford Pantera L Ferrari Dino Maserati Bora Volvo 142E ## -4.5431513 -2.7809399 -3.2053627 -1.0274952 This corresponds to the formula: \\[\\begin{equation} e_t = y_t - \\hat{a}_0 - \\hat{a}_1 x_t. \\tag{3.3} \\end{equation}\\] If we needed to estimate parameters \\(\\hat{a}_0\\) and \\(\\hat{a}_1\\) of the model, we would need to minimise those distances by changing the parameters of the model. The problem is that some errors are positive, while the others are negative. If we just sum them up, they will cancel each other out, and we would loose the information about the distance. The simplest way to get rid of sign and keep the distance is by taking squares of each error and calculating Sum of Squared Errors for the whole sample \\(T\\): \\[\\begin{equation} \\mathrm{SSE} = \\sum_{t=1}^T e_t^2 . \\tag{3.4} \\end{equation}\\] If we now minimise SSE by changing values of parameters \\(\\hat{a}_0\\) and \\(\\hat{a}_1\\), we will find those parameters that would guarantee that the line goes somehow through the cloud of points. Luckily, we do not need to use any fancy optimisers for this, as this has analytical solution (in order to get it, insert (3.3) in (3.4), take derivatives with respect to the parameters \\(\\hat{a}_0\\) and \\(\\hat{a}_1\\) and equate the resulting values to zero): \\[\\begin{equation} \\begin{aligned} \\hat{a}_1 = &amp; \\frac{\\mathrm{cov}(x,y)}{\\mathrm{V}(x)} \\\\ \\hat{a}_0 = &amp; \\bar{y} - \\hat{a}_1 \\bar{x} \\end{aligned} , \\tag{3.5} \\end{equation}\\] where \\(\\bar{x}\\) is the mean of the explanatory variable \\(x_t\\) and \\(\\bar{y}\\) is the mean of the response variables \\(y_t\\). Note that if for some reason \\(\\hat{a}_1=0\\) (for example, because the covariance between \\(x\\) and \\(y\\) is zero, implying that they are not correlated), then the intercept \\(\\hat{a}_0 = \\bar{y}\\), meaning that the global average of the data is the best predictor of the variable \\(y_t\\). This method of estimation of parameters based on the minimisation of SSE, is called “Ordinary Least Squares.” It is simple and does not require any specific assumptions: we just minimise the overall distance by changing the values of parameters. Another thing to note is the connection between the parameter \\(\\hat{a}_1\\) and the correlation coefficient. We have already briefly discussed this in Section 2.6.3, we could estimate two models given the pair of variable \\(x\\) and \\(y\\): Model (3.2); The inverse model \\(x_t = \\hat{b}_0 + \\hat{b}_1 y_t + u_t\\). We could then extract the slope parameters of the two models via (3.5) and get the value of correlation coefficient as a geometric mean of the two: \\[\\begin{equation} r_{x,y} = \\mathrm{sign}(\\hat{b}_1) \\sqrt{\\hat{a}_1 \\hat{b}_1} = \\mathrm{sign}(\\mathrm{cov}(x,y)) \\sqrt{\\frac{\\mathrm{cov}(x,y)}{\\mathrm{V}(x)} \\frac{\\mathrm{cov}(x,y)}{\\mathrm{V}(y)}} = \\frac{\\mathrm{cov}(x,y)}{\\sqrt{V(x)V(y)}} , \\tag{3.6} \\end{equation}\\] which is the formula (2.15). This is how the correlation coefficient was originally derived. While we can do some inference based on simple linear regression, we know that the bivariate relations are not often met in practice: typically a variable is influenced by a set of variables, not just by one. This implies that the correct model would typically include many explanatory variables. This is why we will discuss inference in the next section. "],["linearRegression.html", "3.2 Multiple Linear Regression", " 3.2 Multiple Linear Regression While simple linear regression provides a basic understanding of the idea of capturing the relations between variables, it is obvious that in reality there are more than one external variable that would impact the response variable. This means that instead of (3.1) we should have: \\[\\begin{equation} y_t = a_0 + a_1 x_{1,t} + a_2 x_{2,t} + \\dots + a_{k-1} x_{k-1,t} + \\epsilon_t , \\tag{3.7} \\end{equation}\\] where \\(a_j\\) is a \\(j\\)-th parameter for the respective \\(j\\)-th explanatory variable and there is \\(k-1\\) of them in the model, meaning that when we want to estimate this model, we will have \\(k\\) unknown parameters. The regression line of this model in population (aka expectation conditional on the values of explanatory variables) is: \\[\\begin{equation} \\mu_{y,t} = \\mathrm{E}(y_t | \\mathbf{x}_t) = a_0 + a_1 x_{1,t} + a_2 x_{2,t} + \\dots + a_{k-1} x_{k-1,t} , \\tag{3.8} \\end{equation}\\] while in case of a sample estimation of the model we will use: \\[\\begin{equation} \\hat{y}_t = \\hat{a}_0 + \\hat{a}_1 x_{1,t} + \\hat{a}_2 x_{2,t} + \\dots + \\hat{a}_{k-1} x_{k-1,t} . \\tag{3.9} \\end{equation}\\] While the simple linear regression can be represented as a line on the plane with an explanatory variable and a response variable, the multiple linear regression cannot be easily represented in the same way. In case of two explanatory variables the plot becomes three dimensional and the regression line transforms into regression plane. Figure 3.3: 3D scatterplot of Mileage vs Weight of a car and its Engine Horsepower. Figure 3.3 demonstrates a three dimensional scatterplot with the regression plane, going through the points, similar to how the regression line went through the two dimensional scatterplot 3.1. These sorts of plots are already difficult to read, but the situation becomes even more challenging, when more than two explanatory variables are under consideration: plotting 4D, 5D etc is not a trivial task. Still, what can be said about the parameters of the model even if we cannot plot it in the same way, is that they represent slopes for each variable, in a similar manner as \\(a_1\\) did in the simple linear regression. 3.2.1 OLS estimation In order to show how the estimation of multiple linear regression is done, we need to present it in a more compact form. In order to do that we will introduce the following vectors: \\[\\begin{equation} \\mathbf{x}&#39;_t = \\begin{pmatrix}1 &amp; x_{1,t} &amp; \\dots &amp; x_{k-1,t} \\end{pmatrix}, \\boldsymbol{a} = \\begin{pmatrix}a_0 \\\\ a_{1} \\\\ \\vdots \\\\ a_{k-1} \\end{pmatrix} , \\tag{3.10} \\end{equation}\\] where \\(&#39;\\) symbol is the transposition. This can then be substituted in (3.7) to get: \\[\\begin{equation} y_t = \\mathbf{x}&#39;_t \\boldsymbol{a} + \\epsilon_t . \\tag{3.11} \\end{equation}\\] But this is not over yet, we can make it even more compact, if we pack all those values with index \\(t\\) in vectors and matrices: \\[\\begin{equation} \\mathbf{X} = \\begin{pmatrix} \\mathbf{x}&#39;_1 \\\\ \\mathbf{x}&#39;_2 \\\\ \\vdots \\\\ \\mathbf{x}&#39;_T \\end{pmatrix}, \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_T \\end{pmatrix}, \\boldsymbol{\\epsilon} = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_T \\end{pmatrix} , \\tag{3.12} \\end{equation}\\] where \\(T\\) is the sample size. This leads to the following compact form of multiple linear regression: \\[\\begin{equation} \\mathbf{y} = \\mathbf{X} \\boldsymbol{a} + \\boldsymbol{\\epsilon} . \\tag{3.13} \\end{equation}\\] Now that we have this compact form of multiple linear regression, we can estimate it using linear algebra. Many statistical textbooks explain how the following result is obtained (this involves taking derivative of SSE (3.4) with respect to \\(\\boldsymbol{a}\\) and equating it to zero): \\[\\begin{equation} \\hat{\\boldsymbol{a}} = \\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1} \\mathbf{X}&#39; \\mathbf{y} . \\tag{3.14} \\end{equation}\\] The formula (3.14) is used in all the statistical software, including lm() function from stats package for R. Here is an example with the same mtcars dataset: mtcarsModel01 &lt;- lm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb, mtcars) The simplest plot that we can produce from this model is fitted values vs actuals, plotting \\(\\hat{y}_t\\) on x-axis and \\(y_t\\) on the y-axis: plot(fitted(mtcarsModel01),actuals(mtcarsModel01)) The same plot is produced via plot() method if we use alm() function from greybox instead: mtcarsModel02 &lt;- alm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb, mtcars) plot(mtcarsModel02,1) Figure 3.4: Actuals vs fitted values for multiple linear regression model on mtcars data. This plot can be used for diagnostic purposes and in ideal situation the red line (LOWESS line) should coincide with the grey one, which would mean that we have correctly capture the tendencies in the data, so that all the regression assumptions are satisfied (see Section 3.6). We will come back to the model diagnostics in Section 17. 3.2.2 Quality of a fit In order to get a general impression about the performance of the estimated model, we can calculate several in-sample measures, which could provide us insights about the fit of the model. The first one is based on the OLS criterion, (3.4) and is called either “Root Mean Squared Error” (RMSE) or a “standard error” or a “standard deviation of error” of the regression: \\[\\begin{equation} \\mathrm{RMSE} = \\sqrt{\\frac{1}{T-k} \\sum_{t=1}^T e_t^2 }. \\tag{3.15} \\end{equation}\\] Note that it is divided by the number of degrees of freedom in the model, \\(T-k\\), not on the number of observations. This is needed to correct the in-sample bias of the measure. RMSE does not tell us about the in-sample performance but can be used to compare several models with the same response variable between each other: the lower RMSE is, the better the model fits the data. Note that this measure is not aware of the randomness in the true model and thus will be equal to zero in a model that fits the data perfectly (thus ignoring the existence of error term). This is a potential issue, as we might end up with a poor model that would seem like the best one. Here is how this can be calculated for our model, estimated using alm() function: sigma(mtcarsModel02) ## [1] 2.68094 Another measure is called “Coefficient of Determination” and is calculated based on the following sums of squares: \\[\\begin{equation} \\mathrm{R}^2 = 1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}} = \\frac{\\mathrm{SSR}}{\\mathrm{SST}}, \\tag{3.16} \\end{equation}\\] where SSE$=_{t=1}^T e_t^2 $ is the OLS criterion defined in (3.4), \\[\\begin{equation} \\mathrm{SST}=\\sum_{t=1}^T (y_t - \\bar{y})^2, \\tag{3.17} \\end{equation}\\] is the total sum of squares (where \\(\\bar{y}\\) is the in-sample mean) and \\[\\begin{equation} \\mathrm{SSR}=\\sum_{t=1}^T (\\hat{y}_t - \\bar{y})^2, \\tag{3.18} \\end{equation}\\] is the sum of squares of the regression line. SSE, as discussed above, shows the overall distance of actual values from the regression line. The SST has an apparent connection with the variance of the response variable: \\[\\begin{equation} \\mathrm{V}(y) = \\frac{1}{T-1} \\sum_{t=1}^T (y_t - \\bar{y})^2 = \\frac{1}{T-1} \\mathrm{SST} . \\tag{3.19} \\end{equation}\\] Finally, SSR characterises the deviation of the regression line from the mean. In the linear regression (this is important! This property might be violated in other models), the three sums are related via the following equation: \\[\\begin{equation} \\mathrm{SST} = \\mathrm{SSE} + \\mathrm{SSR}, \\tag{3.20} \\end{equation}\\] which explains why the coefficient of determination (3.16) can be calculated using two different formulae. If we want to interpret the coefficient of determination \\(\\mathrm{R}^2\\), we can imagine the following situations: The model fits the data in the same way as a straight line (mean). In this case SSE would be equal to SST and SSR would be equal to zero (because \\(\\hat{y}_t=\\bar{y}\\)) and as a result the R\\(^2\\) would be equal to zero. The model fits the data perfectly, without any errors. In this situation SSE would be equal to zero and SSR would be equal to SST, because the regression would go through all points (i.e. \\(\\hat{y}_t=y_t\\)). This would make R\\(^2\\) equal to one. In the linear regression model due to (3.20), the coefficient of determination would always lie between zero and one, where zero means that the model does not explain the data at all and one means that it overfits the data. The value itself is usually interpreted as a percentage of variability in data explained by the model. This definition above provides us an important point about the coefficient of determination: it should not be equal to one, and it is alarming if it is very close to one - because in this situation we are implying that there is no randomness in the data, but this contradicts our definition of the statistical model (see Section 1.1). So, in practice we should not maximise R\\(^2\\) and should be careful with models that have very high values of it. At the same time, too low values of R\\(^2\\) are also alarming, as they tell us that the model is not very different from the global mean. So, coefficient of determination in general is not a very good measure for assessing performance of a model. Here how this measure can be calculated in R based on the estimated model: 1 - sigma(mtcarsModel02)^2*(nobs(mtcarsModel02)-nparam(mtcarsModel02)) / (var(actuals(mtcarsModel02))*(nobs(mtcarsModel02)-1)) ## [1] 0.8595764 Note that in this formula we used the relation between SSE and RMSE and between SST and V\\((y)\\), multiplying the values by \\(n-k\\) and \\(n-1\\) respectively. The resulting value tells us that the model has explained 94.7% deviations in the data. Based on coefficient of determination, we can also calculate the coefficient of multiple correlation, which we have already discussed in Section 2.6.4: \\[\\begin{equation} R = \\sqrt{R^2} = \\sqrt{\\frac{\\mathrm{SSR}}{\\mathrm{SST}}} . \\tag{3.21} \\end{equation}\\] Furthermore, the value of coefficient of determination would always increase with the increase of number of variables included in the model. This is because every variable will explain some proportion of the data due to randomness. So, if we add redundant variables, the fit will improve, but the quality of model will decrease. Here is an example: mtcarsData$noise &lt;- rnorm(nrow(mtcarsData),0,10) mtcarsModel02WithNoise &lt;- alm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb+noise, mtcarsData) And here is the value of determination coefficient of the new model: 1 - sigma(mtcarsModel02WithNoise)^2*(nobs(mtcarsModel02WithNoise)-nparam(mtcarsModel02WithNoise)) / (var(actuals(mtcarsModel02WithNoise))*(nobs(mtcarsModel02WithNoise)-1)) ## [1] 0.8595798 The value in the new model will always be higher than in the previous one, no matter how we generate the random fluctuations. This means that some sort of penalisation of the number of variables in the model is required in order to make the measure more reasonable. This is what adjusted coefficient of determination is supposed to do: \\[\\begin{equation} R^2_{adj} = 1 - \\frac{\\mathrm{MSE}}{\\mathrm{V}(y)} = 1 - \\frac{(n-1)\\mathrm{SSE}}{(n-k)\\mathrm{SST}}, \\tag{3.22} \\end{equation}\\] where MSE is the Mean Squared Error (square of RMSE (3.15)). So, instead of dividing sums of squares, in the adjusted R\\(^2\\) we divide the entities that are based on degrees of freedom. Given the presence of \\(k\\) in the formula (3.22), the coefficient will not necessarily increase with the addition of variables - when the variable does not contribute in the reduction of SSE of model substantially, R\\(^2\\) will not go up. Here how it can be calculated for a model in R: setNames(c(1 - sigma(mtcarsModel02)^2 / var(actuals(mtcarsModel02)), 1 - sigma(mtcarsModel02WithNoise)^2 / var(actuals(mtcarsModel02WithNoise))), c(&quot;R^2-adj&quot;,&quot;R^2-adj, Noise&quot;)) ## R^2-adj R^2-adj, Noise ## 0.8021303 0.7927131 What we hope to see in the output above is that the model with the noise will have a lower value of adjusted R\\(^2\\) than the model without it. However, given that we deal with randomness, if you reproduce this example many times, you will see different situation, including those, where introducing noise still increases the value of the parameter. So, you should not fully trust R\\(^2_{adj}\\) either. When constructing a model or deciding what to include in it, you should always use your judgement - make sure that the variables included in the model are meaningful. Otherwise you can easily overfit the data, which would lead to inaccurate forecasts and inefficient estimates of parameters (see Section 3.6 for details). 3.2.3 Interpretation of parameters Finally, we come to the discussion of parameters of a model. As mentioned earlier, each one of them represents the slope of the model. But there is more to the meaning of parameters of the model. Consider the coefficients of the previously estimated model: coef(mtcarsModel02) ## (Intercept) cyl disp hp drat wt ## 17.88963741 -0.41459575 0.01293240 -0.02084886 1.10109551 -3.92064847 ## qsec gear carb ## 0.54145693 1.23321026 -0.25509911 Each of the parameters of this model shows an average effect of each variable on the mileage. They have a simple interpretation and show how the response variable will change on average with the increase of a variable by 1 unit, keeping all the other variables constant. For example, the parameter for wt (weight) shows that with the increase of weight of a car by 1000 pounds, the mileage would decrease on average by 3.921 miles per gallon, if all the other variables do not change. I have made the word “average” boldface three times in this paragraph for a reason. This is a very important point to keep in mind - the parameters will not tell you how variable will change for any specific observation. They do not show how it will change for each point. The regression model capture average tendencies and thus the word “average” is very important in the interpretation. In each specific case, the increase of weight by 1 will lead to different decreases (and even increases in some cases). But if we take the arithmetic mean of those individual effects, it will be close to the value of the parameter in the model. This however is only possible if all the assumptions of regression hold (see Section 3.6). "],["regression-uncertainty.html", "3.3 Regression uncertainty", " 3.3 Regression uncertainty Coming back to the example of mileage vs weight of cars, the estimated simple linear regression on the data was mpg=37.29-5.34wt+et. But what would happen if we estimate the same model on a different sample of data (e.g. 15 first observations instead of 32)? Figure 3.5: Weight vs mileage and two regression lines. Figure 3.5 shows the two lines: the red one corresponds to the larger sample, while the blue one corresponds to the small one. We can see that these lines have different intercepts and slope parameters. So, which one of them is correct? An amateur analyst would say that the one that has more observations is the correct model. But a more experienced statistician would tell you that none of the two is correct. They are both estimated on a sample of data and they both inevitably inherit the uncertainty of the data, making them both incorrect if we compare them to the hypothetical true model. This means that whatever regression model we estimate on a sample of data, it will be incorrect as well. This uncertainty about the regression line actually comes to the uncertainty of estimates of parameters of the model. In order to see it more clearly, consider the example with Speed and Stopping Distances of Cars dataset from datasets package (?cars): Figure 3.6: Speed vs stopping distance of cars While the linear relation between these variables might be not the the most appropriate, it suffices for demonstration purposes. What we will do for this example is fit the model and then use a simple bootstrap technique to get estimates of parameters of the model. We will do that using coefbootstrap() method from greybox package. The bootstrap technique implemented in the function applies the same model to subsamples of the original data and returns a matrix with parameters. This way we get an idea about the empirical uncertainty of parameters: slmSpeedDistance &lt;- alm(dist~speed,cars) slmSpeedDistanceBootstrap &lt;- coefbootstrap(slmSpeedDistance) Based on that we can plot the histograms of the estimates of parameters. par(mfcol=c(1,2)) hist(slmSpeedDistanceBootstrap$coefficients[,1], xlab=&quot;Intercept&quot;, main=&quot;&quot;) hist(slmSpeedDistanceBootstrap$coefficients[,2], xlab=&quot;Slope&quot;, main=&quot;&quot;) Figure 3.7: Distribution of bootstrapped parameters of a regression model Figure 3.7 shows the uncertainty around the estimates of parameters. These distributions look similar to the normal distribution. In fact, if we repeated this example thousands of times, the distribution of estimates of parameters would indeed follow the normal one due to CLT (if the assumptions hold, see Sections 2.2.2 and 3.6). As a result, when we work with regression we should take this uncertainty about the parameters into account. This applies to both parameters analysis and forecasting. 3.3.1 Confidence intervals In order to take this uncertainty into account, we could construct confidence intervals for the estimates of parameters, using the principles discussed in Section 2.4. This way we would hopefully have some idea about the uncertainty of the parameters, and not just rely on average values. If we assume that CLT holds, we could use the t statistics for the calculation of the quantiles of distribution (we need to use t because we do not know the variance of estimates of parameters). But in order to do that, we need to have variances of estimates of parameters. One of possible ways of getting them would be the bootstrap used in the example above. However, this is a computationally expensive operation, and there is a more efficient procedure, which however only works with linear regression models either estimated using OLS or via Maximum Likelihood Estimation assuming Normal distribution (see Section 3.4). In these conditions the covariance matrix of parameters can be calculated using the following formula: \\[\\begin{equation} \\mathrm{V}(\\hat{\\mathbf{a}}) = \\frac{1}{T-k} \\sum_{t=1}^T e_t^2 \\times \\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}. \\tag{3.23} \\end{equation}\\] This matrix will contain variances of parameters on the diagonal and covariances between the parameters on off-diagonals. In this specific case, we only need the diagonal elements. We can take square root of them to obtain standard errors of parameters, which can then be used to construct confidence intervals for each parameter \\(j\\) via: \\[\\begin{equation} a_j \\in (\\hat{a}_j + t_{\\alpha/2}(T-k) s_{\\hat{a}_j}, \\hat{a}_j + t_{1-\\alpha/2}(T-k) s_{\\hat{a}_j}), \\tag{3.23} \\end{equation}\\] where \\(s_{\\hat{a}_j}\\) is the standard error of the parameter \\(\\hat{a}_j\\). All modern software does all these calculations automatically, so we do not need to do them manually. Here is an example: vcov(slmSpeedDistance) ## (Intercept) speed ## (Intercept) 46.648354 -2.7153941 ## speed -2.715394 0.1763243 This is the covariance matrix of parameters, the diagonal elements of which are then used in the confint() method: confint(slmSpeedDistance) ## S.E. 2.5% 97.5% ## (Intercept) 6.8299600 -31.319202 -3.838988 ## speed 0.4199099 3.087659 4.777158 The confidence interval for speed above shows, for example, that if we repeat the construction of interval many times, the true value of parameter speed will lie in 95% of cases between 3.08 and 4.78. This gives an idea about the real effect in the population. We can also present all of this in the following summary (this is based on the alm() model, the other functions will produce different summaries): summary(slmSpeedDistance) ## Response variable: dist ## Distribution used in the estimation: Normal ## Loss function used in estimation: likelihood ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## (Intercept) -17.5791 6.8300 -31.3192 -3.8390 * ## speed 3.9324 0.4199 3.0877 4.7772 * ## ## Error standard deviation: 15.5423 ## Sample size: 50 ## Number of estimated parameters: 3 ## Number of degrees of freedom: 47 ## Information criteria: ## AIC AICc BIC BICc ## 419.1569 419.6786 424.8929 425.9135 This summary provide all the necessary information about the estimates of parameters: their mean values in the column “Estimate,” their standard errors in “Std. Error,” the bounds of confidence interval and finally a star if the interval does not contain zero. This typically indicates that we are certain on the selected confidence level (95% in our example) about the sign of the parameter and that the effect really exists. 3.3.2 Hypothesis testing Another way to look at the uncertainty of parameters is to test a statistical hypothesis. As it was discussed in Section 2.5, I personally think that hypothesis testing is a less useful instrument for these purposes than the confidence interval and that it might be misleading in some circumstances. Nonetheless, it has its merits and can be helpful if an analyst knows what they are doing. In order to test the hypothesis, we need to follow the procedure, described in Section 2.5. The classical hypotheses for the parameters are formulated in the following way: \\[\\begin{equation} \\begin{aligned} \\mathrm{H}_0: a_j = 0 \\\\ \\mathrm{H}_1: a_j \\neq 0 \\end{aligned} . \\tag{3.24} \\end{equation}\\] This formulation of hypotheses comes from the idea that we want to check if the effect estimated by the regression is indeed there (i.e. statistically significantly different from zero). Note however, that as in any other hypothesis testing, if you fail to reject the null hypothesis, this only means that you do not know, we do not have enough evidence to conclude anything. This does not mean that there is no effect and that the respective variable can be removed from the model. In case of simple linear regression, the null and alternative hypothesis can be represented graphically as shown in Figure 3.8. Figure 3.8: Graphical presentation of null and alternative hypothesis in regression context The graph on the left in Figure 3.8 demonstrates how the true model could look if the null hypothesis was true - it would be just a straight line, parallel to x-axis. The graph on the right demonstrates the alternative situation, when the parameter is not equal to zero. We do not know the true model, and hypothesis testing does not tell us, whether the hypothesis is true or false, but if we have enough evidence to reject H\\(_0\\), then we might conclude that we see an effect of one variable on another in the data. Note, as discussed in Section 2.5, the null hypothesis is always wrong, and it will inevitably be rejected with the increase of sample size. Given the discussion in the previous subsection, we know that the parameters of regression model will follow normal distribution, as long as all assumptions are satisfied (including those for CLT). We also know that because the standard errors of parameters are estimated, we need to use Student’s distribution, which takes the uncertainty about the variance into account. Based on this, we can say that the following statistics will follow t with \\(T-k\\) degrees of freedom: \\[\\begin{equation} \\frac{\\hat{a}_j - 0}{s_{\\hat{a}_j}} \\sim t(T-k) . \\tag{3.25} \\end{equation}\\] After calculating the value and comparing it with the critical t-value on the selected significance level or directly comparing p-value based on (3.25) with the significance level, we can make conclusions about the hypothesis. The context of regression provides a great example, why we never accept hypothesis and why in the case of “Fail to reject H\\(_0\\),” we should not remove a variable (unless we have more fundamental reasons for doing that). Consider an example, where the estimated parameter \\(\\hat{a}_1=0.5\\), and its standard error is \\(s_{\\hat{a}_1}=1\\), we estimated a simple linear regression on a sample of 30 observations, and we want to test, whether the parameter in the population is zero (i.e. hypothesis (3.24)) on 1% significance level. Inserting the values in formula (3.25), we get: \\[\\begin{equation*} \\frac{|0.5 - 0|}{1} = 0.5, \\end{equation*}\\] with the critical value for two-tailed test of \\(t_{0.01}(30-2)\\approx 2.76\\). Comparing t-value with the critical one, we would conclude that we fail to reject H\\(_0\\) and thus the parameter is not statistically different from zero. But what would happen if we check another hypothesis: \\[\\begin{equation*} \\begin{aligned} \\mathrm{H}_0: a_1 = 1 \\\\ \\mathrm{H}_1: a_1 \\neq 1 \\end{aligned} . \\end{equation*}\\] The procedure is the same, the calculated t-value is: \\[\\begin{equation*} \\frac{|0.5 - 1|}{1} = 0.5, \\end{equation*}\\] which leads to exactly the same conclusion as before: on 1% significance level, we fail to reject the new H\\(_0\\), so the value is not distinguishable from 1. So, which of the two is correct? The correct answer is “we do not know.” The non-rejection region just tells us that uncertainty about the parameter is so high that it also include the value of interest (0 in case of the classical regression analysis). If we constructed the confidence interval for this problem, we would not have such confusion, as we would conclude that on 1% significance level the true parameter lies in the region \\((-2.26, 3.26)\\) and can be any of these numbers. In R, if you want to test the hypothesis for parameters, I would recommend using lm() function for regression: lmSpeedDistance &lt;- lm(dist~speed,cars) summary(lmSpeedDistance) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 This output tells us that when we consider the parameter for the variable speed, we reject the standard H\\(_0\\) on the pre-selected 1% significance level (comparing the level with p-value in the last column of the output). Note that we should first select the significance level and only then conduct the test, otherwise we would be bending reality for our needs. Finally, in regression context, we can test another hypothesis, which becomes useful, when a lot of parameters of the model are very close to zero and seem to be insignificant on the selected level: \\[\\begin{equation} \\begin{aligned} \\mathrm{H}_0: a_1 = a_2 = \\dots = a_{k-1} = 0 \\\\ \\mathrm{H}_1: a_1 \\neq 0 \\vee a_2 \\neq 0 \\vee \\dots \\vee a_{k-1} \\neq 0 \\end{aligned} , \\tag{3.26} \\end{equation}\\] which translates into normal language as “H\\(_0\\): all parameters (except for intercept) are equal to zero; H\\(_1\\): at least one parameter is not equal to zero.” This is tested based on F statistics with \\(k-1\\), \\(T-k\\) degrees of freedom and is reported, for example, by lm() (see the last line in the previous output). This hypothesis is not very useful, when the parameter are significant and coefficient of determination is high. It only becomes useful in difficult situations of poor fit. The test on its own does not tell if the model is adequate or not. And the F value and related p-value is not comparable with respective values of other models. Graphically, this test checks, whether in the true model the slope of the straight line on the plot of actuals vs fitted is different from zero. An example with the same stopping distance model is provided in Figure 3.9. Figure 3.9: Graphical presentation of F test for regression model. What the test is tries to get insight about, is whether in the true model the blue line coincides with the red line (i.e. the slope is equal to zero, which is only possible, when all parameters are zero). If we have enough evidence to reject the null hypothesis, then this means that the slopes are different on the selected significance level. 3.3.3 Regression model uncertainty Given the uncertainty of estimates of parameters, the regression line itself and the points around it will be uncertain. This means that in some cases we should not just consider the predicted values of the regression \\(\\hat{y}_t\\), but also the uncertainty around them. The uncertainty of the regression line builds upon the uncertainty of parameters and can be measured via the conditional variance in the following way: \\[\\begin{equation} \\mathrm{V}(\\hat{y}_t| \\mathbf{x}_t) = \\mathrm{V}(\\hat{a}_0 + \\hat{a}_1 x_{1,t} + \\hat{a}_2 x_{2,t} + \\dots + \\hat{a}_{k-1} x_{k-1,t}) , \\tag{3.27} \\end{equation}\\] which after some simplifications leads to: \\[\\begin{equation} \\mathrm{V}(\\hat{y}_t| \\mathbf{x}_t) = \\sum_{j=0}^{k-1} \\mathrm{V}(\\hat{a}_j) x^2_{j,t} + 2 \\sum_{j=1}^{k-1} \\sum_{i=0}^{j-1} \\mathrm{cov}(\\hat{a}_i,\\hat{a}_j) x_{i,t} x_{j,t} , \\tag{3.28} \\end{equation}\\] where \\(x_{0,t}=1\\). As we see, the variance of the regression line involves variances and covariances of parameters. This variance can then be used in the construction of the confidence interval for the regression line. Given that each estimate of parameter \\(\\hat{a}_j\\) will follow normal distribution with a fixed mean and variance due to CLT, the predicted value \\(\\hat{y}_t\\) will follow normal distribution as well. This can be used in the construction of the confidence interval, in a manner similar to the one discussed in Section 2.4: \\[\\begin{equation} \\mu \\in (\\hat{y}_t + t_{\\alpha/2}(T-k) s_{\\hat{y}_t}, \\hat{y}_t + t_{1-\\alpha/2}(T-k) s_{\\hat{y}_t}), \\tag{3.29} \\end{equation}\\] where \\(s_{\\hat{y}_t}=\\sqrt{\\mathrm{V}(\\hat{y}_t| \\mathbf{x}_t)}\\). In R, this interval can be constructed via the function predict() with interval=\"confidence\". Note that it can be produced not only for the in-sample value, but for the holdout as well. Here is an example with alm() function: slmSpeedDistanceCI &lt;- predict(slmSpeedDistance,interval=&quot;confidence&quot;) plot(slmSpeedDistanceCI, main=&quot;&quot;) Figure 3.10: Fitted values and confidence interval for the stopping distance model. The same fitted values and interval can be presented differently on the actuals vs fitted plot: plot(fitted(slmSpeedDistance),actuals(slmSpeedDistance), xlab=&quot;Fitted&quot;,ylab=&quot;Actuals&quot;) abline(a=0,b=1,col=&quot;blue&quot;,lwd=2) lines(sort(fitted(slmSpeedDistance)), slmSpeedDistanceCI$lower[order(fitted(slmSpeedDistance))], col=&quot;red&quot;) lines(sort(fitted(slmSpeedDistance)), slmSpeedDistanceCI$upper[order(fitted(slmSpeedDistance))], col=&quot;red&quot;) Figure 3.11: Actuals vs Fitted and confidence interval for the stopping distance model. Figure 3.11 demonstrates the actuals vs fitted plot, together with the 95% confidence interval around the line, demonstrating where the line would be expected to be in 95% of the cases if we re-estimate the model many times. We also see that the uncertainty of the regression line is lower in the middle of the data, but expands in the tails. Conceptually, this happens because the regression line, estimated via OLS, always passes through the average point of the data \\((\\bar{x},\\bar{y})\\) and the variability in this point is lower than the variability in the tails. If we are not interested in the uncertainty of the regression line, but rather in the uncertainty of the observations, we can refer to prediction interval. The variance in this case is: \\[\\begin{equation} \\mathrm{V}(y_t| \\mathbf{x}_t) = \\mathrm{V}(\\hat{a}_0 + \\hat{a}_1 x_{1,t} + \\hat{a}_2 x_{2,t} + \\dots + \\hat{a}_{k-1} x_{k-1,t} + e_t) , \\tag{3.30} \\end{equation}\\] which can be simplified to: \\[\\begin{equation} \\mathrm{V}(y_t| \\mathbf{x}_t) = \\mathrm{V}(\\hat{y}_t| \\mathbf{x}_t) + \\hat{\\sigma}^2, \\tag{3.31} \\end{equation}\\] where \\(\\hat{\\sigma}^2\\) is the variance of the residuals \\(e_t\\). As we see from the formula (3.31), the variance in this case is larger, which will result in wider interval than the confidence one. We can use normal distribution for the construction of the interval in this case (using formula similar to (3.29)), as long as we can assume that \\(\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)\\). In R, this can be done via the very same predict() function with interval=\"prediction\": slmSpeedDistancePI &lt;- predict(slmSpeedDistance,interval=&quot;prediction&quot;) Based on this, we can construct graphs similar to 3.10 and 3.11. Figure 3.12: Fitted values and prediction interval for the stopping distance model. Figure 3.12 shows the prediction interval for values over observations and for actuals vs fitted. As we see, the interval is wider in this case, covering only 95% of observations (there are 2 observations outside it). In forecasting, prediction interval has a bigger importance than the confidence interval. This is because we are typically interested in capturing the uncertainty about the observations, not about the estimate of a line. Typically, the prediction interval would be constructed for some holdout data, which we did not have at the model estimation phase. In the example with stopping distance, we could see what would happen if the speed of a car was, for example, 30mph: slmSpeedDistanceForecast &lt;- predict(slmSpeedDistance,newdata=data.frame(speed=30), interval=&quot;prediction&quot;) plot(slmSpeedDistanceForecast) Figure 3.13: Forecast of the stopping distance for the speed of 30mph. Figure 3.13 shows the point forecast (the expected stopping distance if the speed of car was 30mph) and the 95% prediction interval (we expect that in 95% of the cases, the cars will have the stopping distance between 66.492 and 134.294 feet. "],["likelihoodApproach.html", "3.4 Likelihood Approach", " 3.4 Likelihood Approach We will use different estimation techniques throughout this book, one of the main of which is Maximum Likelihood Estimate (MLE). The very rough idea of the approach is to maximise the chance that each observation in the sample follows a pre-selected distribution with specific set of parameters. In a nutshell, what we try to do when using likelihood for estimation, is fit the distribution function to the data. In order to demonstrate this idea, we start in a non-conventional way, with an example in R. We will then move to the mathematical side of the problem. 3.4.1 An example in R We consider a simple example, when we want to estimate the model \\(y_t = \\mu_y + \\epsilon_t\\) (global average), assuming that the error term follows normal distribution: \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\), which means that \\(y_t \\sim \\mathcal{N}(\\mu_{y}, \\sigma^2)\\). In this case we want to estimate two parameters using likelihood: \\(\\hat{\\mu}_y\\) and \\(\\hat{\\sigma}^2\\). First, we generate the random variable in R and plot its distribution: y &lt;- rnorm(1000, 100, 10) hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) As expected, the distribution of this variable (1000 observations) has the bell shape of Normal distribution. In order to estimate the parameters, for the distribution, we will try them one by one and see how the likelihood and the shape of the fitted curve to this histogram change. We start with \\(\\hat{\\mu}_y=80\\) and \\(\\hat{\\sigma}=10\\) just to see how the probability density function of normal distribution fits the data: hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) lines(c(50:150),dnorm(c(50:150),80,10),col=&quot;red&quot;,lwd=2) abline(v=80,col=&quot;red&quot;,lwd=2) Figure 3.14: ML example with Normal curve and \\(\\hat{\\mu}_y=80\\) and \\(\\hat{\\sigma}=10\\) and we get the following log-likelihood value (we will discuss how this formula can be obtained later): sum(dnorm(y,80,10,log=T)) ## [1] -5744.797 In order for the normal distribution on 3.14 to fit the data well, we need to shift the estimate of \\(\\mu_y\\) to the right, thus increasing the value to, let’s say, \\(\\hat{\\mu}_y=90\\): hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) lines(c(50:150),dnorm(c(50:150),90,10),col=&quot;orange&quot;,lwd=2) abline(v=90,col=&quot;orange&quot;,lwd=2) Figure 3.15: ML example with Normal curve and \\(\\hat{\\mu}_y=90\\) and \\(\\hat{\\sigma}=10\\) Now, in Figure 3.15, the normal curve is much closer to the data, but it is still a bit off. The log-likelihood value in this case is -4225.568, which is higher than the previous one, indicating that we are moving towards the maximum of the likelihood function. Moving it further, setting \\(\\hat{\\mu}_y=100\\), we get: hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) lines(c(50:150),dnorm(c(50:150),100,10),col=&quot;green3&quot;,lwd=2) abline(v=100,col=&quot;green3&quot;,lwd=2) Figure 3.16: ML example with Normal curve and \\(\\hat{\\mu}_y=100\\) and \\(\\hat{\\sigma}=10\\) Figure 3.15 demonstrates a much better fit than in the previous cases with the log-likelihood of -3706.34, which is even higher than in the previous case. We are almost there. In fact, in order to maximise this likelihood, we just need to calculate the sample mean of the variable (this is the MLE of the location parameter in normal distribution) and insert it in the function to obtain: hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) lines(c(50:150),dnorm(c(50:150),mean(y),10),col=&quot;darkgreen&quot;,lwd=2) abline(v=mean(y),col=&quot;darkgreen&quot;,lwd=2) Figure 3.17: ML example with Normal curve and \\(\\hat{\\mu}_y=\\bar{y}\\) and \\(\\hat{\\sigma}=10\\) So the value of \\(\\hat{\\mu}_y=\\bar{y}=\\) 100.192 (where \\(\\bar{y}\\) is the sample mean) maximises the likelihood function, resulting in log-likelihood of -3706.155. In a similar fashion we can get the MLE of the scale parameter \\(\\sigma^2\\) of the model. In this case, we will be changing the height of the distribution. Here is an example with \\(\\hat{\\mu}_y=\\) 100.192 and \\(\\hat{\\sigma}=15\\): hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) lines(c(50:150),dnorm(c(50:150),mean(y),15),col=&quot;royalblue&quot;,lwd=2) abline(v=mean(y),col=&quot;royalblue&quot;,lwd=2) Figure 3.18: ML example with Normal curve and \\(\\hat{\\mu}_y=\\bar{y}\\) and \\(\\hat{\\sigma}=15\\) Figure 3.18 demonstrates that the curve is located lower than needed, which implies that the scale parameter \\(\\hat{\\sigma}\\) is too high. The log-likelihood value in this case is -3842.38. In order to get a better fit of the curve to the data, we need to reduce the \\(\\hat{\\sigma}\\). Here how the situation would look for the case of \\(\\hat{\\sigma}=10\\): hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) lines(c(50:150),dnorm(c(50:150),mean(y),10),col=&quot;darkblue&quot;,lwd=2) abline(v=mean(y),col=&quot;darkblue&quot;,lwd=2) Figure 3.19: ML example with Normal curve and \\(\\hat{\\mu}_y=\\bar{y}\\) and \\(\\hat{\\sigma}=10\\) The fit on Figure 3.19 is better than on Figure 3.18, which is also reflected in the log-likelihood value being equal to -3706.155 instead of -3842.38. The best fit and the maximum of the likelihood is obtained, when the scale parameter is estimated using the formula \\(\\hat{\\sigma}^2 = \\frac{1}{T}\\sum_{t=1}^T\\left(y_t - \\bar{y}\\right)^2\\), resulting in log-likelihood of -3705.913. Note that if we use the unbiased estimate of the variance \\(\\hat{s}^2 = \\frac{1}{T-1}\\sum_{t=1}^T\\left(y_t - \\bar{y}\\right)^2\\), the log-likelihood will not reach the maximum and will be equal to -3705.914. In our special case the difference between the two is infinitesimal, because of the large sample (1000 observations), but it will be more substantial on small samples. Still, the two likelihood values are diffrent, which can be checked in R via the following commands: # The maximum log-likelihood with the biased variance logLik01 &lt;- sum(dnorm(y,mean(y),sqrt(mean((y-mean(y))^2)),log=TRUE)) # The log-likelihood value with the unbiased variance logLik02 &lt;- sum(dnorm(y,mean(y),sd(y),log=TRUE)) # The difference between the two logLik01 - logLik02 All of this is great, but so far we have discussed a very special case, when the data follows normal distribution and we fit the respective model. But what if the model is wrong (no kidding!)? In that case the idea stays the same: we need to find the parameters of the normal distribution, that would guarantee the best possible fit to the non-normal data. Here is an example with MLE of parameters of Normal distribution for the data following Log Normal one: y &lt;- rlnorm(1000, log(80), 0.4) hist(y, main=&quot;&quot;, probability=T, xlim=c(0,300)) lines(c(0:300),dnorm(c(0:300),mean(y),sd(y)),col=&quot;blue&quot;,lwd=2) Figure 3.20: ML example with Normal curve on Log Normal data Figure 3.20 shows that the Normal model does not fit the Log Normal data properly, but this is the best we can get, given our assumptions. The log-likelihood in this case is -5024.897. The much better model would be the Log Normal one: hist(y, main=&quot;&quot;, probability=T, xlim=c(0,300)) lines(c(0:300),dlnorm(c(0:300),mean(log(y)),sd(log(y))),col=&quot;red&quot;,lwd=2) Figure 3.21: ML example with Log Normal curve on Log Normal data The model in Figure 3.21 has the log likelihood of -4885.671. This indicates that the Log Normal model is more appropriate for the data and gives us an idea that it is possible to compare different distributions via the likelihood, finding the better fit to the data. This idea is explored further in the next section. As a final word, when it comes to more complicated models with more parameters and dynamic structure, the specific curves and data become more complicated, but the logic of the likelihood approach stays the same. 3.4.2 Mathematical explanation Now we can discuss the same idea from the mathematical point of view. We use an example of normal distribution and a simple model as before: \\[\\begin{equation} y_t = \\mu_{y} + \\epsilon_t, \\tag{3.32} \\end{equation}\\] where \\(\\mu_{y}\\) is the population location parameter (the true parameter, the global mean). The typical assumption in regression context is that \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\), which means that \\(y_t \\sim \\mathcal{N}(\\mu_{y}, \\sigma^2)\\). We can use this assumption in order to calculate the point likelihood value for each observation based on the PDF of Normal distribution: \\[\\begin{equation} \\mathcal{L} (\\mu_{y}, \\sigma^2 | y_t) = f(y_t | \\mu_{y}, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{\\left(y_t - \\mu_{y,t} \\right)^2}{2 \\sigma^2} \\right). \\tag{3.33} \\end{equation}\\] Very roughly, what the value (3.33) shows is the chance that the specific observation comes from the assumed model with specified parameters (we know that in real world the data does not come from any model, but this interprertation is easier to work with). Note that the likelihood is not the same as probability, because for any continuous random variables the probability for it to be equal to any specific number is equal to zero. However, the idea of likelihood has some similarities with the probability, so we prefer to refer to it as a “chance.” The point likelihood (3.33) is not very helpful on its own, but we can get \\(T\\) values like that, based on our sample of data. We can then summarise it in one number, that would characterise the whole sample, given the assumed distribution, applied model and selected values of parameters: \\[\\begin{equation} \\mathcal{L} (\\boldsymbol{\\theta} | \\mathbf{y}) = \\mathcal{L} (\\mu_{y}, \\sigma^2 | \\mathbf{y}) = \\prod_{t=1}^T f(y_t | \\mu_{y}, \\sigma^2), \\tag{3.34} \\end{equation}\\] where \\(\\boldsymbol{\\theta}\\) is the vector of all parameters in the model (in our example, it is just the two of them). We take the product of likelihoods in (3.34) because we need to get the joint likelihood for all observations and because we can typically assume that the point likelihoods are independent of each other (for example, the value on observation \\(t\\) will not be influenced by the value on \\(t-1\\)). The value (3.34) shows the summary chance that the data comes from the assumed model with specified parameters. Having this value, we can change the values of parameters of the model, getting different value of (3.34) (as we did in the example above). Using an iterative procedure, we can get such estimates of parameters that would maximise the likelihood (3.34), which are called Maximum Likelihood Estimates (MLE) of parameters. However, working with the products in that formula is difficult, so typically we linearise it using natural logarithm, obtaining log-likelihood: \\[\\begin{equation} \\ell (\\boldsymbol{\\theta} | \\mathbf{y}) = \\log \\mathcal{L} (\\boldsymbol{\\theta} | \\mathbf{y}) = -\\frac{T}{2} \\log(2 \\pi \\sigma^2) -\\sum_{t=1}^T \\frac{\\left(y_t - \\mu_{y,t} \\right)^2}{2 \\sigma^2} . \\tag{3.35} \\end{equation}\\] Based on that, we can find some of parameters of the model analytically. For example, we can take derivative of (3.35) with respect to the scale \\(\\hat{\\sigma}^2\\) (which is an estimate of the true parameter \\(\\sigma^2\\)) and equate it to zero in order to find the value that maximises the log-likelihood function in our sample: \\[\\begin{equation} \\frac{d \\ell (\\boldsymbol{\\theta} | \\mathbf{y})}{d \\hat{\\sigma}^2} = -\\frac{T}{2} \\frac{1}{\\hat{\\sigma}^2} + \\frac{1}{2 \\hat{\\sigma}^4}\\sum_{t=1}^T \\left(y_t - \\mu_{y,t} \\right)^2 =0 , \\tag{3.36} \\end{equation}\\] which after multiplication of both sides by \\(2 \\hat{\\sigma}^4\\) leads to: \\[\\begin{equation} T \\hat{\\sigma}^2 = \\sum_{t=1}^T \\left(y_t - \\mu_{y,t} \\right)^2 , \\tag{3.37} \\end{equation}\\] or \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{1}{T}\\sum_{t=1}^T \\left(y_t - \\mu_{y,t} \\right)^2 . \\tag{3.38} \\end{equation}\\] The value (3.38) is in fact a Mean Squared Error (MSE) of the model. If we calculate the value of \\(\\hat{\\sigma}^2\\) using the formula (3.38), we will maximise the likelihood with respect to the scale parameter. In fact, we can insert (3.38) in (3.35) in order to obtain the so called concentrated (or profile) log-likelihood for the normal distribution: \\[\\begin{equation} \\ell^* (\\boldsymbol{\\theta}, \\hat{\\sigma}^2 | \\mathbf{y}) = -\\frac{T}{2}\\left( \\log(2 \\pi e) + \\log \\hat{\\sigma}^2 \\right) . \\tag{3.39} \\end{equation}\\] This function is useful because it simplifies some calculations and also demonstrates the condition, for which the likelihood is maximised: the first part on the right hand side of the formula does not depend on the parameters of the model, it is only the \\(\\log \\hat{\\sigma}^2\\) that does. So, the maximum of the concentrated log-likelihood (3.39) is obtained, when \\(\\hat{\\sigma}^2\\) is minimised, implying the minimisation of MSE, which is the mechanism behind the “Ordinary Least Squares” (OLS) ) estimation method. By doing this, we have just demonstrated that if we assume normality in the model, then the estimates of its parameters obtained via the maximisation of the likelihood coincide with the values obtained from OLS. So, why bother with MLE, when we have OLS? First, the finding above holds for normal distribution only. If we assume a different distribution, we would get different estimates of parameters. In some cases, it might not be possible or reasonable to use OLS, but MLE would be a plausible option (for example, logistic, Poisson and any other non-standard model). Second, the MLE of parameters have good statistical properties: they are consistent and efficient. These properties hold almost universally for many likelihoods under very mild conditions. Note that the MLE of parameters are not necessarily unbiased, but after estimating the model, one can de-bias some of them (for example, calculate the standard deviation of the error via devision of the sum of squared errors by the number of degrees of freedom \\(T-k\\) instead of \\(T\\)). Third, likelihood can be used for the model assessment, even when the standard statistics, such as \\(R^2\\) or F-test are not available. We do not discuss these aspects in this textbook. Finally, it permits the model selection via information criteria. In general, this is not possible to do unless you assume a distribution and maximise the respective likelihood. In some statistical literature, you can notice that information criteria are calculated for the models estimated via OLS, but what the authors of such resources do not tell you is that there is still an assumption of normality behind this (see the link between OLS and MLE of Normal distribution above). Note that the likelihood approach assumes that all parameters of the model are estimated, including location, scale, shape, shift etc of distribution. So typically it has more parameters to estimate than, for example, the OLS does. This is discussed in some detail later in the next section. "],["statisticsNumberOfParameters.html", "3.5 Calculating number of parameters in models", " 3.5 Calculating number of parameters in models When performing model selection and calculating different statistics, it is important to know how many parameters were estimated in the model. While this might seems trivial there are a number of edge cases and wrinkles that are seldom discussed in detail. When it comes to inference based on regression models, the general idea is to calculate the number of all the independent estimated parameters \\(k\\). This typically includes all initial components and all coefficients of the model together with the scale, shape and shift parameters of the assumed distribution (e.g. variance in the Normal distribution). Example 3.1 In a simple regression model: \\(y_t = \\beta_0 + \\beta_1 x_t + \\epsilon_t\\) - assuming Normal distribution for \\(\\epsilon_t\\), using the MLE will result in the estimation of \\(k=3\\): the two parameters of the model (\\(\\beta_0\\) and \\(\\beta_1\\)) and the variance of the error term \\(\\sigma^2\\). If likelihood is not used, then the number of parameters might be different. For example, if we estimate the model via the minimisation of MSE (similar to OLS), then the number of all estimated parameters does not include the variance anymore - it is obtained as a by product of the estimation. This is because the likelihood needs to have all the parameters of distribution in order to be maximised, but with MSE, we just minimise the mean of squared errors, and the variance of the distribution is obtained automatically. While the values of parameters might be the same, the logic is slightly different. Example 3.2 This means that for the same simple linear regression, estimated using OLS, the number of parameters is equal to 2: estimates of \\(\\beta_0\\) and \\(\\beta_1\\). In addition, all the restrictions on the parameters can reduce the number of estimated parameters, when they get to the boundary values. Example 3.3 If we know that the parameter \\(\\beta_1\\) lies between 0 and 1, and in the estimation process it gets to the value of 1 (due to how the optimiser works), it can be considered as a restriction \\(\\beta_1=1\\). So, when estimated via the minimum of MSE with this restriction, this would imply that \\(k=1\\). In general, if a parameter is provided in the model, then it does not count towards the number of all estimated parameters. So, setting \\(b_1=1\\) acts in the same fashion. Finally, if a parameter is just a function of another one, then it does not count towards the \\(k\\) as well. Example 3.4 If we know that in the same simple linear regression \\(\\beta_1 = \\frac{\\beta_0}{\\sigma^2}\\), then the number of all the estimated parameter via the maximum likelihood is 2: \\(\\beta_0\\) and \\(\\sigma^2\\). We will come back to the number of parameters later in this textbook, when we discuss specific models. A final note: typically, the standard maximum likelihood estimators for the scale, shape and shift parameters are biased in small samples and do not coincide with the OLS estimators. For example, in case of Normal distribuiton, OLS estimate of variance has \\(T-k\\) in the denominator, while the likelihood one has just \\(T\\). This needs to be taken into account, when the variance is used in forecasting. "],["assumptions.html", "3.6 Typical assumptions of statistical models", " 3.6 Typical assumptions of statistical models In order for a statistical model to work adequately and not to fail on data, several assumptions about it, when it is applied to the data, should hold. If they do not, then the model might lead to biased or inefficient estimates of parameters and forecasts. Here we briefly discuss the main of them, united in three big groups: Model is correctly specified; Residuals are independent and identicaly distributed (i.i.d.); The explanatory variables are not correlated with anything but the response variable; In Section 17 we also discuss how to diagnose the constructed models and fix the issues in cases, when the assumptions are violated. 3.6.1 Model is correctly specified This implies that: We have not omitted important variables in the model (underfitting the data); We do not have redundant variables in the model (overfitting the data); The necessary transformation of the variables are applied; We do not have outliers in the model. (1): if there are some important variables that we did not include in the model, then the estimates of the parameters might be biased and in some cases quite seriously (e.g. positive sign instead of the negative one). This also means that the point forecasts from the model might be biased as well (systematic under or over forecasting). (2): if there are redundant variables that are not needed in the model, then the estimates of parameters and point forecasts might be unbiased, but inefficient. This implies that the variance of parameters can be lower than needed and the prediction intervals can be narrower than needed. (3): this means that, for example, instead of using a multiplicative model, we apply an additive one. The estimates of parameters and the point forecasts might be biased in this case as well: the model will produce linear trajectory of the forecast, when a non-linear one is needed. (4): in a way, this is similar to (1), the presence of outliers might mean that we have missed some important information, meaning that the estimates of parameters and forecasts would be biased as well. There can be other reasons for outliers as well. For example, we might be using a wrong distributional assumptions. If so, this would imply that the prediction intervals from the model are narrower than needed. 3.6.2 Residuals are i.i.d. There are five assumptions in this group: There is no autocorrelation in the residuals; The residuals are homoscedastic; The expectation of residuals is zero, no matter what; The variable follows the specified distribution; More generally speaking, distribution of residuals does not change over time. (1): we expect that the model captures all the important aspects, so if the residuals are autocorrelated, then something is neglected by the applied model. Typically, this leads to inefficient estimates of parameters and in some cases they can also become biased. As a result, the point forecasts can be less accurate than expected and the prediction intervals might be wrong (wider or narrower than needed). (2): if this is violated, then we say that there is a heteroscedasticity in the model. This means that with a change of variable, the variance of the residuals changes as well. If the model neglects this, then typically the estimates of parameters become inefficient and prediction intervals are wrong: they are wider than needed in some cases and narrower than needed in the other ones. (3): while in sample, this holds automatically in many cases (e.g. when using Least Squares method for regression model estimation), this assumption might be violated in the holdout sample. In this case the point forecasts would be biased, because they typically do not take the non-zero mean of forecast error into account, and the prediction interval might be off as well, because of the wrong estimation of the scale of distribution (e.g. variance is higher than needed). This assumption also implies that the expectation of residuals is zero even conditional on the explanatory variables in the model. If it is not, then this might mean that there is still some important information omitted in the applied model. Note that some models in ADAM framework assume that the expectation of residuals is equal to one instead of zero (e.g. multiplicative error models). The idea of the assumption stays the same, it is only the value that changes. (4): in some cases we are interested in using methods that imply specific distributional assumptions about the model and its residuals. For example, it is assumed in the classical linear model that the error term follows Normal distribution. Estimating this model using MLE with the probability density function of Normal distribution or via minimisation of Mean Squared Error (MSE) would give efficient and consistent estimates of parameters. If the assumption of normality does not hold, then the estimates might be inefficient and in some cases inconsistent. When it comes to forecasting, the main issue in the wrong distributional assumption appears, when prediction intervals are needed: they might rely on a wrong distribution and be narrower or wider than needed. Finally, if we deal with the wrong distribution, then the model selection mechanism might be flawed and would lead to the selection of an inappropriate model. (5): this assumption aligns with (4), but in this specific context implies that all the parameters of distribution stay the same and the shape of distribution does not change. If the former is violated then we might have one of the issues discussed above. If the latter is violated then we might produce biased forecasts and underestimate / overestimate the uncertainty about the future. 3.6.3 The explanatory variables are not correlated with anything but the response variable There are two cases here as well: No multicollinearity; No endogeneity; (1): the effect of multicollinearity implies that the variables included in the model are linearly dependent from each other. In this case, it becomes difficult to distinguish the effect of one variables from the other one. As a result, the estimates of parameters become inefficient and might become biased in some sever cases. In case of forecasting, the effect is not as straight forward, and in some cases might not damage the point forecasts, but can lead to prediction intervals of an incorrect width. It is important to note that this is in a way an assumption about the estimation of the model rather than the model itself: it is unreasonable to assume that explanatory variables are independent - reality is more complicated than we would want it to be, so inevitably some variables will be correlated. The main issue of multicollinearity comes to the difficulties in the model estimation in a sample. If we had all the data in the world, then the issue would not exist. (2): endogeneity applies to the situation, when the dependent variable \\(y_t\\) influences the explanatory variable \\(x_t\\) in the model on the same observation. The relation in this case becomes bi-directional, meaning that the basic model is not appropriate in this situation any more. The parameters and forecasts will typically be biased, and a different estimation method is needed or maybe a different model would need to be constructed in order to fix this. In many cases, in our discussions in this textbook, we assume that all of these assumptions hold. In some of the cases, we will say explicitly, which are violated and what needs to be done in those situations. In Section 17 we will discuss how these assumptions can be checked and how the issues caused by their violation can be fixed. "],["regressionModelBuilding.html", "Chapter 4 Regression model building", " Chapter 4 Regression model building In this Chapter, we discuss more advanced topics related to regression modelling. In a way, this part builds upon elements of Statistical Learning (see, for example, the textbook of Hastie et al., 2009) and focuses on how to select variables for regression model. We start with a fundamental idea of bias-variance trade-off, which lies in the core of many selection methods. We then move to the discussion of information criteria, explaining what they imply, after that - to several existing variable selection approaches, explaining their advantages and limitations. Furthermore, we discuss combination approaches and what they mean in terms of parameters of models. We finish this chapter with an introductory discussion of regularisation techniques (such as LASSO and RIDGE). References "],["modelSelection.html", "4.1 Model selection mechanism", " 4.1 Model selection mechanism There are different ways how to select the most appropriate model for the data. One can use judgment, statistical tests, cross-validation or meta learning. The state of the art one in the field of exponential smoothing relies on the calculation of information criteria and on selection of the model with the lowest value. This approach is discussed in detail in Burnham and Anderson (2004). Here we briefly explain how this approach works and what are its advantages and disadvantages. 4.1.1 Information criteria idea Before we move to the mathematics and well-known formulae, it makes sense to understand what we are trying to do, when we use information criteria. The idea is that we have a pool of model under consideration, and that there is a true model somewhere out there (not necessarily in our pool). This can be presented graphically in the following way: Figure 4.1: An example of a model space This plot 4.1 represents a space of models. There is a true one in the middle, and there are four models under consideration: Model 1, Model 2, Model 3 and Model 4. They might differ in terms of functional form (additive vs. multiplicative), or in terms of included/omitted variables. All models are at some some distance (the grey dashed lines) from the true model in this hypothetic model space: Model 1 is closest while Model 2 is farthest. Models 3 and 4 have similar distances to the truth. In the model selection exercise what we typically want to do is to select the model closest to the true one (Model 1 in our case). This is easy to do when you know the true model: just measure the distances and select the closest one. This can be written very roughly as: \\[\\begin{equation} \\begin{split} d_1 = \\ell^* - \\ell_1 \\\\ d_2 = \\ell^* - \\ell_2 \\\\ d_3 = \\ell^* - \\ell_3 \\\\ d_4 = \\ell^* - \\ell_4 \\end{split} , \\tag{4.1} \\end{equation}\\] where \\(\\ell_j\\) is the position of the \\(j^{th}\\) model and \\(\\ell^*\\) is the position of the true one. One of ways of getting the position of the model is by calculating the log-likelihood (logarithms of likelihood) values for each model, based on the assumed distributions. The likelihood of the true model will always be fixed, so if it is known it just comes to calculating the values for the models 1 - 4, inserting them in the equations in (4.1), and selecting the model that has the lowest distance \\(d_j\\). In reality, however, we never know the true model. We therefore need to find some other way of measuring the distances. The neat thing about the maximum likelihood approach is that the true model has the highest possible likelihood by definition! This means that it is not important to know \\(\\ell^*\\) – it will be the same for all the models. So, we can drop the \\(\\ell^*\\) in the formulae (4.1) and compare the models via their likelihoods \\(\\ell_1, \\ell_2, \\ell_3 \\text{ and } \\ell_4\\) alone: \\[\\begin{equation} \\begin{split} d_1 = - \\ell_1 \\\\ d_2 = - \\ell_2 \\\\ d_3 = - \\ell_3 \\\\ d_4 = - \\ell_4 \\end{split} , \\tag{4.2} \\end{equation}\\] This is a very simple method that allows us to get to the model closest to the true one in the pool. However, we should not forget that we usually work with samples of data instead of the entire population and correspondingly will have only estimates of likelihoods and not the true ones. Inevitably, they will be biased and will need to be corrected. Akaike (1974) showed that the bias can be corrected if the number of parameters in each model is added to the distances (4.2) resulting in the bias corrected formula: \\[\\begin{equation} d_j = k_j - \\ell_j \\tag{4.3}, \\end{equation}\\] where \\(k_j\\) is the number of estimated parameters in model \\(j\\) (this typically includes scale parameters when dealing with Maximum Likelihood Estimates). Akaike (1974) suggests “An Information Criterion” which multiplies both parts of the right-hand side of (4.3) by 2 so that there is a correspondence between the criterion and the well-known likelihood ratio test (Wikipedia, 2020c): \\[\\begin{equation} \\mathrm{AIC}_j = 2 k_j - 2 \\ell_j \\tag{4.4}. \\end{equation}\\] This criterion now more commonly goes by the “Akaike Information Criterion.” Various alternative criteria motivated by similar ideas have been proposed. The following are worth mentioning: AICc (Sugiura, 1978), which is a sample corrected version of the AIC for normal and related distributions, which takes the number of observations into account: \\[\\begin{equation} \\mathrm{AICc}_j = 2 \\frac{T}{T-k_j-1} k_j - 2 \\ell_j \\tag{4.5}, \\end{equation}\\] where \\(T\\) is the sample size. BIC (Schwarz, 1978) (aka “Schwarz criterion”), which is derived from Bayesian statistics: \\[\\begin{equation} \\mathrm{BIC}_j = \\log(T) k_j - 2 \\ell_j \\tag{4.6}. \\end{equation}\\] BICc (McQuarrie, 1999) - the sample-corrected version of BIC, relying on the assumption of normality: \\[\\begin{equation} \\mathrm{BICc}_j = \\frac{T \\log (T)}{T-k_j-1} k_j - 2 \\ell_j \\tag{4.7}. \\end{equation}\\] In general, the use of the sample-corrected versions of the criteria (AICc, BICc) is recommended unless sample size is very large (thousands of observations), in which case the effect of the number of observations on the criteria becomes negligible. The main issue is that corrected versions of information criteria for non-normal distributions need to be derived separately and will differ from (4.5) and (4.7). Still, Burnham and Anderson (2004) recommend using formulae (4.5) and (4.7) in small samples even if the distribution of variables is not normal and the correct formulae are not known. The motivation for this is that the corrected versions still take sample size into account, correcting the sample bias in criteria to some extent. A thing to note is that the approach relies on asymptotic properties of estimators and assumes that the estimation method used in the process guarantees that the likelihood functions of the models are maximised. In fact, it relies on asymptotic behaviour of parameters, so it is not very important whether the maximum of the likelihood in sample is reached or not or whether the final solution is near the maximum. If the sample size changes, the parameters guaranteeing the maximum will change as well so we cannot get the point correctly in sample anyway. However, it is much more important to use an estimation method that will guarantee consistent maximisation of the likelihood. This implies that we might select wrong models in some cases in sample, but that is okay, because if we use the adequate approach for estimation and selection, with the increase of the sample size, we will select the correct model more often than an incorrect one. While the “increase of sample size” might seem as an unrealistic idea in some real life cases, keep in mind that this might mean not just the increase of \\(T\\), but also the increase of the number of series under consideration. So, for example, the approach should select the correct model on average, when you test it on a sample of 10,000 SKUs. Summarising, the idea of model selection via information criteria is to: form a pool of competing models, construct and estimate them, calculate their likelihoods, calculate the information criteria, and finally, select the model that has the lowest value under the information criterion. This approach is relatively fast (in comparison with cross-validation, judgmental selection or meta learning) and has good theory behind it. It can also be shown that for normal distributions selecting time series models on the basis of AIC is asymptotically equivalent to the selection based on leave-one-out cross-validation with MSE. This becomes relatively straightforward, if we recall that typically time series models rely on one step ahead errors \\((e_t = y_t - \\mu_{t|t-1})\\) and that the maximum of the likelihood of Normal distribution gives the same estimates as the minimum of MSE. As for the disadvantages of the approach, as mentioned above, it relies on the in-sample value of the likelihood, based on one step ahead error, and does not guarantee that the selected model will perform well for the holdout for multiple steps ahead. Using the cross-validation or rolling origin for the full horizon could give better results if you suspect that information criteria do not work. Furthermore, any criterion is random on its own, and will change with the sample This means that there is model selection uncertainty and that which model is best might change with new observations. In order to address this issue, combinations of models can be used, which allows mitigating this uncertainty. 4.1.2 Common confusions related to information criteria Similar to the discussion of hypothesis testing, I have decided to collect common mistakes and confusions related to information criteria. Here they are: “AIC relies on Normal distribution.” This is not correct. AIC relies on the value of maximised likelihood function. It will use whatever you provide it, so it all comes to the assumptions you make. Having said that, if you use the sample corrected versions of information criteria, such as AICc or BICc, then the formulae (4.5) and (4.7) are derived for Normal distribution. If you use a different one (not related to Normal, so not Log Normal, Box-Cox Normal, Logit Normal etc), then you would need to derive AICc and BICc for it. Still Burnham and Anderson (2004) argue that even if you do not have the correct formula for your distribution, using (4.5) and (4.7) is better than using the non-corrected versions, because there is at least some correction of the bias caused by sample size. “We have removed outlier from the model, AIC has decreased.” AIC will always decrease if you decrease the sample size and fit the model with the same specification. This is because likelihood function relies on the joint PDF of all observations in sample. If the sample decreases, the likelihood increases. This effect is observed not only in cases, when outliers are removed, but also in case of taking differences of the data. So, when comparing models, make sure that they are constructed on exactly the same data. “We have estimated model with logarithm of response variable, and AIC has decreased” (in comparison with the linear one). AIC is comparable only between models with the same response variable. If you transform the response variable, you inevitably assume a different distribution. For example, taking logarithm and assuming that error term follows normal distribution is equivalent to assuming that the original data follows log-normal distribution. If you want to make information criteria comparable in this case, either estimate the original model with a different distribution or transform AIC for the multiplicative model. “We have used quantile regression, assuming normality and AIC is…” Information criteria only work, when the likelihood with the assumed distribution is maximised, because only then it can be guaranteed that the estimates of parameters will be consistent and efficient. If you assume normality, then you either need to maximise the respective likelihood or minimise MSE - they will give the same solution. If you use quantile regression, then you should use likelihood of Asymmetric Laplace. If you estimate parameters via minimisation of MAE, then Laplace distribution of residuals is a suitable assumption for your model. In the cases when distribution and loss are not connected, the selection mechanism might break and not work as intended. References "],["forecastingProcess.html", "Chapter 5 Forecasts evaluation", " Chapter 5 Forecasts evaluation Forecasts ought to serve a specific purpose. They should not be made “just because” but be useful in the making of a decision. The decision then dictates the kind of forecast that should be made – its form and its time horizon(s). It also dictates how the forecast should be evaluated – a forecast only being as good as the quality of the decisions it enables. Example 5.1 Retailers typically need to order some amount of milk that they will sell over the next week. They do not know how much they will sell so they usually order, hoping to satisfy, let us say, 95% of demand. This situation tells us that the forecasts need to be made a week ahead, they should be cumulative (considering the overal demand during a week before the next order) and that they should focus on an upper bound of a 95% prediction interval. Producing only point forecasts would not be useful in this situation. When you understand how your system works and what sort of forecasts you should produce you can start an evaluation process; measuring the performance of different forecasting models / methods and selecting the most appropriate for your data. There are different ways to measure and compare the performance of models / methods In this chapter, we discuss the most common approaches. "],["errorMeasures.html", "5.1 Measuring accuracy of point forecasts", " 5.1 Measuring accuracy of point forecasts We start with a setting in which we are interested in point forecasts only. In this case we typically start by splitting the available data into train and test sets, apply the models under consideration to the former and produce forecasts on the latter, not showing that part to the models. This is called the “fixed origin” approach: we fix the point in time from which to produce forecasts, we produce them, calculate some sort of error measures and compare the models. There are different error measures that can be used in this case. Which measure ought to be used depends on the specific need. Here we briefly discuss the most important meaures and refer to (Davydenko and Fildes, 2013; Svetunkov, 2019, 2017) for the gory details. The most important error measures are Root Mean Squared Error (RMSE): \\[\\begin{equation} \\mathrm{RMSE} = \\sqrt{\\frac{1}{h} \\sum_{j=1}^h \\left( y_{t+j} - \\hat{y}_{t+j} \\right)^2 }, \\tag{5.1} \\end{equation}\\] and Mean Absolute Error (MAE): \\[\\begin{equation} \\mathrm{MAE} = \\frac{1}{h} \\sum_{j=1}^h \\left| y_{t+j} - \\hat{y}_{t+j} \\right| , \\tag{5.2} \\end{equation}\\] where \\(y_{t+j}\\) is the actual value \\(j\\) steps ahead from the holdout, \\(\\hat{y}_{t+j}\\) is the \\(j\\) steps ahead point forecast (conditional expectation of the model) and \\(h\\) is the forecast horizon. As you see, these error measures aggregate the performance of competing forecasting methods across the forecasting horizon, averaging out the specific performances on each \\(j\\). If this information needs to be retained, then the summation can be dropped to obtain just “SE” and “AE.” It is well-known (see, for example, Kolassa, 2016) that RMSE is minimised by the mean value of a distribution, and MAE is minimised by the median. So, when selecting between the two, you should consider this property. This means, for example, that MAE-based error measures should not be used for the evaluation of models on intermittent demand. The main advantage of these error measures is that they are very simple and have a clear interpretations: they are the “average” distances between the point forecasts and the observed values. They are perfect if you work with only one time series. However, they are not suitable, when you have several time series and want to see the performance of methods across them. This is mainly because they are scale dependent and contain specific units: if you measures sales of apples in pounds, then MAE and RMSE will show the error in pounds. And, as we know, you should not add up pounds of apples with pounds of oranges - the result might not make sense. In order to tackle this issue, different error scaling techniques have been proposed, resulting in a zoo of error measures: MAPE - Mean Absolute Percentage Error: \\[\\begin{equation} \\mathrm{MAPE} = \\frac{1}{h} \\sum_{j=1}^h \\frac{y_{t+j} - \\hat{y}_{t+j}}{y_{t+j}}, \\tag{5.3} \\end{equation}\\] MASE - Mean Absolute Scaled Error (Hyndman and Koehler, 2006): \\[\\begin{equation} \\mathrm{MASE} = \\frac{1}{h} \\sum_{j=1}^h \\frac{|y_{t+j} - \\hat{y}_{t+j}|}{\\bar{\\Delta}_y}, \\tag{5.4} \\end{equation}\\] where \\(\\bar{\\Delta}_y = \\frac{1}{t-1}\\sum_{j=2}^t |\\Delta y_{j}|\\) is the mean absolute value of the first differences \\(\\Delta y_{j}=y_j-y_{j-1}\\) of the in-sample data; rMAE - Relative Mean Absolute Error (Davydenko and Fildes, 2013): \\[\\begin{equation} \\mathrm{rMAE} = \\frac{\\mathrm{MAE}_a}{\\mathrm{MAE}_b}, \\tag{5.5} \\end{equation}\\] where \\(\\mathrm{MAE}_a\\) is the mean absolute error of the model under consideration and \\(\\mathrm{MAE}_b\\) is the MAE of the benchmark model; sMAE - scaled Mean Absolute Error (Petropoulos and Kourentzes, 2015): \\[\\begin{equation} \\mathrm{sMAE} = \\frac{\\mathrm{MAE}}{\\bar{y}}, \\tag{5.6} \\end{equation}\\] where \\(\\bar{y}\\) is the mean of the in-sample data. and others. There is no “best” error measure. All have advantages and disadvantages. For example: MAPE is scale sensitive (if the actual values are measured in thousands of units, the resulting error will be much lower than in the case of hundreds of units) and cannot be estimated on data with zeroes. However, it has a simple interpretation as it shows the percentage error (as the name suggests); MASE avoids the disadvantages of MAPE, but does so at the cost of a simple interpretation due to the division by the first differences of the data (some interpret this as an in-sample one-step-ahead Naïve forecast); rMAE avoids the disadvantages of MAPE, has a simple interpretation (it shows by how much one model is better than the other), but fails, when either \\(\\mathrm{MAE}_a\\) or \\(\\mathrm{MAE}_b\\) for a specific time series is equal to zero; sMAE avoids the disadvantages of MAPE has an interpretation close to it, but breaks down when the data has a trend. When comparing different forecasting methods it can make sense to calculate several of the error measures for comparison. The choice of metric might depend on the specific needs of the forecaster. Here’s a few rules of thumb, however: If you want a robust measure that works consistently, but you do not care about the interpretation, then go with MASE. If you want an interpretation, then either go with rMAE, or sMAE (just keep in mind that if you decide to use rMAE or any other relative measure, you might get attacked by its creator, Andrey Davydenko, who might blame you for stealing his creation, even if you put a reference to his work). You should typically avoid MAPE and other percentage error measures because they are highly influenced by the actual values you have in the holdout. Furthermore, similar to the measures above, there have been proposed RMSE-based scaled and relative error metrics, which would measure the performance of methods in terms of means rather than medians. Here is a brief list of some of them: RMSSE - Root Mean Squared Scaled Error: \\[\\begin{equation} \\mathrm{RMSSE} = \\sqrt{\\frac{1}{h} \\sum_{j=1}^h \\frac{(y_{t+j} - \\hat{y}_{t+j})^2}{\\bar{\\Delta}_y^2}} ; \\tag{5.7} \\end{equation}\\] rRMSE - Relative Root Mean Squared Error: \\[\\begin{equation} \\mathrm{rRMSE} = \\frac{\\mathrm{RMSE}_a}{\\mathrm{RMSE}_b} ; \\tag{5.8} \\end{equation}\\] sRMSE - scaled Root Mean Squared Error: \\[\\begin{equation} \\mathrm{sRMSE} = \\frac{\\mathrm{RMSE}}{\\bar{y}} . \\tag{5.9} \\end{equation}\\] Finally, when aggregating the performance of forecasting methods across several time series, sometimes it makes sense to look at the distribution of errors - this way you will know which of the methods fails seriously and which does a consistently good job. References "],["uncertainty.html", "5.2 Measuring uncertainty", " 5.2 Measuring uncertainty While point forecasts are useful in order to understand what to expect on average, prediction intervals are important in many applications to be able to quantify what to expect in \\(1-\\alpha\\) share of cases. Intervals quantify the uncertainty around the point forecasts and thus the riskiness of the decision. In a way, if you do not have prediction intervals, then you cannot adequately assess the uncertainty about future outcomes. If you cannot say that sales next week will be between 1,000 and 1,200 units with a confidence level of 95% then you cannot say anything useful about future sales because as per the previous discussion point forecasts represent only mean values and typically will not be equal to the actual observations from the holdout sample. Hopefully, all of this explains why the prediction intervals are needed in forecasting. As with point forecasts multiple measures can be used to evaluate prediction intervals. Here are the most popular ones: Coverage, showing the percentage of observations lying inside the interval: \\[\\begin{equation} \\mathrm{coverage} = \\frac{1}{h} \\sum_{j=1}^h \\left( \\mathbb{1}(y_{t+j} &lt; l_{t+j}) \\times \\mathbb{1}(y_{t+j} &gt; u_{t+j}) \\right), \\tag{5.10} \\end{equation}\\] where \\(l_{t+j}\\) is the lower bound and \\(u_{t+j}\\) is the upper bound of the interval and \\(\\mathbb{1}(\\cdot)\\) is the indicator function, returning one, when the condition is true and zero otherwise. Ideally, the coverage should be equal to the confidence level of the interval, but in reality, this can only be observed asymptotically, as the sample size increases due to the inheritted randomness of any sample estimates of parameters; Range, showing the width of the prediction interval: \\[\\begin{equation} \\mathrm{range} = \\frac{1}{h} \\sum_{j=1}^h (u_{t+j} -l_{t+j}); \\tag{5.11} \\end{equation}\\] Mean Interval Score (Gneiting and Raftery, 2007), which shows a combination of the previous two: \\[\\begin{equation} \\begin{aligned} \\mathrm{MIS} = &amp; \\frac{1}{h} \\sum_{j=1}^h \\left( (u_{t+j} -l_{t+j}) + \\frac{2}{\\alpha} (l_{t+j} -y_{t+j}) \\mathbb{1}(y_{t+j} &lt; l_{t+j}) +\\right. \\\\ &amp; \\left. \\frac{2}{\\alpha} (y_{t+j} -u_{t+j}) \\mathbb{1}(y_{t+j} &gt; u_{t+j}) \\right) , \\end{aligned} \\tag{5.12} \\end{equation}\\] where \\(\\alpha\\) is the significance level. If the actual values lie outside of the interval, they get penalised with a ratio of \\(\\frac{2}{\\alpha}\\), proportional to the distance from the interval bound. At the same time the width of the interval positively influences the value of the measure: the wider the interval, the higher the score. The ideal model with \\(\\mathrm{MIS}=0\\) should have all the actual values in the holdout lying on the bounds of the interval and \\(u_{t+j}=l_{t+j}\\), implying that the bounds coincide with each other and that there is no uncertainty about the future (which is not possible in the real life). Pinball loss (Koenker and Bassett, 1978), which measures the accuracy of models in terms of specific quantiles (this is usually applied to different quantiles produced from the model, not just to the lower and upper bounds of 95% interval): \\[\\begin{equation} \\mathrm{pinball} = (1 -\\alpha) \\sum_{y_{t+j} &lt; q_{t+j}, j=1,\\dots,h } |y_{t+j} -q_{t+j}| + \\alpha \\sum_{y_{t+j} \\geq q_{t+j} , j=1,\\dots,h } |y_{t+j} -q_{t+j}|, \\tag{5.13} \\end{equation}\\] where \\(q_{t+j}\\) is the value of the specific quantile of the distribution. What pinball shows, is how well we capture the specific quantile in the data. The lower the value of pinball is, the closer the bound is to the specific quantile of the holdout distribution. If the pinball is equal to zero, then we have done the perfect job in hitting that specific quantile. The main issue with pinball loss is that it is very difficult to assess the quantiles correctly on small samples. For example, in order to get a better idea of how the 0.975 quantile performs, we would need to have at least 40 observations, so that 39 of them would be expected to lie below this bound \\(\\left(\\frac{39}{40} = 0.975\\right)\\). In fact, the quantiles are not always uniquely defined (see, for example, Taylor, 2020), which makes the measurement difficult. Similar to the pinball function, it is possible to propose the expectile-based score, but while it has nice statistical properties (Taylor, 2020), it is more difficult to interpret. Range, MIS and pinball loss are unit-dependent. In order to be able to aggregate them over several time series they need to be scaled (as we did with MAE and RMSE in previous section) either via division by the in-sample mean or in-sample mean absolute differences in order to obtain the scaled counterparts of the measures or via division by the values from the benchmark model in order to obtain the relative one. If you are interested in the overall performance of the model, then MIS provides this information. However, it does not show what specifically happens inside and is difficult to interpret. Coverage and range are easier to interpret but only give information about the specific prediction interval and typically must be traded off against each other (i.e. one can either cover more or have a narrower interval). Academics prefer the pinball for the purposes of uncertainty assessment, as it shows more detailed information about the predictive distribution from each model, but, while it is easier to interpret than MIS, it is still not as straightforward as coverage and range. So, the selection of the measure, again, depends on your specific situation and on the understanding of statistics by decision makers. References "],["rollingOrigin.html", "5.3 Rolling origin", " 5.3 Rolling origin Remark. The text in this section is based on the vignette for the greybox package, written by the author of this textbook. When there is a need to select the most appropriate forecasting model or method for the data, the forecaster usually splits the sample into two parts: in-sample (aka “training set”) and holdout sample (aka out-sample or “test set”). The model is estimated on the in-sample and its forecasting performance evaluated using some error measure on the holdout sample. Using this procedure only once is known as “fixed origin” evaluation. Evaluating a forecast thus, however, might give a misleading impression of its accuracy. If, for example, the time series contains outliers or level shifts a poor model might perform better in fixed origin evaluation than a more appropriate one. An alternative procedure known as “rolling origin” evaluation is much more robust to such issues. In rolling origin evaluation the forecasting origin is repeatedly moved forward and forecasts are produced from each origin (Tashman, 2000). This technique allows obtaining several forecast errors for time series, which gives a better understanding of how the models perform. This can be considered as a time series analogue to cross-validation techniques (Wikipedia, 2020d). Here is a simple graphical representation, courtesy of Nikos Kourentzes. Figure 5.1: Rolling origin illustrated, by Nikos Kourentzes There are different options of how this can be done. 5.3.1 Principles of Rolling origin Figure 5.2 (Svetunkov and Petropoulos, 2018) illustrates the basic idea behind rolling origin. White cells correspond to the in-sample data while the light grey cells correspond to the three-steps-ahead forecasts. The time series in the figure has 25 observations and forecasts are produced from 8 origins starting from origin 15. The model is estimated on the first in-sample set and forecasts are produced. Next, another observation is added to the end of the in-sample set, the test set is advanced and the procedure is repeated. The process stops when there is no more data left. This is a rolling origin with a constant holdout sample size. As a result of this procedure 8 one to three steps ahead forecasts are produced. Based on them we can calculate the preferred error measures and choose the best performing model. Figure 5.2: Rolling origin with constant holdout size Another option for producing forecasts from 8 origins would be to start from origin 17 instead of 15, as shown in Figure 5.3. In this case the procedure continues until origin 22 when the last full set of three-steps-ahead forecasts can be produced produced but then continues with a decreasing forecasting horizon. So the two-steps-ahead forecast is produced from origin 23 and only a one-step-ahead forecast is produced from origin 24. As a result we obtain 8 one-step-ahead forecasts, 7 two-steps-ahead forecasts and 6 three-steps-ahead forecasts. This is a rolling origin with a non-constant holdout sample size, which can be useful with small samples when we don’t have any observations to spare. Figure 5.3: Rolling origin with non-constant holdout size Finally, in both of the cases above we had the increasing in-sample size. However for some research purposes we might need a constant in-sample. Figure 5.4 demonstrates such a setup. In this case, in each iteration we add an observation to the end of the in-sample series and remove one from the beginning (dark grey cells). Figure 5.4: Rolling origin with constant in-sample size 5.3.2 Rolling origin in R The function ro() from greybox package (written by Yves Sagaert and Ivan Svetunkov in 2016 on the way to the International Symposium on Forecasting) implements the rolling origin evaluation for any function you like with a predefined call and returns the desired value. It heavily relies on the two variables: call and value - so it is quite important to understand how to formulate them in order to get the desired results. ro() is a very flexible function but as a result it is not very simple. In this subsection we will see how it work on a couple of examples. We start with a simple example, generating a series from normal distribution: x &lt;- rnorm(100,100,10) We use an ARIMA(0,1,1) model implemented in the stats package: ourCall &lt;- &quot;predict(arima(x=data,order=c(0,1,1)),n.ahead=h)&quot; The call that we specify includes two important elements: data and h. data specifies where the in-sample values are located in the function that we want to use, and it needs to be called “data” in the call. h will tell our function, where the forecasting horizon is specified in the selected function. Note that in this example we use arima(x=data,order=c(0,1,1)), which produces a desired ARIMA(0,1,1) model and then we use predict(..., n.ahead=h), which produces an h steps ahead forecast from that model. Having the call, we need also to specify what the function should return. This can be the conditional mean (point forecasts), prediction intervals, the parameters of a model, or, in fact, anything that the model returns (e.g. name of the fitted model and its likelihood). However, there are some differences in what ro() returns depending on what the function returns. If it is a vector, then ro() will produce a matrix (with values for each origin in columns). If it is a matrix then an array is returned. Finally, if it is a list, then a list of lists is returned. In order not to overcomplicate things, we will collect the conditional mean from the predict() function: ourValue &lt;- c(&quot;pred&quot;) NOTE: If you do not specify the value to return, the function will try to return everything, but it might fail, especially if a lot of values are returned. So, in order to be on the safe side, always provide the value, when possible. Now that we have specified ourCall and ourValue, we can produce forecasts from the model using rolling origin. Let’s say that we want three-steps-ahead forecasts and 8 origins with the default values of all the other parameters: returnedValues1 &lt;- ro(x, h=3, origins=8, call=ourCall, value=ourValue) The function returns a list with all the values that we asked for plus the actual values from the holdout sample. We can calculate some basic error measure based on those values, for example, scaled Mean Absolute Error (Petropoulos and Kourentzes, 2015): apply(abs(returnedValues1$holdout - returnedValues1$pred),1,mean,na.rm=TRUE) / mean(returnedValues1$actuals) ## h1 h2 h3 ## 0.1152795 0.1216502 0.1275300 In this example we use apply() function in order to distinguish between the different forecasting horizons and have an idea of how the model performs for each of them. These numbers do not tell us much on their own, but if we compared the performance of this model with another one, then we could infer if one model is more appropriate for the data than the other one. For example, applying ARIMA(1,1,2) to the same data, we will get: ourCall &lt;- &quot;predict(arima(x=data,order=c(1,1,2)),n.ahead=h)&quot; returnedValues2 &lt;- ro(x, h=3, origins=8, call=ourCall, value=ourValue) ## Warning in log(s2): NaNs produced apply(abs(returnedValues2$holdout - returnedValues2$pred),1,mean,na.rm=TRUE) / mean(returnedValues2$actuals) ## h1 h2 h3 ## 0.1270389 0.1309582 0.1314569 Comparing these errors with the ones from the previous model, we can conclude, which of the approaches is more adequate for the data. We can also plot the forecasts from the rolling origin, which shows how the selected model behaves: par(mfcol=c(2,1)) plot(returnedValues1) plot(returnedValues2) In this example the forecasts from different origins are close to each other. This is because the data is stationary and the model is quite stable. The rolling origin function from the greybox package also allows working with explanatory variables and returning prediction intervals if needed. Some further examples are discussed in the vignette of the package: vignette(\"ro\",\"greybox\"). References "],["tsDecomposition.html", "Chapter 6 From time series components to ETS", " Chapter 6 From time series components to ETS Before we turn to state space models, ETS, ARIMA and other things we need to discuss time series decomposition and the ETS taxonomy. These topics lie at the heart of ETS models and are essential for the understanding of the further material. In this chapter we start with a discussion of time series components, then move to the idea of decomposing time series into distinct components and then move to the conventional ETS taxonomy, as formulated by Hyndman et al. (2008), demonstrating its connection with the previous topics. References "],["tsComponents.html", "6.1 Time series components", " 6.1 Time series components The main idea behind many forecasting techniques is that any time series can contain several unobservable components, such as: Level of the series - the average value for specific period of time, Growth of the series - the average increase or decrease of the value over a period of time, Seasonality - a pattern that repeats itself with a fixed periodicity. This pattern need not literally be seasonal, like beer sales being higher in summer than they are in winter (season of year). Any pattern with a fixed periodicity works: the number of hospital visitors is higher on Mondays than on Saturdaya or Sundays because people tend to stay at home over the weekend (day of week seasonality), and sales are higher during daytime than they are at night (hour of the day seasonality). Error - unexplainable white noise. Each textbook and paper will use slightly different names to refer to these components. For example, in classical decomposition (Warren M. Persons, 1919) it is assumed that (1) and (2) jointly represent a “trend” component so a model will contain error, trend and seasonality. There are modifications of this, which also contain cyclical component(s). When it comes to ETS, the growth component (2) is called “trend,” so the model consists of the four components. We will use the ETS formulation in this textbook. According to this formulation the components can interact with each other in one of two ways: additively or multiplicatively. The pure additive model in this case can be summarised as: \\[\\begin{equation} y_t = l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t , \\tag{6.1} \\end{equation}\\] where \\(l_{t-1}\\) is the level, \\(b_{t-1}\\) is the trend, \\(s_{t-m}\\) is the seasonal component with periodicity \\(m\\) (e.g. 12 for months of year data, implying that something is repeated every 12 months) - all these components are produced on the previous observations and are used on the current one. Finally, \\(\\epsilon_t\\) is the error term, which follows some distribution and has zero mean. Similarly, the pure multiplicative model is: \\[\\begin{equation} y_t = l_{t-1} b_{t-1} s_{t-m} \\varepsilon_t , \\tag{6.2} \\end{equation}\\] where \\(\\varepsilon_t\\) is the error term that has mean of one. The interpretation of the model (6.1) is that the different components add up to each other, so, for example, the sales in January typically increase by the amount \\(s_{t-m}\\), and that there is still some randomness that is not taken into account in the model. The pure additive models can be applied to data that can have positive, negative and zero values. In case of the model (6.2), the interpretation is similar, but the sales change by \\((s_{t-m}-1)\\)% from the baseline. These models only work on data with strictly positive values (data with purely negative values are also possible but rare in practice). It is also possible to define mixed models in which, for example, the trend is additive but the other components are multiplicative: \\[\\begin{equation} y_t = (l_{t-1} + b_{t-1}) s_{t-m} \\varepsilon_t \\tag{6.3} \\end{equation}\\] These models work well in practice when the data has large values far from zero. In other cases, however, they might produce strange results (e.g. negative values on positive data) so the conventional decomposition techniques only consider the pure models. References "],["ClassicalDecomposition.html", "6.2 Classical Seasonal Decomposition", " 6.2 Classical Seasonal Decomposition 6.2.1 How to do? One of the classical textbook methods for decomposing the time series into unobservable components is called “Classical Seasonal Decomposition” (Warren M. Persons, 1919). It assumes either a pure additive or pure multiplicative model, is done using centred moving averages and is focused on approximation, not on forecasting. The idea of the method can be summarised in the following steps: Decide, which of the models to use based on the type of seasonality in the data: additive (6.1) or multiplicative (6.2) Smooth the data using a centred moving average (CMA) of order equal to the periodicity of the data \\(m\\). If \\(m\\) is the an number then the formula is: \\[\\begin{equation} d_t = \\frac{1}{m}\\sum_{i=-(m-1)/2}^{(m-1)/2} y_{t+i}, \\tag{6.4} \\end{equation}\\] which means that, for example, the value on Thursday is the average of values from Monday to Sunday. If \\(m\\) is an even number then a different weighting scheme is typically used, involving the inclusion of additional an value: \\[\\begin{equation} d_t = \\frac{1}{m}\\left(\\frac{1}{2}\\left(y_{t+(m-1)/2}+y_{t-(m-1)/2}\\right) + \\sum_{i=-(m-2)/2}^{(m-2)/2} y_{t+i}\\right), \\tag{6.5} \\end{equation}\\] which means that we use half of the December of the previous year and half of the December of the current year in order to calculate the centred moving average in June. The values \\(d_t\\) are placed in the middle of the window going through the series (e.g. on Thursday the average will contain values from Monday to Sunday). The resulting series is deseasonalised. When we average e.g. sales in a year we automatically remove the potential seasonality, which can be observed individually in each month. A drawback of using CMA is that we inevitably lose \\(\\frac{m}{2}\\) observations at the beginning and the end of the series. In R, the ma() function from the forecast package implements CMA. De-trend the data: For the additive decomposition this is done using: \\({y^\\prime}_t = y_t - d_t\\); For the multiplicative decomposition, it is: \\({y^\\prime}_t = \\frac{y_t}{d_t}\\); If the data is seasonal, then the average value for each period is calculated based on the de-trended series. e.g. we produce average seasonal indices for each January, February, etc. This will give us the set of seasonal indices \\(s_t\\); Calculate the residuals based on what you assume in the model: additive seasonality: \\(e_t = y_t - d_t - s_t\\); multiplicative seasonality: \\(e_t = \\frac{y_t}{d_t s_t}\\); no seasonality: \\(e_t = {y^\\prime}_t\\). Note that the functions in R typically allow you to select between additive and multiplicative seasonality. There is no option for “none” and so even if the data is not seasonal you will nonetheless get values for \\(s_t\\) in the output. Also, notice that the classical decomposition assumes that there is a deseasonalised series \\(d_t\\) but does not make any further split of this variable into level \\(l_t\\) and trend \\(b_t\\). 6.2.2 A couple of examples An example of the classical decomposition in R is the decompose() function from stats package. Here is an example with pure multiplicative model and AirPassengers data: ourDecomposition &lt;- decompose(AirPassengers, type=&quot;multiplicative&quot;) plot(ourDecomposition) We can see that the function has smoothed the original series and produced the seasonal indices. Note that the trend component has gaps at the beginning and at the end. This is because the method relies on CMA (see above). Note also that the error term still contains some seasonal elements, which is a downside of such a simple decomposition procedure. However, the lack of precision in this method is compensated by the simplicity and speed of calculation. Note again that the trend component in decompose() function is in fact \\(d_t = l_{t}+b_{t}\\). Here is an example of decomposition of the non-seasonal data (we assume pure additive model in this example): y &lt;- ts(c(1:100)+rnorm(100,0,10),frequency=12) ourDecomposition &lt;- decompose(y, type=&quot;additive&quot;) plot(ourDecomposition) As you can see, the original data is not seasonal but the decomposition assumes that it is and proceeds with the default approach returning a seasonal component. You get what you ask for. 6.2.3 Other techniques There are other techniques that decompose series into error, trend and seasonal components but make different assumptions about each component. The general procedure, however, always remains the same: (1) smooth the original series, (2) extract the seasonal components, (3) smooth them out. The methods differ in the smoother they use (LOESS, e.g., uses a bisquare function instead of CMA) and in some cases multiple rounds of smoothing are performed to make sure that the components are split correctly. There are many functions in R that implement seasonal decomposition. Here is a small selection: decomp() from the tsutils package does classical decomposition and fills in the tail and head of the smoothed trend with forecasts from exponential smoothing; stl() from the stats package uses a different approach - seasonal decomposition via LOESS. It is an iterative algorithm that smoothes the states and allows them to evolve over time. So, for example, the seasonal component in STL can change; mstl() from the forecast package does the STL for data with several seasonalities; msdecompose() from the smooth package does a classical decomposition for multiple seasonal series. 6.2.4 “Why bother?” “Why decompose?” you may wonder at this point. Understanding the idea behind decompositions and how to perform them helps in understanding ETS, which relies on it. From a practical point of view it can be useful if you want to see if there is a trend in the data and whether the residuals contain outliers or not. It will not show you if the data is seasonal as the seasonality is assumed in the decomposition (I stress this because many students think otherwise). Additionally, when seasonality cannot be added to the model under consideration decomposing the series, predicting the trend and then reseasonalising can be a viable solution. Finally, the values from the decomposition can be used as starting points for the estimation of components in ETS or other dynamic models relying on the error-trend-seasonality. References "],["ETSTaxonomy.html", "6.3 ETS taxonomy", " 6.3 ETS taxonomy Building on the idea of time series components we can move to the ETS taxonomy. ETS stands for “Error-Trend-Seasonality” and defines how specifically the components interact with each other. Based on the type of error, trend and seasonality, (Pegels, 1969) proposed a taxonomy, which was then developed further by (Hyndman et al., 2002) and refined by (Hyndman et al., 2008). According to this taxonomy, error, trend and seasonality can be: Error: either “Additive” (A), or “Multiplicative” (M); Trend: either “None” (N), or “Additive” (A), or “Additive damped” (Ad), or “Multiplicative” (M), or “Multiplicative damped” (Md); Seasonality: either “None” (N), or “Additive” (A), or “Multiplicative” (M). According to this taxonomy, the model (6.1) is denoted as ETS(A,A,A) while the model (6.2) is denoted as ETS(M,M,M), and (6.3) is ETS(M,A,M). The main advantages of the ETS taxonomy are that the components have clear interpretations and that it is flexible, allowing to have 30 models with different types of error, trend and seasonality. The figure below shows examples of different time series with deterministic (they do not change over time) level, trend and seasonality, based on how they interact in the model. The first one shows the additive error case: Figure 6.1: Time series corresponding to the additive error ETS models Things to note from this plot: When the seasonality is multiplicative its amplitude increases with the level of the data while with additive seasonality the amplitude is constant. Compare, e.g., ETS(A,A,A) with ETS(A,A,M): the distance between the highest and the lowest points for the former in the first year is roughly the same as in the last year. In the case of ETS(A,A,M) the distance increases with the increase in the level. When the trend is multiplicative data with exponential growth / decay result. With ETS(A,M,N), for example, we say that there is roughly 5% growth in the data; The damped trend models slow down both additive and multiplicative trends; It is practically impossible to distinguish additive and multiplicative seasonality if the series does not trend because what distinguishes the two – see (1) – is not relevant (compare ETS(A,N,A) and ETS(A,N,M)). Here is a similar plot for the multiplicative error models: Figure 6.2: Time series corresponding to the multiplicative error ETS models They show roughly the same picture as the additive case, the main difference being that the variance of the error increases with the increase of the level of the data - this becomes clearer on ETS(M,A,N) and ETS(M,M,N) data. This property is called heteroscedasticity in statistics and (Hyndman et al., 2008) argue that the main benefit of the multiplicative error models is being able to capture this feature. In the next chapters we will discuss the most important members of the ETS taxonomy. Not all the models in this taxonomy are particularly sensible and some are typically ignored entirely. Although ADAM implements the entire taxonomy we will discuss potential issues and what to expect from them. References "],["ETSTaxonomyMaths.html", "6.4 Mathematical models in the ETS taxonomy", " 6.4 Mathematical models in the ETS taxonomy I hope that it becomes clearer to the reader how the ETS framework is built upon the idea of time series decomposition. By introducing different components and defining their types and by adding the equations for their update, we can construct models that would work better on the time series at hands. The equations discussed in the previous section represent so called “measurement” or “observation” equations of the ETS models. But we should also take into account the potential change in components over time. The “transition” or “state” equation is supposed to reflect this change: they explain, how the level, trend or seasonal components change over time. As discussed in the previous section, given different types of components and their interactions, we end up with 30 models in the taxonomy. Tables 6.1 and 6.2 summarise mathematically all 30 ETS models shown graphically on Figures 6.1 and 6.2 in the ETS Taxonomy chapter, presenting formulae for: Measurement equation; Transition equation; Conditional one step ahead expectation \\(\\mu_{y,t} = \\mu_{y,t|t-1}\\); Multiple steps ahead point forecast \\(\\hat{y}_{t+h}\\); Conditional multiple steps ahead expectation \\(\\mu_{y,t+h|t}\\); In case of the additive error models, the point forecasts correspond to the expectations only when the expectation of the error term is zero, \\(\\text{E}(\\epsilon_t)=0\\), while in case of the multiplicative one the condition is typically that \\(\\text{E}(1+\\epsilon_t)=1\\). Remark. However, note that not all the point forecasts correspond to the conditional expectations. This issue applies to the models with multiplicative trend and / or multiplicative seasonality. This is because SSOE models assume that different states are correlated (they have the same source of error) and as a result multiple steps ahead values (when h&gt;1) of states introduce products of error terms. So, the conditional expectations in these cases might not have analytical forms, and when working with these models, simulations might be required. This does not apply to the one step ahead forecasts, for which all the classical formulae work. Table 6.1: Additive error ETS models Nonseasonal Additive seasonality Multiplicative seasonality No trend \\(\\begin{split} &amp;y_{t} = l_{t-1} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\alpha \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} \\\\ &amp;\\hat{y}_{t+h} = l_{t} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = l_{t-1} + s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\alpha \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = l_{t-1} s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1}} \\\\ &amp;\\mu_{y,t} = l_{t-1} s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m \\end{split}\\) Additive trend \\(\\begin{split} &amp;y_{t} = l_{t-1} + b_{t-1} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\ &amp;b_t = b_{t-1} + \\beta \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} + b_{t-1} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + h b_t \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\ &amp;b_t = b_{t-1} + \\beta \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} + b_{t-1} + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + h b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = (l_{t-1} + b_{t-1}) s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + b_{t-1}} \\\\ &amp;\\mu_{y,t} = (l_{t-1} + b_{t-1}) s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = \\left(l_{t} + h b_{t-1}\\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m \\end{split}\\) Additive damped trend \\(\\begin{split} &amp;y_{t} = l_{t-1} + \\phi b_{t-1} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\ &amp;b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_t \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\ &amp;b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;b_t = \\phi b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + \\phi b_{t-1}} \\\\ &amp;\\mu_{y,t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = \\left(l_{t} + \\sum_{j=1}^h \\phi^j b_t \\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m \\end{split}\\) Multiplicative trend \\(\\begin{split} &amp;y_{t} = l_{t-1} b_{t-1} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\ &amp;b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\ &amp;\\mu_{y,t} = l_{t-1} b_{t-1} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_t^h \\\\ &amp;\\mu_{y,t+h|t} \\text{ - no closed form for} h&gt;1 \\end{split}\\) \\(\\begin{split} &amp;y_{t} = l_{t-1} b_{t-1} + s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\ &amp;b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} b_{t-1} + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_{t-1}^h + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} \\text{ - no closed form for} h&gt;1 \\end{split}\\) \\(\\begin{split} &amp;y_{t} = l_{t-1} b_{t-1} s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}} \\\\ &amp;\\mu_{y,t} = l_{t-1} b_{t-1} s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_{t-1}^h s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} \\text{ - no closed form for} h&gt;1 \\end{split}\\) Multiplicative damped trend \\(\\begin{split} &amp;y_{t} = l_{t-1} b_{t-1}^\\phi + \\epsilon_t \\\\ &amp;l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\ &amp;b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\ &amp;\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_t^{\\sum_{j=1}^h \\phi^j} \\\\ &amp;\\mu_{y,t+h|t} \\text{ - no closed form for} h&gt;1 \\end{split}\\) \\(\\begin{split} &amp;y_{t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\ &amp;b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} \\text{ - no closed form for} h&gt;1 \\end{split}\\) \\(\\begin{split} &amp;y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}} \\\\ &amp;\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} \\text{ - no closed form for} h&gt;1 \\end{split}\\) The multiplicative error models have the same one step ahead expectations as the additive error ones, but due to the multiplication by the error term, the multiple steps ahead conditional expectations between the two models might differ, specifically for the multiplicative trend and multiplicative seasonal models. Table 6.2: Multiplicative error ETS models Nonseasonal Additive seasonality Multiplicative seasonality No trend \\(\\begin{split} &amp;y_{t} = l_{t-1}(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\ &amp;\\mu_{y,t} = l_{t-1} \\\\ &amp;\\hat{y}_{t+h} = l_{t} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = (l_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = l_{t-1} s_{t-m}(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\ &amp;s_t = s_{t-m}(1 + \\gamma \\epsilon_t) \\\\ &amp;\\mu_{y,t} = l_{t-1} s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m \\end{split}\\) Additive trend \\(\\begin{split} &amp;y_{t} = (l_{t-1} + b_{t-1})(1 + \\epsilon_t) \\\\ &amp;l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} + b_{t-1} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + h b_t \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = (l_{t-1} + b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} + b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &amp;b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} + b_{t-1} + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + h b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = (l_{t-1} + b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\ &amp;l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = b_{t-1} + \\beta (l_{t-1} + b_{t-1}) \\epsilon_t \\\\ &amp;s_t = s_{t-m} (1 + \\gamma \\epsilon_t) \\\\ &amp;\\mu_{y,t} = (l_{t-1} + b_{t-1}) s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = \\left(l_{t} + h b_{t-1}\\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m \\end{split}\\) Additive damped trend \\(\\begin{split} &amp;y_{t} = (l_{t-1} + \\phi b_{t-1})(1 + \\epsilon_t) \\\\ &amp;l_t = (l_{t-1} + \\phi b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_t \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = (l_{t-1} + \\phi b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &amp;b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} + \\phi b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = \\phi b_{t-1} + \\beta (l_{t-1} + \\phi b_{t-1}) \\epsilon_t \\\\ &amp;s_t = s_{t-m}(1 + \\gamma \\epsilon_t) \\\\ &amp;\\mu_{y,t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = \\left(l_{t} + \\sum_{j=1}^h \\phi^j b_t \\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m \\end{split}\\) Multiplicative trend \\(\\begin{split} &amp;y_{t} = l_{t-1} b_{t-1} (1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\\\ &amp;\\mu_{y,t} = l_{t-1} b_{t-1} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_t^h \\\\ &amp;\\mu_{y,t+h|t} \\text{ - no closed form} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = (l_{t-1} b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &amp;b_t = b_{t-1} + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} b_{t-1} + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_{t-1}^h + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} \\text{ - no closed form} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = l_{t-1} b_{t-1} s_{t-m} (1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\\\ &amp;s_t = s_{t-m} (1 + \\gamma \\epsilon_t) \\\\ &amp;\\mu_{y,t} = l_{t-1} b_{t-1} s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_{t-1}^h s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} \\text{ - no closed form} \\end{split}\\) Multiplicative damped trend \\(\\begin{split} &amp;y_{t} = l_{t-1} b_{t-1}^\\phi (1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} b_{t-1}^\\phi (1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = b_{t-1}^\\phi (1 + \\beta \\epsilon_t) \\\\ &amp;\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_t^{\\sum_{j=1}^h \\phi^j} \\\\ &amp;\\mu_{y,t+h|t} \\text{ - no closed form} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = (l_{t-1} b_{t-1}^\\phi + s_{t-m})(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &amp;b_t = b_{t-1}^\\phi + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\\\ &amp;\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} \\text{ - no closed form} \\end{split}\\) \\(\\begin{split} &amp;y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} (1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} b_{t-1}^\\phi \\left(1 + \\alpha \\frac{\\epsilon_t}{s_{t-m}}\\right) \\\\ &amp;b_t = b_{t-1}^\\phi \\left(1 + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}}\\right) \\\\ &amp;s_t = s_{t-m} \\left(1 + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}}\\right) \\\\ &amp;\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} \\text{ - no closed form} \\end{split}\\) The formulae summarised above explain the models underlying potential data, but when it comes to their construction and estimation, the \\(\\epsilon_t\\) is substituted by the estimated \\(e_t\\) (which is calculated differently depending on the error type), and time series components and smoothing parameters are also substituted by their estimated analogues (e.g. \\(\\hat{\\alpha}\\) instead of \\(\\alpha\\)). Although there are 30 potential ETS models, not all of them are stable So, Rob Hyndman has reduced the pool of models under consideration in the ets() function of forecast package to the following 19: ANN, AAN, AAdN, ANA, AAA, AAdA, MNN, MAN, MAdN, MNA, MAA, MAdA, MNM, MAM, MAdM, MMN, MMdN, MMM, MMdM. In addition, the multiplicative trend models are difficult and are unstable in cases of data with outliers, so they are switched off in the ets() function by default, which reduces the pool of models further to the first 15. "],["conventional-exponential-smoothing.html", "Chapter 7 Conventional Exponential Smoothing", " Chapter 7 Conventional Exponential Smoothing In this chapter, we discuss the most popular exponential smoothing methods and their connection with the ETS model. We do not go into many details of how the methods were originally derived and how to work with them. Instead, we focus on their connection with ETS and then on the main ideas behind the conventional ETS. The reader interested in the topic of the history of exponential smoothing, how it was developed and what papers contributed towards the development of the field, can refer to the reviews of (Gardner, 1985) and (Gardner, 2006). They summmarise all the progress in the area of exponential smoothing up until 1985 and then until 2006. References "],["SES.html", "7.1 Simple Exponential Smoothing", " 7.1 Simple Exponential Smoothing We start our discussion of exponential smoothing with the original Simple Exponential Smoothing (SES) forecasting method, which was formulated by (Brown, 1956): \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{\\alpha} {y}_{t} + (1 - \\hat{\\alpha}) \\hat{y}_{t}, \\tag{7.1} \\end{equation}\\] where \\(\\hat{\\alpha}\\) is the smoothing parameter, defined by analyst and which is typically restricted with (0, 1) region (this region is actually arbitrary and we will see later what is the correct one). This is one of the simplest forecasting methods, and the smoothing parameter in it is typically interpretted as a weight between the actual value and the one-step-ahead predicted one. If the smoothing parameter is close to zero, then more weight is given to the previous fitted value \\(\\hat{y}_{t}\\) and the new information is neglected. When it is close to one, then mainly the actual value \\({y}_{t}\\) is taken into account. By changing the smoothing parameter value, the forecaster can decide how to approximate the data and filter out the noise. Also, notice that this is a recursive method, meaning that there needs to be some starting point \\(\\hat{y}_1\\) in order to apply (7.1) to the existing data. Different initialisation and estimation methods for SES have been discussed in the literature, but the sttate of the art one is to estimate \\(\\hat{\\alpha}\\) and \\(\\hat{y}_{1}\\) together by minimising some loss function. Typically MSE is used as one, minimising the one step ahead forecast error. 7.1.1 Examples of application Here is an example of how this method works on different time series. We start with generating a stationary series and using es() function from smooth package. Although it implements the ETS model, we will see later the connection between SES and ETS(A,N,N). We start with the stationary time series and \\(\\hat{\\alpha}=0\\): y &lt;- rnorm(100,100,10) ourModel &lt;- es(y, model=&quot;ANN&quot;, h=10, persistence=0) plot(ourModel, 7, main=paste0(&quot;SES with alpha=&quot;,ourModel$persistence)) The SES works well in this case, capturing the deterministic level of the series and filtering out the noise. In this case, it works like a global average applied to the data. As mentioned before, the method is flexible, so if we have a level shift in the data and increase the smoothing parameter, it will adapt and get to the new level. Here is an example: y &lt;- c(rnorm(50,100,10),rnorm(50,130,10)) ourModel &lt;- es(y, model=&quot;ANN&quot;, h=10, persistence=0.1) plot(ourModel, 7, main=paste0(&quot;SES with alpha=&quot;,ourModel$persistence)) With \\(\\hat{\\alpha}=0.1\\), it manages to get to the new level, but now the method starts adapting to noise a little bit - it follows the peaks and troughs and repeats them, but with much smaller magnitude. If we increase the smoothing parameter, it will react to the changes much faster, but it will also react more to noise: If we set \\(\\hat{\\alpha}=1\\), we will end up with Naive forecasting method, which is not appropriate for our example: So, when working with SES, we need to make sure that the reasonable smoothing parameter is selected. This can be done automatically via minimising the MSE: ourModel &lt;- es(y, model=&quot;ANN&quot;, h=10, loss=&quot;MSE&quot;) plot(ourModel, 7, main=paste0(&quot;SES with alpha=&quot;,round(ourModel$persistence,3))) This approach won’t guarantee that we will get the most appropriate \\(\\hat{\\alpha}\\), but it has been shown in the literature that the optimisation of smoothing parameter on average leads to improvements in terms of forecasting. 7.1.2 Why “exponential?” Now, why is it called “exponential”? Because the same method can be represented in a different form, if we substitute \\(\\hat{y}_{t}\\) in right hand side (7.1) by the formula for the previous step: \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{\\alpha} {y}_{t} + (1 -\\hat{\\alpha}) \\left( \\hat{\\alpha} {y}_{t-1} + (1 -\\hat{\\alpha}) \\hat{y}_{t-1} \\right). \\tag{7.2} \\end{equation}\\] By repeating this procedure for each \\(\\hat{y}_{t-1}\\), \\(\\hat{y}_{t-2}\\) etc, we will obtain a different form of the method: \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{\\alpha} {y}_{t} + \\hat{\\alpha} (1 -\\hat{\\alpha}) {y}_{t-1} + \\hat{\\alpha} (1 -\\hat{\\alpha})^2 {y}_{t-2} + \\dots + (1 -\\hat{\\alpha})^t \\hat{y}_1 \\tag{7.3} \\end{equation}\\] or equivalently: \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{\\alpha} \\sum_{j=0}^{t-1} (1 -\\hat{\\alpha})^j {y}_{t-j} + (1 -\\hat{\\alpha})^t \\hat{y}_1 . \\tag{7.4} \\end{equation}\\] Now each actual observation has a weight infront of it. For the most recent observation it is \\(\\hat{\\alpha}\\), for the previous one it is \\(\\hat{\\alpha} (1 -\\hat{\\alpha})\\), then \\(\\hat{\\alpha} (1 -\\hat{\\alpha})^2\\) etc. These form the geometric series or an exponential curve. Here, for example, how it looks, when \\(\\hat{\\alpha} =0.25\\) for a sample of 30 observations: plot(0.25*(1-0.25)^c(0:30), type=&quot;b&quot;, xlab=&quot;Time lags&quot;, ylab=&quot;Weights&quot;) This explains the name “exponential.” The term “smoothing” comes from the idea that the parameter \\(\\hat{\\alpha}\\) should be selected so that the method smoothes the original time series. 7.1.3 Error correction form of SES Finally, an alternative form of SES is known as error correction form and involves some simple permutations, taking that \\(e_t=y_t-\\hat{y}_t\\) is the one step ahead forecast error: \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{y}_{t} + \\hat{\\alpha} e_{t}. \\tag{7.5} \\end{equation}\\] In this form, the smoothing parameter \\(\\hat{\\alpha}\\) regulates how much the model reacts to the forecast error. In this interpretation it no longer needs to be restricted with (0, 1) region, but we would still typically want it to be closer to zero, in order to filter out the noise, not to adapt to it. As you see, this is a very simple method. It is easy to explain it to practitioners and it is very easy to implement in practice. However, this is just a forecasting method, so it just gives a way of generating point forecasts, but does not explain where the error comes from and how to generate prediction intervals. References "],["SESandETS.html", "7.2 SES and ETS", " 7.2 SES and ETS 7.2.1 ETS(A,N,N) There have been several tries to develop statistical models, underlying SES, and we know now that it has underlying ARIMA(0,1,1), local level MSOE (Multiple Source of Error) model (Muth, 1960) and SSOE (Single Source of Error) model (Snyder, 1985). According to (Hyndman et al., 2002), the ETS(A,N,N) model also underlies the SES method. It can be formulated in the following way, as discussed earlier: \\[\\begin{equation} \\begin{split} y_{t} &amp;= l_{t-1} + \\epsilon_t \\\\ l_t &amp;= l_{t-1} + \\alpha \\epsilon_t \\end{split} , \\tag{7.6} \\end{equation}\\] where, as we know from the previous section, \\(l_t\\) is the level of the data, \\(\\epsilon_t\\) is the error term and \\(\\alpha\\) is the smoothing parameter. Note that we use \\(\\alpha\\) without the “hat” symbol, which implies that there is a “true” value of the parameter (which could be obtained if we had all the data in the world or just knew it for some reason). It is easy to show that ETS(A,N,N) underlies SES. In order to see this, we need to take move towards estimation phase and use \\(\\hat{l}_{t-1}=l_{t-1}\\) and move to estimates \\(\\hat{\\alpha}\\) and \\(e_t\\) (the estimate of the error term \\(\\epsilon_t\\)): \\[\\begin{equation} \\begin{split} y_{t} &amp;= \\hat{l}_{t-1} + e_t \\\\ \\hat{l}_t &amp;= \\hat{l}_{t-1} + \\hat{\\alpha} e_t \\end{split} , \\tag{7.7} \\end{equation}\\] and also take that \\(\\hat{y}_t=\\hat{l}_{t-1}\\): \\[\\begin{equation} \\begin{split} y_{t} &amp;= \\hat{y}_{t} + e_t \\\\ \\hat{y}_{t} &amp;= \\hat{y}_{t-1} + \\hat{\\alpha} e_{t-1} \\end{split} . \\tag{7.8} \\end{equation}\\] Inserting the second equation in the first one and substituting \\(y_t\\) with \\(\\hat{y}_t+e_t\\) we get: \\[\\begin{equation} \\hat{y}_t+e_t = \\hat{y}_{t-1} + \\hat{\\alpha} e_{t-1} + e_t , \\tag{7.9} \\end{equation}\\] cancelling out \\(e_t\\) and shifting everything by one step ahead, we obtain the error correction form (7.5) of SES. But now, the main benefit of having the model (7.6) instead of just the method (7.5) is in having a flexible framework, which allows adding other components, selecting the most appropriate ones, estimating parameters in a consistent way, producing prediction intervals etc. In order to see the data that corresponds to the ETS(A,N,N) we can use sim.es() function from smooth package. Here are several examples with different smoothing parameters: y &lt;- vector(&quot;list&quot;,6) initial &lt;- 1000 meanValue &lt;- 0 sdValue &lt;- 20 alphas &lt;- c(0.1,0.3,0.5,0.75,1,1.5) for(i in 1:length(alphas)){ y[[i]] &lt;- sim.es(&quot;ANN&quot;, 120, 1, 12, persistence=alphas[i], initial=initial, mean=meanValue, sd=sdValue) } par(mfcol=c(3,2)) for(i in 1:6){ plot(y[[i]], main=paste0(&quot;alpha=&quot;,y[[i]]$persistence), ylim=initial+c(-500,500)) } This simple simulation shows that the higher \\(\\alpha\\) is, the higher variability is in the data and less predictable the data becomes. This is related with the higher values of \\(\\alpha\\), the level changes faster, also leading to the increased uncertainty about the future values of the level in the data. When it comes to the application of this model to the data, the point forecast corresponds to the conditional h steps ahead mean and is equal to the last observed level: \\[\\begin{equation} \\mu_{y,t+h|t} = \\hat{y}_{t+h} = l_{t} , \\tag{7.10} \\end{equation}\\] this holds because it is assumed that \\(\\text{E}(\\epsilon_t)=0\\), which implies that the conditional h steps ahead expectation of the level in the model is \\(\\text{E}(l_{t+h}|t)=l_t+\\alpha\\sum_{j=1}^{h-1}\\epsilon_{t+j} = l_t\\). Here is an example with automatic parameter estimation in ETS(A,N,N) using es() function from smooth package: y &lt;- sim.es(&quot;ANN&quot;, 120, 1, 12, persistence=0.3, initial=1000) es(y$data, &quot;ANN&quot;, h=12, interval=TRUE, holdout=TRUE, silent=FALSE) ## Time elapsed: 0.04 seconds ## Model estimated: ETS(ANN) ## Persistence vector g: ## alpha ## 0.1802 ## Initial values were optimised. ## ## Loss function type: likelihood; Loss function value: 524.4184 ## Error standard deviation: 31.5279 ## Sample size: 108 ## Number of estimated parameters: 3 ## Number of degrees of freedom: 105 ## Information criteria: ## AIC AICc BIC BICc ## 1054.837 1055.068 1062.883 1063.424 ## ## 95% parametric prediction interval was constructed ## 100% of values are in the prediction interval ## Forecast errors: ## MPE: -2.9%; sCE: -35.3%; Asymmetry: -100%; MAPE: 2.9% ## MASE: 0.997; sMAE: 2.9%; sMSE: 0.1%; rMAE: 1.62; rRMSE: 1.471 As we see, the true smoothing parameter is 0.3, but the estimated one is not exactly 0.3, which is expected, because we deal with an in-sample estimation. Also, notice that with such a high smoothing parameter, the prediction interval is widening with the increase of the forecast horizon. If the smoothing parameter would be lower, then the bounds would not increase, but this might not reflect the uncertainty about the level correctly. Here is an example with \\(\\alpha=0.01\\): ourModel &lt;- es(y$data, &quot;ANN&quot;, h=12, interval=TRUE, holdout=TRUE, silent=FALSE, persistence=0.01) In this case, the prediction interval is wider than needed and the forecast is biased - the model does not keep up to the fast changing time series. So, it is important to correctly estimate the smoothing parameters not only to approximate the data, but also to produce less biased point forecast and more appropriate prediction interval. 7.2.2 ETS(M,N,N) Hyndman et al. (2008) also demonstrate that there is another ETS model, underlying SES. It is the model with multiplicative error, which is formulated in the following way, as mentioned in a previous chapter: \\[\\begin{equation} \\begin{split} y_{t} &amp;= l_{t-1}(1 + \\epsilon_t) \\\\ l_t &amp;= l_{t-1}(1 + \\alpha \\epsilon_t) \\end{split} , \\tag{7.11} \\end{equation}\\] where \\((1+\\epsilon_t)\\) corresponds to the \\(\\varepsilon_t\\) discussed in the previous section. In order to see the connection of this model with SES, we need to revert to the estimation of the model on the data again: \\[\\begin{equation} \\begin{split} y_{t} &amp;= \\hat{l}_{t-1}(1 + e_t) \\\\ \\hat{l}_t &amp;= \\hat{l}_{t-1}(1 + \\hat{\\alpha} e_t) \\end{split} , \\tag{7.12} \\end{equation}\\] where \\(\\hat{y}_t = \\hat{l}_{t-1}\\) and \\(e_t=\\frac{y_t - \\hat{y}_t}{\\hat{y}_t}\\). Substituting these values in (7.12) we obtain: \\[\\begin{equation} \\begin{split} y_{t} &amp;= \\hat{y}_t (1 + e_t) \\\\ \\hat{y}_{t+1} &amp;= \\hat{y}_t \\left(1 + \\hat{\\alpha} \\frac{y_t - \\hat{y}_t}{\\hat{y}_t} \\right) \\end{split} . \\tag{7.13} \\end{equation}\\] Substituting \\(y_t\\) with \\(\\hat{y}_t(1+e_t)\\), shifting the indices one step ahead and inserting the second equation to the first one, we get: \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{y}_t \\left(1 + \\hat{\\alpha} \\frac{y_t - \\hat{y}_t}{\\hat{y}_t} \\right). \\tag{7.14} \\end{equation}\\] Finally, opening the brackets, we get the SES in the form similar to (7.5): \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{y}_t + \\hat{\\alpha} (y_t - \\hat{y}_t). \\tag{7.15} \\end{equation}\\] This example demonstratesonce again the difference between the forecasting method and the forecasting model. When we use SES, we ignore the distributional assumptions, which restricts the usage of the method. The main features of ETS(M,N,N) model in comparison with ETS(A,N,N) are: The variance of the actual values in ETS(M,N,N) increases with the increase of the level \\(l_{t}\\). This allows modelling heteroscedasticity situation in the data; If \\((1+\\epsilon_t)\\) is always positive, then the ETS(M,N,N) model will always produce only positive forecasts (both point and interval). This makes this model applicable to the data with low level; An alternative to (7.11) would be the model (7.6) applied to the data in logarithms (assuming that the data we work with is always positive). However, the ETS(M,N,N) does not rely on exponentiation of the values, making it safe in cases, when very high values are produced by the model (e.g. exp(1000) returns infinity in R). Finally, the point forecast of ETS(M,N,N) corresponds to the conditional h steps ahead mean and is equal to the last observed level, but only if \\(\\text{E}(1+\\epsilon_t)=1\\): \\[\\begin{equation} \\mu_{y,t+h|t} = \\hat{y}_{t+h} = l_{t} . \\tag{7.16} \\end{equation}\\] And here is an example with the ETS(M,N,N) data, which is very similar to the ETS(A,N,N) one: y &lt;- sim.es(&quot;MNN&quot;, 120, 1, 12, persistence=0.3, initial=1000) ourModel &lt;- es(y$data, &quot;MNN&quot;, h=12, interval=TRUE, holdout=TRUE, silent=FALSE) ourModel ## Time elapsed: 0.03 seconds ## Model estimated: ETS(MNN) ## Persistence vector g: ## alpha ## 0.4119 ## Initial values were optimised. ## ## Loss function type: likelihood; Loss function value: 686.0362 ## Error standard deviation: 0.1149 ## Sample size: 108 ## Number of estimated parameters: 3 ## Number of degrees of freedom: 105 ## Information criteria: ## AIC AICc BIC BICc ## 1378.072 1378.303 1386.119 1386.659 ## ## 95% parametric prediction interval was constructed ## 100% of values are in the prediction interval ## Forecast errors: ## MPE: 1.7%; sCE: 55.2%; Asymmetry: 28.2%; MAPE: 8% ## MASE: 1.425; sMAE: 14.6%; sMSE: 2.9%; rMAE: 1.008; rRMSE: 1.023 References "],["ETSExamples.html", "7.3 Several examples of exponential smoothing methods and ETS", " 7.3 Several examples of exponential smoothing methods and ETS There are other exponential smoothing, which include more components, as discussed in the previous section. This includes but is not restricted with: Holt’s (Holt, 2004, originally proposed in 1957), Holt-Winter’s (Winters, 1960), multiplicative trend (Pegels, 1969), Damped trend (originally proposed by Roberts (1982) and then picked up by Gardner and McKenzie (1985)), Damped trend Holt-Winters (Gardner and McKenzie, 1989) and damped multiplicative trend methods (James W. Taylor, 2003a). We will not discuss them here one by one, as we will not use them further in this textbook. More importantly, all of them have underlying ETS models, so we will focus on them instead. We already understand that there can be different components in time series and that they can interact with each other either in an additive or a multiplicative way, which gives us the aforementioned taxonomy. Here are several examples of ETS models with several components and their relations to the conventional exponential smoothing methods. 7.3.1 ETS(A,A,N) This is also sometimes known as local trend model and is formulated as ETS(A,N,N), but with addition of the trend equation. It underlies Holt’s method: \\[\\begin{equation} \\begin{split} y_{t} &amp;= l_{t-1} + b_{t-1} + \\epsilon_t \\\\ l_t &amp;= l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\ b_t &amp;= b_{t-1} + \\beta \\epsilon_t \\end{split} , \\tag{7.17} \\end{equation}\\] where \\(\\beta\\) is the smoothing parameter for the trend component. It has a similar idea as ETS(A,N,N): the states evolve over time, and the speed of their change depends on the values of \\(\\alpha\\) and \\(\\beta\\). Here is an example of the data that corresponds to the ETS(A,A,N) model: y &lt;- sim.es(&quot;AAN&quot;, 120, 1, 12, persistence=c(0.3,0.1), initial=c(1000,20), mean=0, sd=20) plot(y) As you might notice, the trend is not deterministic in this model: both the intercept and the slope change over time. The higher the smoothing parameters are, the more uncertain it is, what the level and the slope will be, thus higher the uncertainty about the future values is. The point forecast h steps ahead from this model is a straight line with a slope \\(b_t\\): \\[\\begin{equation} \\mu_{y,t+h|t} = \\hat{y}_{t+h} = l_{t} + h b_t. \\tag{7.18} \\end{equation}\\] This becomes apparent if one takes the conditional expectations E\\((l_{t+h}|t)\\) and E\\((b_{t+h}|t)\\) in the second and third equations of (7.17). Graphically it will look like this: esModel &lt;- es(y, h=10, silent=FALSE) If you want to experiment with the model and see how its parameters influence the fit and forecast, you can use the following R code: esModel &lt;- es(y$data, h=10, silent=FALSE, persistence=c(0.2,0.1)) where persistence is the vector of smoothing parameters (first \\(\\alpha\\), then \\(\\beta\\)). 7.3.2 ETS(A,Ad,N) This is the model that underlies Damped trend method (Roberts, 1982): \\[\\begin{equation} \\begin{split} y_{t} &amp;= l_{t-1} + \\phi b_{t-1} + \\epsilon_t \\\\ l_t &amp;= l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\ b_t &amp;= \\phi b_{t-1} + \\beta \\epsilon_t \\end{split} , \\tag{7.19} \\end{equation}\\] where \\(\\phi\\) is the dampening parameter, typically lying between 0 and 1. If it is equal to zero, then the model (7.19) reduces to (7.6). If it is equal to one, then it becomes equivalent to (7.17). The dampening parameter slows down the trend, making it non-linear. The typical data that corresponds to ETS(A,Ad,N) is: y &lt;- sim.es(&quot;AAdN&quot;, 120, 1, 12, persistence=c(0.3,0.1), initial=c(1000,20), phi=0.95, mean=0, sd=20) plot(y) The point forecast from this model is a bit more complicated: \\[\\begin{equation} \\mu_{y,t+h|t} = \\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_t. \\tag{7.18} \\end{equation}\\] It corresponds to the slowing down trajectory: esModel &lt;- es(y, h=10, silent=FALSE) 7.3.3 ETS(A,A,M) Finaly, this is an exotic model with additive error and trend, but multiplicative seasonality. Still, we list it here, because it underlies the Holt-Winters method (Winters, 1960): \\[\\begin{equation} \\begin{split} y_{t} &amp;= (l_{t-1} + b_{t-1}) s_{t-m} + \\epsilon_t \\\\ l_t &amp;= l_{t-1} + b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ b_t &amp;= b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\ s_t &amp;= s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1}+b_{t-1}} \\end{split} , \\tag{7.20} \\end{equation}\\] where \\(s_t\\) is the seasonal component and \\(\\gamma\\) is its smoothing parameter. This is one of the potentially unstable models, which due to the mix of components might produce unreasonable forecasts. Still, it might work on the strictly positive high level data. Here how the data for this model can look like: y &lt;- sim.es(&quot;AAM&quot;, 120, 1, 4, persistence=c(0.3,0.1,0.2), initial=c(1000,20), initialSeason=c(0.9,1.1,0.8,1.2), mean=0, sd=20) plot(y) Finally, the point forecast from this model are based on the ETS(A,A,N): \\[\\begin{equation} \\hat{y}_{t+h} = (l_{t} + h b_t) s_{t+h-m\\lceil\\frac{h}{m}\\rceil}, \\tag{7.18} \\end{equation}\\] where \\(\\lceil\\frac{h}{m}\\rceil\\) is the rounded up value of the fraction in the brackets. Remark. The point forecasts produced from this model do not correspond to the conditional expectations. This was discussed in the previous chapter. References "],["ets-assumptions-estimation-and-selection.html", "7.4 ETS assumptions, estimation and selection", " 7.4 ETS assumptions, estimation and selection There are several assumptions that need to hold for the conventional ETS models in order for them to be used in practice appropriately. Some of them have already been discussed in one of the previous sections, and we will not discuss them here again. What is important in our context is that the conventional ETS assumes that the error term \\(\\epsilon_t\\) follows normal distribution with zero mean and variance \\(\\sigma^2\\). As discussed earlier, normal distribution is defined for positive, negative and zero values. This is not a big deal for additive models, which assume that the actual value can be anything. And it is not an issue for the multiplicative models, when we deal with high level positive data (e.g. thousands of units): the variance of the error term will be small enough for the \\(\\epsilon_t\\) not to become less than minus one. However, if the level of the data is low, then the variance of the error term can be large enough for the normally distributed error to cover negative values, less than minus one. This implies that the error term \\(1+\\epsilon_t\\) can become negative, and the model will break. This is a potential flaw in the conventional ETS model with the multiplicative error term. So, what the conventional multiplicative error ETS model assumes in fact is that the data we work with is strictly positive and has high level values. Based on the assumption of normality of error term, the ETS model can be estimated via the maximisation of likelihood, which is equivalent to the minimisation of the mean squared forecast error \\(e_t\\). Note that in order to apply the ETS models to the data, we also need to know the initial values of components, \\(\\hat{l}_0, \\hat{b}_0, \\hat{s}_{-m+2}, \\hat{s}_{-m+3}, \\dots, \\hat{s}_{0}\\). The conventional approach is to estimate these values together with the smoothing parameters during the maximisation of likelihood. As a result, the optimisation might involve a large number of parameters. In addition, the variance of the error term is considered as an additional parameter in the maximum likelihood estimation, so the number of parameters for different models is (here \"*\" stands for any type): ETS(*,N,N) - 3 parameters: \\(\\hat{l}_0\\), \\(\\hat{\\alpha}\\) and \\(\\hat{\\sigma}^2\\); ETS(*,*,N) - 5 parameters: \\(\\hat{l}_0\\), \\(\\hat{b}_0\\), \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\); ETS(*,*d,N) - 6 parameters: \\(\\hat{l}_0\\), \\(\\hat{b}_0\\), \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}\\), \\(\\hat{\\phi}\\) and \\(\\hat{\\sigma}^2\\); ETS(*,N,*) - 4+m-1 parameters: \\(\\hat{l}_0\\), \\(\\hat{s}_{-m+2}, \\hat{s}_{-m+3}, \\dots, \\hat{s}_{0}\\), \\(\\hat{\\alpha}\\), \\(\\hat{\\gamma}\\) and \\(\\hat{\\sigma}^2\\); ETS(*,*,*) - 6+m-1 parameters: \\(\\hat{l}_0\\), \\(\\hat{b}_0\\), \\(\\hat{s}_{-m+2}, \\hat{s}_{-m+3}, \\dots, \\hat{s}_{0}\\), \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}\\), \\(\\hat{\\gamma}\\) and \\(\\hat{\\sigma}^2\\); ETS(*,*d,*) - 7+m-1 parameters: \\(\\hat{l}_0\\), \\(\\hat{b}_0\\), \\(\\hat{s}_{-m+2}, \\hat{s}_{-m+3}, \\dots, \\hat{s}_{0}\\), \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}\\), \\(\\hat{\\gamma}\\), \\(\\hat{\\phi}\\) and \\(\\hat{\\sigma}^2\\). Note that in case of seasonal models we typically make sure that the initial seasonality indices are normalised, so we only need to estimate \\(m-1\\) of them, the last one is calculated based on the linear combination of the others. When it comes to the selection of the most appropriate model, the conventional approach involves the application of all models to the data and then selecting the most appropriate of them based on an information cretiria. In case of the conventional ETS model, this relies on the likelihood value of normal distribution, used in the estimation of the model. Finally, the assumption of normality is used for the generation of prediction intervals from the model. There are typically two ways of doing that: Calculating the variance of multiple steps ahead forecast error and then using it for the intervals calculation; Generating thousands of possible paths for the components of the series and the actual values and then taking the necessary quantiles for the prediction intervals; Typically, (1) is applied for pure additive models, where the closed forms for the variances are known and the assumption of normality holds for several steps ahead. In some special cases of mixed models, there are approximations for variances that work on small horizons. But in all the other cases (2) should be used, despite being typically slower than (1) and producing bounds that differ from run to run due to randomness. "],["state-space-form-of-ets.html", "7.5 State space form of ETS", " 7.5 State space form of ETS One of the main advantages of the ETS model is its state space form, which gives it the flexibility. We would need to revert to linear algebra in this section in order to understand how any ETS model can be presented in a compact state space form. Hyndman et al. (2008) use the following general formulation of the model with the first equation called “measurement equation” and the second one “transition equation”: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp;w(\\mathbf{v}_{t-1}) + r(\\mathbf{v}_{t-1}) \\epsilon_t \\\\ \\mathbf{v}_{t} = &amp;f(\\mathbf{v}_{t-1}) + g(\\mathbf{v}_{t-1}) \\epsilon_t \\end{aligned}, \\tag{7.21} \\end{equation}\\] where \\(\\mathbf{v}_t\\) is the state vector, containing the components of series (level, trend and seasonal), \\(w(\\cdot)\\) is the measurement,\\(r(\\cdot)\\) is the error, \\(f(\\cdot)\\) is the transition and \\(g(\\cdot)\\) is the persistence functions. Depending on the types of components these functions can have different values: Depending on the types of trend and seasonality \\(w(\\mathbf{v}_{t-1})\\) will be equal either to the addition or multiplication of components. The special cases were presented in tables 6.1 and 6.2 in the ETS Taxonomy section. For example, in case of ETS(M,M,M) it is: \\(w(\\mathbf{v}_{t-1}) = l_{t-1} b_{t-1} s_{t-m}\\); If the error is additive, then \\(r(\\mathbf{v}_{t-1})=1\\), otherwise (in case of multiplicative error) it is \\(r(\\mathbf{v}_{t-1})=w(\\mathbf{v}_{t-1})\\); The transition function \\(f(\\cdot)\\) will produce values depending on the types of trend and seasonality and will correspond to the first parts in the tables 6.1 and 6.2 of the transition equations (dropping the error term). This function records how components interact with each other and how they change from one observation to another (thus the term “transition”). An example is the ETS(M,M,M) model, for which the transition function will produce three values: \\(l_{t-1}b_{t-1}\\), \\(b_{t-1}\\) and \\(s_{t-m}\\) respectively for the level, trend and seasonal components. So, if we drop the persistence function \\(g(\\cdot)\\) and the error term \\(\\epsilon_t\\) for a moment, the second equation in (7.21) will be: \\[\\begin{equation} \\begin{aligned} {l}_{t} = &amp;l_{t-1}b_{t-1} \\\\ b_t = &amp;b_{t-1} \\\\ s_t = &amp;s_{t-m} \\end{aligned}, \\tag{7.22} \\end{equation}\\] Finally, the persistence function will differ from one model to another, but in some special cases it can either be: \\(g(\\mathbf{v}_{t-1})=\\mathbf{g}\\), if the error term is additive and \\(g(\\mathbf{v}_{t-1})=f(\\mathbf{v}_{t-1})\\mathbf{g}\\) if it is multiplicative. \\(\\mathbf{g}\\) is the vector of smoothing parameters, called in the ETS context the “persistence vector.” An example of persistence function is the ETS(M,M,M) model, for which it is: \\(l_{t-1}b_{t-1}\\alpha\\), \\(b_{t-1}\\beta\\) and \\(s_{t-m}\\gamma\\) respectively for the level, trend and seasonal components. Uniting this with the transition function (7.22) we get the equation from the table 6.2: \\[\\begin{equation} \\begin{aligned} {l}_{t} = &amp;l_{t-1}b_{t-1} (1+\\alpha\\epsilon_t)\\\\ b_t = &amp;b_{t-1} (1+\\beta\\epsilon_t)\\\\ s_t = &amp;s_{t-m} (1+\\gamma\\epsilon_t) \\end{aligned}, \\tag{7.23} \\end{equation}\\] The compact form (7.21) is thus comfortable to work with and underlies all the 30 ETS models discussed in the sections 5.1 and 5.2. Unfortunately, they cannot be used directly for the derivation of conditional values, so they are needed just for the general understanding of ETS. References "],["ETSParametersBounds.html", "7.6 Parameters bounds", " 7.6 Parameters bounds While, it is accepted by many practitioners and academics that the smoothing parameters of ETS models should lie between zero and one, this is not entirely true for the models. There are, in fact, several possible restrictions on smoothing parameters, and it is worth discussing them separately: Classical or conventional bounds are \\(\\alpha, \\beta, \\gamma \\in (0,1)\\). The idea behind them originates from the exponential smoothing methods, where it is logical to restrict the bounds with this region, because then the smoothing parameters regulate, what weight the actual value \\(y_t\\) will have and what weight will be asigned to the predicted one \\(\\hat{y}_t\\). Hyndman et al. (2008) showed that this condition is sometimes too loose and in other cases is too restrictive to some ETS models. Brenner et al. (1968) was one of the first to show that the bounds are wider than this region for many exponential smoothing methods. Still, the conventional restriction is the most often used, just because it is nice to work with. Usual or traditional bounds are those that satisfy the set of the following equations: \\[\\begin{equation} \\begin{aligned} &amp;\\alpha \\in [0, 1)\\\\ &amp;\\beta \\in [0, \\alpha) \\\\ &amp;\\gamma \\in [0, 1-\\alpha) \\end{aligned}, \\tag{7.24} \\end{equation}\\] This set of restrictions guarantees that the weights decline over time exponentially and the ETS models have the property of “averaging” the values over time. In the lower boundary condition, the components of the model become deterministic and we can say that they are calculated as the simple averages of the values over time. Admissible bounds, satisfying stability condition. The idea here is that the most recent observation should have higher weight than the older ones, which is regulated via the smoothing parameters. However, in this case we do not impose the restriction of exponential decay of weights over time on the models, so they can oscillate or decay harmonially, as long as their absolute values decrease over time. The condition is more complicated mathematically than the previous two and will be discussed later in the textbook for the pure additive models, but here are several examples for bounds, satisfying this condition: ETS(A,N,N): \\(\\alpha \\in (0, 2)\\); ETS(A,A,N): \\(\\alpha \\in (0, 2); \\beta \\in (0, 4-2\\alpha)\\); ETS(A,N,A): \\(\\alpha \\in \\left(\\frac{-2}{m-1}, 2-\\gamma\\right); \\gamma \\in (\\max(-m\\alpha, 0), 2-\\alpha)\\); As you see, the admissible bounds are much wider than the conventional and usual ones. In fact, smoothing parameters can become either negative or greater than one in some cases for some models. Furthermore, the admissible bounds correspond to the parameters restrictions for ARIMA models, underlying some of pure additive ETS models. In a way, they are more natural for the ETS models, because they follow from the formulation and arise naturally. However, their usage in practice has been met with mixed success, with only handful of papers using them instead of (1) or (2) (e.g. Gardner and Diaz-Saiz (2008) mention that they appear in some cases and Snyder et al. (2017) use them in their model). In the R code, the admissible bounds are calculated based on the discount matrix, which will be discussed in the context of pure additive ADAM ETS models in the next chapter. References "],["ADAMETSIntroduction.html", "Chapter 8 Pure additive ADAM ETS", " Chapter 8 Pure additive ADAM ETS The ETS model implemented in ADAM framework is built upon the conventional one but has several important differences. First it is formulated using lags of components rather than the transition of them over time, so the original model (7.21) is written in the following way: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp;w(\\mathbf{v}_{t-\\boldsymbol{l}}) + r(\\mathbf{v}_{t-\\boldsymbol{l}}) \\epsilon_t \\\\ \\mathbf{v}_{t} = &amp;f(\\mathbf{v}_{t-\\boldsymbol{l}}) + g(\\mathbf{v}_{t-\\boldsymbol{l}}) \\epsilon_t \\end{aligned}, \\tag{8.1} \\end{equation}\\] where \\(\\mathbf{v}_{t-\\boldsymbol{l}}\\) is the vector of lagged components and \\(\\boldsymbol{l}\\) is the vector of lags, and all the other functions corresponds to the ones used in (7.21). So, for example, for the ETS(A,A,A) model the lags will be \\(\\boldsymbol{l}&#39;=\\begin{pmatrix}1 &amp; 1 &amp; m\\end{pmatrix}\\), where \\(m\\) is the seasonal periodicity of the data, leading to \\(\\mathbf{v}_{t-\\boldsymbol{l}}&#39;=\\begin{pmatrix} l_{t-1} &amp; b_{t-1} &amp; s_{t-m}\\end{pmatrix}\\). The model (8.1) updates the states exactly in the same way as (7.21) and produces exactly the same values. The main benefit of doing that is that the transition matrix becomes smaller, containing \\(3\\times 3\\) elements in case of ETS(A,A,A) instead of \\((2+m)\\times (2+m)\\) as for the conventional model. The main disadvantage of this approach is in the complications arrising in the derivation of conditional expectation and mean, which still have closed forms, but are more cumbersome. They are discussed later in this chapter for the example of pure additive ETS. Furthermore, ADAM ETS introduces more flexibility, allowing the error term \\(\\epsilon_t\\) to follow non-normal distributions. This impacts the likelihood function and prediction interval, but does not change the mechanism of the update of states. Based on all this, in the next few chapters we will discuss several special cases of the ADAM ETS model and explain how they can be extended. "],["ADAMETSPureAdditive.html", "8.1 Model formulation", " 8.1 Model formulation The pure additive case is interesting, because this is the group of models that has closed forms for both conditional mean and variance. It is formulated in the following way: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp;\\mathbf{w}&#39; \\mathbf{v}_{t-\\boldsymbol{l}} + \\epsilon_t \\\\ \\mathbf{v}_{t} = &amp;\\mathbf{F} \\mathbf{v}_{t-\\boldsymbol{l}} + \\mathbf{g} \\epsilon_t \\end{aligned}, \\tag{8.2} \\end{equation}\\] where \\(\\mathbf{w}\\) is the measurement vector, \\(\\mathbf{F}\\) is the transition matrix and \\(\\mathbf{g}\\) is the persistence vector. An example of a pure additive model is ETS(A,A,A), for which we have the following values: \\[\\begin{equation} \\begin{aligned} \\mathbf{w} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, &amp; \\mathbf{F} = \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}, \\\\ \\mathbf{g} = \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix}, &amp; \\mathbf{v}_{t} = \\begin{pmatrix} l_t \\\\ b_t \\\\ s_t \\end{pmatrix}, &amp; \\boldsymbol{l} = \\begin{pmatrix} 1 \\\\ 1 \\\\ m \\end{pmatrix} \\end{aligned}. \\tag{8.3} \\end{equation}\\] By inserting these values in the equation (8.2), we will obtain the model discussed in the ETS Taxonomy section: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t \\\\ l_t = &amp; l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\ b_t = &amp; b_{t-1} + \\beta \\epsilon_t \\\\ s_t = &amp; s_{t-m} + \\gamma \\epsilon_t \\end{aligned}. \\tag{8.4} \\end{equation}\\] Just to compare, the conventinal ETS(A,A,A), formulated according to (7.21) would have the following transition matrix: \\[\\begin{equation} \\mathbf{F} = \\begin{pmatrix} 1 &amp; 1 &amp; \\mathbf{0}&#39;_{m-1} &amp; 0 \\\\ 0 &amp; 1 &amp; \\mathbf{0}&#39;_{m-1} &amp; 0 \\\\ 0 &amp; 0 &amp; \\mathbf{0}&#39;_{m-1} &amp; 1 \\\\ \\mathbf{0}_{m-1} &amp; \\mathbf{0}_{m-1} &amp; \\mathbf{I}_{m-1} &amp; \\mathbf{0}_{m-1} \\end{pmatrix}, \\tag{8.5} \\end{equation}\\] where \\(\\mathbf{I}_{m-1}\\) is the identity matrix of the size \\((m-1) \\times (m-1)\\) and \\(\\mathbf{0}_{m-1}\\) is the vector of zeroes of size \\(m-1\\). The model (8.2) is more parsimonious and simplifies some of the calculations, making it realistic, for example, to apply models to data with large frequency \\(m\\) (e.g. 24, 48, 52, 365). "],["adamETSPureAdditiveRecursive.html", "8.2 Recursive relation", " 8.2 Recursive relation A useful thing that can be derived from the pure additive model (8.2) is the recursive value, which can be used in several important aspects. First, when we produce forecast for \\(h\\) steps ahead, it is important to understand what the actual value \\(h\\) steps ahead might be, given all the information we have on the observation \\(t\\): \\[\\begin{equation} \\begin{aligned} {y}_{t+h} = &amp;\\mathbf{w}&#39; \\mathbf{v}_{t-h_\\boldsymbol{l}} + \\epsilon_{t+h} \\\\ \\mathbf{v}_{t+h} = &amp;\\mathbf{F} \\mathbf{v}_{t-h_\\boldsymbol{l}} + \\mathbf{g} \\epsilon_{t+h} \\end{aligned}, \\tag{8.6} \\end{equation}\\] where \\(\\mathbf{v}_{t-h_\\boldsymbol{l}}\\) is the vector of previous states, given the lagged values \\(\\boldsymbol{l}\\). In order to obtain the recursion, we need to split the measurement and persisitence vectors together with the transition matrix into parts for the same lags of components, leading to the following transition equation in (8.6): \\[\\begin{equation} \\begin{aligned} {y}_{t+h} = &amp;(\\mathbf{w}_{m_1}&#39; + \\mathbf{w}_{m_2}&#39; + \\dots + \\mathbf{w}_{m_d}&#39;) \\mathbf{v}_{t-h_\\boldsymbol{l}} + \\epsilon_{t+h} \\\\ \\mathbf{v}_{t+h} = &amp;(\\mathbf{F}_{m_1} + \\mathbf{F}_{m_2} + \\dots + \\mathbf{F}_{m_d}) \\mathbf{v}_{t-h_\\boldsymbol{l}} + (\\mathbf{g}_{m_1} + \\mathbf{g}_{m_2} + \\dots \\mathbf{g}_{m_d}) \\epsilon_{t+h} \\end{aligned}, \\tag{8.7} \\end{equation}\\] where \\(m_1, m_2, \\dots, m_d\\) are the distinct seasonal frequencies. So, for example, in case of ETS(A,A,A) model on quarterly data (periodicity is equal to four), \\(m_1=1\\), \\(m_d=4\\), leading to \\(\\mathbf{F}_{m_1} = \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}\\) and \\(\\mathbf{F}_{m_2} = \\begin{pmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\), where the split of the transition matrix is done column-wise. This split of matrices and vectors into distinct sub matrices and subvectors is needed in order to get the correct recursion and obtain the correct conditional expectation and variance. By substituting the values in the transition equation of (8.7) by their previous values until we reach \\(t\\), we get: \\[\\begin{equation} \\begin{aligned} \\mathbf{v}_{t-h_\\boldsymbol{l}} = &amp; \\mathbf{F}_{m_1}^{\\lceil\\frac{h}{m_1}\\rceil-1} \\mathbf{v}_{t} + \\sum_{j=1}^{\\lceil\\frac{h}{m_1}\\rceil-1} \\mathbf{F}_{m_1}^{j-1} \\mathbf{g}_{m_1} \\epsilon_{t+m_1\\lceil\\frac{h}{m_1}\\rceil-j} + \\\\ &amp; \\mathbf{F}_{m_2}^{\\lceil\\frac{h}{m_2}\\rceil-1} \\mathbf{v}_{t} + \\sum_{j=1}^{\\lceil\\frac{h}{m_2}\\rceil-1} \\mathbf{F}_{m_2}^{j-1} \\mathbf{g}_{m_2} \\epsilon_{t+m_2\\lceil\\frac{h}{m_2}\\rceil-j} + \\\\ &amp; \\dots \\\\ &amp; \\mathbf{F}_{m_d}^{\\lceil\\frac{h}{m_d}\\rceil-1} \\mathbf{v}_{t} + \\sum_{j=1}^{\\lceil\\frac{h}{m_d}\\rceil-1} \\mathbf{F}_{m_d}^{j-1} \\mathbf{g}_{m_d} \\epsilon_{t+m_d\\lceil\\frac{h}{m_d}\\rceil-j} \\end{aligned}. \\tag{8.8} \\end{equation}\\] Inserting (8.8) in the measurement equation of (8.7), we will get: \\[\\begin{equation} \\begin{aligned} y_{t+h} = &amp; \\mathbf{w}_{m_1}&#39; \\mathbf{F}_{m_1}^{\\lceil\\frac{h}{m_1}\\rceil-1} \\mathbf{v}_{t} + \\mathbf{w}_{m_1}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{m_1}\\rceil-1} \\mathbf{F}_{m_1}^{j-1} \\mathbf{g}_{m_1} \\epsilon_{t+m_1\\lceil\\frac{h}{m_1}\\rceil-j} + \\\\ &amp; \\mathbf{w}_{m_2}&#39; \\mathbf{F}_{m_2}^{\\lceil\\frac{h}{m_2}\\rceil-1} \\mathbf{v}_{t} + \\mathbf{w}_{m_2}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{m_2}\\rceil-1} \\mathbf{F}_{m_2}^{j-1} \\mathbf{g}_{m_2} \\epsilon_{t+m_2\\lceil\\frac{h}{m_2}\\rceil-j} + \\\\ &amp; \\dots + \\\\ &amp; \\mathbf{w}_{m_d}&#39; \\mathbf{F}_{m_d}^{\\lceil\\frac{h}{m_d}\\rceil-1} \\mathbf{v}_{t} + \\mathbf{w}_{m_d}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{m_d}\\rceil-1} \\mathbf{F}_{m_d}^{j-1} \\mathbf{g}_{m_d} \\epsilon_{t+m_d\\lceil\\frac{h}{m_d}\\rceil-j} + \\\\ &amp; \\epsilon_{t+h} \\end{aligned}. \\tag{8.9} \\end{equation}\\] Substituting the specific values of \\(m_1, m_2, \\dots, m_d\\) in (8.9) will simplify the equation and make it easier to understand. For example, for ETS(A,N,N) \\(m_1=1\\) and all the other frequencies are equal to zero, so the recursion (8.9) simplifies to: \\[\\begin{equation} y_{t+h} = \\mathbf{w}_{1}&#39; \\mathbf{F}_{1}^{h-1} \\mathbf{v}_{t} + \\mathbf{w}_{1}&#39; \\sum_{j=1}^{h-1} \\mathbf{F}_{1}^{j-1} \\mathbf{g}_{1} \\epsilon_{t+h-j} + \\epsilon_{t+h} , \\tag{8.10} \\end{equation}\\] which is the recursion obtained by Hyndman et al. (2008). References "],["pureAdditiveExpectationAndVariance.html", "8.3 Conditional expectation and variance", " 8.3 Conditional expectation and variance Now, why is the recursion (8.9) important? This is because we can take the expectation and variance of (8.9) conditional on the values of the state vector \\(\\mathbf{v}_{t}\\) on the observation \\(t\\) (assuming that the error term is homoscedastic, uncorrelated and has the expectation of zero) in order to get: \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = \\text{E}(y_{t+h}|t) = &amp; \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i}&#39; \\mathbf{F}_{m_i}^{\\lceil\\frac{h}{m_i}\\rceil-1} \\right) \\mathbf{v}_{t} \\\\ \\sigma^2_{h} = \\text{V}(y_{t+h}|t) = &amp; \\left( \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{m_i}\\rceil-1} \\mathbf{F}_{m_i}^{j-1} \\mathbf{g}_{m_i} \\mathbf{g}&#39;_{m_i} (\\mathbf{F}_{m_i}&#39;)^{j-1} \\mathbf{w}_{m_i} \\right) + 1 \\right) \\sigma^2 \\end{aligned}, \\tag{8.11} \\end{equation}\\] where \\(\\sigma^2\\) is the variance of the error term. These two formulae are cumbersome, but they give the analytical solutions to the two statistics. Having obtained both of them, we can construct prediction intervals, assuming, for example, that the error term follows normal distribution: \\[\\begin{equation} y_{t+h} \\in \\text{E}(y_{t+h}|t) \\pm z_{\\frac{\\alpha}{2}} \\sqrt{\\text{V}(y_{t+h}|t)} , \\tag{8.12} \\end{equation}\\] where \\(z_{\\frac{\\alpha}{2}}\\) is quantile of standardised normal distribution for the level \\(\\alpha\\). When it comes to other distributions, in order to get the conditional h steps ahead scale parameter, we can first calculate the variance using (8.11) and then using the relation between the scale and the variance for the specific distribution to get the necessary value. 8.3.1 Example with ETS(A,N,N) For example, for the ETS(A,N,N) model, discussed above, we get: \\[\\begin{equation} \\begin{aligned} \\text{E}(y_{t+h}|t) = &amp; \\mathbf{w}_{1}&#39; \\mathbf{F}_{1}^{h-1} \\mathbf{v}_{t} \\\\ \\text{V}(y_{t+h}|t) = &amp; \\left(\\mathbf{w}_{1}&#39; \\sum_{j=1}^{h-1} \\mathbf{F}_{1}^{j-1} \\mathbf{g}_{1} \\mathbf{g}&#39;_{1} (\\mathbf{F}_{1}&#39;)^{j-1} \\mathbf{w}_{1} + 1 \\right) \\sigma^2 \\end{aligned}, \\tag{8.13} \\end{equation}\\] or by substituting \\(\\mathbf{F}=1\\), \\(\\mathbf{w}=1\\), \\(\\mathbf{g}=\\alpha\\) and \\(\\mathbf{v}_t=l_t\\): \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = &amp; l_{t} \\\\ \\sigma^2_{h} = &amp; \\left((h-1) \\alpha^2 + 1 \\right) \\sigma^2 \\end{aligned}, \\tag{8.14} \\end{equation}\\] which is the same conditional expectation and variance as in the ETS Taxonomy section and in the Hyndman et al. (2008) textbook. References "],["stabilityConditionAdditiveError.html", "8.4 Stability and forecastability conditions", " 8.4 Stability and forecastability conditions Another important aspect of the pure additive model (8.2) is the restriction on the smoothing parameters. This is related to the stability and forecastability conditions of the model. The stability implies that the weights for observations decay, guaranteeing that the newer ones will have higher weights than the older ones. If this condition holds, then the model behaves “steadily,” forgetting eventually the past values. The forecastability does not guarantee that the weights will decay, but it guarantees that the initial value of the state vector will have a constant impact on forecasts, i.e. will not increase in weight with the increase of the forecast horizon. An example of the non-stable, but forecastable model is ETS(A,N,N) with \\(\\alpha=0\\). In this case it reverts to the global level model, where the initial value impacts the forecast, but does not change with the increase of the forecast horizon. In order to obtain both conditions, we need to use a reduced form of ETS by inserting the measurement equation in the transition equation via \\(\\epsilon_t= {y}_{t} - \\mathbf{w}&#39; \\mathbf{v}_{t-\\boldsymbol{l}}\\): \\[\\begin{equation} \\begin{aligned} \\mathbf{v}_{t} = &amp;\\mathbf{F} \\mathbf{v}_{t-\\boldsymbol{l}} + \\mathbf{g} \\left({y}_{t} - \\mathbf{w}&#39; \\mathbf{v}_{t-\\boldsymbol{l}} \\right)\\\\ = &amp; \\left(\\mathbf{F} - \\mathbf{g}\\mathbf{w}&#39; \\right) \\mathbf{v}_{t-\\boldsymbol{l}} + \\mathbf{g} {y}_{t} \\\\ \\end{aligned}. \\tag{8.15} \\end{equation}\\] The matrix \\(\\mathbf{D}=\\mathbf{F} - \\mathbf{g}\\mathbf{w}&#39;\\) is called the discount matrix and it shows how the weights diminish over time. It is the main part of the model that determines, whether the model will be stable / forecastable or not. 8.4.1 Example with ETS(A,N,N) In order to better understand what we plan to discuss later, we can take an example of ETS(A,N,N) model, for which \\(\\mathbf{F}=1\\), \\(\\mathbf{w}=1\\), \\(\\mathbf{g}=\\alpha\\), \\(\\mathbf{v}_t=l_t\\) and \\(\\boldsymbol{l}=1\\). Inserting these values into (8.15), we get: \\[\\begin{equation} \\begin{aligned} l_{t} = &amp; \\left(1 - \\alpha \\right) {l}_{t-1} + \\alpha {y}_{t}, \\end{aligned}. \\tag{8.16} \\end{equation}\\] which corresponds to the formula of Simple Exponential Smoothing (7.1). The discount matrix in this case is \\(\\mathbf{D}=1-\\alpha\\). If we now substitute the values for the level on the right hand side of the equation (8.16) by the previous values of the level, we will obtain the recursion that we have already discussed in a previous section, but now in terms of the “true” components and parameters: \\[\\begin{equation} \\begin{aligned} l_{t} = &amp; {\\alpha} \\sum_{j=0}^{t-1} (1 -{\\alpha})^j {y}_{t-j} + (1 -{\\alpha})^t l_0, \\end{aligned}. \\tag{8.17} \\end{equation}\\] The stability condition for ETS(A,N,N) is that the discount matrix \\(\\mathbf{D}=1-\\alpha\\) is less than or equal to one by absolute value. This way the weights will decay in time because of the exponentiation in (8.17). This condition is satisfied, when \\(\\alpha \\in(0, 2)\\). As for the forecastability condition, in this case it implies that \\(\\lim\\limits_{t\\rightarrow\\infty}(1 -{\\alpha})^t = \\text{const}\\). This is achievable, for example, when \\(\\alpha=0\\), but is violated, when \\(\\alpha&lt;0\\) or \\(\\alpha\\geq 2\\). So, the bounds for the smoothing parameters in the ETS(A,N,N) model, guaranteeing the forecastability of the model (i.e. making it useful) are: \\[\\begin{equation} \\alpha \\in [0, 2) . \\tag{8.18} \\end{equation}\\] 8.4.2 Comming back to the general case In general, the logic is the same as with ETS(A,N,N), but it implies the usage of linear algebra. Due to our lagged formulation, the recursion becomes more complicated: \\[\\begin{equation} \\begin{aligned} \\mathbf{v}_{t} = &amp; \\mathbf{D}_{m_1}^{\\lceil\\frac{t}{m_1}\\rceil} \\mathbf{v}_{0} + \\sum_{j=0}^{\\lceil\\frac{t}{m_1}\\rceil-1} \\mathbf{D}_{m_1}^{j} y_{t - j m_1} + \\\\ &amp; \\mathbf{D}_{m_2}^{\\lceil\\frac{t}{m_2}\\rceil} \\mathbf{v}_{0} + \\sum_{j=0}^{\\lceil\\frac{t}{m_2}\\rceil-1} \\mathbf{D}_{m_2}^j y_{t - j m_2} + \\\\ &amp; \\dots + \\\\ &amp; \\mathbf{D}_{m_d}^{\\lceil\\frac{t}{m_d}\\rceil} \\mathbf{v}_{0} + \\sum_{j=0}^{\\lceil\\frac{t}{m_d}\\rceil-1} \\mathbf{D}_{m_d}^j y_{t - j m_d} \\end{aligned}, \\tag{8.9} \\end{equation}\\] where \\(\\mathbf{D}_{m_i} = \\mathbf{F}_{m_i} - \\mathbf{g}_{m_i} \\mathbf{w}_{m_i}&#39;\\) is the discount matrix for each lagged part of the model. The stability condition in this case is that the absolute values of all the non-zero eigenvalues of the discount matrices \\(\\mathbf{D}_{m_i}\\) are lower than one. This condition can be checked at the model construction stage, ensuring that the selected parameters guarantee the stability of the model. As for the forecastability, the idea is that the initial value of the state vector should not have an increasing impact on the last observed value, which is obtained by inserting (8.9) in the measurement equation: \\[\\begin{equation} \\begin{aligned} y_t = &amp; \\mathbf{w}_{m_1}&#39; \\mathbf{D}_{m_1}^{\\lceil\\frac{t-1}{m_1}\\rceil} \\mathbf{v}_{0} + \\mathbf{w}_{m_1}&#39; \\sum_{j=0}^{\\lceil\\frac{t-1}{m_1}\\rceil-1} \\mathbf{D}_{m_1}^{j} y_{t-1 - j m_1} + \\\\ &amp; \\mathbf{w}_{m_2}&#39; \\mathbf{D}_{m_2}^{\\lceil\\frac{t-1}{m_2}\\rceil} \\mathbf{v}_{0} + \\mathbf{w}_{m_2}&#39; \\sum_{j=0}^{\\lceil\\frac{t-1}{m_2}\\rceil-1} \\mathbf{D}_{m_2}^j y_{t-1 - j m_2} + \\\\ &amp; \\dots + \\\\ &amp; \\mathbf{w}_{m_d}&#39; \\mathbf{D}_{m_d}^{\\lceil\\frac{t-1}{m_d}\\rceil} \\mathbf{v}_{0} + \\mathbf{w}_{m_d}&#39; \\sum_{j=0}^{\\lceil\\frac{t-1}{m_d}\\rceil-1} \\mathbf{D}_{m_d}^j y_{t-1 - j m_d} \\end{aligned}, \\tag{8.19} \\end{equation}\\] and analysing the impact of \\(\\mathbf{v}_0\\) on the actual value \\(y_t\\). In our case forecastability condition implies that: \\[\\begin{equation} \\lim\\limits_{t\\rightarrow\\infty}\\left(\\mathbf{w}_{m_i}&#39;\\mathbf{D}_{m_i}^{\\lceil\\frac{t-1}{m_i}\\rceil} \\mathbf{v}_{0}\\right) = \\text{const for all } i=1, \\dots, d. \\tag{8.19} \\end{equation}\\] "],["ADAMETSAdditiveDistributions.html", "8.5 Distributional assumptions in pure additive ETS", " 8.5 Distributional assumptions in pure additive ETS While the conventional ETS assumes that the error term follows Normal distribution, ADAM ETS proposes some flexibility, implementing the following options for the error term distribution in the additive error models: Normal: \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\); Laplace: \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\); S: \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\); Generalised Normal: \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\); The conditional expectation and stability / forecastability conditions do not change for the model with these new assumptions. The main element that changes is the scale and the width of prediction intervals. Given that scales of these distributions are linearly related to the variance, one can calculate the conditional variance as discussed earlier and then use the formulae from the theory of distributions section in order to obtain the respective scales. Having the scales it becomes straightforward to calculate the needed quantiles for the prediction intervals. The estimation of pure additive ETS models can be done via the maximisation of the likelihood of the assumed distribution, which in some cases coincide with the popular losses (e.g. Normal and MSE, or Laplace and MAE). In addition, the following more exotic options for the additive error models are available in ADAM ETS: Log Normal: \\(\\left(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\right) \\sim \\text{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\). Here \\(\\mu_{y,t} = \\mathbf{w}&#39; \\mathbf{v}_{t-\\boldsymbol{l}}\\), \\(\\sigma^2\\) is the variance of the error term in logarithms and the \\(-\\frac{\\sigma^2}{2}\\) appears due to the restriction \\(\\text{E}(\\epsilon_t)=0\\). Inverse Gaussian: \\(\\left(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\right) \\sim \\mathcal{IG}(1, s)\\); Gamma: \\(\\left(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\right) \\sim \\mathcal{\\Gamma}(s^{-1}, s)\\); The possibility of using these distributions arrises from a reformulation of the original pure additive model (8.2) into: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp;\\mathbf{w}&#39; \\mathbf{v}_{t-\\boldsymbol{l}}\\left(1 + \\frac{\\epsilon_t}{\\mathbf{w}&#39; \\mathbf{v}_{t-\\boldsymbol{l}}}\\right) \\\\ \\mathbf{v}_{t} = &amp;\\mathbf{F} \\mathbf{v}_{t-\\boldsymbol{l}} + \\mathbf{g} \\epsilon_t \\end{aligned}. \\tag{8.20} \\end{equation}\\] The connection between the two formulations becomes apparent, when opening the brackets in the measurement equation of (8.20). Note that in this case the model assumes that the data is strictly positive and while it might be possible to fit the model on the data with negative values, the calculation of the scale and the likelihood might become impossible. Using alternative losses (e.g. MSE) is a possible solution in this case. "],["ADAMETSPureAdditiveExamples.html", "8.6 Examples of application", " 8.6 Examples of application 8.6.1 Non-seasonal data In order to see how the pure additive ADAM ETS works, we will try it out using adam() function from smooth package for R on Box-Jenkins sales data. We start with plotting the data: plot(BJsales) The series seem to exhibit trend, so we will apply ETS(A,A,N) model: adamModel &lt;- adam(BJsales, &quot;AAN&quot;) adamModel ## Time elapsed: 0.03 seconds ## Model estimated using adam() function: ETS(AAN) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 258.9257 ## Persistence vector g: ## alpha beta ## 1.0000 0.2469 ## ## Sample size: 150 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 145 ## Information criteria: ## AIC AICc BIC BICc ## 527.8514 528.2681 542.9046 543.9485 The output of the model summarises, which specific model was estimated, assuming what distribution, how it was estimated, what are the values of smoothing parameters, the sample size, degrees of freedom and also produces information criteria. We can compare this model with the ETS(A,N,N) in order to see, which of them performs better in terms of information criteria (e.g. in terms of AICc): adam(BJsales, &quot;ANN&quot;) ## Time elapsed: 0.01 seconds ## Model estimated using adam() function: ETS(ANN) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 276.0603 ## Persistence vector g: ## alpha ## 0.9996 ## ## Sample size: 150 ## Number of estimated parameters: 3 ## Number of degrees of freedom: 147 ## Information criteria: ## AIC AICc BIC BICc ## 558.1207 558.2850 567.1526 567.5644 In this situation the information criteria for ETS(A,N,N) are higher than for ETS(A,A,N), so we should use the latter for forecasting purposes. We can produced point forecasts and prediction interval (in this example we will constract 90% and 95% ones) and plot them: plot(forecast(adamModel,h=10,interval=&quot;prediction&quot;,level=c(0.9,0.95))) Notice that the smoothing parameters of ETS(A,A,N) are very high, with \\(\\alpha=1\\). This might mean that the maximum of the likelihood is achieved in the admissible bounds. We can try it out and see what happens: adamModel &lt;- adam(BJsales, &quot;AAN&quot;, bounds=&quot;admissible&quot;) adamModel ## Time elapsed: 0.05 seconds ## Model estimated using adam() function: ETS(AAN) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 258.5358 ## Persistence vector g: ## alpha beta ## 1.0541 0.2185 ## ## Sample size: 150 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 145 ## Information criteria: ## AIC AICc BIC BICc ## 527.0716 527.4883 542.1248 543.1687 plot(forecast(adamModel,h=10,interval=&quot;prediction&quot;,level=c(0.9,0.95))) Both smoothing parameters are now higher, which implies that the uncertainty about the future values of states is higher as well, which is then reflected in the slightly wider prediction interval. Although the values are larger than one, the model is still stable. In order to see that we can calculate the discount matrix using the objects returned by the function: discountMatrix &lt;- adamModel$transition - adamModel$persistence %*% adamModel$measurement[nobs(adamModel),,drop=FALSE] eigen(discountMatrix)$values ## [1] 0.79538429 -0.06800887 Notice that the absolute values of the both eigenvalues in the matrix are less than one, which means that the newer observations have higher weights than the older ones and that the absolute values of weights decrease over time, making the model stable. If we want to test ADAM ETS with another distribution, it can be done using the respective parameter (here we use Generalised Normal, estimating the shape together with the other parameters): adamModel &lt;- adam(BJsales, &quot;AAN&quot;, distribution=&quot;dgnorm&quot;) print(adamModel,digits=3) ## Time elapsed: 0.03 seconds ## Model estimated using adam() function: ETS(AAN) ## Distribution assumed in the model: Generalised Normal with shape=1.819 ## Loss function type: likelihood; Loss function value: 258.664 ## Persistence vector g: ## alpha beta ## 1.000 0.239 ## ## Sample size: 150 ## Number of estimated parameters: 6 ## Number of degrees of freedom: 144 ## Information criteria: ## AIC AICc BIC BICc ## 529.327 529.915 547.391 548.863 plot(forecast(adamModel,h=10,interval=&quot;prediction&quot;)) The prediction interval in this case is slightly weider than in the previous one, because \\(\\mathcal{GN}\\) distribution with \\(\\beta=\\) 1.82 has fatter tails than the normal distribution. 8.6.2 Seasonal data Now we will check what happens in the case of seasonal data. We use AirPassengers, which actually has multiplicative seasonality, but for purposes of demonstration we will see what happens, when we use the wrong model. We will withhold 12 observations to look closer at the performance of the ETS(A,A,A) model in this case: adamModel &lt;- adam(AirPassengers, &quot;AAA&quot;, lags=12, h=12, holdout=TRUE) Remark. The lags parameter in this specific case is not necessary, because the function will get the frequency from the ts object automatically. If we were to provide a vector of values instead of the ts object, we would need to specify the correct lag. Note that 1 (lag for level and trend) is not important either - the function will always use it anyway. Remark. In some cases, the optimiser might converge to the local minimum, so if you find the results unsatisfactory, it might make sense to reestimate the model tuning the parameters of the optimiser. Here is an example (we increase the number of iterations in the optimisation and set new starting values for the smoothing parameters): adamModel$B[1:3] &lt;- c(0.2,0.1,0.3) adamModel &lt;- adam(AirPassengers, &quot;AAA&quot;, lags=12, h=12, holdout=TRUE, B=adamModel$B, maxeval=1000) adamModel ## Time elapsed: 0.17 seconds ## Model estimated using adam() function: ETS(AAA) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 515.3469 ## Persistence vector g: ## alpha beta gamma ## 0.2169 0.0001 0.7831 ## ## Sample size: 132 ## Number of estimated parameters: 17 ## Number of degrees of freedom: 115 ## Information criteria: ## AIC AICc BIC BICc ## 1064.694 1070.062 1113.701 1126.808 ## ## Forecast errors: ## ME: 5.753; MAE: 13.859; RMSE: 17.531 ## sCE: 26.298%; Asymmetry: 50.5%; sMAE: 5.28%; sMSE: 0.446% ## MASE: 0.575; RMSSE: 0.56; rMAE: 0.182; rRMSE: 0.17 Notice that because we fit the additive seasonal model to the data with multiplicative seasonality, the smoothing parameter \\(\\gamma\\) has become quite big - the seasonal component needs to be updated in order to keep up for the changing seasonal profile. In addition, because we used holdout parameter, the function now also reports the errors for the point forecasts on that test part of data. This can be useful, when you want to compare the performance of several models on a time series. Here how the forecast from ETS(A,A,A) looks on this data: plot(forecast(adamModel,h=12,interval=&quot;prediction&quot;)) While the fit to the data is far from perfect, due to a pure coincidence the point forecast from this model is quite decent. In order to see how the ADAM ETS decomposes the data into components, we can plot it via the plot() method with which parameter: plot(adamModel,which=12) We can see on this graph that the residuals still contain some seasonality in them, so there is a room for improvement. Most probably, this happened because the data exhibits multiplicative seasonality rather than the additive one. For now, we do not aim to fix this issue. "],["ADAMETSPureMultiplicative.html", "Chapter 9 Pure multiplicative ADAM ETS", " Chapter 9 Pure multiplicative ADAM ETS The pure multiplicative ETS implemented in ADAM framework can be formulated using logarithms, similar to how the pure additive ADAM ETS is formulated in (8.1): \\[\\begin{equation} \\begin{aligned} \\log y_t = &amp; \\mathbf{w}&#39; \\log(\\mathbf{v}_{t-\\boldsymbol{l}}) + \\log(1 + \\epsilon_{t}) \\\\ \\log \\mathbf{v}_{t} = &amp; \\mathbf{F} \\log \\mathbf{v}_{t-\\boldsymbol{l}} + \\log(\\mathbf{1}_k + \\mathbf{g} \\epsilon_t) \\end{aligned}, \\tag{9.1} \\end{equation}\\] where \\(\\mathbf{1}_k\\) is the vector of ones, containing \\(k\\) elements (number of components in the model), \\(\\log\\) is the natural logarithm, applied element-wise to the vectors and all the other values have been discussed in the previous sections. An example of a pure multiplicative model is ETS(M,M,M), for which we have the following values: \\[\\begin{equation} \\begin{aligned} \\mathbf{w} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, &amp; \\mathbf{F} = \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}, &amp; \\mathbf{g} = \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix}, \\\\ \\mathbf{v}_{t} = \\begin{pmatrix} l_t \\\\ b_t \\\\ s_t \\end{pmatrix}, &amp; \\boldsymbol{l} = \\begin{pmatrix} 1 \\\\ 1 \\\\ m \\end{pmatrix}, &amp; \\mathbf{1}_k = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\end{aligned}. \\tag{9.2} \\end{equation}\\] By inserting these values in the equation (9.1), we obtain model in logarithms: \\[\\begin{equation} \\begin{aligned} \\log y_t = &amp; \\log l_{t-1} + \\log b_{t-1} + \\log s_{t-m} + \\log \\left(1 + \\epsilon_{t} \\right) \\\\ \\log l_{t} = &amp; \\log l_{t-1} + \\log b_{t-1} + \\log( 1 + \\alpha \\epsilon_{t}) \\\\ \\log b_{t} = &amp; \\log b_{t-1} + \\log( 1 + \\beta \\epsilon_{t}) \\\\ \\log s_{t} = &amp; \\log s_{t-m} + \\log( 1 + \\gamma \\epsilon_{t}) \\\\ \\end{aligned} , \\tag{9.3} \\end{equation}\\] which after exponentiation becomes equal to the one, discussed in ETS Taxonomy section: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; l_{t-1} b_{t-1} s_{t-m} (1 + \\epsilon_t) \\\\ l_t = &amp; l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ b_t = &amp; b_{t-1} (1 + \\beta \\epsilon_t) \\\\ s_t = &amp; s_{t-m} (1 + \\gamma \\epsilon_t) \\end{aligned}. \\tag{9.4} \\end{equation}\\] An interesting observation is that the model (9.3) will produce values close to the model ETS(A,A,A) applied to the data in logarithms, when the values of smoothing parameters are close to zero. This becomes apparent, when recalling the limit: \\[\\begin{equation} \\lim\\limits_{x \\to 0}\\log(1+x) = x . \\tag{9.5} \\end{equation}\\] Based on that, the model will become close to the following one in cases of small values of smoothing parameters: \\[\\begin{equation} \\begin{aligned} \\log y_t = &amp; \\log l_{t-1} + \\log b_{t-1} + \\log s_{t-m} + \\epsilon_{t} \\\\ \\log l_{t} = &amp; \\log l_{t-1} + \\log b_{t-1} + \\alpha \\epsilon_{t} \\\\ \\log b_{t} = &amp; \\log b_{t-1} + \\beta \\epsilon_{t} \\\\ \\log s_{t} = &amp; \\log s_{t-m} + \\gamma \\epsilon_{t} \\\\ \\end{aligned} , \\tag{9.6} \\end{equation}\\] which is the ETS(A,A,A) applied to the data in logarithms. In many cases the smoothing parameters will be small enough for this limit to hold, so the two models will produce similar forecasts. As a result, model (9.6) can be used instead of (9.3) in these cases in order to get conditional moments and quantiles of distribution. "],["adamETSPuremultiplicativeRecursive.html", "9.1 Recursive relation", " 9.1 Recursive relation Similarly to how it was done for the pure additive model, we can show what the recursive relation will look like for the pure multiplicative model (the logic here is the same, the main difference is in working with logarithms instead of the original values): \\[\\begin{equation} \\begin{aligned} \\log y_{t+h} = &amp; \\mathbf{w}_{m_1}&#39; \\mathbf{F}_{m_1}^{\\lceil\\frac{h}{m_1}\\rceil-1} \\log \\mathbf{v}_{t} + \\mathbf{w}_{m_1}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{m_1}\\rceil-1} \\mathbf{F}_{m_1}^{j-1} \\log \\left(\\mathbf{1}_k + \\mathbf{g}_{m_1} \\epsilon_{t+m_1\\lceil\\frac{h}{m_1}\\rceil-j}\\right) + \\\\ &amp; \\mathbf{w}_{m_2}&#39; \\mathbf{F}_{m_2}^{\\lceil\\frac{h}{m_2}\\rceil-1} \\log \\mathbf{v}_{t} + \\mathbf{w}_{m_2}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{m_2}\\rceil-1} \\mathbf{F}_{m_2}^{j-1} \\log \\left(\\mathbf{1}_k + \\mathbf{g}_{m_2} \\epsilon_{t+m_2\\lceil\\frac{h}{m_2}\\rceil-j}\\right) + \\\\ &amp; \\dots \\\\ &amp; \\mathbf{w}_{m_d}&#39; \\mathbf{F}_{m_d}^{\\lceil\\frac{h}{m_d}\\rceil-1} \\log \\mathbf{v}_{t} + \\mathbf{w}_{m_d}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{m_d}\\rceil-1} \\mathbf{F}_{m_d}^{j-1} \\log \\left(\\mathbf{1}_k + \\mathbf{g}_{m_d} \\epsilon_{t+m_d\\lceil\\frac{h}{m_d}\\rceil-j}\\right) + \\\\ &amp; \\log \\left(1 + \\epsilon_{t+h}\\right) \\end{aligned}. \\tag{9.7} \\end{equation}\\] In order to see how this recursion works, we can take the example of ETS(M,N,N), for which \\(m_1=1\\) and all the other frequencies are equal to zero: \\[\\begin{equation} y_{t+h} = \\exp\\left(\\mathbf{w}_{1}&#39; \\mathbf{F}_{1}^{h-1} \\log\\mathbf{v}_{t} + \\mathbf{w}_{1}&#39; \\sum_{j=1}^{h-1} \\mathbf{F}_{1}^{j-1} \\log \\left(\\mathbf{1}_k + \\mathbf{g}_{1} \\epsilon_{t+h-j}\\right) +\\log \\left(1 + \\epsilon_{t+h}\\right)\\right) , \\tag{9.8} \\end{equation}\\] or after inserting \\(\\mathbf{w}_{1}=1\\), \\(\\mathbf{F}_{1}=1\\), \\(\\mathbf{v}_{t}=l_t\\), \\(\\mathbf{g}_{1}=\\alpha\\) and \\(\\mathbf{1}_k=1\\): \\[\\begin{equation} y_{t+h} = l_t \\prod_{j=1}^{h-1} \\left(1 + \\alpha \\epsilon_{t+h-j}\\right) \\left(1 + \\epsilon_{t+h}\\right) . \\tag{9.9} \\end{equation}\\] This recursion is useful in order to understand how the states evolve over time, but in general it cannot be used for the calculation of moments, as the one for the pure additive ADAM ETS. "],["pureMultiplicativeExpectationAndVariance.html", "9.2 The problem with moments in pure multiplicative ETS", " 9.2 The problem with moments in pure multiplicative ETS The recursion (9.7) obtained in the previous subsection shows how the logarithms of states are influenced by the previous values. While it is possible to calculate the expectation of the logarithm of the variable \\(y_{t+h}\\), in general this does not allow deriving the expectation of the variable in the original scale. This is because of the convolution of terms \\(\\log(\\mathbf{1}_k + \\mathbf{g} \\epsilon_{t+j})\\) for different \\(j\\). In order to better understand this issue, we consider this element for the ETS(M,N,N) model: \\[\\begin{equation} \\log(1+\\alpha\\epsilon_t) = \\log(1-\\alpha + \\alpha(1+\\epsilon_t)). \\tag{9.10} \\end{equation}\\] Whatever we assume about the distribution of the variable \\((1+\\epsilon_t)\\), the distribution of (9.10) will be more complicated than needed. For example, if we assume that \\((1+\\epsilon_t)\\sim\\mathrm{log}\\mathcal{N}(0,\\sigma^2)\\), then the distribution of (9.10) is something like exp three-parameter log normal distribution (Sangal and Biswas, 1970). The convolution of (9.10) for different \\(t\\) does not follow a known distribution, so it is not possible to calculate the conditional expectation and variance based on (9.7). Similar issues arrise if we assume any other distribution. The problem is worsened in case of multiplicative trend and / or multiplicative seasonality models, because then the recursion (9.7) contains several errors on the same observation (e.g. \\(\\log(1+\\alpha\\epsilon_t)\\) and \\(\\log(1+\\beta\\epsilon_t)\\)). The only way to derive the conditional expectation and variance for the pure multiplicative models is to use the formalue in the ETS Taxonomy and manually derive the values in the original scale. This works well only for the ETS(M,N,N) model, for which it is possible to take conditional expectation and variance of the recursion (9.9) in order to obtain: \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = \\mathrm{E}(y_{t+h}|t) = &amp; l_{t} \\\\ \\mathrm{V}(y_{t+h}|t) = &amp; l_{t}^2 \\left( \\left(1+ \\alpha^2 \\sigma^2 \\right)^{h-1} (1 + \\sigma^2) - 1 \\right), \\end{aligned} \\tag{9.11} \\end{equation}\\] where \\(\\sigma^2\\) is the variance of the error term. For the other models, the conditional moments do not have a general closed forms because of the product of \\(\\log(1+\\alpha\\epsilon_t)\\), \\(\\log(1+\\beta\\epsilon_t)\\) and \\(\\log(1+\\gamma\\epsilon_t)\\). It is still possible to derive the moments for special cases of \\(h\\), but this is a tedious process. In order to see that, we demonstrate here how the recursion looks for ETS(M,Md,M) model: \\[\\begin{equation} \\begin{aligned} &amp; y_{t+h} = l_{t+h-1} b_{t+h-1}^\\phi s_{t+h-m} \\left(1 + \\epsilon_{t+h} \\right) = \\\\ &amp; l_{t} b_{t}^{\\sum_{j=1}^h{\\phi^j}} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\prod_{j=1}^{h-1} \\left( (1 + \\alpha \\epsilon_{t+j}) \\prod_{i=1}^{j} (1 + \\beta \\epsilon_{t+i})^{\\phi^{j-i}} \\right) \\prod_{j=1}^{\\lceil\\frac{h}{m}\\rceil} \\left(1 + \\gamma \\epsilon_{t+j}\\right) \\left(1 + \\epsilon_{t+h} \\right) . \\end{aligned} \\tag{9.12} \\end{equation}\\] In general the conditional expectation of the recursion (9.12) does not have a simple form, because of the difficulties in calculating the expectation of \\((1 + \\alpha \\epsilon_{t+j})(1 + \\beta \\epsilon_{t+i})^{\\phi^{j-i}}(1 + \\gamma \\epsilon_{t+j})\\). In a simple example of \\(h=2\\) and \\(m&gt;h\\) the conditional expectation can be simplified to: \\[\\begin{equation} \\mu_{y,t+2} = l_{t} b_{t}^{\\phi+\\phi^2} \\left(1 + \\alpha \\beta \\sigma^2 \\right), \\tag{9.13} \\end{equation}\\] introducing the second moment, the variance of the error term \\(\\sigma^2\\). The case of \\(h=3\\) implies the appearance of the third moment, the \\(h=4\\) - the fourth etc. This is the reason, why there are no closed forms for the conditional moments for the pure multiplicative models with trend and / or seasonality. In some special cases, when smoothing parameters and the variance of error term are low, it is possible to use approximate formulae proposed by Hyndman et al. (2008), and in a special case, when all smoothing parameters are equal to zero or when \\(h=1\\), it is also possible to use the point forecast formulae from the ETS Taxonomy. But in general, the best thing that can be done in this case is the simulation of possible paths from the respective ETS model (using the formulae from the taxonomy) and then calculation of mean and variance based on them. In general, it can be shown that: \\[\\begin{equation} \\hat{y}_{t+h} \\leq \\check{y}_{t+h} \\leq \\mu_{y,t+h} , \\tag{9.14} \\end{equation}\\] where \\(\\mu_{y,t+h}\\) is the conditional h steps ahead expectation, \\(\\check{y}_{t+h}\\) is the conditional h steps ahead geometric expectation (expectation in logarithms) and \\(\\hat{y}_{t+h}\\) is the point forecast (Svetunkov and Boylan, 2020a). References "],["stabilityConditionMultiplicativeError.html", "9.3 Smoothing parameters bounds", " 9.3 Smoothing parameters bounds Similar to the pure additive ADAM ETS, it is possible to have different types of bounds, including the classical, the usual and the admissible ones. However, in case of pure multiplicative models, the classical and the usual restrictions become more reasonable from the point of view of the model itself, while the derivation of admissible bounds becomes a challenging task. In order to see the former, consider the ETS(M,N,N) model, for which the level is updated using the following relation: \\[\\begin{equation} l_t = l_{t-1} (1 + \\alpha\\epsilon_t) = l_{t-1} (1-\\alpha + \\alpha(1+\\epsilon_t)). \\tag{9.15} \\end{equation}\\] As discussed previously, the main benefit of pure multiplicative models is in modelling positive data. So, it is reasonable to assume that \\((1 + \\epsilon_t)&gt;0\\), which then implies that the actual values will always be positive, and that each component of the model should also be positive. This means that \\(\\alpha(1 + \\epsilon_t)&gt;0\\), which implies that \\((1-\\alpha + \\alpha(1+\\epsilon_t))&gt;1-\\alpha\\) or equivalently based on (9.10) \\((1 + \\alpha\\epsilon_t)&gt;1-\\alpha\\) should always hold. Now in order for the model to make sense, the condition \\((1 + \\alpha\\epsilon_t)&gt;0\\) should hold as well, ensuring that the level is always positive. This leads to the following set of inequalities: \\[\\begin{equation} \\begin{aligned} (1 + \\alpha\\epsilon_t)&gt; &amp;0 \\\\ (1 + \\alpha\\epsilon_t)&gt; &amp;1-\\alpha \\end{aligned} . \\tag{9.16} \\end{equation}\\] This can only be satisfied in the case of \\(1-\\alpha\\geq0\\) or \\(\\alpha\\leq1\\). Another bounds can be obtained by analysing the equation (9.15) and using the restriction for positivity of its elements: \\((1-\\alpha + \\alpha(1+\\epsilon_t))&gt;0\\) which can only be achieved, when \\((1+\\epsilon_t)&gt;\\frac{1-\\alpha}{\\alpha}\\), leading to another two inequalities: \\[\\begin{equation} \\begin{aligned} (1 + \\epsilon_t)&gt; &amp;0 \\\\ (1 + \\epsilon_t)&gt; &amp;\\frac{1-\\alpha}{\\alpha} \\end{aligned} , \\tag{9.17} \\end{equation}\\] which can be satisfied only when \\(\\alpha\\geq0\\), because, as we have already shown, the condition \\(1-\\alpha\\geq0\\) should hold. So, in general the bounds \\([0, 1]\\) guarantee that the model ETS(M,N,N) will produce positive values only. The two special cases \\(\\alpha=0\\) and \\(\\alpha=1\\) make sense, because the level in (9.15) will be positive in this case, implying that for the former the model becomes equivalent to the global level, while for the latter the model is equivalent to Random Walk. Using similar logic, it can be shown that the classical restriction \\(\\alpha, \\beta, \\gamma \\in [0, 1]\\) guarantees that the model will always produce positive values. The more restrictive condition of the usual bounds, discussed in Parameters Bounds section makes sense as well, although it might be more restrictive than needed, but it has a different idea: guaranteeing that the model exhibits averaging properties. Finally, the admissible bounds might still make sense for the pure multiplicative models, but the condition for parameters bounds becomes more complicated and implies that the distribution of the error term becomes trimmed from below in order to satisfy (9.16) and (9.17). Very crudely, the conventional restriction from pure additive models can be used to get an approximation to the proper admissible bounds, given the limit (9.5), but this should be used with care, given the discussion above. "],["ADAMETSMultiplicativeDistributions.html", "9.4 Distributional assumptions in pure multiplicative ETS", " 9.4 Distributional assumptions in pure multiplicative ETS The conventional assumption for the error term in ETS is that \\(\\epsilon_t\\sim\\mathcal{N}(0,\\sigma^2)\\), which guarantees that the conditional expectation of the model will be equal to the point forecasts, when the trend and seasonal components are not multiplicative. In general, ETS works well in many cases with this assumption, mainly when the data is strictly positive and the level of series is high (e.g. thousands of units). However, when dealing with lower level data, this assumption might become unhelpful, because the models may start generating non-positive values, which contradicts the idea of pure multiplicative ETS models. Akram et al. (2009) studied the ETS models with multiplicative error and suggested that applying ETS on data in logarithms is a better approach than just using ETS(M,Y,Y) models (here “Y” stands for non-additive components). However, this approach sidesteps the ETS taxonomy, creating a new group of models. An alternative (also discussed in Akram et al. (2009)) is to assume that the error term \\(1+\\epsilon_t\\) follows some distribution for positive data. The authors mentioned log Normal, truncated and Gamma distributions, but never explored them further. Svetunkov and Boylan (2020a) discussed several options for the distribution of the \\(1+\\epsilon_t\\) in ETS and came to conclusion that the most suitable distribution in this case is the Inverse Gaussian. Having said that, other distributions for positive data can be applied as well, but their usage might become complicated, because they need to meet condition \\(\\mathrm{E}(1+\\epsilon_t)=1\\) in order for the expectation to coincide with the point forecasts for models with non-multiplicative trend and seasonality. For example, if the error term follows log Normal distribution, then this restriction implies that the location of the distribution should be non-zero, based on (2.33): \\(1+\\epsilon_t\\sim\\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2},\\sigma^2\\right)\\). Based on that the following distributions are supported by ADAM: Inverse Gaussian: \\(\\left(1+\\epsilon_t \\right) \\sim \\mathcal{IG}(1, s)\\); Gamma: \\(\\left(1+\\epsilon_t \\right) \\sim \\mathcal{\\Gamma}(s^{-1}, s)\\); Log Normal: \\(\\left(1+\\epsilon_t \\right) \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\). The MLE of \\(s\\) in \\(\\mathcal{IG}\\) is straightforward and is: \\[\\begin{equation} \\hat{s} = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{e_{t}^2}{1+e_t} , \\tag{9.18} \\end{equation}\\] where \\(e_t\\) is the estimate of the error term \\(\\epsilon_t\\). However, when it comes to the MLE of scale parameter for the log Normal distribution with the aforementioned restrictions, it is more complicated and is (Svetunkov and Boylan, 2020a): \\[\\begin{equation} \\hat{\\sigma}^2 = 2\\left(1-\\sqrt{ 1-\\frac{1}{T} \\sum_{t=1}^{T} \\log^2(1+e_{t})}\\right). \\tag{9.19} \\end{equation}\\] Finally, MLE of \\(s\\) in \\(\\mathcal{\\Gamma}\\) does not have a closed form. Luckily, as discussed in section 2.5, method of moments can be used to obtain its value: \\[\\begin{equation} \\hat{s} = \\frac{1}{T} \\sum_{t=1}^{T} e_{t}^2 . \\tag{9.20} \\end{equation}\\] Even if we assume that we deal with strictly positive high level data and that \\(\\epsilon_t\\) can be non-positive, it is not necessary to limit the distribution with Normal only. The following distributions can be applied as well: Normal: \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\), implying that \\(y_t = \\mu_t (1+\\epsilon_t) \\sim \\mathcal{N}(\\mu_t, \\mu_t^2 \\sigma^2)\\); Laplace: \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\), meaning that \\(y_t = \\mu_t (1+\\epsilon_t) \\sim \\mathcal{Laplace}(\\mu_t, \\mu_t s)\\); S: \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\), so that \\(y_t = \\mu_t (1+\\epsilon_t) \\sim \\mathcal{S}(\\mu_t, \\sqrt{\\mu_t} s)\\); Generalised Normal: \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\) and \\(y_t = \\mu_t (1+\\epsilon_t) \\sim \\mathcal{GN}(\\mu_t, \\mu_t^\\beta s)\\); Note that the MLE of scale parameters for these distributions will be calculated differently than in the case of pure additive models. For example, for the normal distribution it is: \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{1}{T}\\sum_{t=1}^T \\frac{y_t-\\hat{\\mu}_t}{\\hat{\\mu}_t} , \\tag{9.21} \\end{equation}\\] where the main difference from the additive error case arises from the measurement equation of the multiplicative error models: \\[\\begin{equation} y_t = \\mu_t (1+\\epsilon_t), \\tag{9.22} \\end{equation}\\] implying that \\[\\begin{equation} e_t = \\frac{y_t-\\hat{\\mu}_t}{\\hat{\\mu}_t}. \\tag{9.23} \\end{equation}\\] The estimates of scale can then be used in the estimation phase, when parameters are optimised via the maximisation of respective log-likelihood function. References "],["ADAMETSMultiplicativeExamples.html", "9.5 Examples of application", " 9.5 Examples of application 9.5.1 Non-seasonal data We continue our examples with the same Box-Jenkins sales case by fitting the ETS(M,M,N) model, but this time with a holdout of 10 observations: adamModel &lt;- adam(BJsales, &quot;MMN&quot;, h=10, holdout=TRUE) adamModel ## Time elapsed: 0.03 seconds ## Model estimated using adam() function: ETS(MMN) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 245.3795 ## Persistence vector g: ## alpha beta ## 0.9986 0.2431 ## ## Sample size: 140 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 135 ## Information criteria: ## AIC AICc BIC BICc ## 500.7590 501.2068 515.4672 516.5736 ## ## Forecast errors: ## ME: 3.211; MAE: 3.325; RMSE: 3.778 ## sCE: 14.1%; Asymmetry: 91.6%; sMAE: 1.46%; sMSE: 0.028% ## MASE: 2.813; RMSSE: 2.478; rMAE: 0.924; rRMSE: 0.92 plot(adamModel,7) Note that the function produces the point forecast in this case, which is not equivalent to the conditional expectation! Also, the default distribution for the multiplicative erro models is \\(\\mathcal{IG}\\). Similarly, to how it was done in the previous chapter, the output gives a general summary for the model. We can compare this model with the ETS(A,A,N) via information criteria if we want. For example, here are the AICc for the two models: # ETS(M,M,N) AICc(adamModel) ## [1] 501.2068 # ETS(A,A,N) AICc(adam(BJsales, &quot;AAN&quot;, h=10, holdout=TRUE)) ## [1] 497.3651 The comparison is fair, because both models were estimated via likelihood and both likelihoods are formulated correctly, without omitting any terms (e.g. ets() from forecast package omits the \\(-\\frac{T}{2} \\log\\left(2\\pi e \\frac{1}{T}\\right)\\) for convenience, which makes it incomparable with other models). In this example, it seems tha the pure additive model is more suitable for the data than the pure multiplicative one. Still, if we want to produce forecasts from the model, we can do it, using the same command as in the previous chapter: plot(forecast(adamModel,h=10,interval=&quot;prediction&quot;,level=0.95)) Note that, when we ask for “prediction” intervals, the forecast() function will automatically decide what to use: in case of pure additive model it will use analytical solutions, while in the other cases, it will use simulations. The point forecast obtained from forecast function corresponds to the conditional expectation and is calculated based on the simulations. This also means that it will differ slightly from one run of the function to another (reflecting the uncertainty in the error term), but the difference should be negligible. We can also compare the performance of ETS(M,M,N) with \\(\\mathcal{IG}\\) distribution and the conventional ETS(M,M,N), assuming normality: adamModelNormal &lt;- adam(BJsales, &quot;MMN&quot;, h=10, holdout=TRUE, distribution=&quot;dnorm&quot;) adamModelNormal ## Time elapsed: 0.02 seconds ## Model estimated using adam() function: ETS(MMN) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 245.388 ## Persistence vector g: ## alpha beta ## 0.9996 0.2429 ## ## Sample size: 140 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 135 ## Information criteria: ## AIC AICc BIC BICc ## 500.7759 501.2237 515.4841 516.5905 ## ## Forecast errors: ## ME: 3.211; MAE: 3.325; RMSE: 3.778 ## sCE: 14.1%; Asymmetry: 91.6%; sMAE: 1.46%; sMSE: 0.028% ## MASE: 2.813; RMSSE: 2.478; rMAE: 0.924; rRMSE: 0.92 which are quite similar on this specific example. 9.5.2 Seasonal data The AirPassengers data used in the previous chapter has (as we discussed) multiplicative seasonality. So, the ETS(M,M,M) model might be more suitable than the pure additive one that we used previously: adamModel &lt;- adam(AirPassengers, &quot;MMM&quot;, h=12, holdout=TRUE, silent=FALSE) adamModel ## Time elapsed: 0.15 seconds ## Model estimated using adam() function: ETS(MMM) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 468.2408 ## Persistence vector g: ## alpha beta gamma ## 0.7706 0.0101 0.0009 ## ## Sample size: 132 ## Number of estimated parameters: 17 ## Number of degrees of freedom: 115 ## Information criteria: ## AIC AICc BIC BICc ## 970.4815 975.8500 1019.4892 1032.5956 ## ## Forecast errors: ## ME: -4.365; MAE: 15.406; RMSE: 21.756 ## sCE: -19.955%; Asymmetry: -16.7%; sMAE: 5.869%; sMSE: 0.687% ## MASE: 0.64; RMSSE: 0.694; rMAE: 0.203; rRMSE: 0.211 Notice that the smoothing parameter \\(\\gamma=0\\) in this case, which reflects the idea that we deal with the data with multiplicative seasonality and apply the correct model. Comparing the information criteria (e.g. AICc) with the ETS(A,A,A), this model does a better job at fitting the data. The conditional expectation and prediction interval from this model are better as well: adamForecast &lt;- forecast(adamModel,h=12,interval=&quot;prediction&quot;) plot(adamForecast) If we want to calculate the error measures based on the conditional expectation, we can use the measures() function from greybox package in the following way: measures(adamModel$holdout,adamForecast$mean,actuals(adamModel)) ## ME MAE MSE MPE MAPE ## -4.161384146 15.411913050 472.044359765 -0.013281947 0.033235221 ## sCE sMAE sMSE MASE RMSSE ## -0.190240194 0.058713744 0.006850926 0.639924123 0.693428208 ## rMAE rRMSE rAME asymmetry sPIS ## 0.202788330 0.210985755 0.058473782 -0.152593410 1.862633777 And the plot of the time series decomposition according to ETS(M,M,M) is: plot(adamModel,12) It shows that the residuals are more random for the model than for the ETS(A,A,A), but there still might be some structure left. The autocorrelation and partial autocorrelation functions might help in understanding this better: par(mfcol=c(1,2)) plot(adamModel,10:11) The plot shows that there is still some correlation left in the residuals, which could be either due to pure randomness or due to the imperfect estimation of the model. Tuning the parameters of the optimiser or selecting a different model might solve the problem. Finally, just as an example, we can also fit the most complicated pure multiplicative model, ETS(M,Md,M): adam(AirPassengers, &quot;MMdM&quot;, h=12, holdout=TRUE, silent=FALSE) ## Time elapsed: 0.14 seconds ## Model estimated using adam() function: ETS(MMdM) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 467.3176 ## Persistence vector g: ## alpha beta gamma ## 0.7666 0.0059 0.0001 ## Damping parameter: 0.992 ## Sample size: 132 ## Number of estimated parameters: 18 ## Number of degrees of freedom: 114 ## Information criteria: ## AIC AICc BIC BICc ## 970.6353 976.6884 1022.5257 1037.3037 ## ## Forecast errors: ## ME: 7.548; MAE: 19.099; RMSE: 24.753 ## sCE: 34.508%; Asymmetry: 57.4%; sMAE: 7.276%; sMSE: 0.889% ## MASE: 0.793; RMSSE: 0.79; rMAE: 0.251; rRMSE: 0.24 which does not seem to be significantly better than ETS(M,M,M) on this specific time series. "],["ADAMETSMixedModels.html", "Chapter 10 Further ADAM ETS topics", " Chapter 10 Further ADAM ETS topics Hyndman et al. (2008) proposed five classes of ETS models, based on the types of their components: ANN; AAN; AAdN; ANA; AAA; AAdA; MNN; MAN; MAdN; MNA; MAA; MAdA; MNM; MAM; MAdM; MMN; MMdN; MMM; MMdM; ANM; AAM; AAdM; MMA; MMdA; AMN; AMdN; AMA; AMdA; AMM; AMdM The ets() model from forecast package supports only the Classes 1 - 4. The Class 5 models are not included in the function mainly because they have infinite variances, specifically on long horizons and when the data has low values. Indeed, when the level in one of these models becomes close to zero, there is an increased chance of breaking the model due to the appearance of negative values (think of ETS(A,A,M) model, which might have a negative trend, leading to negative values, which are then multiplied by the positive seasonal indices). That is why in practice these models should only be used, when the level of the series is high. Furthermore, some of the models from the Class 5 are very difficult to estimate and are very sensitive to the smoothing parameters values. All of this gives a rationale for restricting the pool of models to the 19 from classes 1 - 4. Hyndman et al. (2008) demonstrate that models from the class 2 have closed forms of conditional expectation and variance, with the former corresponding to the point forecasts. However, the conditional distribution from these models is not Gaussian, so there are no formulae for the prediction intervals from these models. Yes, in some cases Normal distribution might be a fine approximation for the real one, but in general simulations should be preferred. Class 3 models suffer from similar issues, but the situation worsens: there are no analytical solutions for the conditional mean and variance, and there are only approximations to these statistics. Class 4 models have been discussed in the previous section. To be fair, any mixed model can potentially break, when the level of series is close to zero. For example, ETS(M,A,N) can have the negative trend, which might lead to the negative level and as a result to the multiplication of pure positive error term by the negative components. Estimating such a model on real data becomes a non-trivial task. In addition, as discussed above, simulations are typically needed in order to get adequate estimates of prediction interval for models of classes 2 - 5 and conditional mean and variance for models fo classes 4 - 5. All of this in my opinion means that the more useful classification of ETS models is the following: Pure additive models: ANN; AAN; AAdN; ANA; AAA; AAdA; Pure multiplicative models: MNN; MMN; MMdN; MNM; MMM; MMdM; Mixed models with non-multiplicative trend: MAN; MAdN; MNA; MAA; MAdA; MAM; MAdM; ANM; AAM; AAdM; Mixed models with multiplicative trend: MMA; MMdA; AMN; AMdN; AMA; AMdA; AMM; AMdM; The main idea behind the split to (3) and (4) is because the presence of multiplicative trend makes it almost impossible to derive the formulae for the conditional moments of distribution. So this class of models can be considered as the most challenging. adam() function supports all 30 ETS models, but you should keep in mind the limitations of some of them discussed in this section. References "],["ADAMETSMixedModelsGroup3.html", "10.1 Mixed models with non-multiplicative trend", " 10.1 Mixed models with non-multiplicative trend In this class of models, there are two subclasses: one with non-multiplicative seasonal component (MAN, MAdN, MNA, MAA, MAdA) and another one with the multiplicative one (MAM; MAdM; ANM; AAM; AAdM). The conditional mean for the former models coincides with the point forecasts, while the conditional variance can be calculated using the following recursive formula (Hyndman et al., 2008): \\[\\begin{equation} \\begin{aligned} &amp; \\text{V}(y_{t+h}|t) = (1+\\sigma^2) \\xi_h - \\mu_{t+h|t}^2 \\\\ &amp; \\text{where } \\xi_{1} = \\mu_{t+1|t}^2 \\text{ and } \\xi_h = \\mu_{t+h|t}^2 + \\sigma^2 \\sum_{j=1}^{h-1} c_{j}^2 \\xi_{h-j} \\end{aligned} , \\tag{10.1} \\end{equation}\\] where \\(\\sigma^2\\) is the variance of the error term. As for the second subgroup, the conditional mean corresponds to the point forecasts, when the forecast horizon is less than or equal to the seasonal frequency of the component (i.e. \\(h\\leq m\\)), and there is a cumbersome formula for calculating the conditional mean to some of models in this subgroup for the \\(h&gt;m\\). When it comes to the conditional variance, there exists the formula for some of models in the second subgroup, but they are cumbersome as well. The interested reader is directed to Hyndman et al. (2008), page 85. When it comes to the parameters bounds for the models in this group, the first subgroup of models should have similar bounds to the ones for the respective additive error models (because both underly the same exponential smoothing methods), but with additional restrictions, comming from the multiplicative error. The traditional bounds (aka “usual”) should work fine for these models for the same reasons they work for the pure additive models, although they might be too restrictive in some cases; The admissible bounds for smoothing parameters for the models in this group might be too wide and violate the condition of \\((1+ \\alpha \\epsilon_t)&gt;0, (1+ \\beta \\epsilon_t)&gt;0, (1+ \\gamma \\epsilon_t)&gt;0\\), which is important in order not to break the models. The second subgroup is more challenging in terms of parameters bounds because of the multiplication of states by the seasonal components. References "],["ADAMETSMixedModelsGroup4.html", "10.2 Mixed models with multiplicative trend", " 10.2 Mixed models with multiplicative trend This is the most difficult class of models (MMA; MMdA; AMN; AMdN; AMA; AMdA; AMM; AMdM). These do not have analytical formulae for conditional moments, they are very sensitive to the values of smoothing parameters and may lead to explosive forecasting trajectories. So, in order to obtain conditional expectation, variance and prediction interval from the models of this classs, simulations should be used. One of the issues that can be encountered, when using these models is the explosive trajectories because of the multiplicative trend. As a result, when these models are estimated, it makes sense to set the initial value of trend to 1 or a lower value, so that the optimiser does not encounter difficulties in the calculations. Furthermore, the combination of components for the models in this class makes even less sense than the combination for the previous class of models. For example, the multiplicative trend assumes either the explosive growth or decay as shown on the following two figures: Assuming that either seasonal component, or the error term, or both will have exactly the same impact on the final value irrespective of the point of time is unrealistic for these situation. The more reasonable would be for the amplitude of seasonality to decrease together with the exponential decay of the trend and for the variance of the error term to do the same. But this means that we are talking about the pure multiplicative models, not the mixed ones. There is only one situation, where such mixed models could make sense: when the speed of change of the exponential trajectory is close to zero, meaning that the level of the series does not change rapidly and when the volume of the data is high. In this case the mixed models might perform well and even produce more accurate forecasts than the models from the other classes. When it comes to the parameters bounds, this is a mistery for the mixed models. This is because the recursive relations are complicated and calculating the discount matrix or anything like that becomes a challenging task. Using the usual bounds should still be okay, but keep in mind that these mixed models are typically not very stable, so from my experience the smoothing parameters should be as low as possible, assuming that the initial values guarantee a working model (not breaking at some of observations). "],["ADAMETSSeasonalNormalisation.html", "10.3 Normalisation of seasonal indices in ETS models", " 10.3 Normalisation of seasonal indices in ETS models One of the ideas arrising from time series decomposition, inheritted by the conventional ETS, is the idea of renormalisation of seasonal indices. It comes to one of the two principles, depending on the type of seasonality: If the model has additive seasonality, then the seasonal indices should add up to zero in a specific period of time, e.g. monthly indices should add up to 0 over a yearly period; If the model has multiplicative seasonality, then the geometric mean of seasonal indices over a specific period should be equal to one. in the conventional ETS is substituted by “the arithmetic mean of multiplicative indices should be equal to one,” which does not have good grounds behind it - if we deal with multiplicative effect, then the geometric mean should be used, not the arithmetic one, otherwise by multiplying components by indices we introduce bias in the model. While the normalisation is a natural element of the time series decomposition and works fine for the initial seasonal indices, renormalising the seasonal indices over time might not be a natural idea for the ETS. Hyndman et al. (2008) discuss different mechanisms for the renormalisation of seasonal indices, which as the authors claim are needed in order to make the principles (1) and (2) hold from period to period in the data. However, I argue that this is an unnatural idea for the ETS models, that the indices should only be normalised during the initialisation of the model (at the moment \\(t=0\\)) and that they should vary independently for the rest of the sample. The rationale for this comes from the model itself. To illustrate it, I will use ETS(A,N,A), but the idea can be easily used for any other ETS model with any types of components and any number of seasonal frequencies. Just a reminder, the model can be formulated as: \\[\\begin{equation} \\begin{aligned} y_t = &amp;l_{t-1} + s_{t-m} + \\epsilon_t \\\\ {l}_{t} = &amp;l_{t-1} + \\alpha\\epsilon_t \\\\ s_t = &amp;s_{t-m} + \\gamma\\epsilon_t \\end{aligned}. \\tag{10.2} \\end{equation}\\] Let’s assume that this is the true model for whatever data we have for whatever reasons. In this case the set of equations (10.2) tells us that the seasonal indices change over time, depending on the values of the smoothing parameter \\(\\gamma\\) and each specific values of \\(\\epsilon_t\\), which is assumed to be i.i.d. All seasonal indeces \\(s_{t+i}\\) in a specific period (e.g. monthly indices in a year) can be written down explicitly based on (10.2): \\[\\begin{equation} \\begin{aligned} s_{t+1} = &amp;s_{t+1-m} + \\gamma\\epsilon_{t+1} \\\\ s_{t+2} = &amp;s_{t+2-m} + \\gamma\\epsilon_{t+2} \\\\ \\vdots \\\\ s_{t+m} = &amp;s_{t} + \\gamma\\epsilon_{t+m} \\end{aligned}. \\tag{10.3} \\end{equation}\\] If this is how the data is “generated” and the seasonality evolves over time, then there is only one possibility, for the indices \\(s_{t+1}, s_{t+2}, \\dots, s_{t+m}\\) to add up to zero: \\[\\begin{equation} \\begin{aligned} &amp;s_{t+1}+ s_{t+2}+ \\dots+ s_{t+m} = 0 \\\\ &amp;s_{t+1-m}+ s_{t+2-m}+ \\dots+ s_{t} + \\gamma \\left(\\epsilon_{t+1}+ \\epsilon_{t+2}+ \\dots+ \\epsilon_{t+m}\\right) = 0 \\end{aligned}. \\tag{10.4} \\end{equation}\\] meaning that: the previous indices \\(s_{t+1-m}, s_{t+2-m}, \\dots, s_{t}\\) add up to zero and either \\(\\gamma=0\\), or the sum of error terms \\(\\epsilon_{t+1}, \\epsilon_{t+2}, \\dots, \\epsilon_{t+m}\\) is zero. Note that we do not consider the situation \\(s_{t+1-m}+ \\dots+ s_{t} = - \\gamma \\left(\\epsilon_{t+1}+ \\dots+ \\epsilon_{t+m}\\right)\\) as it does not make sense. The condition (a) can be considered reasonable if the previous indices are normalised. (b) means that the seasonal indices do not evolve over time. However, (c) implies that the error term is not independent, because \\(\\epsilon_{t+m} = -\\epsilon_{t+1}- \\epsilon_{t+2}- \\dots- \\epsilon_{t+m-1}\\), which violates one of the basic assumptions of the model, meaning that (10.4) cannot be considered as the “true” model anymore, as it omits some important elements. The other case, when each seasonal index is updated on each observation \\(t\\) (to make sure that the indices are normalised), does not make sense either. In this situation we have: \\[\\begin{equation} \\begin{aligned} &amp;s_t = s_{t-m} + \\gamma\\epsilon_t \\\\ &amp;s_{t-m+1}+ s_{t-m+2}+ \\dots+ s_{t-1} + s_{t} = 0 \\end{aligned}, \\tag{10.5} \\end{equation}\\] which can be rewritten as \\(s_{t-m} + \\gamma\\epsilon_t = -s_{t+1-m}- s_{t+2-m}- \\dots- s_{t-1}\\), meaning that: \\[\\begin{equation} \\begin{aligned} s_{t-m}+ s_{t+1-m}+ s_{t+2-m}+ \\dots+ s_{t-1} = -\\gamma\\epsilon_t \\end{aligned}, \\tag{10.5} \\end{equation}\\] but due to the normalisation, the sum on the left hand side should be equal to zero, implying that either \\(\\gamma=0\\) or \\(\\epsilon_t=0\\). While former might hold in some cases (deterministic seasonality), the latter cannot hold for all \\(t=1,\\dots,T\\) and violates the assumptions of the model. The discussion in this section demonstrate that the renormalisation of seasonal indices is unnatural for the ETS model and should not be used. Having said that, this does not imply that the initial seasonal indices (those that correspond to the observation \\(t=0\\)) cannot be normalised. In fact, the normalisation of the initial indices allows reducing the number of estimated parameters in the model. References "],["ARIMA.html", "Chapter 11 Conventional ARIMA", " Chapter 11 Conventional ARIMA Another important dynamic element in ADAM is ARIMA model (developed originally by Box and Jenkins, 1976). ARIMA stands for “AutoRegressive Integrated Moving Average,” although the name does not tell much on its own and needs additional explanation, which will be provided in the next sections. The main idea of the model is that the data might have dynamic relations over time, where the new values depend on the values on the previous observations. This becomes more obvious in case of engineering systems and modelling phisical processes. For example, Box and Jenkins (1976) give an example of a series of CO\\(_2\\) output of a furnace, when the input gas rate changes. In this case, the elements of ARIMA process are natural, as the CO\\(_2\\) cannot just drop to zero, when the gas is switched off - it will leave the furnace, in reducing quantity over time (i.e. leaving \\(\\phi_1\\times100\\%\\) of CO\\(_2\\) in the next minute, where \\(\\phi_1\\) is a parameter in the model). Another example, where AR processes are natural is the temperature in the room, measured with 5 minutes intervals. In this case the temperature at 5:30pm will depend on the one at 5:25pm (if the temperature outside the room is lower, then it will go down slightly due to the loss of heat). So, in these examples, ARIMA model can be considered as a true model, but when it comes to time series from the social or business domains, it becomes very difficult to motivate the usage of ARIMA from the from the modelling point of view. For example, the demand on products does not reproduce itself and in real live does not depend on the demand on previous observations, unless we are talking about a repetitive purchases by the same group of consumers. So, if we construct ARIMA for such process, we turn blind eye to the fact that the observed time series relations in the data are most probably spurious. At best, ARIMA in this case can be considered as a very crude approximation of a complex true process (demand is typically influenced by price changes, consumer behaviour and promotional activities). Thus, whenever we work with ARIMA models in social or business domains, we should keep in mind that they are wrong even from the philosophical point of view. Nevertheless, they still can be useful, which is why we discuss them in this chapter. Note that this is a heavy mathematical chapter, and here we will discuss the main theoretical properties of ARIMA processes (i.e. what would happen if the data indeed followed the specified model), moving to more practical aspects in the next chapter. References "],["ARIMAIntro.html", "11.1 Introduction to ARIMA", " 11.1 Introduction to ARIMA ARIMA contains several elements: AR(p) - the AutoRegressive part, showing how the variable is impacted by its values on the previous observations. It contains \\(p\\) lags. For example, the quantity of the liquid in a vessel with an opened tap on some observation will depend on the quantity on the previous steps. This analogy explains the idea of AR part of the model; I(d) - the number of differences \\(d\\) taken in the model (I stands for \"Integrated). Working with differences rather than with the original data means that we deal with changes and rates of changes, not with just values. Technically, differences are needed in order to make data stationary (i.e. with fixed expectation and variance, although there are different definitions of the term stationarity, see below); MA(q) - the Moving Average component, explaining how the variable is impacted by the previous white noise. It contains \\(q\\) lags. Once again, in technical systems, the idea that the random error can impact the value, has a relatively simple explanation. For example, when the liquid drips out of a vessel, we might not be able to predict the air fluctations, which would impact the flow and could be perceived as elements of random noise. This randomness might in turn impact the quantity of liquid in a vessel on a next observation, thus introducing the MA elements in the model. I intentionally do not provide ARIMA examples from the demand forecasting area, as these are much more difficult to motivate and explain than the examples from the more technical areas. Before we continue our discussion, we should define the term stationarity. There are two definitions in the literature, one refers to “strict stationarity,” while the other refers to the “weak stationarity”: Time series is said to be weak stationary, when its conditional expectation and variance are constant and the variance is finite for all times; Time series is strong stationary, when its unconditional joint probability distribution does not change over time. This automatically implies that all its moments are constant (i.e. the process is also weak stationary). The stationarity is essential in ARIMA context and also plays important role in regression analysis. If the series is not stationary, then it might be difficult to estimate its moments correctly using the conventional methods and in some cases it might be not possible to get the correct parameters at all (e.g. there is infinite combination of parameters that would produce the minimum of the selected loss function). In this case, the series is somehow transformed in order to make sure that the moments are finite and constant (e.g. take logarithms or do Box-Cox transform, take differences or detrend the series) and that the model becomes easier to identify. Note that in contrast with ARIMA, the ETS models are almost always all non-stationary and do not require for the series to be stationary. We will see the connection between the two approaches later in this chapter. 11.1.1 AR(p) We start with a simple AR(1) model, which is written as: \\[\\begin{equation} {y}_{t} = \\phi_1 y_{t-1} + \\epsilon_t , \\tag{11.1} \\end{equation}\\] where \\(\\phi_1\\) is the parameter of the model. This formula tells us that the value on the previous observation is carried out to the new one in the proportion of \\(\\phi_1\\). Typically, the parameter \\(\\phi_1\\) is restricted with the region (-1, 1), in order to make the model stationary, but very often in real life \\(\\phi_1\\) actually lies in (0, 1) region. If the parameter is equal to 1, then the model becomes equivalent to Random Walk. The forecast trajectory (conditional expectation several steps ahead) of this model would typically correspond to the exponentially declining curve. Here is a simple example in R of a very basic forecast from AR(1) with \\(\\phi_1=0.9\\): y &lt;- vector(&quot;numeric&quot;, 20) y[1] &lt;- 100 phi &lt;- 0.9 for(i in 2:length(y)){ y[i] &lt;- phi * y[i-1] } plot(y, type=&quot;l&quot;, xlab=&quot;horizon&quot;, ylab=&quot;Forecast&quot;) If for some reason we get \\(\\phi_1&gt;1\\), then the trajectory corresponds to exponential increase, becoming explosive, implying non-stationary behaviour. The model in this case becomes very difficult to work with, even if the parameter is close to one. So it is typically advised to restrict the parameter with stationarity region (we will discuss this in more detail later in this chapter). In general, it is possible to imagine the situation, when the value at the moment of time \\(t\\) would depend on several previous values, so the model AR(p) can be written as: \\[\\begin{equation} {y}_{t} = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t , \\tag{11.2} \\end{equation}\\] where \\(\\phi_i\\) is the parameters for the \\(i\\)-th lag of the model. So, the model assumes that the data on the recent observations is influenced by the \\(p\\) previous observations. The more lags we introduce in the model, the more complicated the forecasting trajectory becomes, potentially introducing harmonic behaviour. Here is an example of AR(3) model \\({y}_{t} = 0.9 y_{t-1} -0.7 y_{t-2} + 0.6 y_{t-3} + \\epsilon_t\\): y &lt;- vector(&quot;numeric&quot;, 30) y[1:3] &lt;- c(100, 75, 30) phi &lt;- c(0.9,-0.7,0.6) for(i in 4:30){ y[i] &lt;- phi[1] * y[i-1] + phi[2] * y[i-2] + phi[3] * y[i-3] } plot(y, type=&quot;l&quot;, xlab=&quot;horizon&quot;, ylab=&quot;Forecast&quot;) No matter what the forecast trajectory of AR model is, it will asymptotically converge to zero, as long as the model is stationary. 11.1.2 MA(q) Before discussing the “Moving Averages” model, we should acknowledge that the name is quite misleading, and that the model has nothing to do with Centred Moving Averages used in time series decomposition or Simple Moving Averages (average of several concequitive observation). The idea of the simplest MA(1) model can be summarised in the following mathematical way: \\[\\begin{equation} {y}_{t} = \\theta_1 \\epsilon_{t-1} + \\epsilon_t , \\tag{11.3} \\end{equation}\\] where \\(\\theta_1\\) is the parameter of the model, typically lying between (-1, 1), showing what part of the error is carried out to the next observation. Because of the conventional assumption that the error term has a zero mean (\\(\\mathrm{E}(\\epsilon_{t})=0\\)), the forecast trajectory of this model is just a straight line coinsiding with zero starting from the \\(h=2\\). For the one step ahead forecast we have: \\[\\begin{equation} \\mathrm{E}({y}_{t+1}|t) = \\theta_1 \\mathrm{E}(\\epsilon_{t}|t) + \\mathrm{E}(\\epsilon_{t+1}|t) = \\theta_1 \\epsilon_{t}. \\tag{11.4} \\end{equation}\\] But starting from \\(h=2\\) there are no observable error terms \\(\\epsilon_t\\), so all the values past that are equal to zero: \\[\\begin{equation} \\mathrm{E}({y}_{t+2}|t) = \\theta_1 \\mathrm{E}(\\epsilon_{t+1}|t) + \\mathrm{E}(\\epsilon_{t+2}|t) = 0. \\tag{11.5} \\end{equation}\\] So, the forecast trajectory for MA(1) model converges to zero, when \\(h&gt;1\\). More generally, MA(q) model is written as: \\[\\begin{equation} {y}_{t} = \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_q \\epsilon_{t-q} + \\epsilon_t , \\tag{11.6} \\end{equation}\\] where \\(\\theta_i\\) is the parameters for the \\(i\\)-th lag of the error term, which are typically restricted with the so called invertibility region (discussed in the next section). In this case, the model assumes that the recent observation is influenced by several errors on previous observations (your mistakes in the past will haunt you in the future). The more lags we introduce, the more complicated the model becomes. As for the forecast trajectory, it will reach zero, when \\(h&gt;q\\). 11.1.3 ARMA(p,q) Connection the models (11.2) and (11.6), we get the more complicated model, ARMA(p,q): \\[\\begin{equation} {y}_{t} = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_q \\epsilon_{t-q} + \\epsilon_t , \\tag{11.7} \\end{equation}\\] which has the properties of the two models discussed above. The forecast trajectory from this model will have a combination of trajectories for AR and MA for \\(h \\leq q\\) and then will correspond to AR(p) for \\(h&gt;q\\). In order to simplify the work with ARMA models, the equation (11.7) is typically rewritten, by moving all terms with \\(y_t\\) to the left hand side: \\[\\begin{equation} {y}_{t} - \\phi_1 y_{t-1} - \\phi_2 y_{t-2} - \\dots - \\phi_p y_{t-p} = \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_q \\epsilon_{t-q} + \\epsilon_t . \\tag{11.8} \\end{equation}\\] Furthermore, in order to make this even more compact, the backshift operator B is introduced, which just shows by how much the subscript of the variable is shifted back in time: \\[\\begin{equation} {y}_{t} B^i = {y}_{t-i}. \\tag{11.9} \\end{equation}\\] Using (11.9), the ARMA model can be written as: \\[\\begin{equation} {y}_{t} (1 - \\phi_1 B - \\phi_2 B^2 - \\dots - \\phi_p B^p) = \\epsilon_t (1 + \\theta_1 B + \\theta_2 B^2 + \\dots + \\theta_q B^q) . \\tag{11.10} \\end{equation}\\] Finally, we can also introduce the AR and MA polynomial functions to make the model even more compact: \\[\\begin{equation} \\begin{aligned} &amp; \\varphi^p(B) = 1 - \\phi_1 B - \\phi_2 B^2 - \\dots - \\phi_p B^p \\\\ &amp; \\vartheta^q(B) = 1 + \\theta_1 B + \\theta_2 B^2 + \\dots + \\theta_q B^q . \\end{aligned} \\tag{11.11} \\end{equation}\\] Inserting the functions (11.11) in (11.10) leads to the compact presentation of ARMA model: \\[\\begin{equation} {y}_{t} \\varphi^p(B) = \\epsilon_t \\vartheta^q(B) . \\tag{11.12} \\end{equation}\\] The model (11.12) can be considered as a compact form of (11.7). It is more difficult to understand and interpret, but easier to work with from mathematical point of view. In addition, this form permits introducing additional elements, which will be discussed later in this chapter. Coming back to the ARMA model (11.7), we might notice, that it assumes convergence to zero, the speed of which is regulated via the parameters. In fact, this implies that the data has the mean of zero, and ARMA becomes useful, when the data is somehow pre-processed, so that it is stationary and varies around zero. This means that if you work with non-stationary and / or with non-zero mean data, the pure AR / MA or ARMA will be inappropriate - some prior transformations are in order. 11.1.4 ARMA with constant One of the simpler ways to deal with the issue with zero forecasts is to introduce the constant (or intercept) in ARMA: \\[\\begin{equation} {y}_{t} = a_0 + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_p \\epsilon_{t-p} + \\epsilon_t \\tag{11.13} \\end{equation}\\] or \\[\\begin{equation} {y}_{t} \\varphi^p(B) = a_0 + \\epsilon_t \\vartheta^q(B) , \\tag{11.12} \\end{equation}\\] where \\(a_0\\) is the constant parameter, which in this case also works as the unconditional mean of the series. The forecast trajectory in this case would converge to \\(a_0\\) instead of zero, but with some minor differences from the ARMA without constant. For example, in case of ARMA(1,1) with constant we will have: \\[\\begin{equation} {y}_{t} = a_0 + \\phi_1 y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\epsilon_t . \\tag{11.14} \\end{equation}\\] The conditional expectation of \\(y_{t+h}\\) for \\(h=1\\) and \\(h=2\\) can be written as (based on the discussions in previous sections): \\[\\begin{equation} \\begin{aligned} &amp; \\mathrm{E}({y}_{t+1}|t) = a_0 + \\phi_1 y_{t} + \\theta_1 \\epsilon_{t} \\\\ &amp; \\mathrm{E}({y}_{t+2}|t) = a_0 + \\phi_1 \\mathrm{E}(y_{t+1}|t) = a_0 + \\phi_1 a_0 + \\phi_1^2 y_{t} + \\phi_1 \\theta_1 \\epsilon_t \\end{aligned} , \\tag{11.15} \\end{equation}\\] or in general for some horizon \\(h\\): \\[\\begin{equation} \\mathrm{E}({y}_{t+h}|t) = \\sum_{j=1}^h a_0\\phi_1^{j-1} + \\phi_1^h y_{t} + \\phi_1^{h-1} \\theta_1 \\epsilon_{t} . \\tag{11.16} \\end{equation}\\] So, the forecast trajectory from this model dampens out, similar to the ETS(A,Ad,N) model, and the rate of dampening is regulated by the value of \\(\\phi_1\\). The following simple example demonstrates this point (I drop the MA(1) part because it does not change the shape of the curve): y &lt;- vector(&quot;numeric&quot;, 20) y[1] &lt;- 100 phi &lt;- 0.9 for(i in 2:length(y)){ y[i] &lt;- 100 + phi * y[i-1] } plot(y, type=&quot;l&quot;, xlab=&quot;horizon&quot;, ylab=&quot;Forecast&quot;) The more complicated ARMA(p,q) models with p&gt;1 will have more complicated trajectories with potential harmonics, but the idea of dampening in AR(p) part of the model stays. Finally, as alternative to adding \\(a_0\\), each actual value of \\(y_t\\) can be centred via \\(y^\\prime_t = y_t - \\bar{y}\\), making sure that the mean of \\(y^\\prime_t\\) is zero and ARMA can be applied to the \\(y^\\prime_t\\) data instead of \\(y_t\\). However, this approach introduces additional steps, but the result on stationary data is typically the same. 11.1.5 I(d) Based on the previous discussion, we can conclude that ARMA cannot be applied to non-stationary data. So, if we deal with one, we need to make it stationary somehow. The convetional way of doing that is by taking differences of the data. The logic behind this is straight forward: if the data is not stationary, then the mean somehow changes over time. This can be, for example, due to a trend in the data. In this case we should be taking about the change of variable \\(y_t\\) rather than the variable itself. So we should work on the following data instead: \\[\\begin{equation} \\Delta y_t = y_t - y_{t-1} = y_t (1 - B), \\tag{11.17} \\end{equation}\\] if the differences have constant mean. The simplest model with differences is I(1), which is also known as the Random walk: \\[\\begin{equation} \\Delta y_t = \\epsilon_t, \\tag{11.18} \\end{equation}\\] which can be reformulated in a simpler, more interpretable form by inserting (11.17) in (11.18) and regrouping elements: \\[\\begin{equation} y_t = y_{t-1} + \\epsilon_t. \\tag{11.19} \\end{equation}\\] The model (11.19) can also be perceived as AR(1) with \\(\\phi_1=1\\). This is a non-stationary model, meaning that the unconditional mean of \\(y_t\\) is not constant. The forecast from this model corresponds to the Na\"{i}ve method with straight line equal to the last observed actual value (again, assuming that \\(\\mathrm{E}(\\epsilon_{t})=0\\) and that other basic assumptions hold): \\[\\begin{equation} \\mathrm{E}(y_{t+h}|t) = \\mathrm{E}(y_{t+h-1}|t) + \\mathrm{E}(\\epsilon_{t+h}|t) = y_{t} . \\tag{11.20} \\end{equation}\\] Another simple model that relies on differences of the data is called Random Walk with drift and is formulated by adding constant \\(a_0\\) to the right hand side of equation (11.18): \\[\\begin{equation} \\Delta y_t = a_0 + \\epsilon_t. \\tag{11.21} \\end{equation}\\] This model has some similarities with the global level model, which is formulated via the actual value rather than differences: \\[\\begin{equation} {y}_{t} = a_0 + \\epsilon_t. \\tag{11.22} \\end{equation}\\] Using a similar regrouping as with the Random Walk, we can obtain a simpler form of (11.21): \\[\\begin{equation} y_t = a_0 + y_{t-1} + \\epsilon_t. \\tag{11.23} \\end{equation}\\] which is, again, equivalent to AR(1) model with \\(\\phi_1=1\\), but this time with a constant. The term “drift” appears because \\(a_0\\) acts as an additional element, showing what the tendecy in the data will be: if it is positive, the model will exhibit positive trend, if it is negative, the trend will be negative. This can be seen for the conditional mean, for example, for the case of \\(h=2\\): \\[\\begin{equation} \\mathrm{E}(y_{t+2}|t) = \\mathrm{E}(a_0) + \\mathrm{E}(y_{t+1}|t) + \\mathrm{E}(\\epsilon_{t+2}|t) = a_0 + \\mathrm{E}(a_0 + y_t + \\epsilon_t|t) = 2 a_0 + y_t , \\tag{11.24} \\end{equation}\\] or in general for the horizon \\(h\\): \\[\\begin{equation} \\mathrm{E}(y_{t+h}|t) = h a_0 + y_t . \\tag{11.25} \\end{equation}\\] In a similar manner we can also introduce second differences of the data (differences of differences) if we suspect that the change of variable over time is not stationary, which would be written as: \\[\\begin{equation} \\Delta^2 y_t = \\Delta y_t - \\Delta y_{t-1} = y_t - y_{t-1} - y_{t-1} + y_{t-2}, \\tag{11.26} \\end{equation}\\] which can also be written in a form using backshift operator: \\[\\begin{equation} \\Delta^2 y_t = y_t(1 - 2B + B^2) = y_t (1-B)^2. \\tag{11.27} \\end{equation}\\] In fact, we can introduce higher level differences if we want (but typically we should not) based on the idea of (11.27): \\[\\begin{equation} \\Delta^d = (1-B)^d. \\tag{11.28} \\end{equation}\\] Based on that, the I(d) model is formualted as: \\[\\begin{equation} \\Delta^d y_t = \\epsilon_t. \\tag{11.29} \\end{equation}\\] 11.1.6 ARIMA(p,d,q) Finally, having made the data stationary via the differences, we can introduce ARMA elements (11.12) to it which would be done on the differenced data, instead of the original \\(y_t\\): \\[\\begin{equation} y_t \\Delta^d(B) \\varphi^p(B) = \\epsilon_t \\vartheta^q(B) , \\tag{11.30} \\end{equation}\\] or in a more general form (11.10) with (11.27): \\[\\begin{equation} y_t (1-B)^d (1 - \\phi_1 B - \\dots - \\phi_p B^p) = \\epsilon_t (1 + \\theta_1 B + \\dots + \\theta_q B^q), \\tag{11.31} \\end{equation}\\] which is ARIMA(p,d,q) model. This model allows producing trends with some values of differences and also inherits the trajectories from both AR(p) and MA(q). This implies that the point forecasts from the model can exhibit quite complicated trajectories, depending on the values of parameters of the model. The model (11.31) is difficult to interpret in a general form, but opening the brackets and moving all elements but \\(y_t\\) to the right hand side typically helps in understanding of each specific model. 11.1.7 Parameters bounds ARMA models have two conditions that need to be satisfied in order for them to be useful and to work appropriately: Stationarity, Invertibility. The condition (1) has already been discussed earlier in this chapter, and is imposed on AR parameters of the model, making sure that the forecast trajectories do not exhibit explosive behaviour (in terms of both mean and variance). (2) is equivalent to the stability condition in ETS and refers to the MA parameters: it guarantees that the old observations do not have increasing impact on the recent ones. The term “invertibility” comes from the idea that any MA process can be represented as an infinite AR process via the inversion of the parameters. For example, MA(1) model, which is written as: \\[\\begin{equation} y_t = \\epsilon_t + \\theta_1 \\epsilon_{t-1} = \\epsilon_t (1 + \\theta_1 B) , \\tag{11.32} \\end{equation}\\] can be rewritten as: \\[\\begin{equation} y_t (1 + \\theta_1 B)^{-1} = \\epsilon_t, \\tag{11.33} \\end{equation}\\] or in a slightly easier to digest form (based on (11.32) and the idea that \\(\\epsilon_{t} = y_{t} - \\theta_1 \\epsilon_{t-1}\\), implying that \\(\\epsilon_{t-1} = y_{t-1} - \\theta_1 \\epsilon_{t-2}\\)): \\[\\begin{equation} y_t = \\theta_1 y_{t-1} - \\theta_1^2 \\epsilon_{t-2} + \\epsilon_t = \\theta_1 y_{t-1} - \\theta_1^2 y_{t-2} + \\theta_1^3 \\epsilon_{t-2} + \\epsilon_t = \\sum_{j=1}^\\infty -1^{j-1} \\theta_1^j y_{t-j} + \\epsilon_t. \\tag{11.34} \\end{equation}\\] The recursion in (11.34) shows that the recent actual value \\(y_t\\) depends on the previous infinite number of values of \\(y_{t-j}\\) for \\(j=\\{1,\\dots,\\infty\\}\\). The parameter \\(\\theta_1\\) in this case is exponentiated and leads to the distribution of weights in this infinite series in an exponential way (reminds SES, doesn’t it?). The invertibility condition makes sure that those weights decline over time with the increase of \\(j\\), so that the older observations do not have an increasing impact on the most recent \\(y_t\\). There are different ways how to check both conditions, the conventional of which is calculating the roots of the polynomial equations: \\[\\begin{equation} \\begin{aligned} &amp; \\varphi^p(B) = 0 \\text{ for AR} \\\\ &amp; \\vartheta^q(B) = 0 \\text{ for MA} \\end{aligned} , \\tag{11.35} \\end{equation}\\] or expanding the functions in (11.35) and substituting \\(B\\) with a variable \\(x\\) (for convenience): \\[\\begin{equation} \\begin{aligned} &amp; 1 - \\phi_1 x - \\phi_2 x^2 - \\dots - \\phi_p x^p = 0 \\text{ for AR} \\\\ &amp; 1 + \\theta_1 x + \\theta_2 x^2 + \\dots + \\theta_q x^q = 0 \\text{ for MA} \\end{aligned} . \\tag{11.36} \\end{equation}\\] Solving the first equation for \\(x\\) in (11.36), we get \\(p\\) roots (some of them might be complex numbers). In order for the model to be stationary all the roots need to be greater than one by absolute value. Similarly, if all the roots of the second equation in (11.36) are greater than one by absolute value, then the model is invertible (aka stable). A special case for both conditions is for the sums of parameters to lie between 0 and 1: \\[\\begin{equation} \\begin{aligned} &amp; 0 &lt; \\sum_{j=1}^p \\phi_j &lt; 1 \\\\ &amp; 0 &lt; \\sum_{j=1}^q \\theta_j &lt; 1 \\end{aligned} . \\tag{11.37} \\end{equation}\\] In this case, the respective stationarity and invertibility conditions are satisfied, which could be used in the model estimation (calculating roots of polynomials is a more difficult task). But note that the condition (11.37) is rather restrictive and not genuinly applicable for all ARIMA models. Finally, in a special case with AR(p) model, \\(0 &lt; \\sum_{j=1}^p \\phi_j &lt; 1\\) and \\(\\sum_{j=1}^p \\phi_j = 1\\), we end up with the moving weighted average, which is a non-stationary model. This becomes apparent from the connection between Simple Moving Average and AR processes (Svetunkov and Petropoulos, 2018). References "],["seasonal-arima.html", "11.2 Seasonal ARIMA", " 11.2 Seasonal ARIMA 11.2.1 Single seasonal ARIMA When it comes to the real data, we typically have not only relations between consequitive observations, but also between observations happening with some fixed lags. In ETS framework, these relations are taken care of via seasonal indices, which are repeated every \\(m\\) observations. In ARIMA framework, this is done via introducing lags in the elements of the model. For example, seasonal AR(P) with lag \\(m\\) can be written similar to AR(p), but with some minor modifications: \\[\\begin{equation} {y}_{t} = \\phi_{m,1} y_{t-m} + \\dots + \\phi_{m,P} y_{t-Pm} + \\varepsilon_t , \\tag{11.38} \\end{equation}\\] where \\(\\phi_{m,i}\\) is the parameter for the lagged \\(im\\) actual value in the model and \\(\\varepsilon_t\\) is the error term of the seasonal AR model. We use the underscore “m” just to show that the parameters here refer to the seasonal part of the model. This will be specifically useful, when we will merge the seasonal and non-seasonal parts of ARIMA. The idea of the model (11.38) on example of monthly data is that the current observation is influenced by the similar value, same month a year ago, then the same month two years ago etc. This is hard to justify from the theoretical point of view, but this model allows capturing complex relations in the data. Similarly to seasonal AR(P), we can have seasonal MA(Q): \\[\\begin{equation} {y}_{t} = \\theta_{m,1} \\varepsilon_{t-m} + \\dots + \\theta_{m,Q} \\varepsilon_{t-Qm} + \\varepsilon_t , \\tag{11.39} \\end{equation}\\] where \\(\\theta_{m,i}\\) is the parameter for the lagged error term in the model. This model is even more difficult to justify than the MA(q), because it is difficult to explain, how the white noise the same month last year can impact the actual value this year. Still, this is a useful instrument for forecasting purposes. Finally, we have the seasonal differences, I(D), which are easier to present using the backshift operator: \\[\\begin{equation} y_t (1-B^m)^D = \\varepsilon_t. \\tag{11.40} \\end{equation}\\] The seasonal differences allow dealing with the seasonality that changes its amplitude in the data, i.e. model the multiplicative seasonality via ARIMA by making the seasonality itself stationary. Combining (11.38), (11.39) and (11.40) we get pure seasonal ARIMA(P,D,Q)\\(_m\\) model in the compact notation, similar to the one we had for ARIMA(p,d,q): \\[\\begin{equation} y_t (1-B^m)^D (1 - \\phi_{m,1} B^m - \\dots - \\phi_{m,P} B^{Pm}) = \\varepsilon_t (1 + \\theta_{m,1} B^m + \\dots + \\theta_{m,Q} B^{Qm}), \\tag{11.41} \\end{equation}\\] or if we introduce the polynomial functions for seasonal AR and MA and use notation similar to (11.27): \\[\\begin{equation} y_t \\Delta^D(B^m) \\varphi^P(B^m) = \\varepsilon_t \\vartheta^Q(B^m), \\tag{11.42} \\end{equation}\\] where \\[\\begin{equation} \\begin{aligned} &amp; \\Delta^D(B^m) = (1-B^m)^D \\\\ &amp; \\varphi^P(B^m) = 1 - \\phi_{m,1} B^m - \\dots - \\phi_{m,P} B^{Pm} \\\\ &amp; \\vartheta^Q(B^m) = 1 + \\theta_{m,1} B^m + \\dots + \\theta_{m,Q} B^{Qm} . \\end{aligned} \\tag{11.43} \\end{equation}\\] Now that we have taken care of the seasonal part of the model, we should not forget that there is a non-seasonal part. If it is in the data, then \\(\\varepsilon_t\\) would not be just a white noise, but could be modelled using a non-seasonal ARIMA(p,d,q): \\[\\begin{equation} \\varepsilon_t \\Delta^d(B) \\varphi^p(B) = \\epsilon_t \\vartheta^q(B), \\tag{11.44} \\end{equation}\\] implying that: \\[\\begin{equation} \\varepsilon_t = \\epsilon_t \\frac{\\vartheta^q(B)}{\\Delta^d(B) \\varphi^p(B)}. \\tag{11.45} \\end{equation}\\] Inserting (11.45) into (11.42), we get the final SARIMA(p,d,q)(P,D,Q)\\(_m\\) model in the compact form after regrouping the polynomials: \\[\\begin{equation} y_t \\Delta^D(B^m) \\varphi^P(B^m) \\Delta^d(B) \\varphi^p(B) = \\epsilon_t \\vartheta^Q(B^m) \\vartheta^q(B) . \\tag{11.46} \\end{equation}\\] The equation (11.46) does not tell us much about what happens in the model, it just shows how different elements interact with each other in it. In order to understand, what SARIMA really means, we need to take an example and see what impacts the current actual value. For example, here what we will have in case of SARIMA(1,0,1)(1,0,1)\\(_4\\) (i.e. applied to quarterly data): \\[\\begin{equation} y_t \\Delta^0(B^4) \\varphi^1(B^4) \\Delta^0(B) \\varphi^1(B) = \\epsilon_t \\vartheta^1(B^4) \\vartheta^1(B) . \\tag{11.47} \\end{equation}\\] Inserting the values of polynomials (11.43), (11.28) and (11.11) in (11.47), we get: \\[\\begin{equation} y_t (1 - \\phi_{4,1} B^4)(1 - \\phi_{1} B) = \\epsilon_t (1 + \\theta_{4,1} B^4) (1 + \\theta_{1} B), \\tag{11.48} \\end{equation}\\] which is slightly easier to understand, but still does not explain how the past values impact the present one. So, we open the brackets and move all the elements except for \\(y_t\\) to the right hand side of the equation to get: \\[\\begin{equation} y_t = \\phi_{1} y_{t-1} + \\phi_{4,1} y_{t-4} - \\phi_{1} \\phi_{4,1} y_{t-5} + \\theta_1 \\epsilon_{t-1} + \\theta_{4,1} \\epsilon_{t-4} + \\theta_{1} \\theta_{4,1} \\epsilon_{t-5} + \\epsilon_t . \\tag{11.49} \\end{equation}\\] So, now we see that SARIMA(1,0,1)(1,0,1)\\(_4\\) implies that the present values is impacted by the value in the previous quarter, the value last year on the same quarter and the value from last year on the previous quarter, which introduces a much more complicated interaction than just an ETS model does. Howver, this complexity is obtained with a minimum number of parameters: we have three lagged actual values and three lagged error terms, but we only have four parameters to estimate, not six. The more complicated SARIMA models would have even more complicated interactions, making it more challenging to interpret the model, but all of that comes with a benefit of having a parsimonious model with just \\(p+q+P+Q\\) parameters to estimate. When it comes to forecasting from such model as SARIMA(1,0,1)(1,0,1)\\(_4\\), the forecasting trajectories would have elements of the classical ARMA model, discussed earlier, converging to zero as long as there is no constant and the model is stationary. The main difference would be in having the seasonal element. Here is an R example of a prediction for such a model for \\(h&gt;m+1\\) (MA part is dropped because the expectation of the error terms is assumed to be equal to zero): y &lt;- vector(&quot;numeric&quot;, 20) y[1:5] &lt;- c(97,87,85,94,95) phi &lt;- c(0.6,0.8) for(i in 6:length(y)){ y[i] &lt;- phi[1] * y[i-1] + phi[2] * y[i-4] - phi[1] * phi[2] * y[i-5] } plot(y, type=&quot;l&quot;, xlab=&quot;horizon&quot;, ylab=&quot;Forecast&quot;) As we see, the values converge to zero due to \\(0&lt;\\phi_1&lt;1\\) and the seasonality disappears because \\(0&lt;\\phi_{4,1}&lt;1\\) as well. So, this is the forecast implied by the SARIMA without differences. If the differences are introduced, then the model would produce non-stationary and seasonaly non-stationary trajectories. 11.2.2 SARIMA with constant In addition, it is possible to add the constant term to the SARIMA model, and it will have a more complex effect on the forecast trajectory, depending on the order of the model. In case of zero differences, the effect will be similar to ARMA, introducing the dampening trajectory, here is an example: y &lt;- vector(&quot;numeric&quot;, 100) y[1:5] &lt;- c(97,87,85,94,95) phi &lt;- c(0.6,0.8) for(i in 6:length(y)){ y[i] &lt;- phi[1] * y[i-1] + phi[2] * y[i-4] - phi[1] * phi[2] * y[i-5] + 8 } plot(y, type=&quot;l&quot;, xlab=&quot;horizon&quot;, ylab=&quot;Forecast&quot;) In case of the model with the differences, the constant would have a two-fold effect: working as a drift for the non-seasonal part and increasing the amplitude of seasonality for the seasonal one. Here is an example from SARIMA(1,0,0)(1,1,0)\\(_4\\) with constant: \\[\\begin{equation} y_t (1 - \\phi_{4,1} B^4)(1 - \\phi_{1} B) (1 - B^4) = \\epsilon_t + a_0 , \\tag{11.50} \\end{equation}\\] which can be reformulated as (after opening brackets and moving elements to the right hand side): \\[\\begin{equation} y_t = \\phi_{1} y_{t-1} + (1+\\phi_{4,1}) y_{t-4} + - (1+\\phi_{4,1}) \\phi_{1} y_{t-5} - \\phi_{4,1} y_{t-8} + \\phi_1 \\phi_{4,1} y_{t-9} + a_0 + \\epsilon_t . \\tag{11.51} \\end{equation}\\] This formula can then be used to see, what the trajectory from such model will be: y &lt;- vector(&quot;numeric&quot;, 100) y[1:9] &lt;- c(96,87,85,94,97,88,86,95,98) phi &lt;- c(0.6,0.8) for(i in 10:length(y)){ y[i] &lt;- phi[1] * y[i-1] + (1+phi[2]) * y[i-4] - (1+ phi[2]) *phi[1] * y[i-5] - phi[2] * y[i-8] + phi[1] * phi[2] * y[i-9] + 0.1 } plot(y, type=&quot;l&quot;, xlab=&quot;horizon&quot;, ylab=&quot;Forecast&quot;) As we see, the trajectory exhibits a drift, coming from the non-seasonal part of the model and a stable seasonality (the amplitude of which does not converge to zero anymore). More complex behaviours for the future trajectories can be obtained with higher orders of seasonal and non-seasonal parts of SARIMA model. 11.2.3 Multiple seasonal ARIMA Using the same approach as with the conventional SARIMA, we can introduce more terms (similar to how it was done by James W. Taylor, 2003b) with several seasonal frequencies. For example, we can have an hour of day, a day of week and a week of year frequencies in the data. Given that we work with the hourly data in this case, we should introduce three seasonal ARIMA elements with seasonalities \\(m_1=24\\), \\(m_2=24 \\times 7\\) and \\(m_3=24 \\times 7 \\times 365\\). In this example we would have AR, I and MA polynomials for each seasonal part of the model, introducing a triple seasonal ARIMA, which is not even easy to formulate in the compact form. This type of model with multiple seasonal components can be called “Multiple Seasonal ARIMA,” MSARIMA. In general, the compact form of the MSARIMA model can be written as: \\[\\begin{equation} y_t \\Delta^{D_n}(B^{m_n}) \\varphi^{P_n}(B^{m_n}) \\dots \\Delta^{D_0}(B^{m_0}) \\varphi^{P_0}(B^{m_0}) = \\epsilon_t \\vartheta^{Q_n}(B^{m_n}) \\dots \\vartheta^{Q_0}(B^{m_0}) , \\tag{11.52} \\end{equation}\\] where \\(n\\) is the number of seasonal cycles, and \\(D_0=d\\), \\(P_0=p\\), \\(Q_0=q\\) and \\(m_0=1\\) for convenience. The slightly more compact and even less comprehensible form of (11.52) is: \\[\\begin{equation} y_t \\prod_{j=0}^n \\Delta^{D_j} (B^{m_j}) \\varphi^{P_j}(B^{m_j}) = \\epsilon_t \\prod_{j=0}^n \\vartheta^{Q_j}(B^{m_j}) , \\tag{11.53} \\end{equation}\\] Conceptually, the model (11.53) is neat, as it captures all the complex relations in the data, but it is not easy to understand and work with, not to mention the potential estimation and order selection problems. In order to understand what the forecast from such model can be, we would need to take a special case, multiply the polynomials and move all the past elements on the right hand side, leaving only \\(y_t\\) on the left hand side one. We have done this before for some examples of ARIMA and SARIMA, so we will not repeat this exercise here. It is worth noting that msarima() function from smooth package implements the model (11.53), although not in this form, but in the state space form, discussed in the next chapter. 11.2.4 Parameters bounds for MSARIMA When it comes to parameters bounds of SARIMA, the logic stays quite similar to the process discussed for the case of non-seasonal model, with the only difference being that instead of analysing the polynomials of a specific part of a model, we need to consider the product of polynomials. So, the stationarity condition for the MSARIMA is for the roots of the following polynomial to be greater than one by absolute value (lie outside the unit circle): \\[\\begin{equation} \\prod_{j=0}^n \\varphi^{P_j}(B^{m_j}) = 0, \\tag{11.54} \\end{equation}\\] while the invertibility condition is for the roots of the following polynomial to lie outside the unit circle: \\[\\begin{equation} \\prod_{j=0}^n \\vartheta^{Q_j}(B^{m_j}) = 0. \\tag{11.55} \\end{equation}\\] Both of these conditions are difficult to check, especially for high frequencies \\(m_j\\): the polynomial equation of order \\(n\\) has \\(n\\) complex roots, so if you fit a multiple seasonal ARIMA on hourly data, where the maximum frequency is \\(24\\times 7\\times 365 = 61,320\\), then the equation will have at least 61,320 roots (this number will increase, if there are lower frequency or non-seasonal orders of the model). Finding all of them is not a trivial task even for modern computers (for example, polyroot() function from base package cannot handle this). So, when considering ARIMA on high frequency data with high seasonal frequency values, it might make sense to find other ways of checking the stationarity and stability conditions. The msarima() and adam() functions in smooth package use the state space form of ARIMA and rely on a slightly different principles of checking the same conditions, and they do that more efficiently than in the case of the convetional approach of finding the roots of polynomials (11.54) and (11.55). References "],["BJApproach.html", "11.3 Box-Jenkins approach", " 11.3 Box-Jenkins approach Know that we are more or less familiar with the idea of ARIMA models, we can move to practicalities. One of the issues with the model as it might become apparent from the previous sections, is the identification of orders p, d, q, P\\(_j\\), D\\(_j\\), Q\\(_j\\) etc. Back in the 20th century, when computers were slow, this was a very difficult task to do, so George Box and Gwilym Jenkins (Box and Jenkins, 1976) developed a methodology for the identification and estimation of ARIMA models. While nowadays there are more efficient ways of order selection for ARIMA, some of their principles are still used in time series analysis and in forecasting. We briefly outline the idea in this section, not purpoting to give the detailed explanation of the approach. 11.3.1 Identifying stationarity Before doing any sort of time series analysis, we need to make the data stationary, which in the context of ARIMA is done via the differences. But before doing anything, we need to understand, whether the data is stationary or not in the first place: over-differencing typically is harmful for the model and would lead to misspecification issues, while in case of under-differencing it might not be possible to correctly identify the model. There different ways of understanding, whether the data is stationary or not. The simples of them is just looking at the data: in some cases it becomes obvious that the mean of the data changes or that there is a trend in the data, so the conclusion would be relatively easy to make. If it is not stationary, then taking differences and analysing the differenced data again would be the way to go, just to make sure that the second differences are not needed. The more formal approach would be to conduct statistical tests, such as ADF (adf.test() from tseries package) or KPSS (kpss.test() from tseries package). Note that they test different hypotheses: In case of ADF it is H\\(_0\\): the data is not stationary; H\\(_1\\): the data is stationary; In case of KPSS, H\\(_0\\): the data is stationary; H\\(_1\\): the data is not stationary; I do not plan discussing, how the tests are conducted and what they imply, but it should suffice to say that ADF is based on estimating parameters of AR model and then testing the hypothesis for those parameters, while KPSS includes the component of Random Walk in a model (with potential trend) and checks, whether the variance of that component is zero or not. Both tests have their own advantages and disadvantages and sometimes might contradict each other. No matter, what test you choose, do not forget what testing a statistical hypothesis means: if you fail to reject H\\(_0\\), it does not mean that it is true. Note that even if you select the test-based appraoch, the procedure should be iterative as well: test the hypothesis, take differences if needed, test hypothesis again etc. This way we can determine the order of differences I(d). When you work with seasonal data, the situation becomes more complicated. Yes, you can probably spot seasonality doing a visual analysis of the data, but it is not easy to conclude, whether the seasonal differences are needed or not. Canova-Hansen test (ch.test() in uroot package) can be used in this case to formally test the hypothesis similar to the one in KPSS test. Only after making sure that the data is stationary, we can move to the analysis of the Autocorrelation and Partial Autocorrelation functions. 11.3.2 Autocorrelation function (ACF) In the core of the Box-Jenkins approach, lies the idea of autocorrelation and partial autocorrelation functions. Autocorrelation is the correlation (see Section 2.6.3) of a variable with itself from a different period of time. Here is an example of autocorrelation coefficient for lag 1: \\[\\begin{equation} \\rho(1) = \\frac{\\sigma_{y_t,y_{t-1}}}{\\sigma_{y_t}\\sigma_{y_{t-1}}} = \\frac{\\sigma_{y_t,y_{t-1}}}{\\sigma_{y_t}^2}, \\tag{11.56} \\end{equation}\\] where \\(\\rho(1)\\) is the “true” autocorrelation coefficient, \\(\\sigma_{y_t,y_{t-1}}\\) is the covariance between \\(y_t\\) and \\(y_{t-1}\\), while \\(\\sigma_{y_t}\\) and \\(\\sigma_{y_{t-1}}\\) are the “true” standard deviations of \\(y_t\\) and \\(y_{t-1}\\). Note that \\(\\sigma_{y_t}=\\sigma_{y_{t-1}}\\), because we are talking about one and the same variable, thus the simpler formula on the right hand side of (11.56). As you see, the formula (11.56) corresponds to the classical correlation coefficient, so the interpretation of this is the same as for the classical one: the value of \\(\\rho(1)\\) shows the closeness of the lagged relation to linear. If it is close to one, then this means that variable has a strong linear relation with itself on the previous observation. It obviously does not tell you anything about the causality, just shows a technical relation between variables, even if in the real life it is spurious. Using the formula (11.56), we can calculate the autocorrelation coefficients for other lags as well, just substituting \\(y_{t-1}\\) with \\(y_{t-2}\\), \\(y_{t-3}\\), \\(\\dots\\), \\(y_{t-\\tau}\\) etc. In a way, \\(\\rho(\\tau)\\) can be considered as a function of a lag \\(\\tau\\), which is in fact called “Autocorrelation function” (ACF). If we know the order of ARIMA process we deal with, then we can plot the values of ACF on y-axis, by changing the \\(\\tau\\) on x-axis. In fact, Box and Jenkins (1976) discuss different theoretical functions for several special cases of ARIMA, which we do not plan to fully repeat here. But, for example, they show that if you deal with AR(1) process, then the \\(\\rho(1)=\\phi_1\\), \\(\\rho(2)=\\phi_1^2\\) etc. This can be seen on the example of \\(\\rho(1)\\) by calculating the covariance: \\[\\begin{equation} \\sigma_{y_t,y_{t-1}} = \\mathrm{cov}(y_t,y_{t-1}) = \\mathrm{cov}(\\phi_1 y_{t-1} + \\epsilon_t, y_{t-1}) = \\mathrm{cov}(\\phi_1 y_{t-1}, y_{t-1}) = \\phi_1 \\sigma_{y_t}^2 , \\tag{11.57} \\end{equation}\\] which when inserted in (11.56) leads to \\(\\rho(1)=\\phi_1\\). The ACF for AR(1) with a positive \\(\\phi_1\\) will have the following shape (on the example of \\(\\phi_1=0.9\\)): Note that \\(\\rho(0)=1\\) just because the value is correlated with itself in this case, so lag 0 is typically dropped as not being useful. The declining shape of the ACF tells us that if \\(y_t\\) is correlated with \\(y_{t-1}\\), then the correlation between \\(y_{t-1}\\) and \\(y_{t-2}\\) will be exactly the same, also implying that \\(y_{t}\\) is somehow correlated with \\(y_{t-2}\\), even if there is no true correlation between them. In fact, it is difficult to say anything for AR process based on ACF exactly because of this temporal relation of the variable with itself. On the other hand, ACF can be used to judge the order of MA(q) process. For example, if we consider MA(1), then the \\(\\rho(1)\\) will depend on the following covariance: \\[\\begin{equation} \\sigma_{y_t,y_{t-1}} = \\mathrm{cov}(y_t,y_{t-1}) = \\mathrm{cov}(\\theta_1 \\epsilon_{t-1} + \\epsilon_t, \\theta_1 \\epsilon_{t-2} + \\epsilon_{t-1}) = \\mathrm{cov}(\\theta_1 \\epsilon_{t-1}, \\epsilon_{t-1}) = \\theta_1 \\sigma^2 , \\tag{11.58} \\end{equation}\\] where \\(\\sigma^2\\) is the variance of the error term. However, the covariance between \\(y_t\\) and \\(y_{t-2}\\) will be equal to zero for the pure MA(1) (given that the usual assumptions hold). In fact, Box and Jenkins (1976) showed that for the moving averages, ACF tells more about the order of the model than for the autoregressive one: ACF will drop rapidly right after the specific lag q for the MA(q) process. When it comes to seasonal models, in case of seasonal AR(P), ACF would decrease exponentially from season to season (e.g you would see a decrease on lags 4, 8, 12 etc for SAR(1) and \\(m=4\\)), while in case of seasonal MA(Q), ACF would drop abruptly, starting from the lag \\((Q+1)m\\) (so, the next seasonal lag from the one that the process has, e.g. on lag 8, if we deal with SMA(1) with \\(m=4\\)). 11.3.3 Partial autocorrelation function (PACF) The other instrument useful for the time series analysis with respect to ARIMA is called “partial ACF.” The idea of this follows from ACF directly. As we have spotted, if the process we deal with follows AR(1), then \\(\\rho(2)=\\phi_1^2\\) just because of the temporal element. In order to get rid of this temporal effect, when calculating the correlation between \\(y_t\\) and \\(y_{t-2}\\) we could remove the correlation \\(\\rho(1)\\) in order to get the clean effect of \\(y_{t-2}\\) on \\(y_t\\). This type of correlation is called “partial” and we will denote it as \\(\\varrho(\\tau)\\). There are different ways how to do that, one of the simplest is to estimate the following regression model: \\[\\begin{equation} y_t = a_1 y_{t-1} + a_2 y_{t-2} + \\dots + a_\\tau y_{t-\\tau} + \\epsilon_t, \\tag{11.59} \\end{equation}\\] where \\(a_i\\) is the parameter for the \\(i\\)-th lag of the model. In this regression, all the relations between \\(y_t\\) and \\(y_{t-\\tau}\\) are captured separately, so the last parameter \\(a_k\\) is clean of all the temporal effects discussed above. We can then use the value \\(\\varrho(\\tau) = a_\\tau\\) as the coefficient, showing this relation. In order to obtain the PACF, we would need to construct and estimate regressions (11.59) for each lag \\(\\tau=\\{1, 2, \\dots, p\\}\\) and get the respective parameters \\(a_1\\), \\(a_2\\), …, \\(a_p\\), which would correspond to \\(\\varrho(1)\\), \\(\\varrho(2)\\), …, \\(\\varrho(p)\\). Just to show what this implies, we consider calculating PACF for AR(1) process. In this case, the true model is: \\[\\begin{equation*} y_t = \\phi_1 y_{t-1} + \\epsilon_t. \\end{equation*}\\] For the first lag we estimate exactly the same model, so that \\(\\varrho(1)=\\phi_1\\). For the second lag we estimate the model: \\[\\begin{equation*} y_t = a_1 y_{t-1} + a_2 y_{t-2} + \\epsilon_t. \\end{equation*}\\] But we know that for AR(1), \\(a_2=0\\), so when estimated in population this would result in \\(\\varrho(2)=0\\) (in case of a sample, this would be a value very close to zero). If we continue with other lags, we will come to the same conclusion: for all lags \\(\\tau&gt;1\\) for the AR(1) we will have \\(\\varrho(\\tau)=0\\). In fact, this is one of the properties of PACF: if we deal with AR(p) process, then PACF drops rapidly to zero right after the lag \\(p\\). When it comes to MA(q) process, PACF behaves differently. In order to understand how it would behave, we take an example of MA(1) model, which is formulated as: \\[\\begin{equation*} y_t = \\theta_1 \\epsilon_{t-1} + \\epsilon_t. \\end{equation*}\\] As it was discussed earlier, MA process can be also represented as an infinite AR (see (11.34) for derivation): \\[\\begin{equation*} y_t = \\sum_{j=1}^\\infty -1^{j-1} \\theta_1^j y_{t-j} + \\epsilon_t. \\end{equation*}\\] If we construct and estimate the regression (11.59) for any lag \\(\\tau\\) for such process we will get \\(\\varrho(\\tau)=-1^{\\tau-1} \\theta_1^\\tau\\). This would correspond to the exponentially decreasing curve (if the parameter \\(\\theta_1\\) is positive, then this will be an alternating series of values), similar to the one we have seen for the AR(1) and ACF. More generally PACF will decline exponentially for MA(q) process, starting from the \\(\\varrho(q)=\\theta_q\\). When it comes to seasonal ARIMA models, the behaviour of PACF would resemble the one of the non-seasonal models, but with lags, multiple to the seasonality \\(m\\). e.g., for the SAR(1) process with \\(m=4\\), the \\(\\varrho(4)=\\phi_{4,1}\\), while \\(\\varrho(8)=0\\). 11.3.4 Summary Summarising the discussions in this section, we can conclude that: For AR(p) process, ACF will decrease either exponentially or alternating (depending on the parameters’ values), starting from the lag \\(p\\); For AR(p) process, PACF will drop abruptly right after the lag \\(p\\); For MA(q) process, ACF will drop abruptly right after the lag \\(q\\); For MA(q) process, PACF will decline either exponentially or alternating (based on the specific values of parameters), starting from the lag \\(q\\). These rules are tempting to use, when determining the appropriate order of ARMA model. However these rules are not necessarily bi-directional: e.g. if we deal with MA(q), ACF drops abruptly right after the lag q, but if ACF drops abruptly after the lag q, then this does not necessarily mean that we deal with MA(q). The former follows directly from the assumed “true” model, while the latter refers to the identification of the model on the data, and there can be different reasons for the ACF to behave in a way it does. The logic here is similar to the following: Example 11.1 All birds have wings. Sarah has wings. Thus Sarah is a bird. Here is Sarah: Figure 11.1: Sarah by Egor Kamelev This small discrepancy led to issues in ARIMA identification over the years. You should not rely fully on Box-Jenkins approach, when identifying the orders of ARIMA, there are more appropriate methods for order selection, which can be used in the context of ARIMA, and we will discuss them in the next chapter. Still, ACF and PACF could be helpful in order to see if anything important is missing in the model, but not on their own. They are useful together with other additional instruments. References "],["ARIMAandETS.html", "11.4 ARIMA and ETS", " 11.4 ARIMA and ETS Box and Jenkins (1976) showed in their textbook that several exponential smoothing methods can be considered as special cases of ARIMA model. Because of that, statisticians have thought for many years that ARIMA is a superior model and payed no attention to the exponential smoothing. It took many years, many papers and a lot of effort Makridakis and Hibon (2000) to show that this is not correct, and that if you are interested in forecasting, then exponential smoothing, being a simpler model, typically does a better job than ARIMA. In fact, it was only after Ord et al. (1997) that the statisticians have started considering ETS as a separate model with its own properties. Furthermore, it seems that some of the conclusions from the previous competitions mainly apply to the Box-Jenkins approach (for example, see Makridakis and Hibon, 1997), pointing out that selecting the correct order of ARIMA models is much more challenging task than the statisticians have thought before. Still, there is a connection between ARIMA and ETS models, which can be beneficial for both models, so it is worth discussing this in a separate section of the textbook. 11.4.1 ARIMA(0,1,1) and ETS(A,N,N) Muth (1960) was one of the first authors who showed that Simple Exponential Smoothing has an underlying ARIMA(0,1,1) model. This becomes apparent, when we study the error correction form of SES: \\[\\begin{equation*} \\hat{y}_{t} = \\hat{y}_{t-1} + \\hat{\\alpha} e_{t-1}. \\end{equation*}\\] Recalling that \\(e_t=y_t-\\hat{y}_t\\), this equation can be rewritten as: \\[\\begin{equation*} y_{t} = y_{t-1} - e_{t-1} + \\hat{\\alpha} e_{t-1} + e_t, \\end{equation*}\\] or after regrouping elements: \\[\\begin{equation*} y_{t} - y_{t-1} = e_t + (\\hat{\\alpha} -1) e_{t-1}. \\end{equation*}\\] Finally, using the backshift operator for ARIMA, substituting the estimated values by their “true” values, we get the ARIMA(0,1,1) model: \\[\\begin{equation*} y_{t}(1 - B) = \\epsilon_t(1 + (\\alpha -1) B) = \\epsilon_t(1 + \\theta_1 B), \\end{equation*}\\] where \\(\\theta_1 = \\alpha-1\\). This relation was one of the first hints that \\(\\alpha\\) in SES should lie in a wider interval: based on the fact that \\(\\theta_1 \\in (-1, 1)\\), the smoothing parameter \\(\\alpha \\in (0, 2)\\). This is exactly the same region we get, when we deal with admissible bounds of ETS(A,N,N) model. This connection between the parameters of ARIMA(0,1,1) and ETS(A,N,N) is useful on its own, because we can transfer the properties of ETS to ARIMA. For example, we know that the level of ETS(A,N,N) will change slowly, when \\(\\alpha\\) is close to zero. The similar behaviour would be observed in ARIMA(0,1,1) with \\(\\theta_1\\) close to -1. In addition, we know that ETS(A,N,N) reverts to Random Walk, when \\(\\alpha=1\\), which corresponds to \\(\\theta_1=0\\). So, the closer \\(\\theta_1\\) to zero, the more abrupt behaviour the ARIMA model exhibits. In cases of \\(\\theta_1&gt;0\\), the behaviour of the model becomes even more uncertain. In a way, this relation gives us the idea of what to expect from more complicated ARIMA(p,d,q) models, when the parameters for moving average are negative - the model should typically behave smoother, although this might differ from one model to another. The main conceptual difference between ARIMA(0,1,1) and ETS(A,N,N) is that the latter still makes sense, when \\(\\alpha=0\\), while in case of ARIMA(0,1,1) the condition \\(\\theta_1=-1\\) is unacceptable. The global level model with \\(\\alpha=0\\) corresponds to justa a different model, ARIMA(0,0,0) with constant. Finally, the connection between the two models tells us that if we have ARIMA(0,1,q) model, then this model would be suitable for the data called “level” in ETS framework. The length of \\(q\\) would define the weights distribution in the model. The specific impact of each MA parameter on the actual values would differ, depending on the order \\(q\\) and values of parameters. The forecast from the ARIMA(0,1,q) would be a straight line, parallel to the x-axis for \\(h\\geq q\\). In order to demonstrate the connection between the two models we consider the following example in R using functions sim.es(), es() and ssarima() from smooth package: # Generate data from ETS(A,N,N) with alpha=0.2 y &lt;- sim.es(&quot;ANN&quot;, obs=120, persistence=0.2) # Estimate ETS(A,N,N) esModel &lt;- es(y$data, &quot;ANN&quot;) # Estimate ARIMA(0,1,1) ssarimaModel &lt;- ssarima(y$data, c(0,1,1), initial=&quot;optimal&quot;) Given the the two models in smooth have the same initialisation mechanism, they should be equivalent. The values of their losses and information criteria should be the same: # Loss values setNames(c(esModel$lossValue, ssarimaModel$lossValue), c(&quot;ETS(A,N,N)&quot;,&quot;ARIMA(0,1,1)&quot;)) ## ETS(A,N,N) ARIMA(0,1,1) ## 485.6427 485.6427 # AIC setNames(c(AIC(esModel), AIC(ssarimaModel)), c(&quot;ETS(A,N,N)&quot;,&quot;ARIMA(0,1,1)&quot;)) ## ETS(A,N,N) ARIMA(0,1,1) ## 977.2855 977.2855 In addition, their parameters should be related based on the formula discussed above. The following two lines should produce the same values: # Smoothing parameter and theta_1 setNames(c(esModel$persistence, ssarimaModel$MA+1), c(&quot;ETS(A,N,N)&quot;,&quot;ARIMA(0,1,1)&quot;)) ## ETS(A,N,N) ARIMA(0,1,1) ## 0.1659913 0.1659913 Finally, the fit and the forecasts from the two models should be exactly the same if the parameters are linearly related: par(mfcol=c(2,1)) plot(esModel,7) plot(ssarimaModel,7) We expect the ETS(A,N,N) and ARIMA(0,1,1) models to be equivalent in this example because they are estimated using the respective functions es() and ssarima(), which are implemented in the same way, using the same framework. If the framework, initialisation, construction or estimation would be different, then the relation between the applied models might be not exact, but approximate. 11.4.2 ARIMA(0,2,2) and ETS(A,A,N) Nerlove and Wage (1964) showed that there is an underlying ARIMA(0,2,2) for the Holts method, althought they do not say that exlicitly in their paper. Skipping the derivations, the relation between Holts method and the ARIMA model are expressed in the following two equations about their parameters (in the form of ARIMA discussed in this textbook): \\[\\begin{equation*} \\begin{aligned} &amp;\\theta_1 = \\alpha + \\beta - 2 \\\\ &amp;\\theta_2 = 1 -\\alpha \\end{aligned} \\end{equation*}\\] We also know from the previous discussion that Holt’s method has underlying ETS(A,A,N) model, thus there is a connection between this model and ARIMA(0,2,2). This means that ARIMA(0,2,2) will produce linear forecasting trajectories for the data and that MA parameters of the model regulate the speed of update of the values. In fact, ARIMA(0,2,q) will produce straight line as a forecasting trajectory for any \\(h\\geq q\\). Similarly to the ARIMA(0,1,1) vs ETS(A,N,N), one of the important differences between the models is that the boundary values for parameters are not possible for ARIMA(0,2,2): \\(\\alpha=0\\) and \\(\\beta=0\\) are possible in ETS, but the respective \\(\\theta_1=2\\) and \\(\\theta_2=-1\\) are not. The model that corresponds to the situation, when \\(\\beta=0\\), but \\(\\alpha \\neq 0\\) is formulated as ARIMA(0,1,0) with drift. The global trend ARIMA could hypothetically appear in the boundary case with \\(\\theta_1=-2\\) and \\(\\theta_2=1\\), implying the following model: \\[\\begin{equation*} y_t (1 - B)^2 = \\epsilon_t - 2\\epsilon_{t-1} + \\epsilon_{t-2} = \\epsilon_t (1 - B)^2 , \\end{equation*}\\] which tells us that in ARIMA framework, the global trend model is only available as a global mean on second differences of the data, there is no proper equivalent ARIMA for this. Finally, the ETA(A,A,N) and ARIMA(0,2,2) will fit the data similarly and produce the same forecasts as long as they are constructed, initialised and estimated in the same way. 11.4.3 ARIMA(1,1,2) and ETS(A,Ad,N) Roberts (1982) proposed damped trend exponential smoothing, showing that it is related to ARIMA(1,1,2), with the following connection between the parameters of the two: \\[\\begin{equation*} \\begin{aligned} &amp;\\theta_1 = \\alpha - 1 + \\phi (\\beta - 1) \\\\ &amp;\\theta_2 = \\phi(1-\\alpha) \\\\ &amp;\\phi_1 = \\phi \\end{aligned} . \\end{equation*}\\] At the same time, the damped trend method has underlying ETS(A,Ad,N), so there is a connection between the two models. Recalling that ETS(A,Ad,N) reverts to ETS(A,A,N), when \\(\\phi=1\\), we can see a similar property in ATIMA: when \\(\\phi_1=1\\), the model should be reformulated as ARMIMA(0,2,2) instead of ARIMA(1,1,2). Given the direct connection between the dampening parameters and the AR(1) parameter of the two models, we can conclude that AR(1) in the model defines the dampening effect of the forecasting trajectory. This is something that we have already noticed in a previous section. However, we should acknowledge that the dampening only happens, when \\(\\phi_1 \\in (0,1)\\). The case of \\(\\phi_1&gt;1\\) is unacceptable in ARIMA framework and is not very useful in case of ETS, producing explosive exponential trajectories. The case of \\(\\phi_1 \\in (-1, 0)\\) is possible, but is less useful in practice, as the trajectory will be oscilating. The lesson to learn from the connection between the two models is that AR(p) part of ARIMA can act as a dampening element for the forecasting trajectories, although the specific shape would depend on the value of \\(p\\) and the values of parameters. 11.4.4 ARIMA and other ETS models The pure additive seasonal ETS models also have connection with ARIMA, but the resulting models are not parsimonious. For example, ETS(A,A,A) is related to SARIMA(0,1,m+1)(0,1,0)\\(_m\\) Chatfield (1977) with some restrictions on parameters, and if we were to work with SARIMA and wanted to model the seasonal time series, we would probably apply SARIMA(0,1,1)(0,1,1)\\(_m\\) instead of this larger model. When it comes to pure multiplicative and mixed ETS models, there are no appropriate ARIMA analogues for them. For example, Chatfield (1977) showed that there are no ARIMA models for the exponential smoothing with multiplicative seasonal component. In fact, this makes ETS distinct from ARIMA. The closest one can get to a pure multiplicative model is the ARIMA aplpied to logarithmically transformed data, when the smoothing parameters of ETS are close to zero, coming from the limit (9.5). 11.4.5 ETS + ARIMA Finally, it is possible to have a combination of ETS and ARIMA, based on the discussion above, but not all combinations would be meaningful and helpful. For example, fitting a combination of ETS(A,N,N)+ARIMA(0,1,1) is not a good idea due to the connection of the two models. However, doing ETS(A,N,N) and adding ARIMA(1,0,0) component might be useful - the resulting model would exhibit the dampening trends as discussed before, but would have fewer parameters to estimate than ETS(A,Ad,N). In fact Gardner (1985) pointed out that using AR(1) together with some exponential smoothing methods improves the forecasting accuracy, so this sort of combination of the two models is potentially beneficial for ETS. In the next chapter we will discuss how specifically the two models can be united in one framework. References "],["ARIMAExampleInR.html", "11.5 Examples of application", " 11.5 Examples of application 11.5.1 Non-seasonal data Using the time series from the Box-Jenkins textbook (Box and Jenkins, 1976), we will fit ARIMA model to the data, but based on our judgment rather than their approach. Just a reminder, here how the data looks (series BJsales): It seems to exhibit the trend in the data, so we can consider ARIMA(1,1,2), ARIMA(0,2,2) and ARIMA(1,1,1). We do not consider models with drift in this example, because they would imply the same slope over time, which does not seem to be the case here: adamModelARIMA &lt;- vector(&quot;list&quot;,3) adamModelARIMA[[1]] &lt;- adam(BJsales, &quot;NNN&quot;, orders=c(1,1,2), h=10, holdout=TRUE) adamModelARIMA[[2]] &lt;- adam(BJsales, &quot;NNN&quot;, orders=c(0,2,2), h=10, holdout=TRUE) adamModelARIMA[[3]] &lt;- adam(BJsales, &quot;NNN&quot;, orders=c(1,1,1), h=10, holdout=TRUE) names(adamModelARIMA) &lt;- c(&quot;ARIMA(1,1,2)&quot;, &quot;ARIMA(0,2,2)&quot;, &quot;ARIMA(1,1,1)&quot;) Note that we need to tell adam to use model=\"NNN\" in order to switch off the ETS part of the model. Comparing information criteria (we will use AICc) of the three models, we can select the most appropriate one: sapply(adamModelARIMA, AICc) ## ARIMA(1,1,2) ARIMA(0,2,2) ARIMA(1,1,1) ## 503.8505 497.0136 505.7461 Note that this comparison is possible in adam() (and in ssarima() and msarima()) because the implemented ARIMA is formulated in state space form, sidestepping the issue of the conventional ARIMA (where taking differences reduces the sample size). Based on this comparison, it looks like the ARIMA(0,2,2) is the most appropriate model (among the three) to the data. Here how the fit and the forecast from the model look (figure 11.2): plot(adamModelARIMA[[2]], 7) Figure 11.2: BJSales series and ARIMA(0,2,2) Comparing this model with the ETS(A,A,N), we will see a slight difference, because the two models are initialised and estimated differently: adam(BJsales, &quot;AAN&quot;, h=10, holdout=TRUE, silent=FALSE) ## Time elapsed: 0.02 seconds ## Model estimated using adam() function: ETS(AAN) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 243.4587 ## Persistence vector g: ## alpha beta ## 0.9997 0.2403 ## ## Sample size: 140 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 135 ## Information criteria: ## AIC AICc BIC BICc ## 496.9173 497.3651 511.6255 512.7319 ## ## Forecast errors: ## ME: 3.225; MAE: 3.338; RMSE: 3.793 ## sCE: 14.162%; Asymmetry: 91.7%; sMAE: 1.465%; sMSE: 0.028% ## MASE: 2.824; RMSSE: 2.488; rMAE: 0.927; rRMSE: 0.923 If we are interested in a more classical Box-Jenkins approach, we can always analyse the residuals of the constructed model and try improving it further. Here is an example of ACF and PACF of the residuals of the ARIMA(0,2,2): par(mfcol=c(1,2)) plot(adamModelARIMA[[2]], c(10,11), main=&quot;&quot;) Figure 11.3: ACF and PACF of ARIMA(0,2,2) on BJSales data As we see from the plot above, all autocorrelation coefficients lie inside the confidence interval, implying that there are no significant AR / MA lags to include in the model. 11.5.2 Seasonal data Similarly to the previous cases, we use Box-Jenkins AirPassengers data, which exhibits a multiplicative seasonality and a trend. We will model this using SARIMA(0,2,2)(0,1,1)\\(_{12}\\), SARIMA(0,2,2)(1,1,1)\\(_{12}\\) and SARIMA(0,2,2)(1,1,0)\\(_{12}\\) models, which are selected to see what type of seasonal ARIMA is more appropriate to the data: adamModelSARIMA &lt;- vector(&quot;list&quot;,3) adamModelSARIMA[[1]] &lt;- adam(AirPassengers, &quot;NNN&quot;, lags=c(1,12), orders=list(ar=c(0,0), i=c(2,1), ma=c(2,1)), h=12, holdout=TRUE) adamModelSARIMA[[2]] &lt;- adam(AirPassengers, &quot;NNN&quot;, lags=c(1,12), orders=list(ar=c(0,1), i=c(2,1), ma=c(2,1)), h=12, holdout=TRUE) adamModelSARIMA[[3]] &lt;- adam(AirPassengers, &quot;NNN&quot;, lags=c(1,12), orders=list(ar=c(0,1), i=c(2,1), ma=c(2,0)), h=12, holdout=TRUE) names(adamModelSARIMA) &lt;- c(&quot;SARIMA(0,2,2)(0,1,1)[12]&quot;, &quot;SARIMA(0,2,2)(1,1,1)[12]&quot;, &quot;SARIMA(0,2,2)(1,1,0)[12]&quot;) Note that now that we have seasonal component, we need to provide the SARIMA lags: 1 and \\(m=12\\) and need to provide orders differently - as a list, specifying the values for AR, I and MA separately. This is done because the SARIMA implemented in adam() supports multiple seasonality (e.g. you can have lags=c(1,24,24*7) if you want). The resulting information criteria of models are: sapply(adamModelSARIMA, AICc) ## SARIMA(0,2,2)(0,1,1)[12] SARIMA(0,2,2)(1,1,1)[12] SARIMA(0,2,2)(1,1,0)[12] ## 1086.192 1193.389 1143.419 It looks like the first model is slightly better than the other two, so we will use it in order to produce forecasts: plot(forecast(adamModelSARIMA[[1]], h=12, interval=&quot;prediction&quot;)) This model is directly comparable with ETS models, so here, for example, AICc of ETS(M,A,M) on the same data: AICc(adamModelETS &lt;- adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE)) ## [1] 973.1933 Which is lower than for the SARIMA model. This means that ETS(M,A,M) is more appropriate to the data in terms of information criteria. Let’s see if there is a way to improve ETS(M,A,M) by adding some ARMA components (figure 11.4): par(mfcol=c(1,2)) plot(adamModelETS, c(10,11), main=&quot;&quot;) Figure 11.4: ACF and PACF of ETS(M,A,M) on AirPassengers data Judging by the plots, there are some significant correlation coefficients for some lags, but it is not clear, whether they appear due to the type I error or not. Just to check, we will see if adding SARIMA(0,0,0)(0,0,1)\\(_{12}\\) helps (reduces AICc) in this case: AICc(adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE, order=list(ma=c(0,1)), lags=c(1,12))) ## [1] 1011.15 As we see, the increased complexity does not decrease the AICc (probably because now we need to estimate 13 parameters more than in just ETS(M,A,M)), so we should not add the SARIMA component. We could try adding other SARIMA elements to see if they improve the model or not, the reader is suggested to do that as an additional exercise. References "],["ADAMARIMA.html", "Chapter 12 ADAM ARIMA", " Chapter 12 ADAM ARIMA There are different ways to formulate and implement ARIMA. The one discussed in the previous section is the conventional way, and the model in that case can be estimate directly, assuming that its initialisation happens some time before the Big Bang: the conventional ARIMA assumes that there is no starting point of the model, we just observe a specific piece of data from a population without any beginning or end. Obviously this assumption is idealistic and does not necessarily agree with reality (imagine the series of infinitely lasting sales of Siemens S45 mobile phones. Do you even remember such thing?). But besides the conventional formulation, there is also a state space form of ARIMA, implemented in SSOE (Hyndman et al., 2008). Svetunkov and Boylan (2020b) adapted this state space model for supply chain forecasting, developing an order selection mechanism, sidesteping the hypothesis testing and focusing on information criteria. However, the main issue with this approach is that the resulting ARIMA model works very slow on the data with large frequencies (because the transition matrix becomes huge). Luckily, there is an alternative SSOE state space formulation, using the same idea of lags as ADAM ETS. This model is already implemented in msarima() function of smooth package and was also used as the basis for the ADAM ARIMA. References "],["StateSpaceARIMA.html", "12.1 State space ARIMA", " 12.1 State space ARIMA 12.1.1 Additive ARIMA In order to develop state space ARIMA, we will use the most general multiple seasonal ARIMA, discussed in the previous section: \\[\\begin{equation*} y_t \\prod_{j=0}^n \\Delta^{D_j} (B^{m_j}) \\varphi^{P_j}(B^{m_j}) = \\epsilon_t \\prod_{j=0}^n \\vartheta^{Q_j}(B^{m_j}) , \\end{equation*}\\] This model can be represented in an easier to digest form by expanding the polynomials on the left hand side of the equation and moving all the previous values to the right hand side and then expanding the MA polynomials: \\[\\begin{equation} y_t = \\sum_{j=1}^K \\eta_j y_{t-j} + \\sum_{j=1}^K \\theta_j \\epsilon_{t-j} + \\epsilon_t . \\tag{12.1} \\end{equation}\\] Here \\(K\\) is the order of the highest polynomial, calculated as \\(K=\\max\\left(\\sum_{j=0}^n (P_j + D_j)m_j, \\sum_{j=0}^n Q_j m_j\\right)\\). If, for example, the MA order is higher than the sum of ARI orders, then polynomials \\(\\eta_i=0\\) for \\(i&gt;\\sum_{j=0}^n (P_j + D_j)m_j\\). The same property holds for the opposite situation of the sum of ARI orders being higher than the MA orders. Based on this we could define states for each of the previous elements: \\[\\begin{equation} v_{i,t-i} = \\eta_i y_{t-i} + \\theta_i \\epsilon_{t-i}, \\tag{12.2} \\end{equation}\\] leading to the following model based on (12.2) and (12.1): \\[\\begin{equation} y_t = \\sum_{j=1}^K v_{j,t-j} + \\epsilon_t . \\tag{12.3} \\end{equation}\\] This can be considered as a measurement equation of the state space ARIMA. Now if we consider the previous values of \\(y_t\\) based on (12.3), for \\(y_{t-i}\\), it will be equal to: \\[\\begin{equation} y_{t-i} = \\sum_{j=1}^K v_{j,t-j-i} + \\epsilon_{t-i} . \\tag{12.4} \\end{equation}\\] The value (12.4) can be inserted into (12.2), in order to get the transition equation: \\[\\begin{equation} v_{i,t-i} = \\eta_i \\sum_{j=1}^K v_{j,t-j-i} + (\\eta_i + \\theta_i) \\epsilon_{t-i}. \\tag{12.5} \\end{equation}\\] This leads to the SSOE state space model based on (12.4) and (12.5): \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = \\sum_{j=1}^K v_{j,t-j} + \\epsilon_t \\\\ &amp;v_{i,t} = \\eta_i \\sum_{j=1}^K v_{j,t-j} + (\\eta_i + \\theta_i) \\epsilon_{t} \\text{ for each } i=\\{1, 2, \\dots, K \\} \\end{aligned}, \\tag{12.6} \\end{equation}\\] which can be formulated in the conventional form as a pure additive model: \\[\\begin{equation*} \\begin{aligned} &amp;{y}_{t} = \\mathbf{w}&#39; \\mathbf{v}_{t-\\boldsymbol{l}} + \\epsilon_t \\\\ &amp;\\mathbf{v}_{t} = \\mathbf{F} \\mathbf{v}_{t-\\boldsymbol{l}} + \\mathbf{g} \\epsilon_t \\end{aligned}, \\end{equation*}\\] with the following values for matrices: \\[\\begin{equation} \\begin{aligned} \\mathbf{F} = \\begin{pmatrix} \\eta_1 &amp; \\eta_1 &amp; \\dots &amp; \\eta_1 \\\\ \\eta_2 &amp; \\eta_2 &amp; \\dots &amp; \\eta_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\eta_K &amp; \\eta_K &amp; \\dots &amp; \\eta_K \\end{pmatrix}, &amp; \\mathbf{w} = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}, \\\\ \\mathbf{g} = \\begin{pmatrix} \\eta_1 + \\theta_1 \\\\ \\eta_2 + \\theta_2 \\\\ \\vdots \\\\ \\eta_K + \\theta_K \\end{pmatrix}, &amp; \\mathbf{v}_{t} = \\begin{pmatrix} v_{1,t} \\\\ v_{2,t} \\\\ \\vdots \\\\ v_{K,t} \\end{pmatrix}, &amp; \\boldsymbol{l} = \\begin{pmatrix} 1 \\\\ 2 \\\\ \\vdots \\\\ K \\end{pmatrix} \\end{aligned}. \\tag{12.7} \\end{equation}\\] States in this model do not have any specific meaning, they just represent a combination of actual values and error terms, some portion of ARIMA model. Furthermore, there are zero states in this model, corresponding to zero polynomials of ARI and MA. These can be dropped to make the model even more compact. 12.1.2 An example In order to better understand what the state space model (12.6) implies, we consider an example of SARIMA(1,1,2)(0,1,0)\\(_4\\): \\[\\begin{equation*} {y}_{t} (1- \\phi_1 B)(1-B)(1-B^4) = \\epsilon_t (1 + \\theta_1 B + \\theta_2 B^2), \\end{equation*}\\] which can be rewritten in the expanded form: \\[\\begin{equation*} {y}_{t} (1-\\phi_1 B - B + \\phi_1 B^2 - B^4 +\\phi_1 B^5 + B^5 - \\phi_1 B^6) = \\epsilon_t (1 + \\theta_1 B + \\theta_2 B^2), \\end{equation*}\\] or after moving the previous values to the right hand side: \\[\\begin{equation*} {y}_{t} = (1+\\phi_1) {y}_{t-1} - \\phi_1 {y}_{t-2} + {y}_{t-4} - (1+\\phi_1) {y}_{t-5} + \\phi_1 {y}_{t-6} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\epsilon_t . \\end{equation*}\\] The polynomials in this case can be written as: \\[\\begin{equation*} \\begin{aligned} &amp; \\eta_1 = 1+\\phi_1 \\\\ &amp; \\eta_2 = -\\phi_1 \\\\ &amp; \\eta_3 = 0 \\\\ &amp; \\eta_4 = 1 \\\\ &amp; \\eta_5 = - (1+\\phi_1) \\\\ &amp; \\eta_6 = \\phi_1 \\end{aligned} , \\end{equation*}\\] leading to 6 states, one of which can be dropped (the third one, for which both \\(\\eta_3=0\\) and \\(\\theta_3=0\\)). The state space ARIMA can then be written as: \\[\\begin{equation*} \\begin{aligned} &amp;{y}_{t} = \\sum_{j=1,2,4,5,6} v_{j,t-j} + \\epsilon_t \\\\ &amp; v_{1,t} = (1+\\phi_1) \\sum_{j=1}^6 v_{j,t-j} + (1+\\phi_1+\\theta_1) \\epsilon_t \\\\ &amp; v_{2,t} = -\\phi_1 \\sum_{j=1}^6 v_{j,t-j} + (-\\phi_1+\\theta_2) \\epsilon_t \\\\ &amp; v_{4,t} = \\sum_{j=1}^6 v_{j,t-j} + \\epsilon_t \\\\ &amp; v_{5,t} = -(1+\\phi_1) \\sum_{j=1}^6 v_{j,t-j} -(1+\\phi_1) \\epsilon_t \\\\ &amp; v_{6,t} = \\phi_1 \\sum_{j=1}^6 v_{j,t-j} + \\phi_1 \\epsilon_t \\end{aligned} . \\end{equation*}\\] This model looks more complicated than the original ARIMA in the conventional form, but it bring the model to the same ground as ETS in ADAM, making them directly comparable via information criteria and allowing to easily combine the two models, not to mention compare ARIMA of any order with another ARIMA (e.g. with different orders of differencing). 12.1.3 State space ARIMA with constant If we want to add the constant to the model, we need to modify the equation (12.1): \\[\\begin{equation} y_t = \\sum_{j=1}^K \\eta_j y_{t-j} + \\sum_{j=1}^K \\theta_j \\epsilon_{t-j} + a_0 + \\epsilon_t . \\tag{12.8} \\end{equation}\\] This then leads to the appearance of the new state: \\[\\begin{equation} v_{K+1,t} = a_0 , \\tag{12.9} \\end{equation}\\] which leads to the modified measurement equation: \\[\\begin{equation} y_t = \\sum_{j=1}^{K+1} v_{j,t-j} + \\epsilon_t , \\tag{12.10} \\end{equation}\\] and the modified transition states: \\[\\begin{equation} \\begin{aligned} &amp; v_{i,t} = \\eta_i \\sum_{j=1}^{K+1} v_{j,t-j} + (\\eta_i + \\theta_i) \\epsilon_{t} , \\text{ for } i=\\{1, 2, \\dots, K\\} \\\\ &amp; v_{K+1, t} = v_{K+1, t-1} . \\end{aligned} \\tag{12.11} \\end{equation}\\] The state space equations (12.10) and (12.11) lead to the following matrices: \\[\\begin{equation} \\begin{aligned} \\mathbf{F} = \\begin{pmatrix} \\eta_1 &amp; \\dots &amp; \\eta_1 &amp; \\eta_1 \\\\ \\eta_2 &amp; \\dots &amp; \\eta_2 &amp; \\eta_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\eta_K &amp; \\dots &amp; \\eta_K &amp; \\eta_K \\\\ 0 &amp; \\dots &amp; 0 &amp; 1 \\end{pmatrix}, &amp; \\mathbf{w} = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\\\ 1 \\end{pmatrix}, \\\\ \\mathbf{g} = \\begin{pmatrix} \\eta_1 + \\theta_1 \\\\ \\eta_2 + \\theta_2 \\\\ \\vdots \\\\ \\eta_K + \\theta_K \\\\ 0 \\end{pmatrix}, &amp; \\mathbf{v}_{t} = \\begin{pmatrix} v_{1,t} \\\\ v_{2,t} \\\\ \\vdots \\\\ v_{K,t} \\\\ v_{K+1,t} \\end{pmatrix}, &amp; \\boldsymbol{l} = \\begin{pmatrix} 1 \\\\ 2 \\\\ \\vdots \\\\ K \\\\ 1 \\end{pmatrix} \\end{aligned}. \\tag{12.12} \\end{equation}\\] Note that the constant term introduced in this model has different meaning, depending on the differences of the model. For example, if all \\(D_j=0\\), then it acts as an intercept, while for the \\(d=1\\), it will act as a drift. 12.1.4 Multiplicative ARIMA In order to connect ARIMA with ETS, we also need to define cases for multiplicative models. This implies that the error term \\((1+\\epsilon_t)\\) is multiplied by components of the model. The state space ARIMA in this case is formulated using logarithms in the following way: \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = \\exp \\left( \\sum_{j=1}^K \\log v_{j,t-j} + \\log(1+\\epsilon_t) \\right) \\\\ &amp;\\log v_{i,t} = \\eta_i \\sum_{j=1}^K \\log v_{j,t-j} + (\\eta_i + \\theta_i) \\log(1+\\epsilon_t) \\text{ for each } i=\\{1, 2, \\dots, K \\} \\end{aligned}. \\tag{12.13} \\end{equation}\\] The model (12.13) can be written in the following more general form: \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = \\exp \\left( \\mathbf{w}&#39; \\log \\mathbf{v}_{t-\\boldsymbol{l}} + \\log(1+\\epsilon_t) \\right) \\\\ &amp;\\log \\mathbf{v}_{t} = \\mathbf{F} \\log \\mathbf{v}_{t-\\boldsymbol{l}} + \\mathbf{g} \\log(1+\\epsilon_t) \\end{aligned}, \\tag{12.14} \\end{equation}\\] where \\(\\mathbf{w}\\), \\(\\mathbf{F}\\), \\(\\mathbf{v}_t\\), \\(\\mathbf{g}\\) and \\(\\boldsymbol{l}\\) are defined as before for the additive ARIMA, e.g. in equation (12.12). This model is equivalent to applying ARIMA to log-transformed data, but at the same time shares some similarities with pure multiplicative ETS. The main advantage of this formulation is that this model has analytical solutions for the conditional moments and has well defined h steps ahead distributions, which simplifies the work with it in contrast with the pure multiplicative ETS models. In order to distinguish the additive ARIMA from the multiplicative one, we will use the notation “logARIMA” for the latter in this book, pointing out at what such model is equivalent to (applying ARIMA to the log-transformed data). "],["ADAMARIMARecursive.html", "12.2 Recursive relation", " 12.2 Recursive relation Both additive and multiplicative ARIMA models can be written in the recursion form, similar to pure additive ETS. The formulae would be cumbersome in this case, but would have closed forms. Here they are for the pure additive ARIMA: \\[\\begin{equation} y_{t+h} = \\sum_{i=1}^K \\mathbf{w}_{i}&#39; \\mathbf{F}_{i}^{\\lceil\\frac{h}{i}\\rceil-1} \\mathbf{v}_{t} + \\mathbf{w}_{i}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{i}\\rceil-1} \\mathbf{F}_{i}^{j-1} \\mathbf{g}_{i} \\epsilon_{t+i\\lceil\\frac{h}{i}\\rceil-j} + \\epsilon_{t+h} , \\tag{12.15} \\end{equation}\\] and for the pure multiplicative one: \\[\\begin{equation} \\log y_{t+h} = \\sum_{i=1}^K \\mathbf{w}_{i}&#39; \\mathbf{F}_{i}^{\\lceil\\frac{h}{i}\\rceil-1} \\log \\mathbf{v}_{t} + \\mathbf{w}_{i}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{i}\\rceil-1} \\mathbf{F}_{i}^{j-1} \\mathbf{g}_{i} \\log (1+\\epsilon_{t+i\\lceil\\frac{h}{i}\\rceil-j}) + \\log(1+ \\epsilon_{t+h}) , \\tag{12.16} \\end{equation}\\] where \\(i\\) corresponds to each lag of the model from 1 to \\(K\\), \\(\\mathbf{w}_{i}\\) is the measurement vector, \\(\\mathbf{g}_{i}\\) is the persistence vector, both including only \\(i\\)-th elements, \\(\\mathbf{F}_{i}\\) is the transition matrix, including only \\(i\\)-th column. Based on this recursion, we can calculate conditional moments of ADAM ARIMA. 12.2.1 Moments of ADAM ARIMA In case of the pure additive ARIMA model, the moments correspond to the ones discussed in the pure additive ETS section and follows directly from (12.15): \\[\\begin{equation*} \\begin{aligned} \\mu_{y,t+h} = \\mathrm{E}(y_{t+h}|t) = &amp; \\sum_{i=1}^K \\left(\\mathbf{w}_{i}&#39; \\mathbf{F}_{i}^{\\lceil\\frac{h}{i}\\rceil-1} \\right) \\mathbf{v}_{t} \\\\ \\sigma^2_{h} = \\mathrm{V}(y_{t+h}|t) = &amp; \\left( \\sum_{i=1}^K \\left(\\mathbf{w}_{i}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{i}\\rceil-1} \\mathbf{F}_{i}^{j-1} \\mathbf{g}_{i} \\mathbf{g}&#39;_{i} (\\mathbf{F}_{i}&#39;)^{j-1} \\mathbf{w}_{i} \\right) + 1 \\right) \\sigma^2 \\end{aligned} . \\end{equation*}\\] When it comes to the multiplicative ARIMA model, using the same idea with recursive relation as in the pure additive ETS section, we can obtain the logarithmic moments based on (12.16): \\[\\begin{equation} \\begin{aligned} \\mu_{\\log y,t+h} = \\mathrm{E}(\\log y_{t+h}|t) = &amp; \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i}&#39; \\mathbf{F}_{m_i}^{\\lceil\\frac{h}{m_i}\\rceil-1} \\right) \\log \\mathbf{v}_{t} \\\\ \\sigma^2_{\\log y,h} = \\mathrm{V}(\\log y_{t+h}|t) = &amp; \\left( \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{m_i}\\rceil-1} \\mathbf{F}_{m_i}^{j-1} \\mathbf{g}_{m_i} \\mathbf{g}&#39;_{m_i} (\\mathbf{F}_{m_i}&#39;)^{j-1} \\mathbf{w}_{m_i} \\right) + 1 \\right) \\sigma_{\\log (1+\\epsilon)}^2 \\end{aligned}, \\tag{12.17} \\end{equation}\\] where \\(\\sigma_{\\log (1+\\epsilon)}^2\\) is the variance of the error term in logarithms. The obtained logarithmic moments can then be used to get the ones in the original scale, after making assumptions about the distribution of the random variable. For example, if we assume that \\(\\left(1+\\epsilon_t \\right) \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma_{\\log (1+\\epsilon)}^2}{2}, \\sigma_{\\log (1+\\epsilon)}^2\\right)\\), then the conditional expectation and variance can be calculated as: \\[\\begin{equation} \\begin{aligned} &amp; \\mu_{y,t+h} = \\mathrm{E}(y_{t+h}|t) = \\exp \\left(\\mu_{\\log y,t+h} + \\frac{\\sigma^2_{\\log y,h}}{2} \\right) \\\\ &amp; \\sigma^2_{h} = \\mathrm{V}(y_{t+h}|t) = \\left(\\exp\\left( \\sigma^2_{\\log y,h} \\right) - 1 \\right)\\exp\\left(2 \\times \\mu_{\\log y,t+h} + \\sigma^2_{\\log y,h} \\right) \\end{aligned}. \\tag{12.18} \\end{equation}\\] If some other distributions are assumed in the model, then the connection between the logarithmic and normal moments should be used in order to get the conditional expectation and variance. If these relations are not available, then simulations can be used in order to obtain the numeric approximations. 12.2.2 Parameters bounds Finally, modifying the recursions (12.15) and (12.16), we can get the stability condition for the parameters, similar to the one for pure additive ETS. The advantage of the pure multiplicative ARIMA formulated in the form (12.14) is that the adequate stability condition can be obtained. In fact, it will be the same as for the pure additive ARIMA and / or ETS. The ARIMA model will be stable, when the absolute values of all non-zero eigenvalues of the discount matrices \\(\\mathbf{D}_{i}\\) are lower than one, given that: \\[\\begin{equation} \\mathbf{D}_{i} = \\mathbf{F}_{i} - \\mathbf{g}_{i} \\mathbf{w}_{i}&#39; . \\tag{12.19} \\end{equation}\\] Hyndman et al. (2008) show that the stability condition corresponds to the invertibility condition of ARIMA, so the model can either be checked via the discount matrix (12.19) or via the MA polynomials (11.55). When it comes to stationarity, state space ARIMA is always non-stationary if the differences \\(d \\neq 0\\). So, there needs to be a different mechanism for the stationarity check. The simplest thing to do would be to expand the AR(p) polynomials, ignoring I(d), fill in the transition matrix \\(\\mathbf{F}\\) and then calculate its eigenvalues. If they are lower than one by absolute values, then the model is stationary. The same condition can be checked via the roots of polynomial of AR(p) (11.54). If both stability and stationarity conditions for ARIMA are satisfied, then we will call the bounds that the AR / MA parameters form “admissible,” similar to how they are called in ETS. Note that there are no “usual” or “traditional” bounds for ARIMA. References "],["ADAMARIMADistributions.html", "12.3 Distributional assumptions of ADAM ARIMA", " 12.3 Distributional assumptions of ADAM ARIMA Following the same idea as in pure additive and pure multiplicative ETS models, we can have state space ARIMA with different distributions, but with distributions aligning more appropriately with the types of models. For additive ARIMA: Normal: \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\); Laplace: \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\); S: \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\); Generalised Normal: \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\); Asymmetric Laplace: \\(\\epsilon_t \\sim \\mathcal{ALaplace}(0, s, \\alpha)\\) For multiplicative ARIMA: Inverse Gaussian: \\(\\left(1+\\epsilon_t \\right) \\sim \\mathcal{IG}(1, s)\\); Log Normal: \\(\\left(1+\\epsilon_t \\right) \\sim \\text{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\). The restrictions imposed on the parameters of the model correspond to the ones for ETS: in case of pure additive models, they ensure that the conditional h steps ahead mean is not impacted by the location of distribution (thus \\(\\mu_\\epsilon=0\\)); in case of pure multiplicative models, they ensure that the conditional h steps ahead mean is just equal to the point forecast (thus imposing \\(\\mathrm{E}(1+\\epsilon_t)=1\\)). 12.3.1 Conditional distributions When it comes to conditional distribution of variables, ADAM ARIMA with the assumptions discussed above has closed forms for all of them. For example, if we work with additive ARIMA, then according to recursive relation (12.15) the h steps ahead value follows the same distribution but with different conditional mean and variance. For example, if \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\), then \\(y_{t+h} \\sim \\mathcal{GN}(\\mu_{y,t+h}, s_{h}, \\beta)\\), where \\(s_{h}\\) is the conditional h steps ahead scale, found from the connection between variance and scale in Generalised Normal distribution via: \\[\\begin{equation*} s_h = \\sqrt{\\frac{\\sigma^2_h \\Gamma(1/\\beta)}{\\Gamma(3/\\beta)}}. \\end{equation*}\\] Using similar principles, we can calculate scale parameters for the other distributions. When it comes to the multiplicative models, the conditional distribution has the closed form in case of log Normal (it is log Normal as well), but does not have it in case of Inverse Gaussian. In the former case, the logarithmic moments can be directly used to define the parameters of distribution, i.e. if \\(\\left(1+\\epsilon_t \\right) \\sim \\text{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\), then \\(y_{t+h} \\sim \\text{log}\\mathcal{N}\\left(\\mu_{\\log y,t+h}, \\sigma^2_{\\log y,h} \\right)\\). In the latter case, simulations need to be used in order to get the quantile, cumulative and density functions. "],["ets-arima-1.html", "12.4 ETS + ARIMA", " 12.4 ETS + ARIMA Coming back to the topic of ETS and ARIMA, we can now look at it from the SSOE state space point of view. 12.4.1 Pure additive models A pure additive ETS + ARIMA model can be formulated in the general form, which we have already discussed several times in this textbook: \\[\\begin{equation*} \\begin{aligned} &amp;{y}_{t} = \\mathbf{w}&#39; \\mathbf{v}_{t-\\boldsymbol{l}} + \\epsilon_t \\\\ &amp;\\mathbf{v}_{t} = \\mathbf{F} \\mathbf{v}_{t-\\boldsymbol{l}} + \\mathbf{g} \\epsilon_t \\end{aligned}, \\end{equation*}\\] but now the matrices and vectors of the model contain ETS and ARIMA components, stacked one after another. For example, if we want to construct ETS(A,N,A)+ARIMA(2,0,0), we can formulate this model as: \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = l_{t-1} + s_{t-m} + v_{1,t-1} + v_{2,t-2} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\alpha \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\epsilon_t \\\\ &amp;v_{1,t} = \\phi_1 v_{1,t-1} + \\phi_1 v_{2,t-2} + \\phi_1 \\epsilon_t \\\\ &amp;v_{2,t} = \\phi_1 v_{1,t-1} + \\phi_2 v_{2,t-2} + \\phi_2 \\epsilon_t \\end{aligned}, \\tag{12.20} \\end{equation}\\] where \\(\\phi_1\\) is the parameter of the AR(1) part of the model. This model represented in the conventional additive SSOE state space model leads to the following matrices and vectors: \\[\\begin{equation} \\begin{aligned} \\mathbf{w} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, &amp; \\mathbf{F} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\phi_1 &amp; \\phi_1 \\\\ 0 &amp; 0 &amp; \\phi_2 &amp; \\phi_2 \\end{pmatrix}, \\\\ \\mathbf{g} = \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\phi_1 \\\\ \\phi_2 \\end{pmatrix}, &amp; \\mathbf{v}_{t} = \\begin{pmatrix} l_t \\\\ s_t \\\\ v_{1,t-1} \\\\ v_{2,t-2} \\end{pmatrix}, &amp; \\boldsymbol{l} = \\begin{pmatrix} 1 \\\\ m \\\\ 1 \\\\ 2\\end{pmatrix} \\end{aligned}. \\tag{12.21} \\end{equation}\\] So, in this formulation the states of ETS and ARIMA are independent and form a combination of models only in the measurement equation. In a way, this model becomes similar to fitting first ETS to the data and then ARIMA to the residuals, but estimating both elements at the same time. ADAM introduces the flexibility necessary for fitting any ETS+ARIMA combination, but not all combinations make sense. For example, here how ETS(A,N,N)+ARIMA(0,1,1) would look like: \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = l_{t-1} + v_{1,t-1} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\alpha \\epsilon_t \\\\ &amp;v_{1,t} = v_{1,t-1} + (1+\\theta_1) \\epsilon_t \\end{aligned}. \\tag{12.22} \\end{equation}\\] In the transition part of the model (12.22), the two equations duplicate each other, because they same exactly the same mechanism of update of states. In fact, as we know from a previous chapter, ETS(A,N,N) and ARIMA(0,1,1) are equivalent, when \\(\\alpha=1+\\theta_1\\). If we estimate this model, then we are duplicating the state, in a way splitting it into two parts with some arbitrary weights. This can be seen if we insert the transition equations in the measurement one, obtaining: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp; l_{t-2} + \\alpha \\epsilon_{t-1} + v_{1,t-2} + (1+\\theta_1) \\epsilon_{t-1} + \\epsilon_t =\\\\ &amp; l_{t-2} + v_{1,t-2} + (1+\\theta_1+\\alpha) \\epsilon_{t-1} + \\epsilon_t \\end{aligned}, \\tag{12.23} \\end{equation}\\] which leads to an infinite combination of values of parameters \\(\\theta\\) and \\(\\alpha\\) that would produce exactly the same fit. So, the model (12.22) does not have unique parameters and thus is not identifiable. In some other cases, some parts of models might be unidentifiable as well, so it makes sense to switch to a more complicated model. For example, if we have ETS(A,A,N)+ARIMA(0,2,3), then some parts of the models will be duplicated (because ETS(A,A,N) is equivalent to ARIMA(0,2,2)), so it would be more reasonable to switch to pure ARIMA(0,2,3) instead. These examples show that, when using ETS+ARIMA, model building needs to be done with care, not to get an unreasonable model that cannot be identefied. As a general recommendation, keep the ETS and ARIMA connection in mind, when deciding, what to construct. And here is a short list of guidlines of what to do in some special cases: For ETS(A,N,N)+ARIMA(0,1,q): use ARIMA(0,1,q) in case of with \\(q &gt;1\\), use ETS(A,N,N) in case of \\(q\\leq 1\\); For ETS(A,A,N)+ARIMA(0,2,q): use ARIMA(0,2,q) in case of with \\(q &gt;2\\), use ETS(A,A,N) in case of \\(q \\leq 2\\); For ETS(A,Ad,N)+ARIMA(p,1,q): use ARIMA(p,1,q), when either \\(p&gt;1\\) or \\(q&gt;2\\), use ETS(A,Ad,N), when \\(p \\leq 1\\) and \\(q \\leq 2\\). When it comes to seasonal models, the relation between ETS and ARIMA is mroe complex, and it is highly improbable to get to equivalent ARIMA models, so it makes sense to make sure that the three rules above hold for the non-seasonal part of the model. 12.4.2 Pure multiplicative models When it comes to the multiplicative error and mixed ETS models, then the ETS+ARIMA might not have the same issues as the pure additive one. This is because the multiplicative ETS and multiplicative ARIMA are formulated differently. An example is an ETS(M,N,N)+logARIMA(0,1,1), which is formulated as: \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = l_{t-1} v_{1,t-1} (1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\ &amp;\\log v_{1,t} = \\log v_{1,t-1} + (1+\\theta_1) \\log (1 + \\epsilon_t) \\end{aligned}. \\tag{12.24} \\end{equation}\\] The last equation in (12.24) can be rewritten as \\(v_{1,t} = v_{1,t-1} (1 + \\epsilon_t)^{(1+\\theta_1)}\\), demonstrating the difference between the transition equation of ETS(M,N,N) and multiplicative ARIMA(0,1,1). Still, the two models will be similar in cases, when \\(\\alpha\\) is close to zero and (respectively) \\(\\theta\\) is close to -1. So this combination of models should be treated with care, along with other potentially similar combinations. The following combinations of the two models can be considered as potentially unidentifiable under some conditions: ETS(M,N,N)+logARIMA(0,1,1); ETS(M,M,N)+logARIMA(0,2,2); ETS(M,Md,N)+logARIMA(1,1,1). In additions, the recommendations discussed for the pure additive ETS+ARIMA can be applied here for the pure multiplicative ETS+ARIMA to guarantee that the resulting model is identifiable no matter what. Finally, mixing additive ETS with multiplicative ARIMA or multiplicative ETS with additive ARIMA does not make sense and only complicates the model building process, so, we do not consider these exotic cases in the book. "],["ADAMARIMAExamples.html", "12.5 Examples of application", " 12.5 Examples of application Building upon the example with AirPassengers data from the previous section, we will construct multiplicative ARIMA models and see, which one of them is the most appropriate for the data. As a reminder, the best additive ARIMA model was SARIMA(0,2,2)(0,2,2)\\(_{12}\\), which had AICc of 1086.192. We will do something similar here, but using Log Normal distribution, thus working with logARIMA. In order to understand what model can be used in this case, we can take logarithm of data and see whaat happens with the components of time series: plot(log(AirPassengers)) We still have the trend in the data and the seasonality now corresponds to the additive one rather than the multiplicative (as expected). While we might still need the second differences for the non-seasonal part of the model, taking first differences for the seasonal should suffice. So we can test several models with different options for ARIMA orders: adamModelLogSARIMA &lt;- vector(&quot;list&quot;,3) adamModelLogSARIMA[[1]] &lt;- adam(AirPassengers, &quot;NNN&quot;, lags=c(1,12), orders=list(ar=c(0,0), i=c(1,1), ma=c(1,1)), h=12, holdout=TRUE, distribution=&quot;dlnorm&quot;) adamModelLogSARIMA[[2]] &lt;- adam(AirPassengers, &quot;NNN&quot;, lags=c(1,12), orders=list(ar=c(0,0), i=c(2,1), ma=c(2,2)), h=12, holdout=TRUE, distribution=&quot;dlnorm&quot;) adamModelLogSARIMA[[3]] &lt;- adam(AirPassengers, &quot;NNN&quot;, lags=c(1,12), orders=list(ar=c(1,0), i=c(1,1), ma=c(2,1)), h=12, holdout=TRUE, distribution=&quot;dlnorm&quot;) names(adamModelLogSARIMA) &lt;- c(&quot;logSARIMA(0,1,1)(0,1,1)[12]&quot;, &quot;logSARIMA(0,2,2)(0,1,1)[12]&quot;, &quot;logSARIMA(1,1,2)(0,1,1)[12]&quot;) The thing that is different between the models is the non-seasonal part. Using the connection with ETS, the first model should work on local level data, the second should be optimal for the local trend series and the third one is placed somewhere in between the two. We can compare the models based on AICc: sapply(adamModelLogSARIMA, AICc) ## logSARIMA(0,1,1)(0,1,1)[12] logSARIMA(0,2,2)(0,1,1)[12] ## 978.8511 1096.8616 ## logSARIMA(1,1,2)(0,1,1)[12] ## 994.0898 It looks like the logSARIMA(0,1,1)(0,1,1)\\(_{12}\\) is more appropriate for the data. In order to make sure that we did not miss anything, we analyse the residuals of this model: par(mfcol=c(2,1)) plot(adamModelLogSARIMA[[1]],10:11) We can see that there are no significant coefficient on either ACF or PACF, so there is nothing else to improve in this model. We can then produce forecast from the model and see how it performed on the holdout sample: adamModelLogSARIMA[[1]] ## Time elapsed: 0.28 seconds ## Model estimated using adam() function: SARIMA(0,1,1)[1](0,1,1)[12] ## Distribution assumed in the model: Log Normal ## Loss function type: likelihood; Loss function value: 471.0603 ## ARMA parameters of the model: ## MA: ## theta1[1] theta1[12] ## -0.2982 -0.5720 ## ## Sample size: 132 ## Number of estimated parameters: 16 ## Number of degrees of freedom: 116 ## Information criteria: ## AIC AICc BIC BICc ## 974.1206 978.8511 1020.2455 1031.7944 ## ## Forecast errors: ## ME: -12.392; MAE: 13.422; RMSE: 18.746 ## sCE: -56.649%; Asymmetry: -91.2%; sMAE: 5.113%; sMSE: 0.51% ## MASE: 0.557; RMSSE: 0.598; rMAE: 0.177; rRMSE: 0.182 plot(forecast(adamModelLogSARIMA[[1]],h=12,interval=&quot;prediction&quot;)) The ETS model closest to the logSARIMA(0,1,1)(0,1,1)\\(_{12}\\) would probably be ETS(M,M,M): adamModelETS &lt;- adam(AirPassengers, &quot;MMM&quot;, h=12, holdout=TRUE) adamModelETS ## Time elapsed: 0.13 seconds ## Model estimated using adam() function: ETS(MMM) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 468.2408 ## Persistence vector g: ## alpha beta gamma ## 0.7706 0.0101 0.0009 ## ## Sample size: 132 ## Number of estimated parameters: 17 ## Number of degrees of freedom: 115 ## Information criteria: ## AIC AICc BIC BICc ## 970.4815 975.8500 1019.4892 1032.5956 ## ## Forecast errors: ## ME: -4.365; MAE: 15.406; RMSE: 21.756 ## sCE: -19.955%; Asymmetry: -16.7%; sMAE: 5.869%; sMSE: 0.687% ## MASE: 0.64; RMSSE: 0.694; rMAE: 0.203; rRMSE: 0.211 Comparing information criteria, ETS(M,M,M) should be preferred to logARIMA, but in terms of accuracy on the holdout, logARIMA is more accurate than ETS on this data. "],["ADAMX.html", "Chapter 13 Explanatory variables in ADAM", " Chapter 13 Explanatory variables in ADAM Having the state space model (8.1) allows easily extending the model to include additional components and explanatory variables. Furthremore, parameters for these additional components can either be fixed or change over time. The model becomes more complicated in the latter case and more difficult to estimate, but nonetheless still potentially useful. In practice, the need for explanatory variables arises, when there are some external factors influencing the response variable, which cannot be ignored and impact the final forecasts. Examples of such variables in demand forecasting context include price changes, promotional activities, temperature etc. In some cases the changes in these factors would not have a substantial impact on the demand, but in the others they would be essential for improving the accuracy. While inclusion of explanatory variables in context of ARIMA models is relatively well studied idea, in case of ETS, there is only a handful of papers on the topic. One of such papers is Koehler et al. (2012), which discusses the mechanism of outlier detection and approximation of outliers via an ETSX model (ETS with explanatory variables). The authors show that if an outlier appears at the end of series, then it will have a serious impact on the final forecast. However, if it appears either in the middle or in the beginning of series, the impact on the final forecast is typically negligible. This is relevant to our discussion, because there is a direct link between dealing with outlier in Koehler et al. (2012) and including explanatory variables in ETSX in terms of how the ETS model is formulated in these two cases. In this chapter we discuss the main aspects of ADAM with explanatory variables, how it is formulated and how the more advanced models can be built upon it. We will do the discussions on the example of ADAM ETSX, keeping in mind that the same principles will hold for ADAM ARIMAX as well, because the two are formulated in the same way. The more general dynamic model with explanatory variables is called “ADAMX” in this and further chapters. References "],["adamx-model-formulation.html", "13.1 ADAMX: Model formulation", " 13.1 ADAMX: Model formulation As discussed previously, there are fundamentally two types of ETS models: Additive error model (which was discussed in Hyndman et al., 2008 in Chapter 9), Multiplicative error model. The inclusion of explanatory variables in ADAMX is determined by the type of the error, so that in case of (1) the measurement equation of the model is: \\[\\begin{equation} {y}_{t} = a_{0,t} + a_{1,t} x_{1,t} + a_{2,t} x_{2,t} + \\dots + a_{n,t} x_{n,t} + \\epsilon_t , \\tag{13.1} \\end{equation}\\] where \\(a_{0,t}\\) is the point value based on all ETS components (for example, \\(a_{0,t}=l_{t-1}\\) in case of ETS(A,N,N)), \\(x_{i,t}\\) is the \\(j\\)-th explanatory variable, \\(a_{i,t}\\) is the parameter for that component and \\(n\\) is the number of explanatory variables. We will call the estimated parameters of such model \\(\\hat{a}_{i,t}\\). In the simple case, the transition equation for such model would imply that the parameters \\(a_{i,t}\\) do not change over time: \\[\\begin{equation} \\begin{aligned} &amp;a_{1,t} = a_{1,t-1} \\\\ &amp;a_{2,t} = a_{2,t-1} \\\\ &amp;\\vdots \\\\ &amp;a_{n,t} = a_{n,t-1} \\end{aligned} . \\tag{13.2} \\end{equation}\\] Complex mechanisms for the states update can be proposed instead of (13.2), but we do not discuss them at this point. Typically, the initial values of parameters would be estimated at the optimisation stage, either based on likelihood or some other loss function, so the index \\(t\\) can be dropped, substituting \\(a_{1,t}=a_{1}\\). When it comes to the mulitplicative error, the multiplication should be used instead of addition. However, it is easier to formulate the model in logarithms in order to linearise it: \\[\\begin{equation} \\log {y}_{t} = \\log a_{0,t} + a_{1,t} x_{1,t} + a_{2,t} x_{2,t} + \\dots + a_{n,t} x_{n,t} + \\log(1+ \\epsilon_t). \\tag{13.1} \\end{equation}\\] Note that if log-log model is required, all that needs to be done, is that \\(x_{i,t}\\) should be substituted by \\(\\log x_{j,t}\\). One of the other ways to formulate the ADAMX model is to move the explanatory variables \\(x_{i,t}\\) in the measurement vector \\(\\mathbf{w}_{t}\\), making it change over time, to move the parameters in the state vector, add diagonal matrix to the existing transition matrix and set values of the persistence vector for the parameters of explanatory variables to zero. The general state space model does not change in that case, but the pure ones can be specifically represented as: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp; \\mathbf{w}&#39;_t \\mathbf{v}_{t-\\boldsymbol{l}} + \\epsilon_t \\\\ \\mathbf{v}_t = &amp; \\mathbf{F} \\mathbf{v}_{t-\\boldsymbol{l}} + \\mathbf{g} \\epsilon_t \\end{aligned} \\tag{13.3} \\end{equation}\\] and \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp; \\exp\\left(\\mathbf{w}&#39;_t \\log \\mathbf{v}_{t-\\boldsymbol{l}} + \\log(1 + \\epsilon_t)\\right) \\\\ \\log \\mathbf{v}_t = &amp; \\mathbf{F} \\log \\mathbf{v}_{t-\\boldsymbol{l}} + \\log(\\mathbf{1}_k + \\mathbf{g} \\epsilon_t) \\end{aligned}. \\tag{13.4} \\end{equation}\\] So, the only thing that changes in these models is the time varying measurement vector \\(\\mathbf{w}&#39;_t\\) instead of the fixed one. For example, in case of ETSX(A,A,A) we will have: \\[\\begin{equation} \\begin{aligned} \\mathbf{F} = \\begin{pmatrix} 1 &amp; 1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 1 \\end{pmatrix}, &amp; \\mathbf{w}_t = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ x_{1,t} \\\\ \\vdots \\\\x_{n,t} \\end{pmatrix}, &amp; \\mathbf{g} = \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}, \\\\ &amp; \\mathbf{v}_{t} = \\begin{pmatrix} l_t \\\\ b_t \\\\ s_t \\\\ a_{1,t} \\\\ \\vdots \\\\ a_{n,t} \\end{pmatrix}, &amp; \\boldsymbol{l} = \\begin{pmatrix} 1 \\\\ 1 \\\\ m \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\end{aligned}, \\tag{13.5} \\end{equation}\\] which is equivalent to the combination of equations (13.1) and (13.2), giving us: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; l_{t-1} + b_{t-1} + s_{t-m} + a_{1,t} x_{1,t} + \\dots + a_{n,t} x_{n,t} + \\epsilon_t \\\\ l_t = &amp; l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\ b_t = &amp; b_{t-1} + \\beta \\epsilon_t \\\\ s_t = &amp; s_{t-m} + \\gamma \\epsilon_t \\\\ a_{1,t} = &amp;a_{1,t-1} \\\\ \\vdots &amp;\\\\ a_{n,t} = &amp;a_{n,t-1} \\end{aligned}. \\tag{13.6} \\end{equation}\\] Alternatively, the state, measurement and persistence vectors and transition matrix can be split into two, separatign the ETS and X parts in the state space equations. When all the smoothing parameters of the ETS part of the model are equal to zero, the ETSX reverts to a deterministic model, directly related to the multiple linear regression. For example, in case of ETSX(A,N,N) with \\(\\alpha=0\\) we get: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; l_{t-1} + a_{1,t} x_{1,t} + \\dots + a_{n,t} x_{n,t} + \\epsilon_t \\\\ l_t = &amp; l_{t-1} \\\\ a_{1,t} = &amp; a_{1,t-1} \\\\ \\vdots &amp; \\\\ a_{n,t} = &amp; a_{n,t-1} \\end{aligned}, \\tag{13.7} \\end{equation}\\] where \\(l_t=a_0\\) is the intercept of the model. (13.7) can be rewritten in the conventional way, dropping the transition part of the state space model: \\[\\begin{equation} y_{t} = a_0 + a_{1} x_{1,t} + \\dots + a_{n} x_{n,t} + \\epsilon_t . \\tag{13.8} \\end{equation}\\] So, in general ADAMX implies that we are dealing with regression with time varying intercept, where the principles of this variability are defined by the ADAM components and smoothing parameters (e.g. intercept can vary seasonally). Similar properties are obtained with the multiplicative error model, with the main difference that the impact of explanatory variables on the response variable will vary with the changes of the intercept. References "],["ADAMXConventionalConditionalMoments.html", "13.2 Conditional expectation and variance of ADAMX", " 13.2 Conditional expectation and variance of ADAMX 13.2.1 The ADAMX with known explanatory variables ETS models have a serious limitation, which we will discuss in one of the latter chapters: they assume that the parameters of the model are known, i.e. there is no variability in them and that the in-sample values are fixed no matter what. This limitation also impacts the ETSX part. While in case of point forecasts this is not an issue, this impacts the conditional variance and prediction intervals. As a result, the conditional mean and variance of the conventional ADAMX assume that the parameters \\(a_0, \\dots a_n\\) are also known, leading to the following formulae in case of pure additive model, based on what was discussed in the section on pure additive models: \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = \\text{E}(y_{t+h}|t) = &amp; \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i,t}&#39; \\mathbf{F}_{m_i}^{\\lceil\\frac{h}{m_i}\\rceil-1} \\right) \\mathbf{v}_{t} \\\\ \\text{V}(y_{t+h}|t) = &amp; \\left( \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i,t}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{m_i}\\rceil-1} \\mathbf{F}_{m_i}^{j-1} \\mathbf{g}_{m_i} \\mathbf{g}&#39;_{m_i} (\\mathbf{F}_{m_i}&#39;)^{j-1} \\mathbf{w}_{m_i,t} \\right) + 1 \\right) \\sigma^2 \\end{aligned}, \\tag{13.9} \\end{equation}\\] the main difference from the conventional model being is the index \\(t\\) in the measurement vector. As an example, here how the two statistics will look in case of ETSX(A,N,N): \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = \\text{E}(y_{t+h}|t) = &amp; l_{t} + \\sum_{i=1}^n a_i x_{i,t+h} \\\\ \\text{V}(y_{t+h}|t) = &amp; \\left((h-1) \\alpha^2 + 1 \\right) \\sigma^2 \\end{aligned}, \\tag{13.10} \\end{equation}\\] where the variance ignores the potential variability rising from the explanatory variables because of the ETS limitations. As a result, the prediction and confidence intervals for the ADAMX model would typically be narrower than needed and would only be adequate in cases of large samples, where law of large numbers would start working, reducing the variance of parameters (this is assuming that the typical assumptions of the model hold). 13.2.2 ADAMX with random explanatory variables Note that the ADAMX works well in cases, when the future values of \\(x_{i,t+h}\\) are known, which is not always the case. It is a realistic assumption, when we have control over the explanatory variables (e.g. prices and promotions for our product). But in the case, when the variables are out of our control, they need to be forecasted somehow. In this case we are assuming that each \\(x_{i,t}\\) is a random variable with some dynamic conditional one step ahead expectation \\(\\mu_{x_{i,t}}\\) and a one step ahead variance \\(\\sigma^2_{x_{i,1}}\\). Note that in this case we treat the available explanatory variables as models on their own, not just as values given to us from above. This assumption of randomness will change the conditional moments of the model. Here what we will have in case of ETSX(A,N,N) (given that the typical assumptions hold): \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = \\text{E}(y_{t+h}|t) = &amp; l_{t} + \\sum_{i=1}^n a_i \\mu_{x_{i,t+h}} \\\\ \\text{V}(y_{t+h}|t) = &amp; \\left((h-1) \\alpha^2 + 1 \\right) \\sigma^2 + \\sum_{i=1}^n a^2_i \\sigma^2_{x_{i,h}} + 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j \\sigma_{x_{i,h},x_{j,h}} \\end{aligned}, \\tag{13.11} \\end{equation}\\] where \\(\\sigma^2_{x_{i,h}}\\) is the variance of \\(x_{i}\\) h steps ahead, \\(\\sigma_{x_{i,h},x_{j,h}}\\) is the h steps ahead covariance between the explanatory variables \\(x_{i,h}\\) and \\(x_{j,h}\\), both conditional on the information available at the observation \\(t\\). similarly, if we are interested in one step ahead point forecast from the model, it should take the randomness of explanatory variables into account and become: \\[\\begin{equation} \\begin{aligned} \\mu_{y,t} = &amp; \\left. \\mathrm{E}\\left(l_{t-1} + \\sum_{i=1}^n a_i x_{i,t} + \\epsilon_{t} \\right| t-1 \\right) = \\\\ = &amp; l_{t-1} + \\sum_{i=1}^n a_i \\mu_{x_{i,t}} \\end{aligned}. \\tag{13.12} \\end{equation}\\] So, in case of ADAMX with random explanatory variables, the model should be constructed based on the expectations of those variables, not the random values themselves. This does not appear in the context of the classical linear regression, because it does not rely on the one step ahead recursion. But this explains, for example, why Athanasopoulos et al. (2011) found that some models with predicted explanatory variables works better than the model with the variables themselves. This becomes important, when estimating the model, such as ETS(A,N,N), when the following is constructed: \\[\\begin{equation} \\begin{aligned} \\hat{y}_{t} = &amp; \\hat{l}_{t-1} + \\sum_{i=1}^n \\hat{a}_{i,t} \\hat{x}_{i,t} \\\\ e_t = &amp; y_t - \\hat{y}_{t} \\\\ \\hat{l}_{t} = &amp; \\hat{l}_{t-1} + \\hat{\\alpha} e_t \\\\ \\hat{a}_{i,t} = &amp; \\hat{a}_{i,t-1} \\text{ for each } i \\in \\{1, \\dots, n\\} \\end{aligned}, \\tag{13.12} \\end{equation}\\] where \\(\\hat{x}_{i,t}\\) is the in-sample conditional one step ahead mean for the explanatory variable \\(x_i\\). Summarising this section, the adequate ADAMX model needs to be able to work in at least two regimes: (1) assuming that the explanatory variable is known, (2) assuming that the explanatory variable is random. Finally, as discussed previously, the conditional moments for the pure multiplicative and mixed models do not have closed forms in general, implying that the simulations need to be carried out. The situation becomes more challenging in case of random explanatory variables, because that randomness needs to be introduced in the model itself and propagated throught the time series. This is not a trivial task, which we will discuss later in this textbook. References "],["ADAMXDynamic.html", "13.3 Dynamic X in ADAMX", " 13.3 Dynamic X in ADAMX Note: the model discussed in this section assumes a very specific dynamics of parameters (that they are correlated with other states of the model), aligning with what the conventional ETS assumes. It does not treat parameters as independent as the MSOE state space models do. But this type of model works well with categorical variables as I show later in this section. As discussed earlier in this chapter, the parameters of the explanatory variables in ADAMX can be assumed to stay constant over time or can be assumed to vary according to some mechanism. The most reasonable mechanism in SSOE framework is the one relying on the same error for different components of the model. Osman and King (2015) proposed one of such mechanisms, relying on the differences of the data. The main motivation of the research was to make the dynamic ADAMX model stable, which is a challenging task. However, this mechanism relies on the assumption of non-stationarity of the explanatory variables, which does not always make sense (for example, it is not reasonable in case of promotional data). An alternative approach that we will discuss in this section, is the one originally proposed by Svetunkov (1985) based on stochastic approximation mechanism and further developed in Svetunkov and Svetunkov (2014). In this method, we consider the following regression model: \\[\\begin{equation} y_{t} = a_{0,t-1} + a_{1,t-1} x_{1,t} + \\dots + a_{n,t-1} x_{n,t} + \\epsilon_t , \\tag{13.13} \\end{equation}\\] where all parameters vary over time and \\(a_{0,t}\\) represents the value from the conventional pure additive ETS model. The updating mechanism for the parameters is straight forward and relies on the ratio of the error term and the respective explanatory variables: \\[\\begin{equation} a_{i,t} = a_{i,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_i \\frac{\\epsilon_t}{x_{i,t}} \\text{ for each } i \\in \\{1, \\dots, n\\}, \\text{ if } x_{i,t}\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. , \\tag{13.14} \\end{equation}\\] where \\(\\delta_i\\) is the smoothing parameter of the \\(i\\)-th explanatory variable. The same model can be represented in the state space form, based on the equations, similar to (13.3): \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp; \\mathbf{w}&#39;_t \\mathbf{v}_{t-\\boldsymbol{l}} + \\epsilon_t \\\\ \\mathbf{v}_t = &amp; \\mathbf{F} \\mathbf{v}_{t-\\boldsymbol{l}} + \\mathrm{diag}\\left(\\mathbf{w}_t\\right)^{-1} \\mathbf{g} \\epsilon_t \\end{aligned} \\tag{13.15} \\end{equation}\\] where \\(\\mathrm{diag}\\left(\\mathbf{w}_t\\right)^{-1}=\\mathbf{I}_{k+n} \\odot (\\mathbf{w}_t \\mathbf{1}_{k+n})\\) (where \\(\\mathbf{I}_{k+n}\\) is the identity matrix for \\(k\\) ADAM components and \\(n\\) explanatory variables and \\(\\odot\\) is Hadamard product for element-wise multiplication). This is the inverse of the diagonal matrix based on the measurement vector, for which those values that cannot be inverted (due to division by zero) are substitute by zeroes in order to reflect the condition in (13.14). In addition to what (13.3) contained, we add smoothing parameters \\(\\delta_i\\) in the persistence vector \\(\\mathbf{g}\\) for each of the explanatory variables. If the error term is multiplicative, then the model changes to: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; \\exp \\left(a_{0,t-1} + a_{1,t-1} x_{1,t} + \\dots + a_{n,t-1} x_{n,t} + \\log(1+ \\epsilon_t) \\right) \\\\ a_{i,t} = &amp; a_{i,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_i \\frac{\\log(1+\\epsilon_t)}{x_{i,t}} \\text{ for each } i \\in \\{1, \\dots, n\\}, \\text{ if } x_{i,t}\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\end{aligned} . \\tag{13.16} \\end{equation}\\] The formulation (13.16) differs from the conventional pure multiplicative ETS model because the smoothing parameter \\(\\delta_i\\) is not included inside the error term \\(1+\\epsilon_t\\), which simplifies some derivations and makes model easier to work with. Mixed ETS models can also have explanatory variables, but we suggest to align the type of explanatory variables model with the error term. Finally, in order to distinguish the ADAMX with static parameters from the ADAMX with dynamic ones, we will use the letters “S” and “D” in the names of models. So, the model (13.7) can be called ETSX(A,N,N){S}, while the model (13.16), assuming that \\(a_{0,t-1}=l_{t-1}\\), would be called ETSX(M,N,N){D}. We use curly brackets in order to split the ETS states from the type of X. Furthermore, given that the model with static regressors is assumed in many contexts to be the default one, the ETSX(*,*,*){S} model can also be denoted just ETSX(*,*,*). 13.3.1 Conditional moments of dynamic ADAMX Similar to how it was discussed in the previous section, we can have two cases in the dynamic model: (1) when explanatory variables are assumed to be known, (2) when explanatory variables are assumed to be random. For illustrative purposes, we will use a non-seasonal model for which the lag vector contains ones only, keeping in mind that other pure additive models can be easily used instead. The cases of other ETS models are not discussed in this part in detail - the moments for these models need to be calculated based on simulations. So, as discussed previously, the model can be written in the following general way, assuming that all \\(\\boldsymbol{l}=1\\): \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp; \\mathbf{w}&#39;_t \\mathbf{v}_{t-1} + \\epsilon_t \\\\ \\mathbf{v}_t = &amp; \\mathbf{F} \\mathbf{v}_{t-1} + \\mathrm{diag}\\left(\\mathbf{w}_t\\right)^{-1} \\mathbf{g} \\epsilon_t \\end{aligned} . \\tag{13.17} \\end{equation}\\] Based on this model, we can get the recursive relation for \\(h\\) steps ahead, similar to how it was done in one of the previos sections: \\[\\begin{equation} \\begin{aligned} {y}_{t+h} = &amp; \\mathbf{w}&#39;_{t+h} \\mathbf{v}_{t+h-1} + \\epsilon_{t+h} \\\\ \\mathbf{v}_{t+h-1} = &amp; \\mathbf{F} \\mathbf{v}_{t+h-2} + \\mathrm{diag}\\left(\\mathbf{w}_{t+h-1}\\right)^{-1} \\mathbf{g} \\epsilon_{t+h-1} \\end{aligned} , \\tag{13.18} \\end{equation}\\] where the second equation can be represented based on the values available on observation \\(t\\): \\[\\begin{equation} \\mathbf{v}_{t+h-1} = \\mathbf{F}^{h-1} \\mathbf{v}_{t} + \\sum_{j=1}^{h-1} \\mathbf{F}^{h-1-j} \\mathrm{diag}\\left(\\mathbf{w}_{t+j}\\right)^{-1} \\mathbf{g} \\epsilon_{t+j} . \\tag{13.19} \\end{equation}\\] Substituting the equation (13.19) in the measurement equation of (13.18) leads to the final recursion: \\[\\begin{equation} {y}_{t+h} = \\mathbf{w}&#39;_{t+h} \\mathbf{F}^{h-1} \\mathbf{v}_{t} + \\mathbf{w}&#39;_{t+h} \\sum_{j=1}^{h-1} \\mathbf{F}^{h-1-j} \\mathrm{diag}\\left(\\mathbf{w}_{t+j}\\right)^{-1} \\mathbf{g} \\epsilon_{t+j} + \\epsilon_{t+h} . \\tag{13.20} \\end{equation}\\] 13.3.2 Known explanatory variables Based on this recursion, we can calculate the conditional mean and variance for the model. First, we assume that the explanatory variables are controlled by an analyst, so that they are not random: \\[\\begin{equation} \\begin{aligned} &amp; \\mu_{y,t+h} = \\text{E}(y_{t+h}|t) = \\mathbf{w}&#39;_{t+h} \\mathbf{F}^{h-1} \\mathbf{v}_{t} \\\\ &amp; \\text{V}(y_{t+h}|t) = \\left(\\mathbf{w}&#39;_{t+h} \\sum_{j=1}^{h-1} \\mathbf{F}^{h-1-j} \\mathrm{diag}\\left(\\mathbf{w}_{t+j}\\right)^{-1} \\mathbf{g} \\right)^2 \\sigma^2 + \\sigma^2 \\end{aligned} . \\tag{13.21} \\end{equation}\\] The formulae for conditional moments in this case look similar to the ones from the pure additive ETS model with only difference being the interaction with time varying measurument vector. 13.3.3 Random explanatory variables In the case of random explanatory variables, the conditional expectation is straightforward and is similar to the one in the static ADAMX model: \\[\\begin{equation} \\mu_{y,t+h} = \\text{E}(y_{t+h}|t) = \\boldsymbol{\\mu}&#39;_{w,t+h} \\mathbf{F}^{h-1} \\mathbf{v}_{t} , \\tag{13.22} \\end{equation}\\] where \\(\\boldsymbol{\\mu}&#39;_{w,t+h}\\) is the vector of conditional h steps ahead expectations for each element in the \\(\\mathbf{w}_{t+h}\\). In case of ETS components, the vector would contain ones. However, when it comes to the conditional variance, it is more complicated, because it introduces complex interactions between variances of different variables and error term. As a result, it would be easier to get the correct variance based on simulations, assuming that the explanatory variables and the error term change according to some assumed distributions. References "],["stability-and-forecastability-conditions-of-adamx.html", "13.4 Stability and forecastability conditions of ADAMX", " 13.4 Stability and forecastability conditions of ADAMX It can be shown that any ADAMX{S} is not stable (as it was defined for pure additive ETS models), meaning that the weights of such model do not decline exponentially to zero. This becomes apparent, when we compare the explanatory part of any ADAMX with a deterministic model, discussed in the context of pure additive models. For example, we have already discussed that when \\(\\alpha=0\\) in ETS(A,N,N), then the model becomes equivalent to the global level, looses the stability condition, but still can be forecastable. Similarly, the X part of ADAMX{S} will always be unstable, but can be forecastable. However, this is unreasonable from the model building point of view: if for example, the ETS part of the model is stable but the model does not pass stability check just because of the X part, then the check itself is incorrect. Furthremore, there are no issues constructing ARIMAX models, but Hyndman et al. (2008) claim that there are issues with stability of ETSX. This only means that the stability / forecastability conditions should be checked for the dynamic part of the model (ETS or ARIMA) separately, ignoring the X part. Technically, this implies creating separate transition matrix, persistence and measurement vectors and calculating the discount matrix for the ETS part in order to check already discussed stability and forecastability conditions. When it comes to the dynamic ADAMX, then the situation changes, because now the smoothing parameters for the coefficients of the model determine, how weights decline over time. It can be shown based on (8.9) that the values of the state vector on the observation \\(t\\) can be calculated via the recursion (here we provide formula for the non-seasonal case, keeping in mind that in case of the seasonal one, the derivation and the main message will be similar): \\[\\begin{equation} \\mathbf{v}_{t} = \\prod_{j=1}^{t-1}\\mathbf{D}_{t-j} \\mathbf{v}_{0} + \\sum_{j=0}^{t-1} \\prod_{i=0}^{j} \\mathbf{D}_{t-i} y_{t-j}, \\tag{13.23} \\end{equation}\\] where \\(\\mathbf{D}_t=\\mathbf{F} - \\mathrm{diag}\\left(\\mathbf{w}_{t}\\right)^{-1} \\mathbf{g} \\mathbf{w}_{t}&#39;\\) is the time varying discount matrix. The main issue in the case of dynamic ADAMX is that the stability condition varies over time together with the values of explanatory variables. So, it is not possible to derive it for the general case. In order to make sure that the model is stable, we need for all eigenvalues of each \\(\\mathbf{D}_{j}\\) for all \\(j=\\{1,\\dots,t\\}\\) to lie in the unit circle. Alternatively, we can introduce a new condition. We say that the model is stable on average if the eigenvalues of \\(\\mathbf{\\bar{D}}=\\frac{1}{t}\\sum_{j=1}^t\\mathbf{D}_t\\) all lie in the unit circle. This way, some of observations might have a higher impact on the final value, but they will be canceled out by those that have much lower weights. This condition can be checked during the model estimation, similar to how the conventional stability condition is checked. As for the forecastability condition, for the ADAMX{D} it should be (based on and the same logic as in section 6.4): \\[\\begin{equation} \\lim\\limits_{t\\rightarrow\\infty}\\left(\\mathbf{w}&#39;_{t}\\prod_{j=1}^{t-1}\\mathbf{D}_{t-j} \\mathbf{v}_{0}\\right) = \\text{const} . \\tag{13.24} \\end{equation}\\] However, for the reasons discussed earlier in this subsection, this condition will always be violated for the ADAMX models, just because the explanatory variables in \\(\\mathbf{w}_{t}\\) have their own variability and typically do not converge to a stable value with the increase of the sample size. So, if a forecastability condition needs to be checked for either ADAMX{D} or ADAMX{S}, we recommend checking it separately for the dynamic part of the model. References "],["ETSXDynamicCategories.html", "13.5 Dealing with categorical variables in ADAMX", " 13.5 Dealing with categorical variables in ADAMX When dealing with categorical variables in regression context, they are typically expanded to a set of dummy variables. So, for example, having a variable promotions that can be “light,” “medium” and “heavy” for different observations \\(t\\), we would expand it to three dummy variables, promolight, promomedium and promoheavy, each one of which is equal to 1, when the respective promotion type is used and equal to zero otherwise. When including these variables in the model, we would drop one of them (which is sometimes called pivot variable) and have a model with two dummy variables of a type: \\[\\begin{equation} y_t = a_0 + a_1 x_{1,t} + \\dots + a_n x_{n,t} + d_1 promolight_t + d_2 promomedium_t + \\epsilon_t, \\tag{13.25} \\end{equation}\\] where \\(d_i\\) is the parameter for the \\(i\\)-th dummy variable. The same procedure can be done in the context of ADAMX, and the principles will be exactly the same for ADAMX{S}. However, when it comes to the dynamic model, the parameters have time indeces, and there can be different ways of formulating the model. Here is the first one: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; a_{0,t-1} + a_{1,t-1} x_{1,t} + \\dots + a_{n,t-1} x_{n,t} + d_1 promolight_t + d_2 promomedium_t + \\epsilon_t \\\\ a_{i,t} = &amp; a_{i,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_i \\frac{\\log(1+\\epsilon_t)}{x_{i,t}} \\text{ for each } i \\in \\{1, \\dots, n\\}, \\text{ if } x_{i,t}\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\\\ d_{1,t} = &amp; d_{1,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_{n+1} \\epsilon_t, \\text{ if } promolight_t\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\\\ d_{2,t} = &amp; d_{2,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_{n+2} \\epsilon_t, \\text{ if } promomedium_t\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\end{aligned} . \\tag{13.26} \\end{equation}\\] Here we assume that each specific category of the variable promotion changes over time on its own with their own smoothing parameters \\(\\delta_{n+1}\\) and \\(\\delta_{n+1}\\). Alternatively, we can assume that they have the same smoothing parameters, implying that the changes of the parameters are similar throughout different categories of the variable: \\[\\begin{equation} \\begin{aligned} d_{1,t} = &amp; d_{1,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_{n+1} \\epsilon_t, \\text{ if } promolight_t\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\\\ d_{2,t} = &amp; d_{2,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_{n+1} \\epsilon_t, \\text{ if } promomedium_t\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\end{aligned} . \\tag{13.27} \\end{equation}\\] This case becomes useful in making the connection between the ETSX and the conventional seasonal ETS model. Let’s assume that we deal with quarterly data with no trend and we have a categorical variable quarterOfYear, which can be First, Second, Third and Fourth, depending on the specific observation. For convenience, I will call the parameters for the dummy variables, created from this categorical variable \\(s_{1,t}, s_{2,t}, s_{3,t} \\text{ and } s_{4,t}\\). Based on (13.27), the model can then be formulated as: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; l_{t-1} + s_{1,t} quarterOfYear_{1,t} + s_{2,t} quarterOfYear_{2,t} \\\\ &amp; + s_{3,t} quarterOfYear_{3,t} + s_{4,t} quarterOfYear_{4,t} + \\epsilon_t \\\\ l_t = &amp; l_{t-1} + \\alpha \\epsilon_t \\\\ s_{i,t} = &amp; s_{i,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta \\epsilon_t \\text{ for each } i \\in \\{1, \\dots, 4\\}, \\text{ if } quarterOfYear_{i,t}\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\end{aligned} . \\tag{13.28} \\end{equation}\\] We intentionally added all 4 dummy variables here, so that they separate seasonal effect from the level component. While in the regression and ETSX{S} contexts, this does not make much sense, in the ETSX{D} we avoid the trap of dummy variables due to the dynamic update of parameters. Having done that, we have just formulated the conventional ETS(A,N,A) model using a set of dummy variables, the difference being that the latter relies on the lag of component: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; l_{t-1} + s_{t-4} + \\epsilon_t \\\\ l_t = &amp; l_{t-1} + \\alpha \\epsilon_t \\\\ s_t = &amp; s_{t-4} + \\gamma \\epsilon_t \\\\ \\end{aligned} . \\tag{13.29} \\end{equation}\\] So, this comparison shows on one hand that the mechanism of ADAMX{D} is natural for the ADAM model, and on the other hand that using the same smoothing parameters for categorical variables can be a reasonable idea, especially in cases, when we do not have grounds to assume that each category of variable should evolve over time independently. "],["ETSXRExample.html", "13.6 Examples of application", " 13.6 Examples of application For this example, we will use the data of Road Casualties in Great Britain 1969–84, Seatbelts dataset in datasets package for R, which contains several variables, the description for which is provided in the documentation for the data (can be accessed via ?Seatbelts command). The variable of interest in this case is drivers, and the dataset contains more variables than needed, so we will restrict the data with drivers, kms (distance driven), PetrolPrice and law - the latter three seem to influence the number of injured / killed drivers in principle: SeatbeltsData &lt;- Seatbelts[,c(&quot;drivers&quot;,&quot;kms&quot;,&quot;PetrolPrice&quot;,&quot;law&quot;)] The dynamics of these variables over time is shown on figure 13.1 plot(SeatbeltsData) Figure 13.1: The time series dynamics of variables from Seatbelts dataset. It is apparent that the drivers variable exhibits seasonality, but does not seem to have a trend. The type of seasonality is difficult to determine, but we will assume that it is multiplicative. So a simple ETS(M,N,M) model applies to the data will produce the following (we will withhold the last 12 observations for the forecast evaluation): adamModelETSMNM &lt;- adam(SeatbeltsData[,&quot;drivers&quot;],&quot;MNM&quot;,h=12,holdout=TRUE) plot(forecast(adamModelETSMNM,h=12,interval=&quot;prediction&quot;)) This simple model already does a fine job in fitting and forecasting the data, although the forecast is biased and is lower than needed because of the sudden drop in the level of series, which can only be explained by the introduction of the new law in the UK in 1983, making the seatbelts compulsory for drivers. Due to the sudden drop, the smoothing parameter for the level of series is higher than needed, leading to wider intervals and less accurate forecasts, here is the output of the model: adamModelETSMNM ## Time elapsed: 0.11 seconds ## Model estimated using adam() function: ETS(MNM) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 1125.85 ## Persistence vector g: ## alpha gamma ## 0.4157 0.0000 ## ## Sample size: 180 ## Number of estimated parameters: 15 ## Number of degrees of freedom: 165 ## Information criteria: ## AIC AICc BIC BICc ## 2281.699 2284.626 2329.594 2337.193 ## ## Forecast errors: ## ME: 113.732; MAE: 113.732; RMSE: 135.656 ## sCE: 80.736%; Asymmetry: 100%; sMAE: 6.728%; sMSE: 0.644% ## MASE: 0.659; RMSSE: 0.602; rMAE: 0.486; rRMSE: 0.534 In order to further explore the data we will produce the scatterplots and boxplots between the variables using spread() function from greybox package: spread(SeatbeltsData) Figure 13.2: The relation between variables from Seatbelts dataset The plot on Figure 13.2 shows that there is a negative relation between kms and drivers: the higher the distance driven, the lower the total of car drivers killed or seriously injuried. A similar relation is observed between the PetrolPrice and drivers (when the prices are high, people tend to drive less, thus causing less incidents). Interestingly, the increase of both variables causes the variance of the response variable to decrease (heteroscedasticity effect). Using multiplicative error model and including the variables in logarithms in this case might address this potential issue. Note that we do not need to take the logarithm of drivers, as we already use the model with multiplicative error. Finally, the legislation of a new law seems to have caused the decrease in the number of causalities. In order to have a better model in terms of explanatory and predictive power, we should include all three variables in the model. This is how we can make it in ADAM: adamModelETSXMNM &lt;- adam(SeatbeltsData,&quot;MNM&quot;,h=12,holdout=TRUE, formula=drivers~log(kms)+log(PetrolPrice)+law) plot(forecast(adamModelETSXMNM,h=12,interval=&quot;prediction&quot;)) The parameter formula in general is not compulsory and can either be substituted by formula=drivers~. or dropped completely - the function would fit the model of the first variable in the matrix from everything else. We need it in our case, because we introduce log-transformations of some of explanatory variables. The forecast from the second model is slightly more accurate and, what is even more important, the prediction interval is narrower, because now the model takes the external information into account. Here is the summary of the second model: adamModelETSXMNM ## Time elapsed: 0.4 seconds ## Model estimated using adam() function: ETSX(MNM) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 1114.11 ## Persistence vector g (excluding xreg): ## alpha gamma ## 0.204 0.000 ## ## Sample size: 180 ## Number of estimated parameters: 18 ## Number of degrees of freedom: 162 ## Information criteria: ## AIC AICc BIC BICc ## 2264.221 2268.469 2321.694 2332.725 ## ## Forecast errors: ## ME: 96.603; MAE: 97.555; RMSE: 123.83 ## sCE: 68.576%; Asymmetry: 97.1%; sMAE: 5.771%; sMSE: 0.537% ## MASE: 0.566; RMSSE: 0.55; rMAE: 0.417; rRMSE: 0.488 The model with explanatory variables is already more precise than the simple univariate ETS(M,N,M) (e.g. MASE on the holdout is lower), but we could try introducing the update of the parameters for the explanatory variables, just to see how it works (it might be unnecessary for this data): adamModelETSXMNMD &lt;- adam(SeatbeltsData,&quot;MNM&quot;,h=12,holdout=TRUE, formula=drivers~log(kms)+log(PetrolPrice)+law,regressors=&quot;adapt&quot;) plot(forecast(adamModelETSXMNMD,h=12,interval=&quot;prediction&quot;)) In this specific case, the difference between the ETSX and ETSX{D} models is infinitesimal in terms of the accuracy of final forecasts and prediction intervals. Here is the output of the model: adamModelETSXMNMD ## Time elapsed: 0.58 seconds ## Model estimated using adam() function: ETSX(MNM){D} ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 1114.062 ## Persistence vector g (excluding xreg): ## alpha gamma ## 0.118 0.000 ## ## Sample size: 180 ## Number of estimated parameters: 21 ## Number of degrees of freedom: 159 ## Information criteria: ## AIC AICc BIC BICc ## 2270.124 2275.973 2337.177 2352.361 ## ## Forecast errors: ## ME: 98.835; MAE: 99.67; RMSE: 125.95 ## sCE: 70.161%; Asymmetry: 97.3%; sMAE: 5.896%; sMSE: 0.555% ## MASE: 0.578; RMSSE: 0.559; rMAE: 0.426; rRMSE: 0.496 We can spot that the error measures of the dynamic model are a bit higher than the ones from the static one (e.g., compare MASE and RMSSE of models). In addition, the information criteria are slightly lower for the static model, so based on all of this, we should probably use the static one for the forecasting and anlytical purposes. In order to see the effect of the explanatory variables on the number of incidents with drivers, we can look at the parameters for those variables: adamModelETSXMNM$initial$xreg ## log.kms. log.PetrolPrice. law ## -0.1018604 -0.3038194 -0.2388061 Based on that, we can point out that the introduction of the law reduced on average the number of incidents by approximately 24%, while the increase of the petrol price by 1% leads on average to decrease in the number of incidents by 0.3%. Finally, the distance has a negative impact on incidents as well, reducing it on average by 0.1% for each 1% increase in the distance. All of this is the standard interpretation of parameters, which we can use based on the estimated model. We will discuss how to do analysis using ADAM in future chapters, introducing the standard errors and confidence intervals for the parameters. Finally, adam() has some shortcuts in cases, when a matrix of variables is provided with no formula, assuming that the necessary expansion has already been done. This leads to the decrease in computational time of the function and becomes especially useful when working on large samples of data. Here is an example with ETSX(M,N,N): SeatbeltsDataExpanded &lt;- ts(model.frame(drivers~log(kms)+log(PetrolPrice)+law, SeatbeltsData), start=start(SeatbeltsData), frequency=frequency(SeatbeltsData)) colnames(SeatbeltsDataExpanded) &lt;- make.names(colnames(SeatbeltsDataExpanded)) adamModelETSXMNMExpanded &lt;- adam(SeatbeltsDataExpanded,&quot;MNM&quot;,lags=12,h=12,holdout=TRUE) "],["ADAMETSEstimation.html", "Chapter 14 Estimation of ADAM models", " Chapter 14 Estimation of ADAM models Now that we have discussed the properties and issues of some of ETS models, we need to understand how to estimate them. As mentioned earlier, when we apply a model to the data, we assume that it is suitable and see how it fits the data and produces forecasts. In this case all the parameters in the model are substituted by the estimated ones (observed in sample) and the error term becomes an estimate of the true one. In general this means that the state space model (8.1) is substituted by: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp;w(\\hat{\\mathbf{v}}_{t-\\boldsymbol{l}}) + r(\\hat{\\mathbf{v}}_{t-\\boldsymbol{l}}) e_t \\\\ \\hat{\\mathbf{v}}_{t} = &amp;f(\\hat{\\mathbf{v}}_{t-\\boldsymbol{l}}) + \\hat{g}(\\hat{\\mathbf{v}}_{t-\\boldsymbol{l}}) e_t \\end{aligned}, \\tag{14.1} \\end{equation}\\] implying that the initial values of components and the smoothing parameters of the model are estimated. An example is the ETS(A,A,A) model applied to the data: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; \\hat{l}_{t-1} + \\hat{b}_{t-1} + \\hat{s}_{t-m} + e_t \\\\ \\hat{l}_t = &amp; \\hat{l}_{t-1} + \\hat{b}_{t-1} + \\hat{\\alpha} e_t \\\\ \\hat{b}_t = &amp; \\hat{b}_{t-1} + \\hat{\\beta} e_t \\\\ \\hat{s}_t = &amp; \\hat{s}_{t-m} + \\hat{\\gamma} e_t \\end{aligned}, \\tag{14.2} \\end{equation}\\] where the initial values \\(\\hat{l}_0, \\hat{b}_0\\) and \\(\\hat{s}_{-m+2}, ... \\hat{s}_0\\) are estimated and then influence all the future values of components via the recursion (14.2). The estimation itself does not happen on its own, a complicated process of minimisation / maximisation of the pre-selected loss function by changing the values of parameters is involved. The results of this will differ depending on: what distribution you assume, what loss you use, what the initial values of parameters that you feed to the optimiser are, what the parameters of the optimiser are, what the sample size is, how many parameters you need to estimate and what restriction you impose on parameters. All of these aspects will be covered in this chapter. Note that the discussions in this chapter are widely applicable to other dynamic models, such as ARIMA, which will be discussed later in this textbook. "],["ADAMETSEstimationLikelihood.html", "14.1 Maximum Likelihood Estimation", " 14.1 Maximum Likelihood Estimation The maximum likelihood estimation (MLE) of ADAM model relies on the distributional assumptions of each specific model and might differ from one to another. There are several options for the distribution supported by adam() function in smooth package, here we briefly discuss how the estimation is done for each one of them. We start with the additive error models, for which the assumptions, log-likelihoods and MLE of scales are provided in Table 14.1. The likelihoods are derived based on the probability density functions, discussed in the chapter 2, by taking logarithms of the product of pdfs for all in sample observations. For example, this is what we get for the normal distribution (we already had a simple example with this earlier in the book): \\[\\begin{equation} \\begin{aligned} \\mathcal{L}(\\boldsymbol{\\theta}, {\\sigma}^2 | \\mathbf{y}) = &amp; \\prod_{t=1}^T \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{\\left(y_t - \\mu_t \\right)^2}{2 \\sigma^2} \\right)\\right) \\\\ \\mathcal{L}(\\boldsymbol{\\theta}, {\\sigma}^2 | \\mathbf{y}) = &amp; \\frac{1}{\\left(\\sqrt{2 \\pi \\sigma^2}\\right)^T} \\exp \\left( \\sum_{t=1}^T -\\frac{\\epsilon_t^2}{2 \\sigma^2} \\right) \\\\ \\ell(\\boldsymbol{\\theta}, {\\sigma}^2 | \\mathbf{y}) = &amp; \\log \\mathcal{L}(\\boldsymbol{\\theta}, {\\sigma}^2 | \\mathbf{y}) \\\\ \\ell(\\boldsymbol{\\theta}, {\\sigma}^2 | \\mathbf{y}) = &amp; -\\frac{T}{2} \\log(2 \\pi \\sigma^2) -\\frac{1}{2} \\sum_{t=1}^T \\frac{\\epsilon_t^2}{\\sigma^2} \\end{aligned}, \\tag{14.3} \\end{equation}\\] where \\(\\mathbf{y}\\) is the vector of all in-sample actual values. As for the scale, it is obtained by solving the equation after taking derivative of the log-likelihood (14.3) with respect to \\(\\sigma^2\\) and setting it equal to zero. We don’t discuss the concentrated log-likelihoods (obtained after inserting the estimated scale in the respective log-likelihood function), because they are not useful in understanding how the model is estimated, but knowing how to calculate scale helps, because it simplifies the model estimation process. Table 14.1: Likelihood approach for additive error models Assumption log-likelihood MLE of scale Normal \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\) \\(-\\frac{T}{2} \\log(2 \\pi \\sigma^2) -\\frac{1}{2} \\sum_{t=1}^T \\frac{\\epsilon_t^2}{\\sigma^2}\\) \\(\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T e_t^2\\) Laplace \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\) \\(-T \\log(2 s) -\\sum_{t=1}^T \\frac{|\\epsilon_t|}{s}\\) \\(\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T |e_t|\\) S \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\) \\(-T \\log(4 s^2) -\\sum_{t=1}^T \\frac{\\sqrt{|\\epsilon_t|}}{s}\\) \\(\\hat{s} = \\frac{1}{2T} \\sum_{t=1}^T \\sqrt{|e_t|}\\) Generalised Normal \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\) \\(\\begin{split} &amp;T\\log\\beta -T \\log(2 s \\Gamma\\left(\\beta^{-1}\\right)) -\\\\ &amp;\\sum_{t=1}^T \\frac{\\left|\\epsilon_t\\right|^\\beta}{s}\\end{split}\\) \\(\\hat{s} = \\sqrt[^{\\beta}]{\\frac{\\beta}{T} \\sum_{t=1}^T\\left| e_t \\right|^{\\beta}}\\) Asymmetric Laplace \\(\\epsilon_t \\sim \\mathcal{ALaplace}(0, s, \\alpha)\\) \\(\\begin{split} &amp;T\\log\\left(\\alpha(1-\\alpha)\\right) -T \\log(s) -\\\\ &amp;\\sum_{t=1}^T \\frac{\\epsilon_t (\\alpha - I(\\epsilon_t \\leq 0))}{s} \\end{split}\\) \\(\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T e_t(\\alpha -I(e_t \\leq 0))\\) Inverse Gaussian \\(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\sim \\mathcal{IG}(1, s)\\) \\(\\begin{split} &amp;-\\frac{T}{2} \\log \\left(2 \\pi s \\left(\\frac{y_t}{\\mu_{y,t}}\\right)^3 \\right) -\\\\ &amp;\\frac{3}{2}\\sum_{t=1}^T \\log y_t -\\frac{1}{2s} \\sum_{t=1}^{T} \\frac{\\epsilon_t^2}{\\mu_{y,t}y_t}\\end{split}\\) \\(\\hat{s} = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{e_t^2}{\\hat{\\mu}_{y,t} y_t}\\) Gamma \\(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\sim \\mathcal{\\Gamma}(s^{-1}, s)\\) \\(\\begin{split} &amp;-T \\log \\Gamma \\left(s^{-1}\\right) - \\frac{T}{s} \\log s + \\\\ &amp;\\frac{1}{s} \\sum_{t=1}^T \\log \\left(\\frac{y_t/\\mu_{y,t}}{ \\exp(y_t/\\mu_{y,t})}\\right) - \\sum_{t=1}^T \\log y_t\\end{split}\\) \\(\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T \\left(\\frac{e_t}{\\mu_{y,t}}\\right)^2\\) * Log Normal \\(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\) \\(\\begin{split} &amp;-\\frac{T}{2} \\log \\left(2 \\pi \\sigma^2\\right) -\\\\ &amp;\\frac{1}{2\\sigma^2} \\sum_{t=1}^{T} \\left(\\log \\left(\\frac{y_t}{\\mu_{y,t}}\\right)+\\frac{\\sigma^2}{2}\\right)^2 -\\\\ &amp;\\sum_{t=1}^T \\log y_t \\end{split}\\) \\(\\hat{\\sigma}^2 = 2\\left(1-\\sqrt{ 1-\\frac{1}{T} \\sum_{t=1}^{T} \\log^2\\left(\\frac{y_t}{\\hat{\\mu}_{y,t}}\\right)}\\right)\\) (*) Note that the MLE of scale parameter for Gamma distribution (formulated in ADAM) does not exist. While there are no proofs for it, it seems that the maximum of the likelihood of Gamma distribution is achieved, when \\(\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T \\left(\\frac{e_t}{\\mu_{y,t}}\\right)^2\\), which corresponds to the estimate based on method of moments. Other distributions can be used in ADAM ETS as well (for example, Logistic distribution or Student’s t), but we do not discuss them here. Note that for the additive error models \\(y_t = \\mu_{y,t}+\\epsilon_t\\), thus some formulae in Table 14.1 are simplified. In all of the cases in 14.1 the assumptions imply that the actual value follows the same distribution, but with a different location and / or scale. For example, for the Normal distribution we have: \\[\\begin{equation} \\begin{aligned} &amp; \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2) \\\\ &amp; \\mathrm{or} \\\\ &amp; y_t = \\mu_{y,t}+\\epsilon_t \\sim \\mathcal{N}(\\mu_{y,t}, \\sigma^2) \\end{aligned}. \\tag{14.4} \\end{equation}\\] When it comes to the multiplicative error models, the likelihoods become slightly different. For example, when we assume that \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\) in the multiplicative error model, this implies that: \\[\\begin{equation} y_t = \\mu_{y,t}(1+\\epsilon_t) \\sim \\mathcal{N}(\\mu_{y,t}, \\mu_{y,t}^2 \\sigma^2) . \\tag{14.5} \\end{equation}\\] As a result the log-likelihoods would have the \\(\\mu_{y,t}\\) part in the formulae. Similar logic is applicable to \\(\\mathcal{Laplace}\\), \\(\\mathcal{S}\\), \\(\\mathcal{GN}\\) and \\(\\mathcal{ALaplace}\\) distributions. From the practical point of view, these assumptions imply that the scale (and variance) of the distribution of \\(y_t\\) changes together with the level of the series. When it comes to the \\(\\mathcal{IG}\\) and log\\(\\mathcal{N}\\), the assumptions are imposed on the \\(1+\\epsilon_t\\) part of the model, the respective likelihoods do not involve the expectation \\(\\mu_{y,t}\\), but the formulation still implies that the variance of the data increases together with the increase of the level of data. All the likelihoods for the multiplicative error models are summarised in Table 14.2. Table 14.2: Likelihood approach for multiplicative error models Assumption log-likelihood MLE of scale Normal \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\) \\(\\begin{split} &amp;-\\frac{T}{2} \\log(2 \\pi \\sigma^2) -\\frac{1}{2} \\sum_{t=1}^T \\frac{\\epsilon_t^2}{\\sigma^2} -\\\\ &amp;\\sum_{t=1}^T \\log |\\mu_{y,t}|\\end{split}\\) \\(\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T e_t^2\\) Laplace \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\) \\(\\begin{split} &amp;-T \\log(2 s) -\\sum_{t=1}^T \\frac{|\\epsilon_t|}{s} -\\\\ &amp;\\sum_{t=1}^T \\log \\mu_{y,t}\\end{split}\\) \\(\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T |e_t|\\) S \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\) \\(\\begin{split} &amp;-T \\log(4 s^2) -\\sum_{t=1}^T \\frac{\\sqrt{|\\epsilon_t|}}{s} -\\\\ &amp;\\sum_{t=1}^T \\log \\mu_{y,t}\\end{split}\\) \\(\\hat{s} = \\frac{1}{2T} \\sum_{t=1}^T \\sqrt{|e_t|}\\) Generalised Normal \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\) \\(\\begin{split} &amp;T\\log\\beta -T \\log(2 s \\Gamma\\left(\\beta^{-1}\\right)) -\\\\ &amp;\\sum_{t=1}^T \\frac{\\left|\\epsilon_t\\right|^\\beta}{s} -\\sum_{t=1}^T \\log \\mu_{y,t}\\end{split}\\) \\(\\hat{s} = \\sqrt[^{\\beta}]{\\frac{\\beta}{T} \\sum_{t=1}^T\\left| e_t \\right|^{\\beta}}\\) Asymmetric Laplace \\(\\epsilon_t \\sim \\mathcal{ALaplace}(0, s, \\alpha)\\) \\(\\begin{split} &amp;T\\log\\left(\\alpha(1-\\alpha)\\right) -T \\log(s) -\\\\ &amp;\\sum_{t=1}^T \\frac{\\epsilon_t (\\alpha - I(\\epsilon_t \\leq 0))}{s} -\\sum_{t=1}^T \\log \\mu_{y,t}\\end{split}\\) \\(\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T e_t(\\alpha -I(e_t \\leq 0))\\) Inverse Gaussian \\(1+\\epsilon_t \\sim \\mathcal{IG}(1, s)\\) \\(\\begin{split} &amp;-\\frac{T}{2} \\log \\left(2 \\pi s \\left(1+\\epsilon_{t}\\right)^3 \\right) -\\\\ &amp;\\frac{3}{2}\\sum_{t=1}^T \\log y_t -\\frac{1}{2s} \\sum_{t=1}^{T} \\frac{\\epsilon_t^2}{1+\\epsilon_t}\\end{split}\\) \\(\\hat{s} = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{e_t^2}{1+e_t}\\) Gamma \\(1+\\epsilon_t \\sim \\mathcal{\\Gamma}(s^{-1}, s)\\) \\(\\begin{split} &amp;-T \\log \\Gamma \\left(s^{-1}\\right) - \\frac{T}{s} \\log s + \\\\ &amp;\\frac{1}{s} \\sum_{t=1}^T \\log \\left(\\frac{1+\\epsilon_t}{\\exp(1+\\epsilon_t)}\\right) - \\sum_{t=1}^T \\log y_t\\end{split}\\) \\(\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T e_t^2\\) * Log Normal \\(1+\\epsilon_t \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\) \\(\\begin{split} &amp;-\\frac{T}{2} \\log \\left(2 \\pi \\sigma^2\\right) -\\\\ &amp;\\frac{1}{2\\sigma^2} \\sum_{t=1}^{T} \\left(\\log \\left(1+\\epsilon_t\\right)+\\frac{\\sigma^2}{2}\\right)^2 -\\\\ &amp;\\sum_{t=1}^T \\log y_t \\end{split}\\) \\(\\hat{\\sigma}^2 = 2\\left(1-\\sqrt{ 1-\\frac{1}{T} \\sum_{t=1}^{T} \\log^2\\left(1+e_t\\right)}\\right)\\) When it comes to practicalities, in adam() function, in the optimiser, on each iteration, after fitting the model the scales from Tables 14.1 and 14.2 are calculated and then used in the log-likelihoods based on the respective distribution functions, for additive error models: \\(\\mathcal{N}\\) - dnorm(x=actuals, mean=fitted, sd=scale, log=TRUE) from stats package; \\(\\mathcal{Laplace}\\) - dlaplace(q=actuals, mu=fitted, scale=scale, log=TRUE) from greybox package; \\(\\mathcal{S}\\) - ds(q=actuals, mu=fitted, scale=scale, log=TRUE) from greybox package; \\(\\mathcal{GN}\\) - dgnorm(x=actuals, mu=fitted, alpha=scale, beta=beta, log=TRUE) implemented in greybox package based on gnorm package (the version on CRAN is outdated); \\(\\mathcal{ALaplace}\\) - dalaplace(q=actuals, mu=fitted, scale=scale, alpha=alpha, log=TRUE) from greybox package; \\(\\mathcal{IG}\\) - dinvgauss(x=actuals, mean=fitted, dispersion=scale/fitted, log=TRUE) from statmod package; log\\(\\mathcal{N}\\) - dlnorm(x=actuals, meanlog=fitted-scale^2/2, sdlog=scale, log=TRUE) from stats package; \\(\\mathcal{\\Gamma}\\) - dgamma(x=actuals, shape=1/scale, scale=scale*fitted, log=TRUE) from stats package. and for multiplicative error models: \\(\\mathcal{N}\\) - dnorm(x=actuals, mean=fitted, sd=scale*fitted, log=TRUE); \\(\\mathcal{Laplace}\\) - dlaplace(q=actuals, mu=fitted, scale=scale*fitted, log=TRUE); \\(\\mathcal{S}\\) - ds(q=actuals, mu=fitted, scale=scale*sqrt(fitted), log=TRUE); \\(\\mathcal{GN}\\) - dgnorm(x=actuals, mu=fitted, alpha=scale*fitted^beta, beta=beta, log=TRUE); \\(\\mathcal{ALaplace}\\) - dalaplace(q=actuals, mu=fitted, scale=scale*fitted, alpha=alpha, log=TRUE); \\(\\mathcal{IG}\\) - dinvgauss(x=actuals, mean=fitted, dispersion=scale/fitted, log=TRUE); log\\(\\mathcal{N}\\) - dlnorm(x=actuals, meanlog=fitted-scale^2/2, sdlog=scale, log=TRUE); \\(\\mathcal{\\Gamma}\\) - dgamma(x=actuals, shape=1/scale, scale=scale*fitted, log=TRUE). Note that in cases of \\(\\mathcal{GN}\\) and \\(\\mathcal{ALaplace}\\), additional parameters (namely \\(\\beta\\) and \\(\\alpha\\)) are needed. If the user does not provide them, then they are estimated together with the other parameters via the maximisation of respective likelihoods. The MLE of parameters of ADAM ETS models makes them comparable with each other irrespective of the types of components and distributional assumptions. As a result, model selection based on information criteria can be done using auto.adam() function from smooth, which will select the most appropriate ETS model with the most suitable distribution. 14.1.1 An example in R adam() function in smooth package has the parameter distribution, which allows selecting between several options discussed in this chapter, based on the respective density functions (see the list above with dnorm() etc). Here is a brief example in R with ADAM ETS(M,A,M) applied to the AirPassengers data with several distributions: adamModel &lt;- vector(&quot;list&quot;,5) adamModel[[1]] &lt;- adam(AirPassengers, &quot;MAM&quot;, distribution=&quot;dnorm&quot;, h=12, holdout=TRUE) adamModel[[2]] &lt;- adam(AirPassengers, &quot;MAM&quot;, distribution=&quot;dlaplace&quot;, h=12, holdout=TRUE) adamModel[[3]] &lt;- adam(AirPassengers, &quot;MAM&quot;, distribution=&quot;dgnorm&quot;, h=12, holdout=TRUE) adamModel[[4]] &lt;- adam(AirPassengers, &quot;MAM&quot;, distribution=&quot;dinvgauss&quot;, h=12, holdout=TRUE) adamModel[[5]] &lt;- adam(AirPassengers, &quot;MAM&quot;, distribution=&quot;dgamma&quot;, h=12, holdout=TRUE) In this case, the function will select the most appropriate ETS model for each distribution. We can see what was selected in each case and compare the models using information criteria: setNames(sapply(adamModel, AICc), c(&quot;dnorm&quot;,&quot;dlaplace&quot;,&quot;dgnorm&quot;,&quot;dinvgauss&quot;,&quot;dgamma&quot;)) ## dnorm dlaplace dgnorm dinvgauss dgamma ## 972.5669 975.2124 977.9902 973.7769 973.1933 We could compare the performance of models in detail, but, for the purposes of this demonstration, it should suffice to say that among the four models considered above, based on the AICc value, the model with the Normal distribution should be preferred. This process of fit and selection can be automated using auto.adam() model, which accepts the vector of distributions to test and by default would consider distribution=c(\"dnorm\", \"dlaplace\", \"ds\", \"dgnorm\", \"dlnorm\", \"dinvgauss\", \"dgamma\"): adamModel &lt;- auto.adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE) This command should return the ADAM ETS(M,A,M) model with the most appropriate distribution, selected based on the AICc. "],["non-mle-based-loss-functions.html", "14.2 Non MLE-based loss functions", " 14.2 Non MLE-based loss functions 14.2.1 MSE and MAE An alternative approach for estimating ADAM ETS is using the conventional loss functions, such as MSE, MAE etc. In this case, the model selection using information criteria would not work, but this might not matter, when you have already decided what model to use and want to improve it. In some special cases (as discussed earlier), the minimisation of these losses would give the same results as the maximisation of some likelihoods, namely: Minimum of MSE corresponds to the maximum of likelihood of Normal distribution; Minimum of MAE corresponds to the maximum of likelihood of Laplace distribution; Minimum of pinball function (Koenker and Bassett, 1978, quantile regression) corresponds to the maximum of likelihood of Asymmetric Laplace distribution (Yu and Zhang, 2005); The main difference between using these losses and maximising respective likelihoods is in the number of estimated parameters: the latter implies that the scale is estimated together with the other parameters, while the former does not consider it. Having said that, the assumed distribution does not necessarily depend on the used loss and vice versa. For example, we can assume that the actuals follow Inverse Gaussian distribution, but estimate the model using MAE. The estimates of parameters might not be as efficient as in the case of MLE, but it is still possible to do. The error term, which is minimised in respective losses depends on the error type: Additive error: \\(e_t = y_t - \\hat{\\mu}_{y,t}\\); Multiplicative error: \\(e_t = \\frac{y_t - \\hat{\\mu}_{y,t}}{\\hat{\\mu}_{y,t}}\\). This follows directly from the respective ETS models. 14.2.2 HAM Along with the discussed MSE and MAE, there is also HAM - “Half Absolute Moment”: \\[\\begin{equation} \\mathrm{HAM} = \\frac{1}{T} \\sum_{j=1}^T \\sqrt{\\left|e_t\\right|}, \\tag{14.6} \\end{equation}\\] the minimum of which corresponds to the maximum of the likelihood of S distribution. The idea of this estimator is to minimise the errors that happen very often, close to the estimate. It will typically ignore the outliers and focus on the values that happen most often. As a result, if used for the integer values, the minimum of HAM would correspond to the mode of that distribution. I donot have a proof of this property, but it becomes apparent, given that the square root in (14.6) would reduce the influence of all values lying above 1 and increase the values of everything that lies between (0, 1) (e.g. \\(\\sqrt{0.16}=0.4\\), but \\(\\sqrt{16}=4\\)). Similar to HAM, one can calculate other fractional losses, which would be even less sensitive to outliers and more focused on the frequently appearing values, e.g. by using the \\(\\sqrt[^\\alpha]{\\left|e_t\\right|}\\) with \\(\\alpha&gt;1\\). 14.2.3 LASSO and RIDGE While this is not a well studied approach yet, it is possible to use LASSO (Tibshirani, 1996) and RIDGE for the estimation of ETS models (James et al., 2017 give a good overview of these losses with examples in R), which can be formulated as (at least this is how it is formualted in ADAM ETS): \\[\\begin{equation} \\begin{aligned} \\mathrm{LASSO} = &amp;(1-\\lambda) \\sqrt{\\frac{1}{T} \\sum_{j=1}^T e_t^2} + \\lambda \\sum |\\hat{\\theta}| \\\\ \\mathrm{RIDGE} = &amp;(1-\\lambda) \\sqrt{\\frac{1}{T} \\sum_{j=1}^T e_t^2} + \\lambda \\sum \\hat{\\theta}^2, \\end{aligned} \\tag{14.7} \\end{equation}\\] where \\(\\theta\\) is the vector of all parameters in the model and \\(\\lambda\\) is the regularisation parameter. The idea of these losses is in shrinkage of parameters. If \\(\\lambda=0\\), then the losses become equivalent to the MSE, when \\(\\lambda=1\\), the optimiser would minimise the values of parameters, ignoring the MSE part. In the context of ETS, it makes sense to shrink everything but the initial level. In order for different components to shrink with a similar speed, they need to be normalised, so here are some tricks used in ADAM ETS: The smoothing parameters are left intact, because they typicall lie between 0 and 1, where 0 means that the respective states are not updated over time; Damping parameter is modified to shrink towards 1, enforcing no dampening of the trend via: \\(\\hat{\\phi}^\\prime=1-\\hat{\\phi}\\); The initial level is normalised using the formula: \\(\\hat{l}_0&#39; = \\frac{\\hat{l}_0 - \\bar{y}_m}{\\hat{\\sigma}_{y,m}}\\), where \\(\\bar{y}_m\\) is the mean and \\(\\hat{\\sigma}_{y,m}\\) is the standard deviation of the first \\(m\\) actual observations, where \\(m\\) is the highest frequency of the data (if \\(m=1\\), then both values are taken based on the first two observations). Shrinking it to zero implies that it becomes equivalent to the mean of the first \\(m\\) observations; If the trend is additive, then the initial trend component is scaled using: \\(\\hat{b}_0&#39; = \\frac{\\hat{b}_0}{\\hat{\\sigma}_{y,m}}\\), making sure that it shrinks towards zero (no trend). In case of the multiplicative trend, this becomes: \\(\\hat{b}_0&#39; = \\log{\\hat{b}_0}\\) , making it shrink towards 1 in the original scale (again, no trend case). The multiplicative trend can be anything between 0 and \\(\\infty\\) with 1 corresponding to no trend. Taking logarithm of it allows balancing out the cases, when \\(\\hat{b}_0&lt;1\\) with \\(\\hat{b}_0&gt;1\\); If the seasonal component is additive, then the normalisation similar to the one in trend is done for every seasonal index: \\(\\hat{s}_i&#39; = \\frac{\\hat{s}_i}{\\hat{\\sigma}_{y,m}}\\), making sure that they shrink towards zero. If the seasonal component is multiplicative, then the logarithm of components is taken: \\(\\hat{s}_i&#39; = \\log{\\hat{s}_i}\\), making sure that they shrink towards 1 in the original scale. If there are explanatory variables and the error term of the model is additive, then the respective parameters are divided by the standard deviations of the respective variables. In case of multiplicative error term, nothing is done, because the parameters in this case would typically be close to zero anyway (see a chapter on the ADAM ETSX). Finally, in order to make \\(\\lambda\\) slightly more meaningful, in case of additive error model, we also divide the MSE part of the loss by \\(\\mathrm{V}\\left({\\Delta}y_t\\right)\\), where \\({\\Delta}y_t=y_t-y_{t-1}\\). This sort of scaling helps in both cases, when there is a trend in the data and when the data does not exhibit one. We do not do anything for the multiplicative error models, because typically the error in this case is already quite small. All of these tricks make sure that different components shrink towards zero simultaneously, not breaking the model. If we would not do these steps, then, for example, the initial trend could dominate in \\(\\hat{\\theta}\\) and would shrink faster than the smoothing parameters. As a result, one can potentially use the model with trend and seasonality and use regularisation in order to shrink the unnecessary parameters towards zero. The adam() function does not select the most appropriate \\(\\lambda\\) and will set it equal to zero if it is not provided by the user. Note that both LASSO and RIDGE are experimental options. If you have better ideas of how to implement them, send me a message, I will improve the mechanism in adam(). 14.2.4 Custom losses It is also theoretically possible to use other non-standard loss functions. adam() function allows doing that via the parameter loss. For example, we could estimate an ETS(A,A,N) model on the BJsales data using an absolute cubic loss (note that the parameters actual, fitted and B are compulsory for the function): lossFunction &lt;- function(actual, fitted, B){ return(mean(abs(actual-fitted)^3)); } adam(BJsales, &quot;AAN&quot;, loss=lossFunction, h=10, holdout=TRUE) where actual is the vector of actual values \\(y_t\\), fitted is the estimate of the one step ahead conditional mean \\(\\hat{\\mu}_{y,t}\\) and \\(B\\) is the vector of all estimated parameters, \\(\\hat{\\theta}\\). This way, one can use more advanced estimators, such as, for example, M-estimators (Barrow et al., 2020). 14.2.5 Examples in R adam() has two parameters, one regulating the assumed distribution, and another one, regulating how the model will be estimated, what loss will be used for these purposes. Here are examples with combinations of different losses and an assumed Inverse Gaussian distribution for ETS(M,A,M) on AirPassengers data. We start with MSE, MAE and HAM: adamModel &lt;- vector(&quot;list&quot;,6) names(adamModel) &lt;- c(&quot;likelihood&quot;,&quot;MSE&quot;, &quot;MAE&quot;, &quot;HAM&quot;, &quot;LASSO&quot;, &quot;Huber&quot;) adamModel[[1]] &lt;- adam(AirPassengers, &quot;MAM&quot;, distribution=&quot;dinvgauss&quot;, h=12, holdout=TRUE, loss=&quot;likelihood&quot;) adamModel[[2]] &lt;- adam(AirPassengers, &quot;MAM&quot;, distribution=&quot;dinvgauss&quot;, h=12, holdout=TRUE, loss=&quot;MSE&quot;) adamModel[[3]] &lt;- adam(AirPassengers, &quot;MAM&quot;, distribution=&quot;dinvgauss&quot;, h=12, holdout=TRUE, loss=&quot;MAE&quot;) adamModel[[4]] &lt;- adam(AirPassengers, &quot;MAM&quot;, distribution=&quot;dinvgauss&quot;, h=12, holdout=TRUE, loss=&quot;HAM&quot;) In these three cases the models, assuming the same distribution for the error term are estimated using MSE, MAE and HAM. Their smoothing parameters should differ, with MSE producing fitted values closer to the mean, MAE - closer to the median and HAM - closer to the mode (but not exactly the mode) of the distribution. In addition, we introduce ADAM ETS(M,A,M) estimated using LASSO with arbitrarily selected \\(\\lambda=0.1\\): adamModel[[5]] &lt;- adam(AirPassengers, &quot;MAM&quot;, distribution=&quot;dinvgauss&quot;, h=12, holdout=TRUE, loss=&quot;LASSO&quot;, lambda=0.1) And, finally, we estimate the same model using a custom loss, which in this case is Huber loss with the threshold of 1.345: # Huber loss with a threshold of 1.345 lossFunction &lt;- function(actual, fitted, B){ errors &lt;- actual-fitted; return(sum(errors[errors&lt;=1.345]^2) + sum(abs(errors)[errors&gt;1.345])); } adamModel[[6]] &lt;- adam(AirPassengers, &quot;MAM&quot;, distribution=&quot;dinvgauss&quot;, h=12, holdout=TRUE, loss=lossFunction) Now we can compare the performance of the six models. First, we can compare the smoothing parameters: round(sapply(adamModel,&quot;[[&quot;,&quot;persistence&quot;),3) ## likelihood MSE MAE HAM LASSO Huber ## alpha 0.778 0.766 0.874 0.907 0.026 0.858 ## beta 0.006 0.006 0.021 0.024 0.025 0.010 ## gamma 0.000 0.000 0.000 0.001 0.183 0.000 What we sould observe in this case is that LASSO should have the lowest smoothing parameters, because they are shrunk directly in the model. Likelihood and MSE should probably give similar values, because they both rely on squared errors, but not the same because the likelihood of Inverse Gaussian also has additional elements in it. Unfortunately, we do not have information criteria for the models 2 - 6 in this case, because the likelihood function is not maximised with these losses, so it’s not possible to compare them via the in-sample statistics. But we can compare their holdout sample performance: round(sapply(adamModel,&quot;[[&quot;,&quot;accuracy&quot;),3)[c(&quot;ME&quot;,&quot;MAE&quot;,&quot;MSE&quot;),] ## likelihood MSE MAE HAM LASSO Huber ## ME 8.184 7.253 4.571 3.603 -16.753 30.633 ## MAE 19.514 18.805 16.489 16.075 19.547 38.131 ## MSE 633.863 601.831 496.085 496.800 635.992 1853.751 And we can also produce forecasts from them and plot those forecasts for the visual inspection: adamModelForecasts &lt;- lapply(adamModel,forecast, h=12, interval=&quot;prediction&quot;) layout(matrix(c(1:6),3,2,byrow=TRUE)) for(i in 1:6){ plot(adamModelForecasts[[i]], main=paste0(&quot;ETS(MAM) estimated using &quot;,names(adamModel)[i])) } What we observe in this case, is that different losses led to different forecasts and prediction intervals (we still assume Inverse Gaussian distribution) and that in case of LASSO, the shrinkage is so strong that the seasonality is shrunk to zero. What can potentially be done to make this practical is the rolling origin evaluation for different losses and then comparison of forecast errors between them in order to select the most efficient one. References "],["multistepLosses.html", "14.3 Multistep losses", " 14.3 Multistep losses Another class of losses that can be used in the estimation of ADAM models is the multistep losses. The idea behind them is to produce the point forecast for \\(h\\) steps ahead from each observation in sample and then calculate a measure based on that, which will be minimised by the optimiser in order to find the most suitable values of parameters. There is a lot of literature on this topic, Svetunkov et al. (2021) studied them in detail, showing that their usage implies shrinkage of smoothing parameters in case of ETS models. In this section we will discuss the most popular multistep losses, see what they imply and make a connection between these losses and predictive likelihoods from the ETS models. 14.3.1 \\(\\mathrm{MSE}_h\\) - MSE for h steps ahead One of the simplest estimators is the \\(\\mathrm{MSE}_h\\) - mean squared \\(h\\)-steps ahead error: \\[\\begin{equation} \\mathrm{MSE}_h = \\frac{1}{T-h} \\sum_{t=1}^{T-h} e_{t+h|t}^2 , \\tag{14.8} \\end{equation}\\] where \\(e_{t+h|t} = y_{t+h} - \\hat{\\mu}_{t+h|t}\\) is the conditional \\(h\\) steps ahead forecast error on the observation \\(t+h\\) produced from the point at time \\(t\\). This estimator is sometimes used to fit a model several times, for each horizon from 1 to \\(h\\) steps ahead, resulting in \\(h\\) different values of parameters for each \\(j=1, \\ldots, h\\). The estimation process in this case becomes at least \\(h\\) times more complicated than estimating one model, but is reported to result in increased accuracy. Svetunkov et al. (2021) show that MSE\\(_h\\) is proportional to the h steps ahead forecast error variance \\(V(y_{t+h}|t)=\\sigma^2_{h}\\), which implies that the minimisation of (14.8) leads to the minimisation of the variance and in turn to the minimisation of both one step ahead MSE and a combination of smoothing parameters of a model. This becomes more obvious in the case of pure additive ETS, where the analytical formulae for variance are available. The parameters are shrunk towards zero in case of ETS, making the model deterministic. The effect is neglected on large samples, when the ratio \\(\\frac{T-h}{T-1}\\) becomes close to one. In case of ARIMA, the shrinkage mechanism is similar, making models closer to the deterministic ones, but the direction of shrinkage is more complicating. The strength of shrinkage is proportional to the forecast horizon \\(h\\) and is weakened with the increase of the sample size. Svetunkov et al. (2021) demonstrate that the minimum of MSE\\(_h\\) corresponds to the maximum of the predictive likelihood based on the normal distribution, assuming that \\(\\epsilon_t \\sim N(0,\\sigma^2)\\). The log-likelihood in this case is: \\[\\begin{equation} \\ell_{\\mathrm{MSE}_h}(\\theta, {\\sigma^2_h} | \\mathbf{y}) = -\\frac{T-h}{2} \\left( \\log(2 \\pi) + \\log \\sigma^2_h \\right) -\\frac{1}{2} \\sum_{t=1}^{T-h} \\left( \\frac{\\eta_{t+h|}^2}{\\sigma^2_h} \\right) , \\tag{14.9} \\end{equation}\\] where \\(\\eta_{t+h|} \\sim N(0, \\sigma_h^2)\\) is the h steps ahead forecast error, conditional on the information available at time \\(t\\), \\(\\theta\\) is the vector of all estimated parameters of the model and \\(\\mathbf{y}\\) is the vector of \\(y_{t+h}\\) for all \\(t=1,..,T-h\\). The MLE of the scale parameter in (14.9) coincides with the MSE\\(_h\\): \\[\\begin{equation} \\hat{\\sigma}_h = \\frac{1}{T-h} \\sum_{t=1}^{T-h} e_{t+h|t}^2 , \\tag{14.10} \\end{equation}\\] where \\(e_{t+h|t}\\) is the in sample estimate of the \\(\\eta_{t+h}\\). The formula (14.9) can be used for the calculation of information criteria and in turn for the model selection in cases, when MSE\\(_h\\) is used for the model estimation. Svetunkov et al. (2021) demonstrate that (14.8) is more efficient than the conventional MSE\\(_1\\) one, when the true smoothing parameters are close to zero and is less efficient otherwise. On smaller samples, MSE\\(_h\\) produces biased estimates of parameters due to shrinkage. This can still be considered as advantage if you are interested in forecasting and do not want the smoothing parameters to vary a lot. 14.3.2 TMSE - Trace MSE An alternative to MSE\\(_h\\) is to produce 1 to \\(h\\) steps ahead forecasts and calculate the respective forecast errors. Then, based on that, we can calculate the overall measure, which we will call “Trace MSE”: \\[\\begin{equation} \\mathrm{TMSE} = \\sum_{j=1}^h \\frac{1}{T-h} \\sum_{t=1}^{T-h} e_{t+j|t}^2 . \\tag{14.11} \\end{equation}\\] The benefit of this estimator is in minimising the error for the whole 1 to \\(h\\) steps ahead in one model - there is no need to construct \\(h\\) models, minimising MSE\\(_j\\), \\(j=1,...,h\\). However, this comes with a cost: typically short term forecast errors have lower MSE than the longer term ones, so if we just sum their squares up, we are mixing different values, and the minimisation will be done mainly for the ones on the longer horizons. TMSE does not have a related predictive likelihood, so it is difficult to study its properties, but the simulations show that it tends to produce less biased and more efficient estimates of parameters than MSE\\(_h\\). Kourentzes et al. (2019b) showed that TMSE performs well in comparison with the conventional MSE\\(_1\\) and MSE\\(_h\\) in terms of forecasting accuracy and does not take as much time as the estimation of \\(h\\) models using MSE\\(_h\\). 14.3.3 GTMSE - Geometric Trace MSE An estimator that addresses the issues of TMSE, is the GTMSE, which is derived from a so called General Predictive Likelihood (GPL by Svetunkov et al., 2021). The word “Geometric” sums up how the value is calculated: \\[\\begin{equation} \\mathrm{GTMSE} = \\sum_{j=1}^h \\log \\left(\\frac{1}{T-h} \\sum_{t=1}^{T-h} e_{t+j|t}^2 \\right). \\tag{14.12} \\end{equation}\\] Logarithms in the formula (14.12) bring the MSEs on different horizons to the same level, so that the both short term and long term errors are minimised with a similar power. As a result, the shrinkage effect in this estimator is milder than in MSE\\(_h\\) and in TMSE and the estimates of parameters are less biased and more efficient on smaller samples. It still has the benefits of other multistep estimators, shrinking the parameters towards zero. Although it is possible to derive a predictive likelihood that would be maximised, when GTMSE is minimised, it relies on unrealistic assumptions of independence of multistep forecast errors (they are always correlated as long as smoothing parameters are not zero). 14.3.4 MSCE - Mean Squared Cumulative Error This estimator alings the loss function with a specific inventory decision: ordering based on the lead time \\(h\\): \\[\\begin{equation} \\mathrm{MSCE} = \\frac{1}{T-h} \\sum_{t=1}^{T-h} \\left( \\sum_{j=1}^h e_{t+j|t} \\right)^2 . \\tag{14.13} \\end{equation}\\] Kourentzes et al. (2019a) demonstrated that it produces more accurate forecasts in cases of intermittent demand and leads to less revenue losses. Svetunkov et al. (2021) show that the shrinkage effect is much stronger in this estimator than in the other estimators discussed in this section. In addition, it is possible to derive a predictive log-likelihood related to this estimator: \\[\\begin{equation} \\ell_{\\mathrm{MSCE}}(\\theta, {\\varsigma^2_h} | \\mathbf{z}) = -\\frac{T-h}{2} \\left( \\log(2 \\pi) + \\log {\\varsigma^2_h} \\right) -\\frac{1}{2} \\sum_{t=1}^{T-h} \\left( \\frac{\\left(\\sum_{j=1}^h \\eta_{t+j|t}\\right)^2}{2 {\\varsigma^2_h}} \\right) , \\tag{14.14} \\end{equation}\\] where \\(\\mathbf{z}\\) is the cumulative sum of actual values, the vector of \\(z_t=\\sum_{j=1}^h y_{t+j}\\) for all \\(t=1, \\ldots, T-h\\) and \\({\\varsigma^2_h}\\) is the variance of the cumulative error term, the MLE of which is equal to (14.13). Having the likelihood (14.14) permits the model selection and combination using information criteria and also means that the parameters estimated using MSCE will be asymptotically consistent and efficient. 14.3.5 GPL - General Predictive Likelihood Finally, Svetunkov et al. (2021) studied the General Predictive Likelihood for normally distributed variable from Clements and Hendry (1998), p.77, logarithm version of which can be written as: \\[\\begin{equation} \\ell_{\\mathrm{GPL}_h}(\\theta, \\mathbf{{\\Sigma}} | \\mathbf{Y}) = -\\frac{T-h}{2} \\left( h \\log(2 \\pi) + \\log | \\mathbf{{\\Sigma}}| \\right) -\\frac{1}{2} \\sum_{t=1}^T \\left( \\mathbf{E_t^\\prime} \\mathbf{{\\Sigma}}^{-1} \\mathbf{E_t} \\right) , \\tag{14.15} \\end{equation}\\] where \\(\\mathbf{{\\Sigma}}\\) is the conditional covariance matrix for variable \\(\\mathbf{Y}_t\\), \\(\\mathbf{Y}\\) is the matrix consisting of for all \\(t=1, \\ldots, T-h\\) and \\(\\mathbf{E_t}^{\\prime} = \\begin{pmatrix} \\eta_{t+1|t} &amp; \\eta_{t+2|t} &amp; \\ldots &amp; \\eta_{t+h|t} \\end{pmatrix}\\) is the vector of 1 to \\(h\\) steps ahead forecast errors. Svetunkov et al. (2021) showed that the maximisation of the likelihood (14.15) is equivalent to minimising the generalised variance of the error term, \\(|\\mathbf{\\hat{\\Sigma}}|\\), where: \\[\\begin{equation} \\mathbf{\\hat{\\Sigma}} = \\frac{1}{T-h} \\sum_{t=1}^{T-h} \\mathbf{E_t} \\mathbf{E_t^\\prime} = \\begin{pmatrix} \\hat{\\sigma}_1^2 &amp; \\hat{\\sigma}_{1,2} &amp; \\dots &amp; \\hat{\\sigma}_{1,h} \\\\ \\hat{\\sigma}_{1,2} &amp; \\hat{\\sigma}_2^2 &amp; \\dots &amp; \\hat{\\sigma}_{2,h} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\hat{\\sigma}_{1,h} &amp; \\hat{\\sigma}_{2,h} &amp; \\dots &amp; \\hat{\\sigma}_h^2 \\end{pmatrix} , \\tag{14.16} \\end{equation}\\] where \\(\\hat{\\sigma}_{i,j}\\) is the covariance between \\(i\\)-th and \\(j\\)-th steps ahead forecast errors. Svetunkov et al. (2021) show that this estimator encompassess all the other estimators discussed in this section: minimising MSE\\(_h\\) is equivalent to minimising the \\(\\hat{\\sigma}^2_{h}\\); minimising TMSE is equivalent to minimising the trace of the matrix \\(\\mathbf{\\hat{\\Sigma}}\\); minimising GTMSE is the same as minimising the determinant of \\(\\mathbf{\\hat{\\Sigma}}\\) but with the restriction that all off-diagonal elements are equal to zero; minimising MSCE produces the same results as minimising the sum of all elements in \\(\\mathbf{\\hat{\\Sigma}}\\). However, maximum of GPL is equivalent to the maximum of the conventional one step ahead likelihood for a normal model in case, when all the basic assumptions hold. In other cases, they would be different, but it is still not clear, whether the difference would be favouring the conventional likelihood of the GPL. Nonetheless, GPL, being the likelihood, guarantees that the estimates of parameters will be efficient and consistent and permits model selection and combination via information criteria. 14.3.6 Other multistep estimators It is also possible to derive multistep estimators based on MAE, HAM and other error measures. adam() unofficially supports the following multistep losses by analogie with MSE\\(_h\\), TMSE and MSCE discussed in this section: MAE\\(_h\\); TMAE; MACE; HAM\\(_h\\); THAM; CHAM. 14.3.7 An example in R In order to see how different estimators perform, we will us Box-Jenkins Sales data, ETS(A,A,N) model and \\(h=10\\): adamModel &lt;- vector(&quot;list&quot;,6) names(adamModel) &lt;- c(&quot;MSE&quot;,&quot;MSEh&quot;,&quot;TMSE&quot;,&quot;GTMSE&quot;,&quot;MSCE&quot;,&quot;GPL&quot;) for(i in 1:length(adamModel)){ adamModel[[i]] &lt;- adam(BJsales, &quot;AAN&quot;, loss=names(adamModel)[i], h=10, holdout=TRUE) } We can compare the smoothing parameters of these models to see how the shrinkage effect worked in different estimators: round(sapply(adamModel,&quot;[[&quot;,&quot;persistence&quot;),5) ## MSE MSEh TMSE GTMSE MSCE GPL ## alpha 0.99969 0.99997 0.99907 0.99999 0.99936 0.99143 ## beta 0.24029 0.00489 0.00000 0.14493 0.11485 0.00088 In the table above we can see that \\(\\beta\\) is close to zero for the estimators that impose harder shrinkage on parameters: MSE\\(_h\\), TMSE and MSCE. MSE does not shrink the parameters, while GTMSE has the mild shrinkage effect. While the models estimated using these losses are in general not comparable in-sample (although MSE, MSE\\(_h\\), MSCE and GPL could be compared via information criteria if they are scaled appropriately), they are comparable on the holdout: round(sapply(adamModel,&quot;[[&quot;,&quot;accuracy&quot;),5)[c(&quot;ME&quot;,&quot;MAE&quot;,&quot;MSE&quot;),] ## MSE MSEh TMSE GTMSE MSCE GPL ## ME 3.22544 0.34613 1.10232 3.44956 3.38311 0.18437 ## MAE 3.33751 1.10853 1.44602 3.53724 3.47792 1.12323 ## MSE 14.38952 1.69945 2.98891 16.26285 15.69350 1.61258 In this specific case, ETS(A,A,N) estimated using MSE\\(_h\\) produced more accurate forecasts than based on the other estimators. Repeating the experiment on many samples and selecting the approach that produces more accurate forecasts would allow to select the most appropriate approach for the combination of the model + data. References "],["ADAMInitialisation.html", "14.4 Initialisation of ADAM", " 14.4 Initialisation of ADAM In order to construct a model, we need to initialise it, defining the values of \\(\\mathbf{v}_{-m+1}, \\dots, \\mathbf{v}_0\\) - initial states of the model. There are different ways of doing that, but here we only discuss the following three: Optimisation of initials, Backcasting, Provided values. The first option implies that the values of initial states are found in the same procedure as the other parameters of the model. (2) means that the initials are refined iteratively, when the model is fit to the data from observation \\(t=1\\) to \\(t=T\\) and backwards. Finally, (3) is when a user knows initials and provides them to the model. As a side note, we assume in ADAM that the model is initialised at the moment just before \\(t=1\\), we do not believe that it was initialised some time before the Big Bang (as ARIMA typically does) and we do not initialise it at the start of the sample. This way we make all models in ADAM comparable, making them work on exactly the same sample, no matter how many differences are taken or how many seasonal components they contain. 14.4.1 Optimisation vs backcasting In case of optimisation, all the parameters of model are estimated together. This includes (depending on the type of model): Smoothing parameters of ETS, Smoothing parameters for the regression part of the model, Dampening parameter of ETS, Parameters of ARIMA: both AR(p) and MA(q), Initials of ETS, Initials of ARIMA, Initial values for explanatory variables, Constant / drift for ARIMA, Other additional parameters needed by assumed distribuitons. The more complex the selected model is, the more parameters we will need to estimate, and all of this will happen in one and the same iterative process in the optimiser: Choose parameters, Fit the model, Calculate loss function, Compare the loss with the previous one, Update the parameters based on (4), Go to (2) and repeat until a specific criterion is met. The stopping criteria can be different and specified by the user. There are several options considered by the optimiser of adam(): Maximum number of iterations (maxeval), which is equal to \\(40\\times k\\), where \\(k\\) is the number of all estimated parameters; The relative precision of the optimiser (xtol_rel) with default value of \\(10^{-6}\\), which regulates the relative change of parmaters; The absolute precision of the optimiser (xtol_abs) with default value of \\(10^{-8}\\), which regulates the absolute change of parmaters; The stopping criterion in case of the relative change in the loss function (ftol_rel) with default value of \\(10^{-8}\\); All these parameters are explained in more detail in the documentation of nloptr() function from nloptr package for R. adam() accepts several other stopping criteria, which can be found in the documentation of the function. The mechanism explained above can become quite complicated if a big complex model is constructed and might take a lot of time and manual tuning of parameters in order to get to the optimum. In some cases, it is worth considering to reduce the number of estimated parameters, and one way of doing so is the backcasting. In case of backcasting we do not need to estimate initials of ETS, ARIMA and regression. What model does in this case is goes through the series from \\(t=1\\) to \\(t=T\\), fitting to the data and then reverses and goes back from \\(t=T\\) to \\(t=1\\) based on the following state space model: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp;w(\\mathbf{v}_{t+\\boldsymbol{l}}) + r(\\mathbf{v}_{t+\\boldsymbol{l}}) \\epsilon_t \\\\ \\mathbf{v}_{t} = &amp;f(\\mathbf{v}_{t+\\boldsymbol{l}}) + g(\\mathbf{v}_{t+\\boldsymbol{l}}) \\epsilon_t \\end{aligned}. \\tag{14.17} \\end{equation}\\] The new values of \\(\\mathbf{v}_t\\) for \\(t&lt;1\\) are then used in order to fit the model to the data again. The procedure can be repeated several times in order for the initial states to converge to more reasonable values. The backcasting procedure implies the extended fitting process for the model, removing the need to estimate all the initials. It works especially well, on large samples of data (thousands of observations) and with models with several seasonal components. The bigger your model is, the more time the optimisation will take and the more likely backcasting would do better. On the other hand, you might also prefer backcasting to optimisation in cases of small samples, when you do not have more than two seasons of data - estimation of initial seasonal components might become challenging and can potentially lead to overfitting. When talking about specific models, ADAM ARIMA works better (faster and more accurate) with backcasting than with optimisation, because it does not need to estimate as many parameters as in the latter case. ADAM ETS, on the other hand, typically works quite well in case of optimisation, when there is enough data to train the model on. Last but not least, if you introduce explanatory variables, then optimising the initial states might be a better option than backcasting, unless you use a dynamic ETSX / ARIMAX, because the initial parameters for the explanatory variables will not be updated in the latter case. It is also important to note that the information criteria of models with backcasting are typically lower than in case of the optimised initials. This is because the difference in the number of estimated parameters is substantial in these two cases and the models are initialised differently. So, it is advised not to mix the model selection between the two initialisation techniques. Nonetheless, no matter what initialisation method is used, we need to start the fitting process from \\(t=1\\), and this cannot be done unless we provide some pre-initialised values for parameters to the optimiser. The better we guess the initial values, the faster the optimiser will converge to the optimum. adam() uses several heuristics in this stage, explained in more detail in the next subsections. 14.4.2 Pre-initialisation of ADAM parameters In this subsection we discuss how the value of smoothing parameter, damping parameter and coefficients of ARIMA are preset before the initialisation. All the things discussed here are heuristic, developed based on my experience and many experiments with ADAM ETS. Depending on the type of model, the vector of estimated parameters will contain different values. We start with smoothing parameters of ETS: For the unsafe mixed models ETS(A,A,M), ETS(A,M,A), ETS(M,A,A) and ETS(M,A,M): \\(\\hat{\\alpha}=0.01\\), \\(\\hat{\\beta}=0\\) and \\(\\hat{\\gamma}=0\\). This is needed because the models listed above are very sensitive to the changes in smoothing parameters and might fail for time series with level close to zero; For the one of the most complicated and sensitive models ETS(M,M,A) \\(\\hat{\\alpha}=\\hat{\\beta}=\\hat{\\gamma}=0\\). The combination of additive seasonality and multiplicative trend is one of the most difficult ones. The multiplcative error makes estimation even more challenging in cases of the low level data. So starting from the deterministic model, that will work for sure is a safe option; ETS(M,A,N) is slightly easier to estimate than ETS(M,A,M) and ETS(M,A,A), so \\(\\hat{\\alpha}=0.2\\), \\(\\hat{\\beta}=0.01\\). The low value for the trend is needed to avoid the difficult situations with low level data, when the fitted values become negative; ETS(M,M,N) and ETS(M,M,M) have \\(\\hat{\\alpha}=0.1\\), \\(\\hat{\\beta}=0.05\\) and \\(\\hat{\\gamma}=0.01\\), making the ternd and seasonal components a bit more conservative. The high values are not needed in this model as they might lead to explosive behaviour; Other models with multiplicative components (ETS(M,N,N), ETS(M,N,A), ETS(M,N,M), ETS(A,N,M), ETS(A,M,N) and ETS(A,M,M)) are slightly easier to estimate and harder to break, so their parameters are set to \\(\\hat{\\alpha}=0.1\\), \\(\\hat{\\beta}=0.05\\) and \\(\\hat{\\gamma}=0.05\\); Finally, pure additive models are initialised with \\(\\hat{\\alpha}=0.1\\), \\(\\hat{\\beta}=0.05\\) and \\(\\hat{\\gamma}=0.11\\). Their parameter space is the widest, and the models do not break on any data. The smoothing parameter for the explanatory variables is set to \\(\\hat{\\delta}=0.01\\) in case of additive error and \\(\\hat{\\delta}=0\\) in case of the multiplicative one. The latter is done because the model might break if some of ETS components are additive. If dampening parameter is needed in the model, then its pre-initialised value is \\(\\hat{\\phi}=0.95\\). In case of ARIMA, the parameters are pre-initialised based on ACF and PACF. First, the in sample actual values are differenced, according to the selected order \\(d\\) and all \\(D_j\\), after which the ACF and PACF are calculated. Then the initials for AR parameters are taken from the PACF, while the initials for MA parameters are taken from ACF, making sure that the sum of parameters is not greater than one in both cases. If it is, then the parameters are renormalised to satisfy the condition. The reason behind this mechanism is to get a potentially correct direction towards the optimal parameters of the model and make sure that the initial values satisfy the very basic stationarity and invertibility conditions. In cases, when it is not possible to calculate ACF and PACF for the specified lags and orders, AR parameters are set to -0.1, while the MA parameters are set to 0.1, making sure that the conditions mentioned above hold. If the skewness parameter of Asymmetric Laplace distribution is estimated, then its initial value is set to 0.5, corresponding to the median of the data. In case of Generalised Normal distribution, the shape parameter is set to 2 (if it is estimated), makign the optimiser start from the conventional Normal distribution. The pre-initialisations described above guarantee that the model is estimable for a wide variety of time series and that the optimiser will reach the optimum in a limited time. If for a specific case, it does not work, a user can provide their own vector of pre-initialised parameters via the parameter B in ellipsis of the model. Furthermore, the typical bounds for the parameters can be tuned as well. For example, the bounds for smoothing parameters in ADAM ETS are (-5, 5), and they are needed only to simplify the optimisation procedure. The function will check the violation of either usual or admissible bounds inside the optimiser, but having some ideas of where to search for optimal parameters, helps. A user can provide their own vector for lower bound via lb and for the upper one via ub. 14.4.3 Pre-initialisation of ADAM states, ETS The pre-initialisation of states of ADAM ETS differs depending on whether the model is seasonal or not. If it is, then the multiple seasonal decomposition is done using msdecompose() function from smooth with the seasonality set to “multiplicative” if either error or seasonal component of ETS is multiplicative. After that: Initial level is then equal to the first initial value from the function (which is the back forecasted de-seasonalised series); The value is corrected if regressors are included to remove their impact on the value (either by subtracting the fitted of the regression part or by dividing by them - depending on the type of error); If trend is additive and seasonality is multiplicative, then the trend component is obtained by multiplying the initial level and trend from the decomposition (remember, the assumed model is multiplicative in this case) and then subtracting the previous level; If trend is multiplicative and seasonality is additive, then the initials are added and then divided by the previous level to get the initial multiplicative trend component; If there is no seasonality and trend is multiplicative, then the initial trend is set to 1. This is done in order to avoid the potentially explosive behaviour of the model; If the trend is multiplicative and level is negative, then the level is substituted by the first actual value. This might happen in some weird cases of time series with low values; When it comes to seasonal components, if we have pure additive, or pure multiplicative ETS model or ETS(A,Z,M), then we use the seasonal indices, obtained from the msdecompose() function, making sure that they are normalised. The type of seasonality in msdecompose() corresponds to the seasonal component of ETS in this case, and nothing additional needs to be done; The situation is more challenging with ETS(M,Z,A), for which the decomposition would return the multiplicative seasonal components. In order to convert them to the additive, we take their logarithm and multiply them by the minimum value of the actual time series. This way we guarantee that the seasonal components are closer to the optimal ones. In case of the non-seasonal model, the algorithm is easier: The initial level is equal to either arithmetic or geometric mean (depending on the type of trend component) of the first \\(\\max(m_1,\\dots,m_n)\\) observations, where \\(m_j\\) is the seasonal periodicity. If the length of this mean is smaller than 20% of the sample, then the arithmetic mean of the first 20% of actual values is used; If regressors are included, then the value is modified, similar to how it is done in the seasonal ETS; If the model has additive trend then its initial value is equal to the mean difference between first \\(\\max(m_1,\\dots,m_n)\\) observations; In case of multiplicative trend, initial value is equal to the to the geometric mean of ratios between first \\(\\max(m_1,\\dots,m_n)\\) observations; In cases of the small samples (less than 2 seasonal periods), the procedure is similar to the one above, but the seasonal indices are obtained by taking the actual values and either subtracting an arithmetic mean or dividing them by the geometric one of the first \\(m_j\\) observations, normalising them afterwards. Finally, to make sure that the safe initials were provided, for the ETS(M,Z,Z) models, if the initial level contains negative value, then it is substituted by the global mean of the series. The pre-initialisation described here is not simple, but it guarantees that any ETS model can be constructed and estimated almost to any data. Yes, there might still be some issues with mixed ETS models, but the mechanism used in ADAM is quite robust. 14.4.4 Pre-initialisation of ADAM states, ARIMA Each state \\(v_{i,t}\\) needs to be initialised with \\(i\\) values (e.g. 1 for the first state, 2 for the second etc). This leads in general to more initial values for states than the SSARIMA from Svetunkov and Boylan (2020b): \\(\\frac{K(K+1)}{2}\\) instead of \\(K\\). However, this formulation has a more compact transition matrix, leading to computational improvements in terms of applying the model to the data with large \\(K\\) (e.g. multiple seasonalities). Besides, we can reduce the number of initial seeds to estimate either by using a different initialisation procedure (e.g. backcasting) or estimating directly \\(y_t\\) and \\(\\epsilon_t\\) for \\(t=\\{-K+1, -K+2, \\dots, 0\\}\\) to obtain the initials for each state via the formula (12.5). In order to reduce the number of estimated parameters to \\(K\\), we can take the conditional expectations for the states, in which case we will have: \\[\\begin{equation*} \\mathrm{E}(v_{i,t} | t) = \\eta_i y_{t} \\text{ for } t=\\{-K+1, -K+2, \\dots, 0\\}, \\end{equation*}\\] and then use these expectations for the initialisation of ARIMA. A the same time, we can express the actual value in terms of the state and error from (12.2) for the last state \\(K\\): \\[\\begin{equation} y_{t} = \\frac{v_{K,t} - \\theta_K \\epsilon_{t}}{\\eta_K}. \\tag{14.18} \\end{equation}\\] We select the last state \\(K\\) because it has the highest number of initials to estimate among all states. We can then insert the value (14.18) in each formula for each state for \\(i=\\{1, 2, \\dots, K-1\\}\\) and take their expectations: \\[\\begin{equation} \\mathrm{E}(v_{i,t}|t) = \\frac{\\eta_i}{\\eta_K} \\mathrm{E}(v_{K,t}|t) \\text{ for } t=\\{-i+1, -i+2, \\dots, 0\\}. \\tag{14.19} \\end{equation}\\] So the process then comes to estimating the initials states of \\(v_{K,t}\\) for \\(t=\\{-K+1, -K+2, \\dots, 0\\}\\) and then propagating them to the other states. However, this strategy will only work for the states corresponding to ARI elements of model. In case of MA(q), using the same principle of initialisation via the conditional expectation, we can set the initial MA states to zero and estimate only ARI states. This is a crude but relatively simple way to pre-initialise ADAM ARIMA. Having said all that, we need to point out that it is advised to use backcasting in case of ADAM ARIMA model - this is a more reliable and a faster procedure for initialisation of ARIMA than the optimisation. 14.4.5 Pre-initialisation of ADAM states, Regressors and constant When it comes to the initials for the regressors, they are obtained from the parameters of the alm() model based on the rules below: The model with logarithm of response variable is constructed, if the error term is multiplicative and one of the following distributions has been selected: Normal, Laplace, S, Generalised Normal or Asymmetric Laplace; Otherwise the model is constructed based on provided formula and selected distribution; In both cases, the global trend is added to the formula to make sure that its effect on the values of parameters is reduced; If the data contains categorical variables (aka “factors” in R), then the factors are expanded to dummy variable, adding the baseline value as well. While the classical multiple regression would not be estimable in this situation, the dynamic models like ETSX and ARIMAX can work with the full set of levels of categorical variable. In order to get the missing level, the intercept is added to the parameters of dummy variables, after which the obtained vector is normalised. This way we can get, for example, all seasonal components if we want to model seasonality via X part of the model, not merging one of the components with level. Finally, the initialisation of constant (if it is needed in the model) is done depending on the selected model. In case of ARIMA with all \\(D_j=0\\), the mean of the data is used. In all the other cases either the arithmetic mean of difference or geometric mean of ratios of all actual values is used. This is because the constant acts as a drift in the model in this situation. The impact of the constant is removed from the level in ETS and the states of ARIMA by either subtraction, or division. References "],["multiple-frequencies-in-adam-ets.html", "Chapter 15 Multiple frequencies in ADAM ETS", " Chapter 15 Multiple frequencies in ADAM ETS James W. Taylor (2003a) proposed an exponential smoothing model with double seasonality and applied it to energy data. Since then, the topic was developed by Gould et al. (2008), Taylor (2008), Taylor (2010), De Livera (2010) and De Livera et al. (2011). In this chapter we will discuss some of the proposed models, how they relate to the ADAM framework and can be implemented. Roughly, the idea of a model with multiple seasonalities is in introducing additional seasonal components. For the general framework this means that the state vector (for example, in a model with trend and seasonality) becomes: \\[\\begin{equation} \\mathbf{v}_t&#39; = \\begin{pmatrix} l_t &amp; b_t &amp; s_{1,t} &amp; s_{2,t} &amp; \\dots &amp; s_{n,t} \\end{pmatrix}, \\tag{15.1} \\end{equation}\\] where \\(n\\) is the number of seasonal components (e.g. hour of day, hour of week and hour of year components). The lag matrix in this case becomes: \\[\\begin{equation} \\boldsymbol{l}&#39;=\\begin{pmatrix}1 &amp; 1 &amp; m_1 &amp; m_2 &amp; \\dots &amp; m_n \\end{pmatrix}, \\tag{15.2} \\end{equation}\\] where \\(m_i\\) is the \\(i\\)-th seasonal periodicity. While, in theory there can be combinations between additive and multiplicative seasonal components, we argue that such a mixture does not make sense, and the components should align with each other. This means that in case of ETS(M,N,M), all seasonal components should be multiplicative, while in ETS(A,A,A) they should be additive. This results fundamentally in two types of models: Additive seasonality: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = \\check{y}_t + s_{1,t-m_1} + \\dots + s_{n,t-m_n} \\epsilon_t \\\\ &amp; \\vdots \\\\ &amp; s_{1,t} = s_{1,t-m_1} + \\gamma_1 \\epsilon_t \\\\ &amp; \\vdots \\\\ &amp; s_{n,t} = s_{n,t-m_n} + \\gamma_n \\epsilon_t \\end{aligned}, \\tag{15.3} \\end{equation}\\] where \\(\\check{y}_t\\) is the point value based on all non-seasonal components (e.g. \\(\\check{y}_t=l_{t-1}\\) in case of no trend model) and \\(\\gamma_i\\) is the \\(i\\)-th seasonal smoothing parameter. Multiplicative seasonality: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = \\check{y}_t \\times s_{1,t-m_1} \\times \\dots \\times s_{n,t-m_n} \\times(1+\\epsilon_t) \\\\ &amp; \\vdots \\\\ &amp; s_{1,t} = s_{1,t-m_1} (1 + \\gamma_1 \\epsilon_t) \\\\ &amp; \\vdots \\\\ &amp; s_{n,t} = s_{n,t-m_n} (1+ \\gamma_n \\epsilon_t) \\end{aligned}. \\tag{15.4} \\end{equation}\\] Depending on a specific model, the number of seasonal components can be 1, 2, 3 or more (although more than 3 might not make much sense from modelling point of view). De Livera (2010) introduced components based on fourier terms, updated over time via smoothing parameters. This feature is not yet fully supported in adam(), but it is possible to substitute some of seasonal components (especially those that have fractional periodicity) with fourier terms via explanatory variables and update them over time. The explanatory variables idea was discussed in the previous chapter. References "],["estimation-of-multiple-seasonal-model.html", "15.1 Estimation of multiple seasonal model", " 15.1 Estimation of multiple seasonal model 15.1.1 ADAM ETS issues Estimating a multiple seasonal ETS model is a challenging task, because the number of parameters becomes large. The number of parameters related to seasonal components is equal in general to \\(\\sum_{j=1}^n m_j + n\\). For example, in case of hourly data, a triple seasonal model for hours of day, hours of week and hours of year will have: \\(m_1 = 24\\), \\(m_2 = 24 \\times 7 = 168\\) and \\(m_3= 7 \\times 24 \\times 365 = 61320\\), resulting overall in \\(24 + 168 + 61320 + 3 = 61498\\) parameters related to seasonal components to estimate. This is not a trivial task and would take hours to converge to optimum, unless the pre-initials are already close to optimum. So, if you want to construct multiple seasonal ADAM ETS model, it makes sense to use a different initialisation, reducing the number of estimated parameters. A possible solution in this case is backcasting. The number of parameters in our example would reduce from 61498 to 3, substantially speeding up the model estimation process. Another consideration is fitting model to the data. In the conventional ETS, the size of transition matrix is equal to the number of initial parameters, which makes it too slow to be practical on high frequency data (multiplying a matrix \\(61498 \\times 61498\\) matrix by a vector with rows is a difficult task even for modern computers). But due to the lagged structure of ADAM models, construction of multiple seasonal models does not take as much time, because we end up multiplying a matrix of \\(3 \\times 3\\) by a vector with 3 rows (skipping level and trend, which would add two more elements). So, in ADAM, the main computational burden comes from recursive relation in transition equation of the state space model, because this operation needs to be repeated at least \\(T\\) times, whatever the sample size \\(T\\) is. As a result, you would want to get to the optimum with as fewer iterations as possible, not needing to refit the model with different parameters to the same data many times. This gives another motivation for reducing the number of parameters to estimate (and thus for using backcasting). Another potential simplification would be to use deterministic seasonality for some of seasonal frequencies. The possible solution in this case is to use explanatory variables for the higher frequency states (see discussion in the (next section)[#ETSXMultipleSeasonality]) or use multiple seasonal ETS, setting some of smoothing parameters equal to zero. Finally, given that we deal with large samples of data, some of states of ETS might become more reactive than needed, having higher than needed smoothing parameters. One of possible ways to overcome this limitation is by using multistep loss functions. For example, Kourentzes and Trapero (2018) showed that using such loss functions as TMSE in the estimation of ETS models on high frequency data leads to improvements in accuracy due to the shrinkage of parameters towards zero, mitigating the potential overfitting issue. The only problem with this approach is that it is more computationally expensive and thus would take more time (at least \\(h\\) times more, where \\(h\\) is the length of the forecast horizon). 15.1.2 ADAM ARIMA issues It is also possible to fit multiple seasonal ARIMA to the high frequency data, and, for example, Taylor (2010) used triple seasonal ARIMA on example of two time series, and demonstrated that it produced more accurate forecasts than other ARIMAs under consideration, even slightly outperforming ETS. The main issue with ARIMA arises in the model selection direction. While in case of ETS, one can decide, what model to use based on judgment (e.g. there is no obvious trend, and the amplitude increases with the increase of level, so we will fit ETS(M,N,M) model), ARIMA requires more careful consideration of possible orders of the model. Selecting appropriate orders of ARIMA is not a trivial task on its own, but selecting the orders on high frequency data (where correlations might appear significant just because of the sample size) becomes even more challenging task than usual. Furthremore, while on monthly data we typically maximum AR and MA orders of the model with 3 or 5, in case of high frequency data this does not look natural anymore. If the first seasonal component has lag of 24, then in theory anything up until 24 might be useful for the model. Long story short, be prepared for the lengthy investigation of appropriate ARIMA orders. While ADAM ARIMA implements an efficient order selection mechanism for ARIMA, it does not guarantee that the most appropriate model will be applied to the data. Inevitably, you would need to analyse the residuals, add higher orders and see if there is an improvement in performance of the model. The related issue to this in context of ADAM ARIMA is the dimensionality problem. The more orders you introduce in the model, the bigger transition matrix becomes. This leads to the same issues as in the ADAM ETS, discussed in the previous subsection. There is no unique recipe in this difficult situation, but using backcasting addresses some of these issues. You might also want to fine tune the optimiser to get a balance between speed and accuracy in the estimation of parameters (see discussion in Subection 12.4). References "],["ETSXMultipleSeasonality.html", "15.2 Using explanatory variables for multiple seasonalities", " 15.2 Using explanatory variables for multiple seasonalities The conventional way of introducing several seasonal components has several issues: It only works with the data with fixed periodicity (the problem sometimes referred to as “fractional frequency”): if \\(m_i\\) is not fixed and changes from period to period, the model becomes disaligned. An example of such problem is fitting ETS on daily data with \\(m=365\\), while there are leap years that contain 366 days; If the model is fit on high frequency data, the problem of parameters estimation becomes non-trivial. Indeed, on daily data with \\(m=365\\), we need to estimate 364 initial seasonal indices together with the other parameters; Different seasonal indices would “compete” with each other for each observation, thus making the model overfit the data. An example is the daily data with \\(m_1=7\\) and \\(m_2=365\\), where both seasonal components are updated on each observation based on the same error, but with different smoothing parameters. The situation becomes even more complicated, when the model has more than two seasonal components. But there are at least two ways of resolving these issues in ADAM framework. The first is based on the idea of De Livera (2010) and the dynamic ETSX. In this case we need to generate fourier series and use them as explanatory variables in the model, switching on the mechanism of adaptation. For example, for the pure additive model, in this case, we will have: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = \\check{y}_t + \\sum_{i=1}^p a_{i,t-1} x_{i,t} + \\epsilon_t \\\\ &amp; \\vdots \\\\ &amp; a_{i,t} = a_{i,t-1} + \\delta_i \\frac{\\epsilon_t}{x_{i,t}} \\text{ for each } i \\in \\{1, \\dots, p\\} \\end{aligned}, \\tag{15.5} \\end{equation}\\] where \\(p\\) is the number of fourier harmonics. In this case, we can introduce the conventional seasonal part of the model for the fixed periodicity (e.g. days of week) in \\(\\check{y}_t\\) and use the updated harmonics for the non-fixed one. This approach is not the same as the one in De Livera (2010), but might lead to similar results. The only issue here is in the selection of the number of harmonics, which can be done via the variables selection mechanism, but would inevitably increase computational time. The second option is based on the idea of dynamic model with categorical variables. In this case, instead of trying to fix the problem with days of year, we first introduce the categorical variables for days of week and then for the weeks of year (or months of year if we can assume that the effects of months are more appropriate). After that we can introduce both categorical variables in the model, using the similar adaptation mechanism to (15.5). In fact, if some of variables have fixed periodicity, we can substitute them with the conventional seasonal components. So, for example, in this case, ETSX(M,N,M)[7]{D} could be written as: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = l_{t-1} s_{t-7} \\times \\prod_{i=1}^q \\exp(a_{i,t-1} x_{i,t}) (1 + \\epsilon_t) \\\\ &amp; l_t = l_{t-1} (1 + \\alpha\\epsilon_t) \\\\ &amp; s_t = s_{t-7} (1 + \\gamma\\epsilon_t) \\\\ &amp; a_{i,t} = a_{i,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta \\log(1+\\epsilon_t) \\text{ for each } i \\in \\{1, \\dots, q\\}, \\text{ if } x_{i,t} = 1 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\end{aligned}, \\tag{15.6} \\end{equation}\\] where \\(q\\) is the number of levels in the categorical variable (for weeks of year, this should be 53). The number of parameters to estimate in this case might be greater than the number of harmonics in the first case, but this type of model resolves all three issues as well and does not have the dilema about selecting the number of harmonics. References "],["MultipleFrequenciesDSTandLeap.html", "15.3 Dealing with daylight saving and leap years", " 15.3 Dealing with daylight saving and leap years Another problem that arises in case of data with high frequency is the change of local time due to daylight saving (DST). This happens in some countries two times a year: in Spring the time is moved one hour forward (typically at 1am to 2am), while in the Autumn it is moved back one hour. The implications of this are terrifying from forecasting point of view, because one day of year has 23 hours, while the other one has 25 hours, while all the business processes are aligned to the local time. This means that if the conventional seasonal ETS model with \\(m=24\\) is fit to the data, it will only work correctly in a half of year. Well, it will adapt to the new patterns after some times, but this implies that the smoothing parameter \\(\\gamma\\) will be higher than needed. There are two solutions to this problem: 1. Shift the periodicity for one day, when the time changes from 24 to either 23, or 25, depending on the time of year; 2. Introduce categorical variables for factors, which will mark specific hours of day; The former is more difficult to formalise mathematically and implement in software, but the latter relies on the already discussed mechanism of ETSX{D} with categorical variables and should be more straightforward. Given the connection between seasonality in the conventional ETS model and the ETSX{D} with categorical variables for seasonality, both approaches should be equivalent in terms of parameters estimation and final forecasts. Similarly, the problem with leap years can be solved either using the shift from \\(m=365\\) to \\(m=366\\) on 29th February in a spirit of the option (1), or using the categorical variables, approach (2). There is a difference, however: the former would be suitable for the data with only one leap year, where the estimation of the seasonal index for 29th February might be difficult, while the latter assumes the separate estimation of the parameter (so it has one more parameter to estimate). However, given the discussion in the previous section, maybe we should not bother with \\(m=365\\) in the first place and rethink the problem, if possible. Having 52 / 53 weeks in a year has similar difficulties, but at least does not involve the estimation of so many initial seasonal states. "],["ADAMMultipleFrequenciesExamples.html", "15.4 Examples of application", " 15.4 Examples of application 15.4.1 ADAM ETS In order to see how ADAM can be applied to high frequency data, we will use taylor series from forecast package. This is half-hourly electricity demand in England and Wales from Monday 5 June 2000 to Sunday 27 August 2000, used in James W. Taylor (2003b). library(zoo) y &lt;- zoo(forecast::taylor, order.by=as.POSIXct(&quot;2000/06/05&quot;)+(c(1:length(forecast::taylor))-1)*60*30) ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo plot(y) Note that when you have data with DST or Leap years, adam() will automatically correct the seasonal lags if your data contains specific dates (as zoo objects have, for example). The series above does not exhibit an obvious trend, but has two seasonal cycles: half-hour of day and day of week. Seasonality seems to be multiplicative. We will try several different models and see how they compare. In all the cases below, we will use backcasting as initialisation of the model. We will use the last 336 observations (\\(48 \\times 7\\)) as the holdout, just to see whether models perform adequately or not. First, it is ADAM ETS(M,N,M) with lags=c(48,7*48): adamModelETSMNM &lt;- adam(y, &quot;MNM&quot;, lags=c(1,48,336), initial=&quot;back&quot;, h=336, holdout=TRUE) adamModelETSMNM ## Time elapsed: 0.21 seconds ## Model estimated using adam() function: ETS(MNM)[48, 336] ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 25798.77 ## Persistence vector g: ## alpha gamma1 gamma2 ## 0.8874 0.1125 0.1125 ## ## Sample size: 3696 ## Number of estimated parameters: 4 ## Number of degrees of freedom: 3692 ## Information criteria: ## AIC AICc BIC BICc ## 51605.55 51605.56 51630.41 51630.45 ## ## Forecast errors: ## ME: -424.363; MAE: 758.61; RMSE: 1048.348 ## sCE: -481.882%; Asymmetry: -57.2%; sMAE: 2.564%; sMSE: 0.126% ## MASE: 1.167; RMSSE: 1.111; rMAE: 0.113; rRMSE: 0.128 plot(adamModelETSMNM,7) As you might notice the model was constructed in 0.21 seconds, and while it might not be the most accurate model for the data, it fits the data well and produces reasonable forecasts. So, it is a good starting point. If we want to improve upon it, we can try one of multistep estimators, for example GTMSE: adamModelETSMNMGTMSE &lt;- adam(y, &quot;MNM&quot;, lags=c(1,48,336), initial=&quot;back&quot;, h=336, holdout=TRUE, loss=&quot;GTMSE&quot;) This time the function will take much more time (on my computer it takes around 1.5 minutes), but hopefully will produce more accurate forecasts due to shrinkage of smoothing parameters: adamModelETSMNMGTMSE ## Time elapsed: 22.65 seconds ## Model estimated using adam() function: ETS(MNM)[48, 336] ## Distribution assumed in the model: Normal ## Loss function type: GTMSE; Loss function value: -2309.774 ## Persistence vector g: ## alpha gamma1 gamma2 ## 0.0420 0.1776 0.1734 ## ## Sample size: 3696 ## Number of estimated parameters: 3 ## Number of degrees of freedom: 3693 ## Information criteria are unavailable for the chosen loss &amp; distribution. ## ## Forecast errors: ## ME: 241.047; MAE: 392.087; RMSE: 523.07 ## sCE: 273.719%; Asymmetry: 66.9%; sMAE: 1.325%; sMSE: 0.031% ## MASE: 0.603; RMSSE: 0.554; rMAE: 0.059; rRMSE: 0.064 Comparing, for example, RMSSE of the two models, we can conclude that the one with TMSE was more accurate than the one estimated using the conventional likelihood. Another potential way of improvement for the model is the inclusion of AR(1) term, as for example done by Taylor (2010). This will take more time, but might lead to some improvements in the accuracy: adamModelETSMNMAR &lt;- adam(y, &quot;MNM&quot;, lags=c(1,48,336), initial=&quot;back&quot;, orders=c(1,0,0), h=336, holdout=TRUE, maxeval=1000) Note that estimating ETS+ARIMA models is a complicated task, but by default the number of iterations would be restricted by 160, which might not be enough to get to the minimum of the loss. This is why I increased the number of iterations in the code above to 1000. If you want to get more feedback on how the optimisation has been carried out, you can ask function to print details via print_level=41. adamModelETSMNMAR ## Time elapsed: 1.94 seconds ## Model estimated using adam() function: ETS(MNM)[48, 336]+ARIMA(1,0,0) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 24108.2 ## Persistence vector g: ## alpha gamma1 gamma2 ## 0.1129 0.2342 0.3180 ## ## ARMA parameters of the model: ## AR: ## phi1[1] ## 0.6923 ## ## Sample size: 3696 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 3691 ## Information criteria: ## AIC AICc BIC BICc ## 48226.39 48226.41 48257.47 48257.54 ## ## Forecast errors: ## ME: 257.38; MAE: 435.476; RMSE: 561.237 ## sCE: 292.266%; Asymmetry: 67.2%; sMAE: 1.472%; sMSE: 0.036% ## MASE: 0.67; RMSSE: 0.595; rMAE: 0.065; rRMSE: 0.069 In this specific example, we see that the ADAM ETS(M,N,M)+AR(1) leads to a small improvement in accuracy. 15.4.2 ADAM ETSX Another option of dealing with multiple seasonalities, as discussed above, is ETSX model. We start with a static model, which captures half-hours of day via its seasonal component and days of week frequency via explanatory variable. We will use temporaldummy() function from greybox package for this. This function works much better, when the data contains proper time stamps and, for example, is of class zoo or xts: x1 &lt;- temporaldummy(y,type=&quot;day&quot;,of=&quot;week&quot;,factors=TRUE) x2 &lt;- temporaldummy(y,type=&quot;hour&quot;,of=&quot;day&quot;,factors=TRUE) taylorData &lt;- data.frame(y=y,x1=x1,x2=x2) This function is especially useful when dealing with DST and Leap years (see Section 15.3), because it will encode the dummy variables based on dates, allowing to sidestep the issue with changing frequency in the data. We can now fit the ADAM ETSX model with dummy variables for days of week: adamModelETSXMNN &lt;- adam(taylorData, &quot;MNN&quot;, h=336, holdout=TRUE, initial=&quot;back&quot;) In the code above we use the initialisation via backacasting, because otherwise the calculation will take much more time. Here is what we get as a result: adamModelETSXMNN ## Time elapsed: 0.58 seconds ## Model estimated using adam() function: ETSX(MNN) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 30155.54 ## Persistence vector g (excluding xreg): ## alpha ## 0.6182 ## ## Sample size: 3696 ## Number of estimated parameters: 2 ## Number of degrees of freedom: 3694 ## Information criteria: ## AIC AICc BIC BICc ## 60315.08 60315.09 60327.51 60327.53 ## ## Forecast errors: ## ME: -1664.294; MAE: 1781.472; RMSE: 2070.321 ## sCE: -1889.878%; Asymmetry: -92.3%; sMAE: 6.021%; sMSE: 0.49% ## MASE: 2.74; RMSSE: 2.194; rMAE: 0.266; rRMSE: 0.253 The resulting model produces the biased forecasts (they are consistently higher than needed). This is mainly because the smoothing parameter \\(\\alpha\\) is too high and the model changes the level to frequently. We can see that in the plot of the state: plot(adamModelETSXMNN$states[,1], ylab=&quot;Level&quot;) As we see, the level component not only contains the level, but also absorbed seasonality, which causes the issue with forecasting accuracy. However, the obtained value did not happen due to randomness - this is what the model does, when seasonality is fixed and is not allowed to evolve over time. In order to reduce the sensitivity of the model, we can shrink the smoothing parameter using a multistep estimator (discussed in Section 14.3). Note however that these estimators are typically slower than the conventional ones, so they might take more computational time: adamModelETSXMNNGTMSE &lt;- adam(taylorData, &quot;MNN&quot;, h=336, holdout=TRUE, initial=&quot;back&quot;, loss=&quot;GTMSE&quot;) adamModelETSXMNNGTMSE ## Time elapsed: 38.15 seconds ## Model estimated using adam() function: ETSX(MNN) ## Distribution assumed in the model: Normal ## Loss function type: GTMSE; Loss function value: -2044.705 ## Persistence vector g (excluding xreg): ## alpha ## 0.0153 ## ## Sample size: 3696 ## Number of estimated parameters: 1 ## Number of degrees of freedom: 3695 ## Information criteria are unavailable for the chosen loss &amp; distribution. ## ## Forecast errors: ## ME: 105.462; MAE: 921.897; RMSE: 1204.967 ## sCE: 119.757%; Asymmetry: 18%; sMAE: 3.116%; sMSE: 0.166% ## MASE: 1.418; RMSSE: 1.277; rMAE: 0.138; rRMSE: 0.147 While the performance of model with GTMSE has improved due to the shrinkage of \\(\\alpha\\) to zero, the seasonal states are still deterministic and do not adapt to the changes in data. We could adapt them via regressors=\"adapt\", but then we would be constructing the ETS(M,N,M)[48,336] model but in a less efficient way. Alternatively, we could assume that one of the seasonal states is deterministic and, for example, construct the ETSX(M,N,M) model: adamModelETSXMNMGTMSE &lt;- adam(taylorData, &quot;MNM&quot;, lags=48, h=336, holdout=TRUE, initial=&quot;back&quot;, loss=&quot;GTMSE&quot;, formula=y~x1) adamModelETSXMNMGTMSE plot(adamModelETSXMNMGTMSE,7) ## Time elapsed: 32.25 seconds ## Model estimated using adam() function: ETSX(MNM) ## Distribution assumed in the model: Normal ## Loss function type: GTMSE; Loss function value: -2071.366 ## Persistence vector g (excluding xreg): ## alpha gamma ## 0.0138 0.0755 ## ## Sample size: 3696 ## Number of estimated parameters: 2 ## Number of degrees of freedom: 3694 ## Information criteria are unavailable for the chosen loss &amp; distribution. ## ## Forecast errors: ## ME: 145.779; MAE: 829.688; RMSE: 1055.461 ## sCE: 165.538%; Asymmetry: 27%; sMAE: 2.804%; sMSE: 0.127% ## MASE: 1.276; RMSSE: 1.118; rMAE: 0.124; rRMSE: 0.129 We can see an improvement in comparison with the previous model, so the seasonal states do change over time, which means that the deterministic seasonality is not appropriate in our example. However, in some other cases it might be more suitable, producing more accurate forecasts than the models assuming stochastic seasonality (i.e. via multiple seasonal ETS / ARIMA models). 15.4.3 ADAM ARIMA Another model we can try on this data is ARIMA. We have not yet discussed the order selection mechanism for ARIMA, so I will construct a model based on my judgment. Keeping in mind that ETS(A,N,N) is equivalent to ARIMA(0,1,1), and that the changing seasonality in ARIMA context can be modelled with seasonal differences, I will construct SARIMA(0,1,1)(0,1,1)\\(_{336}\\), skipping the frequencies for half-hour of day. Hopefully, this will be enough to model: (a) changing level of data; (b) changing seasonal amplitude. Here how we can construct this model using adam(): adamModelARIMA &lt;- adam(y, &quot;NNN&quot;, lags=c(1,336), initial=&quot;back&quot;, orders=list(i=c(1,1),ma=c(1,1)), h=336, holdout=TRUE) adamModelARIMA ## Time elapsed: 0.2 seconds ## Model estimated using adam() function: SARIMA(0,1,1)[1](0,1,1)[336] ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 26098.76 ## ARMA parameters of the model: ## MA: ## theta1[1] theta1[336] ## 0.5086 -0.1977 ## ## Sample size: 3696 ## Number of estimated parameters: 3 ## Number of degrees of freedom: 3693 ## Information criteria: ## AIC AICc BIC BICc ## 52203.53 52203.53 52222.17 52222.20 ## ## Forecast errors: ## ME: 49.339; MAE: 373.387; RMSE: 499.661 ## sCE: 56.027%; Asymmetry: 18.6%; sMAE: 1.262%; sMSE: 0.029% ## MASE: 0.574; RMSSE: 0.529; rMAE: 0.056; rRMSE: 0.061 plot(adamModelARIMA,7) This model is directly comparable with ADAM ETS via information criteria, and as we can see is worse than ADAM ETS(M,N,M)+AR(1) and multiple seasonal ETS(M,N,M) in terms of AICc. But it is better in terms of RMSSE, producing more accurate forecasts. We could analyse the residuals of this model and iteratively test, whether the addition of AR terms and halfhour of day seasonality improves the accuracy of the model. We could also try ARIMA models with different distributions, compare them and select the most appropriate one. The reader is encouraged to do this task on their own. References "],["ADAMIntermittent.html", "Chapter 16 ADAM for Intermittent Demand", " Chapter 16 ADAM for Intermittent Demand So far we have discussed data that has regular occurrence. This is a characteristic of a well established product that is sold every observation. For example, daily sales of bread in a supermarket would have this regularity. However, there are time series, where non-zero values do not happen on every observation. In the context of demand forecasting, this is called “Intermittent demand.” The conventional example of such demand is monthly sales of jet engines: they will contain a lot of zeroes, when nobody buys the product and then all of a sudden several units, again followed by zeroes. One of the simplest definitions of intermittent demand is that it is the demand that happens at irregular frequency. While at a first glance it might seem that it is an exotic problem, intermittent demand can be encountered in many areas, when the frequency of measurement is high enough. For example, daily sales of a specific type of tomatoes in a store might exhibit regular demand, but the same sales on hourly or minute frequency would exhibit intermittence. So, the problem is universal and might appear in almost any context. Sometimes the term “count data” (or “integer-valued data”) is used in a similar context, but there is a difference between this term and intermittent data. “Count data” implies that demand can take integer values only and can be typically modelled via Poisson, Binomial or Negative Binomial distributions. It does not necessarily contain zeroes and does not explicitly allow demand to happen at random. If there are zeroes, then it is assumed that they are just one of the possible values of a distribution. In case of intermittent demand, we explicitly acknowledge that demand might not happen, but if it happens then the demand size will be greater than zero. Furthermore, intermittent demand does not necessarily need to be integer-valued. For example, daily energy consumption for charging electric vehicles would typically be intermittent (because the vehicle owners do not charge them every day), but the non-zero consumption will not be integer. Having said that, count distributions can be used in some cases of intermittent demand, but they do not necessarily always provide a good approximation of complex reality. Before we move towards the proper discussion of the topic in context of ADAM, we should acknowledge that at the heart of what follows, there lies the following model (Croston, 1972): \\[\\begin{equation} y_t = o_t z_t , \\tag{16.1} \\end{equation}\\] where \\(o_t\\) is the demand occurrence variable, which can be either zero or one and has some probability of occurrence, \\(z_t\\) is the demand sizes captured by a model (for example, ADAM ETS) and \\(y_t\\) is the final observed demand. This model in context of intermittent demand was originally proposed by Croston (1972), but similar models (e.g. Hurdle and Zero Inflated Poisson) exist in other, non-forecasting related contexts. In this chapter we will discuss the intermittent state space model that (16.1), both parts of which can be modelled via ADAM models, and we will see how they can be used, what they imply and how they connect to the convetional regular demand. If ETS model is used for \\(z_t\\) then (16.1) is called iETS. So, iETS(M,N,N) model refers to the intermittent state space model, where demand sizes are modelled via ETS(M,N,N). ETS can also be used for occurrence part of the model, so if the discussion is focused on demand occurrence part of the model (as in Subsection 16.1), we will use ``oETS’’ instead. While ARIMA can be used in this context as well, it is not yet implemented for the occurrence part of the model. So we will focus the discussio on ADAM ETS. Furthermore, depending on how the occurrence part is modelled, these notations can be expanded to include references to specific parts of the occurrence part of the model. This is discussed in detail in Subsection 16.1. This chapter is based on Svetunkov and Boylan (2019). References "],["ADAMOccurrence.html", "16.1 Occurrence part of the model", " 16.1 Occurrence part of the model The general model (16.1) assumes that demand occurs randomly and that the variable \\(o_t\\) can be either zero (no demand) or one (there is some demand). While this process can be modelled using different distributions, we propose using Bernoulli with a time varying probability (in the most general case): \\[\\begin{equation} o_t \\sim \\text{Bernoulli} \\left(p_t \\right) , \\tag{16.2} \\end{equation}\\] where \\(p_t\\) is the probability of occurrence. In this section we will discuss different types of models for this probability. Each one has some idea behind it, and there are different mechanisms of the model construction, estimation, error calculation, update of the states and the generation of forecasts for each of them. The Probability Mass Function (PMF) of Bernoulli distribution based on this formulation is: \\[\\begin{equation} f_o(o_t, p_t) = p_t^{o_t}(1-p_t)^{1-o_t}. \\tag{16.3} \\end{equation}\\] The occurrence part of the model can be estimated via the maximisation of the log-likelihood function, which comes directly from (16.3), and in the most general case is: \\[\\begin{equation} \\ell \\left(\\boldsymbol{\\theta}_o | o_t \\right) = \\sum_{o_t=1} \\log(\\hat{p}_t) + \\sum_{o_t=0} \\log(1-\\hat{p}_t) , \\tag{16.4} \\end{equation}\\] where \\(\\hat{p}_t\\) is the in-sample conditional one step ahead expectation of the probability on observation \\(t\\), given the information on observation \\(t-1\\), which depends on the vector of estimated parameters for the occurrence part of the model \\(\\boldsymbol{\\theta}_o\\). In order to demonstrate the difference between specific types of oETS models, we will use the following artificial data: y &lt;- ts(c(rpois(20,0.25),rpois(20,0.5),rpois(20,1),rpois(20,2),rpois(20,3),rpois(20,5))) Here how the data looks like: plot(y) The probability of occurrence in this model increases together with the demand sizes. This example corresponds to the situation of intermittent demand of a product becoming regular. 16.1.1 Fixed probability model, oETS\\(_F\\) We start with the simplest case of the fixed probability of occurrence, oETS\\(_F\\) model: \\[\\begin{equation} o_t \\sim \\text{Bernoulli}(p) , \\tag{16.5} \\end{equation}\\] This model assumes that demand happens with the same probability no matter what. This might sound exotic, because in practice, there might be many factors influencing customers’ desire to purchase and the impact of these factors might change over the year. But this is a very basic model, which at least can be used as a benchmark on intermittent demand data. Furthermore, it might be suitable for modelling demand on expensive high-tech products, such as jet engines, which is ``very slow’’ in its nature and typically does not evolve much over time. When estimated via maximisation of likelihood function (16.4), the probability of occurrence is equal to: \\[\\begin{equation} \\hat{p} = \\frac{T_1}{T}, \\tag{16.6} \\end{equation}\\] where \\(T_1\\) is the number of non-zero observations and \\(T\\) is the number of all the available observations in sample. The occurrence part of the model, oETS\\(_F\\) can be constructed using oes() function from smooth package: oETSFModel1 &lt;- oes(y, occurrence=&quot;fixed&quot;, h=10, holdout=TRUE) oETSFModel1 ## Occurrence state space model estimated: Fixed probability ## Underlying ETS model: oETS[F](MNN) ## Vector of initials: ## level ## 0.6273 ## ## Error standard deviation: 1.0929 ## Sample size: 110 ## Number of estimated parameters: 1 ## Number of degrees of freedom: 109 ## Information criteria: ## AIC AICc BIC BICc ## 147.2861 147.3231 149.9866 150.0736 plot(oETSFModel1) The plot above demonstrates the dynamics of the occurrence variable \\(o_t\\) and the fitted and predicted probabilities. The oETS\\(_F\\) model produces the straight line for the probability of 0.63, ignoring the fact that in our example the probability of occurrence has increased over time. 16.1.2 Odds ratio model, oETS\\(_O\\) In this model, it is assumed that the update of the probability is driven by the occurrence of variable. It is more complicated than the previous as the probability now changes over time and can be modelled, for example, with ETS(M,N,N) model: \\[\\begin{equation} \\begin{aligned} &amp; o_t \\sim \\text{Bernoulli} \\left(p_t \\right) \\\\ &amp; p_t = \\frac{\\mu_{a,t}}{\\mu_{a,t}+1} \\\\ &amp; a_t = l_{a,t-1} \\left(1 + \\epsilon_{a,t} \\right) \\\\ &amp; l_{a,t} = l_{a,t-1}( 1 + \\alpha_{a} \\epsilon_{a,t}) \\\\ &amp; \\mu_{a,t} = l_{a,t-1} \\end{aligned}, \\tag{16.7} \\end{equation}\\] where \\(l_{a,t}\\) is the unobserved level component, \\(\\alpha_{a}\\) is the smoothing parameter, the error term \\(1+\\epsilon_{a,t}\\) is positive, has means of one and follows an unknown distribution and \\(\\mu_{a,t}\\)is the conditional expectation for the unobservable shape parameters \\(a_t\\). The measurement and transition equations in (16.7) can be substituted by any other ADAM ETS or ARIMA model if it is reasonable to assume that the dynamics of probability has some additional components. This model is called ``odds ratio’’, because the probability of occurrence in (16.7) is calculated using the classical logistic transform. This also means that \\(\\mu_{a,t}\\) is equal to: \\[\\begin{equation} \\label{eq:oETS_O_oddsRatio} \\mu_{a,t} = \\frac{p_t}{1 - p_t} . \\end{equation}\\] When \\(\\mu_{a,t}\\) increases in the oETS model, the odds ratio increases as well, meaning that the probability of occurrence goes up. Svetunkov and Boylan (2019) explain that this model is in theory more appropriate for the demand on products becoming obsolescent, but given the updating mechanism it should also work fine on other types of intermittent demand data. When it comes to the application of the model to the data, its construction is done via the following set of equations (example with oETS\\(_O\\)(M,N,N)): \\[\\begin{equation} \\begin{aligned} &amp; \\hat{p}_t = \\frac{\\hat{\\mu}_{a,t}}{\\hat{\\mu}_{a,t}+1} \\\\ &amp; \\hat{\\mu}_{a,t} = \\hat{l}_{a,t-1} \\\\ &amp; \\hat{l}_{a,t} = \\hat{l}_{a,t-1}( 1 + \\hat{\\alpha}_{a} e_{a,t}) \\\\ &amp; 1+e_{a,t} = \\frac{u_t}{1-u_t} \\\\ &amp; u_{t} = \\frac{1 + o_t - \\hat{p}_t}{2} \\end{aligned}, \\tag{16.8} \\end{equation}\\] where \\(e_{a,t}\\) is the proxy for the unobservable error term \\(\\epsilon_{a,t}\\) and \\(\\hat{\\mu}_t\\) is the estimate of \\(\\mu_{a,t}\\), conditional expectation discussed in Subsection 6.4. If a multiple steps ahead forecast for the probability is needed from this model, then the formulae discussed in Subsection 6.4 can be used to get \\(\\hat{\\mu}_{a,t}\\), which then can be inserted in the first equation of (16.8) to get the final conditional multiple steps ahead probability of occurrence. Finally, in order to estimate the model (16.8), the likelihood (16.4) can be used. The occurrence part of the model oETS\\(_O\\) is constructed using the very same oes() function, but also allows specifying the ETS model to use. For example, here is the oETS\\(_O\\)(M,M,N) model: oETSOModel &lt;- oes(y, model=&quot;MMN&quot;, occurrence=&quot;odds-ratio&quot;, h=10, holdout=TRUE) oETSOModel ## Occurrence state space model estimated: Odds ratio ## Underlying ETS model: oETS[O](MMN) ## Smoothing parameters: ## level trend ## 0.0144 0.0000 ## Vector of initials: ## level trend ## 0.0633 1.0546 ## ## Error standard deviation: 2.6666 ## Sample size: 110 ## Number of estimated parameters: 4 ## Number of degrees of freedom: 106 ## Information criteria: ## AIC AICc BIC BICc ## 99.4099 99.7908 110.2118 111.1071 plot(oETSOModel) The constructed model introduces the trend component, capturing the changing probability of occurrence and reflecting well the fact that it increases over time. 16.1.3 Inverse odds ratio model, oETS\\(_I\\) Using similar approach to the oETS\\(_O\\) model, we can formulate the “inverse odds ration” model oETS\\(_I\\)(M,N,N): \\[\\begin{equation} \\begin{aligned} &amp; o_t \\sim \\text{Bernoulli} \\left(p_t \\right) \\\\ &amp; p_t = \\frac{1}{1+\\mu_{b,t}} \\\\ &amp; b_t = l_{b,t-1} \\left(1 + \\epsilon_{b,t} \\right) \\\\ &amp; l_{b,t} = l_{b,t-1}( 1 + \\alpha_{b} \\epsilon_{b,t}) \\\\ &amp; \\mu_{b,t} = l_{b,t-1} \\end{aligned}, \\tag{16.9} \\end{equation}\\] where similarly to (16.8), \\(l_{b,t}\\) is the unobserved level component, \\(\\alpha_{b}\\) is the smoothing parameter, the error term \\(1+\\epsilon_{b,t}\\) is positive, has means of one and follows an unknown distribution and \\(\\mu_{b,t}\\)is the conditional expectation for the unobservable shape parameters \\(b_t\\). The main difference of this model with the previous is in the different mechanism of probability calculation, which focuses on the probability of “inoccurrence,” i.e. on zeroes of data rather than on ones. This type of model should be more appropriate for cases of demand building up (Svetunkov and Boylan, 2019). The probability calculation mechanism in (16.9) implies that \\(\\mu_{b,t}\\) is equal to: \\[\\begin{equation} \\mu_{b,t} = \\frac{1-p_t}{p_t} . \\end{equation}\\] The construction of the model (16.9) is similar to (16.8): \\[\\begin{equation} \\begin{aligned} &amp; \\hat{p}_t = \\frac{1}{1+\\hat{\\mu}_{b,t}} \\\\ &amp; \\hat{\\mu}_{b,t} = \\hat{l}_{b,t-1} \\\\ &amp; \\hat{l}_{b,t} = l_{b,t-1}( 1 + \\hat{\\alpha}_{b} e_{b,t}) \\\\ &amp; 1+e_{b,t} = \\frac{1-u_t}{u_t} \\\\ &amp; u_{t} = \\frac{1 + o_t - \\hat{p}_t}{2} \\end{aligned}, \\tag{16.10} \\end{equation}\\] where \\(e_{b,t}\\) is the proxy for the unobservable error term \\(\\epsilon_{b,t}\\) and \\(\\hat{\\mu}_{b,t}\\) is the estimate of \\(\\mu_{b,t}\\). Once again, we refer an interested reader to Subsection 6.4 for the discussion of the multiple steps ahead conditional expectations from the model. Svetunkov and Boylan (2019) show that the oETS\\(_I\\)(M,N,N) model can also be estimated using Croston’s method, as long as we can assume that the probability does not change over time substantially. In this case the demand intervals can be used instead of \\(\\hat{\\mu}_{b,t}\\) in (16.10). So the iETS(M,N,N)\\(_I\\)(M,N,N) can be considered as the model underlying Croston’s method. The function oes() implements the oETS\\(_I\\) model as well. For example, here is the oETS\\(_I\\)(M,M,N) model: oETSIModel &lt;- oes(y, model=&quot;MMN&quot;, occurrence=&quot;inverse-odds-ratio&quot;, h=10, holdout=TRUE) oETSIModel ## Occurrence state space model estimated: Inverse odds ratio ## Underlying ETS model: oETS[I](MMN) ## Smoothing parameters: ## level trend ## 0.0773 0.0000 ## Vector of initials: ## level trend ## 68.5419 0.8792 ## ## Error standard deviation: 3.8638 ## Sample size: 110 ## Number of estimated parameters: 4 ## Number of degrees of freedom: 106 ## Information criteria: ## AIC AICc BIC BICc ## 103.3851 103.7660 114.1870 115.0823 plot(oETSIModel) Similarly to the oETS\\(_O\\), the model captures the trend in the probability of occurrence, but will have different smoothing parameters. 16.1.4 General oETS model, oETS\\(_G\\) Uniting the models oETS\\(_O\\) with oETS\\(_I\\), we can obtain the “general” model, which in the most general case can be summarised in the following way: \\[\\begin{equation} \\begin{aligned} &amp; p_t = f_p(\\mu_{a,t}, \\mu_{b,t}) \\\\ &amp; a_t = w_a(\\mathbf{v}_{a,t-\\boldsymbol{l}}) + r_a(\\mathbf{v}_{a,t-\\boldsymbol{l}}) \\epsilon_{a,t} \\\\ &amp; \\mathbf{v}_{a,t} = f_a(\\mathbf{v}_{a,t-\\boldsymbol{l}}) + g_a(\\mathbf{v}_{a,t-\\boldsymbol{l}}) \\epsilon_{a,t} \\\\ &amp; b_t = w_b(\\mathbf{v}_{b,t-\\boldsymbol{l}}) + r_b(\\mathbf{v}_{b,t-\\boldsymbol{l}}) \\epsilon_{b,t} \\\\ &amp; \\mathbf{v}_{b,t} = f_b(\\mathbf{v}_{b,t-\\boldsymbol{l}}) + g_b(\\mathbf{v}_{b,t-\\boldsymbol{l}}) \\epsilon_{b,t} \\end{aligned} , \\tag{16.11} \\end{equation}\\] where \\(\\epsilon_{a,t}\\), \\(\\epsilon_{b,t}\\), \\(\\mu_{a,t}\\) and \\(\\mu_{b,t}\\) have been defined in previous subsectionsa and the other elements correspond to the ADAM model discussed in section 8. Note that two models similar to the one discussed in Section 8 are used for modelling of \\(a_t\\) and \\(b_t\\). The general formula for the probability in case of the multiplicative error model is: \\[\\begin{equation} p_t = \\frac{\\mu_{a,t}}{\\mu_{a,t}+\\mu_{b,t}} , \\tag{16.12} \\end{equation}\\] while for the additive one, it is: \\[\\begin{equation} p_t = \\frac{\\exp(\\mu_{a,t})}{\\exp(\\mu_{a,t})+\\exp(\\mu_{b,t})} . \\tag{16.13} \\end{equation}\\] This is because both \\(\\mu_{a,t}\\) and \\(\\mu_{b,t}\\) need to be strictly positive, while the additive error models support the real plane. The canonical oETS model assumes that the pure multiplicative model is used for the both \\(a_t\\) and \\(b_t\\). This type of model is positively defined for any values of error, trend and seasonality, which is essential for the values of \\(a_t\\) and \\(b_t\\) and their expectations. If a combination of additive and multiplicative error models is used, then the additive part should be exponentiated prior to the usage of the formulae for the calculation of the probability. So, \\(f_p(\\cdot)\\) function maps the expectations from models A and B to the probability of occurrence, depending on the error type of the respective models: \\[\\begin{equation} p_t = f_p(\\mu_{a,t}, \\mu_{b,t}) = \\left \\lbrace \\begin{aligned} &amp; \\frac{\\mu_{a,t}}{\\mu_{a,t} + \\mu_{b,t}} &amp; \\text{ when both have multiplicative errors} \\\\ &amp; \\frac{\\mu_{a,t}}{\\mu_{a,t} + \\exp(\\mu_{b,t})} &amp; \\text{ when model B has additive error} \\\\ &amp; \\frac{\\exp(\\mu_{a,t})}{\\exp(\\mu_{a,t}) + \\mu_{b,t}} &amp; \\text{ when model A has additive error} \\\\ &amp; \\frac{\\exp(\\mu_{a,t})}{\\exp(\\mu_{a,t}) + \\exp(\\mu_{b,t})} &amp; \\text{ when both have additive errors} \\end{aligned} . \\right. \\tag{16.14} \\end{equation}\\] An example of the oETS model is oETS\\(_G\\)(M,N,N)(M,N,N): \\[\\begin{equation} \\begin{aligned} &amp; o_t \\sim \\text{Bernoulli} \\left(p_t \\right) \\\\ &amp; p_t = \\frac{\\mu_{a,t}}{\\mu_{a,t}+\\mu_{b,t}} \\\\ \\\\ &amp; a_t = l_{a,t-1} \\left(1 + \\epsilon_{a,t} \\right) \\\\ &amp; l_{a,t} = l_{a,t-1}( 1 + \\alpha_{a} \\epsilon_{a,t}) \\\\ &amp; \\mu_{a,t} = l_{a,t-1} \\\\ \\\\ &amp; b_t = l_{b,t-1} \\left(1 + \\epsilon_{b,t} \\right) \\\\ &amp; l_{b,t} = l_{b,t-1}( 1 + \\alpha_{b} \\epsilon_{b,t}) \\\\ &amp; \\mu_{b,t} = l_{b,t-1} \\\\ \\end{aligned}, \\tag{16.15} \\end{equation}\\] where all the parameters have already been defined in previous subsections. More advanced models can be constructing for \\(a_t\\) and \\(b_t\\) by specifying the ETS models for each part and / or adding explanatory variables. The construction of this model is done via the following set of equations: \\[\\begin{equation} \\begin{aligned} &amp; e_{a,t} = \\frac{u_t}{1-u_t} -1 \\\\ &amp; \\hat{a}_t = \\hat{l}_{a,t-1} \\\\ &amp; \\hat{l}_{a,t} = \\hat{l}_{a,t-1}( 1 + \\alpha_{a} e_{a,t}) \\\\ &amp; e_{b,t} = \\frac{1-u_t}{u_t} -1 \\\\ &amp; \\hat{b}_t = \\hat{l}_{b,t-1} \\\\ &amp; \\hat{l}_{b,t} = \\hat{l}_{b,t-1}( 1 + \\alpha_{b} e_{b,t}) \\end{aligned} . \\tag{16.16} \\end{equation}\\] There is a separate function for the oETS\\(_G\\) model, called oesg(). It has twice more parameters than oes(), because it allows fine tuning of the models for the both variables \\(a_t\\) and \\(b_t\\). This gives an additional flexibility. For example, here is how we can use ETS(M,N,N) for the \\(a_t\\) and ETS(A,A,N) for the \\(b_t\\), resulting in oETS\\(_G\\)(M,N,N)(A,A,N): oETSGModel1 &lt;- oesg(y, modelA=&quot;MNN&quot;, modelB=&quot;AAN&quot;, h=10, holdout=TRUE) oETSGModel1 ## Occurrence state space model estimated: General ## Underlying ETS model: oETS[G](MNN)(AAN) ## ## Sample size: 110 ## Number of estimated parameters: 6 ## Number of degrees of freedom: 104 ## Information criteria: ## AIC AICc BIC BICc ## 102.4709 103.2865 118.6738 120.5905 plot(oETSGModel1) We can also analyse separately models for \\(a_t\\) and \\(b_t\\). Here is, for example, the model A: oETSGModel1$modelA ## Occurrence state space model estimated: Odds ratio ## Underlying ETS model: oETS[G](MNN)_A ## Smoothing parameters: ## level ## 0 ## Vector of initials: ## level ## 0.3545 ## ## Error standard deviation: 2.2879 ## Sample size: 110 ## Number of estimated parameters: 2 ## Number of degrees of freedom: 108 ## Information criteria: ## AIC AICc BIC BICc ## 94.4709 94.5831 99.8719 100.1355 The experiments that I have done so far show that oETS\\(_G\\) very seldomly brings improvements in comparison with oETS\\(_O\\) or oETS\\(_I\\) in terms of forecasting accuracy. Besides, selecting models for each of the parts is a challenging task. So, this model is theoretically nice, being more general than the other oETS models, but is not very practical. Still it is useful because we can introduce different oETS model by restricting \\(a_t\\) and \\(b_t\\). For example, we can get: oETS\\(_F\\), when \\(\\mu_{a,t} = \\text{const}\\), \\(\\mu_{b,t} = \\text{const}\\) for all \\(t\\); oETS\\(_O\\), when \\(\\mu_{b,t} = 1\\) for all \\(t\\); oETS\\(_I\\), when \\(\\mu_{a,t} = 1\\) for all \\(t\\); oETS\\(_D\\), when \\(\\mu_{a,t} + \\mu_{b,t} = 1\\), \\(\\mu_{a,t} \\leq 1\\) for all \\(t\\) (discussed in the next subsection); oETS\\(_G\\), when there are no restrictions. 16.1.5 Direct probability model, oETS\\(_D\\) The last model in the family of oETS is the “Direct probability.” It appears, when the following restriction is imposed on the oETS\\(_G\\): \\[\\begin{equation} \\mu_{a,t} + \\mu_{b,t} = 1, \\mu_{a,t} \\in [0, 1] . \\tag{16.17} \\end{equation}\\] This restriction is inspired by the mechanism for the probability update proposed by Teunter et al. (2011) (TSB method). In their paper they use SES to model the time varying probability of occurrence. Based on this idea and the restriction (16.17) we can formulate oETS\\(_D\\)(M,N,N) model, which will underly the occurrence part of the TSB method: \\[\\begin{equation} \\begin{aligned} &amp; o_t \\sim \\text{Bernoulli} \\left(\\mu_{a,t} \\right) \\\\ &amp; a_t = l_{a,t-1} \\left(1 + \\epsilon_{a,t} \\right) \\\\ &amp; l_{a,t} = l_{a,t-1}( 1 + \\alpha_{a} \\epsilon_{a,t}) \\\\ &amp; \\mu_{a,t} = \\min(l_{a,t-1}, 1) \\end{aligned}. \\tag{16.18} \\end{equation}\\] There is also an option with the additive error for the occurrence part (also underlying TSB), which has a different, more complicated form: \\[\\begin{equation} \\begin{aligned} &amp; o_t \\sim \\text{Bernoulli} \\left(\\mu_{a,t} \\right) \\\\ &amp; a_t = l_{a,t-1} + \\epsilon_{a,t} \\\\ &amp; l_{a,t} = l_{a,t-1} + \\alpha_{a} \\epsilon_{a,t} \\\\ &amp; \\mu_{a,t} = \\max \\left( \\min(l_{a,t-1}, 1), 0 \\right) \\end{aligned}. \\tag{16.19} \\end{equation}\\] The estimation of the oETS\\(_D\\)(M,N,M) model can be done using the following set of equations: \\[\\begin{equation} \\begin{aligned} &amp; \\hat{\\mu}_{a,t} = \\hat{l}_{a,t-1} \\\\ &amp; \\hat{l}_{a,t} = \\hat{l}_{a,t-1}( 1 + \\hat{\\alpha}_{a} e_{a,t}) \\end{aligned}, \\tag{16.20} \\end{equation}\\] where \\[\\begin{equation} e_{a,t} = \\frac{o_t (1 - 2 \\kappa) + \\kappa - \\hat{\\mu}_{a,t}}{\\hat{\\mu}_{a,t}}, \\tag{16.21} \\end{equation}\\] and \\(\\kappa\\) is a very small number (for example, \\(\\kappa = 10^{-10}\\)), needed only in order to make the model estimable. The estimate of the error term in case of the additive model is much simpler and does not need any specific tricks to work: \\[\\begin{equation} e_{a,t} = o_t - \\hat{\\mu}_{a,t} , \\tag{16.22} \\end{equation}\\] which is directly related to TSB method. Note that equation (16.20) does not contain \\(\\min\\) function, because the estimated error (16.21) will always guarantee that the level will lie between 0 and 1 as long as the smoothing parameter lies in the [0, 1] region (which is the conventional assumption for both ETS(A,N,N) and ETS(M,N,N) models). This also applies for the oETS\\(_D\\)(A,N,N) model, where the \\(\\max\\) and \\(\\min\\) functions can be dropped as long as the smoothing parameter lies in [0,1]. An important feature of this model is that it allows probability to become either 0 or 1, thus implying either that there is no demand on the product at all or that the demand on product has become regular. No other oETS model permits that - they assume that probability might become very close to bounds, but can never reach them. When it comes to initialising the oETS\\(_D\\) model, the values are calculated directly from the data without any additional transformations. Here’s an example of the application of the oETS\\(_D\\)(M,M,N) to the same artificial data: oETSDModel &lt;- oes(y, model=&quot;MMN&quot;, occurrence=&quot;d&quot;, h=10, holdout=TRUE) oETSDModel ## Occurrence state space model estimated: Direct probability ## Underlying ETS model: oETS[D](MMN) ## Smoothing parameters: ## level trend ## 0.0857 0.0857 ## Vector of initials: ## level trend ## 0.2409 1.1281 ## ## Error standard deviation: 2.3618 ## Sample size: 110 ## Number of estimated parameters: 4 ## Number of degrees of freedom: 106 ## Information criteria: ## AIC AICc BIC BICc ## 208.3728 208.7538 219.1747 220.0701 plot(oETSDModel) The empirical analysis I have done so far on different datasets shows that oETS\\(_D\\) model works efficiently in many cases and produces accurate forecasts. So, if you are unsure, which of the oETS models to choose for your intermittent data, I would recommend starting with oETS\\(_D\\). 16.1.6 Model selection in oETS There are two dimensions for the model selection in the oETS model: Selection of the type of occurrence; Selection of the model type for the occurrence part. It is not a rocket science, what can be done in order to select the most appropriate model for the occurrence part is the IC based selection. Given the likelihood (16.4), we can try each of the models, calculate the number of all estimated parameters and then select the one that has the lowest information criterion. The demand occurrence models discussed in this section will have: oETS\\(_F\\): 1 parameter for the probability of occurrence; oETS\\(_O\\), oETS\\(_I\\) and oETS\\(_D\\): initial values, smoothing and dampening parameters; oETS\\(_G\\): initial values, smoothing and dampening parameters for models A and B; For example, if the oETS(M,N,N) model is constructed, the overall number of parameters for the models will be: oETS(M,N,N)\\(_F\\) - 1 parameter: the probability of occurrence \\(\\hat{p}\\) and the scale parameter for the demand sizes; oETS(M,N,N)\\(_O\\), oETS(M,N,N)\\(_I\\) and oETS(M,N,N)\\(_D\\) - 2: the initial value of level and the smoothing parameter; oETS(M,N,N)\\(_G\\) - 4: the initial values of \\(\\hat{l}_{a,0}\\) and \\(\\hat{l}_{b,0}\\) and the smoothing parameters \\(\\hat{\\alpha}_a\\) and \\(\\hat{\\alpha}_b\\). This implies that the selection between models in (2) will come to the best fit to the demand occurrence data, while oETS(M,N,N)\\(_G\\) will only be selected if it provides much better fit to the data. Given that intermittent demand typically does not have many observations, selection of oETS(M,N,N)\\(_G\\) becomes highly improbable. When it comes to the selection of the most appropriate demand occurrence model (e.g. between local level and local trend models), then the approach would be similar: estimate the pool of models via likelihood, calculate their number of parameters, select the model with the lowest AIC. Given that the likelihood part of the demand occurrence part comes to probabilities, the selection in the occurrence part can be done based on the likelihood (16.4) independently of the demand sizes part of the model. References "],["ADAMDemandSizes.html", "16.2 Demand sizes part of the model", " 16.2 Demand sizes part of the model So far we have discussed the occurrence part \\(o_t\\) of the model and how to capture the probability of demand occurrence \\(p_t\\). But this is only a half of the intermittent state space model. The second one is the model for the demand sizes \\(z_t\\), which focuses on how many units of product will be sold if our customers decide to buy in a specific period of time. This can be modelled with any ADAM model, but has its own implications. We start discussion with analysis of iETS(M,N,N)\\(_F\\) model, which can be formulate as: \\[\\begin{equation} \\begin{aligned} &amp; y_t = o_t z_t \\\\ &amp; z_t = l_{z,t-1}(1 + \\epsilon_{z,t}) \\\\ &amp; l_{z,t} = l_{z,t-1}(1 + \\alpha_{z} \\epsilon_{z,t}) \\\\ &amp; o_t \\sim \\text{Bernoulli}(p) \\\\ \\end{aligned}, \\tag{16.23} \\end{equation}\\] where the subscript \\(z\\) refers to the components and parameters of demand sizes. This model assumes that there is always a potential demand on the product which evolves over time, even when \\(o_t=0\\), we just do not always observe it. All the properties of this model have alread been discussed in chapter 9. The main challenge appears, when this model needs to be constructed and estimated, because \\(z_t\\) is not observable, when \\(o_t=0\\). In these situations the error term cannot be esimated, but according to the model it still exists, thus impacting the level of demand \\(l_{z,t}\\). In order to construct the model in the cases of no demand, we propose taking the conditional expectation for these periods, given the last available non-zero observations. This means that the model can be constructed using the following set of equations \\[\\begin{equation} \\begin{aligned} &amp; e_{z,t} = \\frac{z_t - \\hat{\\mu}_{z,t}}{\\hat{\\mu}_{z,t}}, \\text{ when } o_t=1 \\\\ &amp; \\hat{\\mu}_{z,t} = \\hat{l}_{z,t-1} \\\\ &amp; \\hat{l}_{z,t} = \\left \\lbrace \\begin{aligned} &amp; \\hat{l}_{z,t-1} (1 + \\hat{\\alpha}_z e_t ), &amp; \\text{ when } o_t=1 \\\\ &amp; \\hat{l}_{z,t-1} , &amp; \\text{ when } o_t=0 \\end{aligned} \\right. \\end{aligned}. \\tag{16.24} \\end{equation}\\] This is only possible if \\(\\mathrm{E}(1+\\epsilon_{z,t})=1\\), which is an important assumption for multiplicative error models, discussed in Section 9.4. If this is violated, then the formula for the calculation of the level in (16.24) will become more complicated, involving the expectation of products of random variables. In a similar way, we can construct more complicated models for the demand sizes. In a more general case this can be written as: \\[\\begin{equation} \\begin{aligned} &amp; e_{z,t} = \\frac{z_t - \\hat{\\mu}_{z,t}}{\\hat{\\mu}_{z,t}}, \\text{ when } o_t=1 \\\\ &amp; \\hat{\\mathbf{v}}_{t} = \\left \\lbrace \\begin{aligned} &amp; f(\\hat{\\mathbf{v}}_{t-\\boldsymbol{l}}) + g(\\hat{\\mathbf{v}}_{t-\\boldsymbol{l}}) e_t, &amp; \\text{ when } o_t=1 \\\\ &amp; f(\\hat{\\mathbf{v}}_{t-\\boldsymbol{l}}) , &amp; \\text{ when } o_t=0 \\end{aligned} \\right. \\end{aligned}, \\tag{16.25} \\end{equation}\\] where all the functions and vectors have been defined for the original ADAM ETS model (8.1) in Section 8. 16.2.1 Additive vs multiplicative ETS for demand sizes iETS supports any type of ETS model, including pure additive, pure multiplicative and mixed ones. But the selection of the appropriate model should be done based on the understanding of the problem. Typically, we expect demands to be non-negative: people want to buy our product, and usually the business does not want to buy from customers. In this case, we should use pure multiplicative models, as they will always produce meaningful results, as long as the assumption of positivity of \\((1+\\epsilon_{z,t})\\) holds. This is important because the data would typically have low volume and the model might generate unreasonable (negative) pont and interval forecasts if a non-positive distribution is used (e.g. Normal). Thus, it is important to use either Inverse Gaussian, or Gamma, or Log Normal distribution for the error term of the demand sizes part of the model, when the volume of data is low and you expect the non-zero values to be strictly positive. The main difficulty with pure multiplicative models arrises from the construction point of view - as discussed in Section 9.2 the point forecasts of such model in general do not correspond to the conditional expectations (the only exclusion is the ETS(M,N,N) model). At the same time, the construction of the model for demand sizes assumes that the conditional expectations are equal to point forecasts, when demand is not observed. If this is violated then (16.25) is no longer the correct way to construct the model. This problem becomes especially important for the models with multiplicative trend, where the conditional expectation might differ from point forecasts substantially. Still, point forecasts can be considered as proxies for the conditional expectation, especially when smoothing parameters are close to zero. In the boundary case with \\(\\alpha=0\\) and \\(\\beta=0\\) in ETS(M,M,N), the conditional expectation coincides with the point forecast. The higher the smoothing parameters are, the bigger descrepancy will be, implying that the model for the demand sizes is constructed incorrectly. The pure additive models do not have the issue with the conditional expectation and thus can be constructed easily in case of intermittent demand. But as discussed earlier, they might violated the non-negativity assumption of the model. So, in practice they should be used with care. 16.2.2 Using ARIMA for demand sizes Finally, ADAM ARIMA can be used for demand sizes as well, making iARIMA model. All the discussions in the previous subsection would apply to ARIMA as well, keeping in mind that ADAM ARIMA can be either pure additive or pure multiplicative. Given that the multiplicative ARIMA is formulated via logarithms, and still has the error term with the expectation of one, any ARIMA model can be used for the variable \\(z_t\\) and can be constructed via (16.25). This can also be used for the cases, when a pure multiplicative model with trend is needed, and there are difficulties with constructing ETS(M,M,N) (i.e. smoothing parameters are not close to zero). The relation between ARIMA and ETS might be useful in this case. For example, instead of constructing ETS(M,M,N) we can construct logARIMA(0,2,2) in this case, sidestepping the aforementioned problem. "],["ADAMIntermittentFull.html", "16.3 The full ADAM model", " 16.3 The full ADAM model Uniting demand occurrence with the demand sizes parts of the model, we can now discuss the full iETS model, which in the most general form can be represented as: \\[\\begin{equation} \\begin{aligned} &amp; y_t = o_t z_t , \\\\ &amp; {z}_{t} = w_z(\\mathbf{v}_{z,t-\\boldsymbol{l}}) + r_z(\\mathbf{v}_{z,t-\\boldsymbol{l}}) \\epsilon_{z,t} \\\\ &amp; \\mathbf{v}_{z,t} = f_z(\\mathbf{v}_{z,t-\\boldsymbol{l}}) + g_z(\\mathbf{v}_{z,t-\\boldsymbol{l}}) \\epsilon_{z,t} \\\\ &amp; \\\\ &amp; o_t \\sim \\text{Bernoulli} \\left(p_t \\right) , \\\\ &amp; p_t = f_p(\\mu_{a,t}, \\mu_{b,t}) \\\\ &amp; a_t = w_a(\\mathbf{v}_{a,t-\\boldsymbol{l}}) + r_a(\\mathbf{v}_{a,t-\\boldsymbol{l}}) \\epsilon_{a,t} \\\\ &amp; \\mathbf{v}_{a,t} = f_a(\\mathbf{v}_{a,t-\\boldsymbol{l}}) + g_a(\\mathbf{v}_{a,t-\\boldsymbol{l}}) \\epsilon_{a,t} \\\\ &amp; b_t = w_b(\\mathbf{v}_{b,t-\\boldsymbol{l}}) + r_b(\\mathbf{v}_{b,t-\\boldsymbol{l}}) \\epsilon_{b,t} \\\\ &amp; \\mathbf{v}_{b,t} = f_b(\\mathbf{v}_{b,t-\\boldsymbol{l}}) + g_b(\\mathbf{v}_{b,t-\\boldsymbol{l}}) \\epsilon_{b,t} \\end{aligned} , \\tag{16.26} \\end{equation}\\] where the elements of the demand size and demand occurrence parts have been defined in Sections 8 and 16.1.4 respectively. The model (16.26) can also be considered as a more general one to the conventional ADAM ETS and ARIMA models. And if the probability of occurrence \\(p_t\\) is equal to one for all observations, then the model reverts to them. Another important thing to note about this model is that it relies on the following assumptions: The demand sizes variable \\(z_t\\) is continuous. While it might sound artificial, Svetunkov and Boylan (2019) showed that such model does not perform worse than count data models; Potential demand size may change over time even when \\(o_t=0\\). This means that the states evolve over time even when demand is not observed; Demand sizes and demand occurrence are independent. This simplifies many of the further derivations and makes model estimable. If the assumption is violated, then a different model with different properties would need to be constructed. My gut feeling tells me that even if this is violated, the model (16.26) would work well. Depending on the specific model for each part and restrictions on \\(\\mu_{a,t}\\) and \\(\\mu_{b,t}\\), we might have different types of iETS models. In order to distinguish one model from another, we introduce the notation of iETS models of the form “iETS(demand sizes model)\\(_\\text{type of occurrence}\\)(model A type)(model B type).” For example, in the iETS(M,N,N)\\(_G\\)(A,N,N)(M,M,N) the first brackets say that ETS(M,N,N) was applied to the demand sizes, the underscore letter points out that this is the “general probability” model, which has ETS(A,N,N) for the model A and ETS(M,M,N) for the model B. If only one variable is needed (either \\(a_t\\) or \\(b_t\\)), then the redundant brackets are dropped, and the notation is simplified, for example, to: iETS(M,N,N)\\(_O\\)(M,N,N). If the same type of model is used for both demand sizes and demand occurrence, then the second brackets can be dropped as well, simplifying this further to: iETS(M,N,N)\\(_O\\) (odds ratio model with ETS(M,N,N) for all parts). All these models are implemented in adam() function for smooth package in R. Similar notations and principles can be used for models based on ARIMA. Note that oARIMA is not yet implemented, but in theory a model like iARIMA(0,1,1)\\(_O\\)(0,1,1) could be constructed in ADAM framework. Last but not least, in some cases we might have explanatory variables (such as promotions, prices, weather etc) that would impact both demand occurrence and demand sizes. In ADAM, we can include them in respective oes() and adam() functions. Just remember that when you include explanatory variables in the occurrence part, you are modelling the probability of occurrence, not the occurrence itself. So, for example, a promotional effect in this situation would mean that there is a higher chance of having sales. In some other situations we might not need dynamic models, such as ETS and ARIMA, and can focus on the static regression. While adam() supports this as well, the alm() function from greybox might be more suitable in this situation. It supports similar parameters, but its occurrence parameter accepts either the type of transform (plogis for logit model and pnorm for the probit one) or a previously estimated occurrence model (either from alm() or from oes()). 16.3.1 Maximum Likelihood Estimation While there are different ways of estimating the parameters of ADAM model (16.26), it is worth focusing on likelihood estimation. The log-likelihood of the model should consist of several parts: The PDF of demand sizes part of the model, when demand occurs; The probability of occurrence; The probability of inoccurrence. When demand occurs the likelihood is: \\[\\begin{equation} \\mathcal{L}(\\boldsymbol{\\theta} | y_t, o_t=1) = p_t f_z(z_t | \\mathbf{v}_{z,t-\\boldsymbol{l}}) , \\tag{16.27} \\end{equation}\\] while in the opposite case it is: \\[\\begin{equation} \\mathcal{L}(\\boldsymbol{\\theta} | y_t, o_t=0) = (1-p_t) f_z(z_t | \\mathbf{v}_{z,t-\\boldsymbol{l}}), \\tag{16.28} \\end{equation}\\] where \\(\\boldsymbol{\\theta}\\) includes all the estimated parameters of the model and parameters of assumed distribution (i.e. scale). Based on the equations (16.27) and (16.28), we can summarise the likelihood for the whole sample of \\(T\\) observations: \\[\\begin{equation} \\mathcal{L}(\\boldsymbol{\\theta} | \\textbf{Y}) = \\prod_{o_t=1} p_t \\prod_{o_t=0} (1-p_t) \\prod_{t=1}^T f_z(z_t | \\mathbf{v}_{z,t-\\boldsymbol{l}}) , \\tag{16.29} \\end{equation}\\] or in logarithms: \\[\\begin{equation} \\ell(\\boldsymbol{\\theta} | \\textbf{Y}) = \\sum_{t=1}^T f_z(z_t | \\mathbf{v}_{z,t-\\boldsymbol{l}}) + \\sum_{o_t=1} \\log(p_t) + \\sum_{o_t=0} \\log(1-p_t), \\tag{16.30} \\end{equation}\\] where \\(f_z(z_t | \\mathbf{v}_{z,t-\\boldsymbol{l}})\\) can be substituted by a likelihood of the assumed distribution from the list of candidates in Section 14.1 (substituting \\(T\\) in formulae in Tables 14.1 and 14.2 by \\(T_1\\)). The main issue in calculating the likelihood (16.30) is that the demand sizes are not observable when \\(o_t=0\\). This means that we cannot calculate the likelihood using the conventional approach, we need to use something else. Svetunkov and Boylan (2019) proposed using Expectation Maximisation (EM) algorithm for this purpose, which is typically done in the following stages: Take Expectation of the likelihood; Maximise it with the obtained parameters; Go to (1) with the new set of parameters if the likelihood has not converged to maximum. A classical example with EM is when there are several samples with different parameters and we need to split them, but we do not know where specific observations belongs to and what is the probability that each observation belongs to one of the groups. In our context, it is a slightly different idea: we know probabilities, but we do not observe some of demand sizes. If we take the expectation of (16.30) with respect to the unobserved demand sizes, we will get: \\[\\begin{equation} \\begin{aligned} \\ell(\\boldsymbol{\\theta} | \\textbf{Y}) &amp; = \\sum_{o_t=1} \\log f_z \\left(z_{t} | \\mathbf{v}_{z,t-\\boldsymbol{l}} \\right) + \\sum_{o_t=0} \\text{E} \\left( \\log f_z \\left(z_{t} | \\mathbf{v}_{z,t-\\boldsymbol{l}} \\right) \\right) \\\\ &amp; + \\sum_{o_t=1} \\log(p_t) + \\sum_{o_t=0} \\log(1- p_t) \\end{aligned}. \\tag{16.31} \\end{equation}\\] Luckily, the expectation in (16.31) is known in statistics as “Differential Entropy” (it is actually negative differential entropy in the formula above). It will differ from one case to another, depending on the assumed demand sizes distribution. Table 16.1 summarises differential entropies for the distributions used in ADAM. Table 16.1: Differential entropies for different distributions. \\(\\Gamma(\\cdot)\\) is the Gamma function, while \\(\\psi(\\cdot)\\) is the digamma function. Assumption Differential Entropy Normal \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\) \\(\\frac{1}{2}\\left(\\log(2\\pi\\sigma^2)+1\\right)\\) Laplace \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\) \\(1+\\log(2s)\\) S \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\) \\(2+2\\log(2s)\\) Generalised Normal \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\) \\(\\beta^{-1}-\\log\\left(\\frac{\\beta}{2s\\Gamma\\left(\\beta^{-1}\\right)}\\right)\\) Asymmetric Laplace \\(\\epsilon_t \\sim \\mathcal{ALaplace}(0, s, \\alpha)\\) \\(1+\\log(2s)\\) Inverse Gaussian \\(1+\\epsilon_t \\sim \\mathcal{IG}(1, s)\\) \\(\\frac{1}{2}\\left(\\log \\pi e s -\\log(2) \\right)\\) Gamma \\(1+\\epsilon_t \\sim \\mathcal{\\Gamma}(s^{-1}, s)\\) \\(s^{-1} + \\log \\Gamma\\left(s^{-1} \\right) + \\left(1-s^{-1}\\right)\\psi\\left(s^{-1}\\right)\\) Log Normal \\(1+\\epsilon_t \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\) \\(\\frac{1}{2}\\left(\\log(2\\pi\\sigma^2)+1\\right)-\\frac{\\sigma^2}{2}\\) The majority of formulae for differential entropy in Table 16.1 are taken from Wikipedia (2021b) with the exclusion of the one for \\(\\mathcal{IG}\\), which was derived by Mudholkar and Tian (2002). These values can be inserted instead of the \\(\\text{E} \\left( \\log f_z \\left(z_{t} | \\mathbf{v}_{z,t-\\boldsymbol{l}} \\right) \\right)\\) in the formula (16.31), leading to the expected likelihood for respective distributions. Luckily, the EM process in our specific situation does not need to be iterative - the obtained likelihood can then be maximised directly by changing the values of parameters \\(\\boldsymbol{\\theta}\\). It is also possible to derive analytical formulae for parameters of some of distributions based on (16.31) and the values from Table 16.1. For example, in case of \\(\\mathcal{IG}\\) the estimate of scale parameter is: \\[\\begin{equation} \\hat{s} = \\frac{1}{T} \\sum_{o_t=1}\\frac{e_t^2}{1+e_t}. \\tag{16.32} \\end{equation}\\] In fact, it can be shown that the likelihood estimates of scales for different distributions correspond to the conventional formulae from Section 14.1, but with the summation over \\(o_t=1\\) instead of all the observations. Note however that the division in (16.32) is done by the whole sample \\(T\\). This implies that the estimate of scale will be biased, similarly to the classical bias of the sample variance (2.1). However, in the full ADAM model, the estimate of scale is biased not only in sample, but asymptotically as well, implying that with the increase of the sample size it will be consistently lower than needed. This is because the summation is done over the non-zero values, while the division is done over the whole sample. This proportion of non-zeroes will impact the scale in (16.32), deflating its value. The only situation, when the bias will be reduced is when the probability of occurrence reaches 1 (demand on product becomes regular). Still, the value (16.32) will maximise the expected likelihood (16.31) and is useful for inference. However, if one needs to construct prediction intervals, this bias needs to be addressed, which can be done using the following correction: \\[\\begin{equation} \\hat{s}^\\prime = \\frac{T}{T_1-k} \\hat{s}, \\tag{16.33} \\end{equation}\\] where \\(k\\) is the number of all estimated parameters. 16.3.2 Conditional expectation and variance Now that we have discussed how the module is formulated and how it can be estimated, we can move to the discussion of conditional expectation and variance from the model. The former is needed in order to produce point forecasts, while the latter might be needed for different inventory decisions. The conditional \\(h\\) steps ahead expectation of the model can be obtained easily based on the assumption of independence of demand occurrence and demand sizes we have discussed earlier in Section 16.3: \\[\\begin{equation} \\mu_{y,t+h|t} = \\mu_{o,t+h|t} \\mu_{z,t+h|t}, \\tag{16.34} \\end{equation}\\] where \\(\\mu_{o,t+h|t}\\) is the conditional expectation of the occurrence variable (the conditional \\(h\\) steps ahead probability of occurrence) and \\(\\mu_{z,t+h|t}\\) is the conditional expectation of the demand sizes variable \\(z_t\\). So, the forecast from the model (16.26) relies on the probability of occurrence of the variable and will reflect an average demand per period of time. As a result, in some cases it might be less than one, implying that the product is not sold every day. Consequentially, Kourentzes (2014) argues that a term “demand rate” should be used in this context instead of the conditional expectation. However, any forecasting model will produce “demand per period” forecasts, they just typically assume that the probability of occurrence is equal to one (\\(p_t=1\\)) for all observations. So, there is no conceptual difference between the forecasts produced by regular and intermittent demand models and I personally do not see point in using “demand rate” term. As for the conditional variance, it is slightly trickier than the conditional expectation, because the variance of a product involves not only variances but expectations as well (assuming that two variables are independent): \\[\\begin{equation} \\mathrm{V}(y_t) = \\mathrm{V}(o_t) \\mathrm{V}(z_t) + \\mathrm{E}(o_t)^2 \\mathrm{V}(z_t) + \\mathrm{V}(o_t) \\mathrm{E}(z_t)^2 . \\tag{16.35} \\end{equation}\\] Given that we use Bernoulli distribution for the variable \\(o_t\\), its variance is equal to \\(\\mu_{o,t+h|t} (1-\\mu_{o,t+h|t})\\). In our context this implies that the conditional \\(h\\) steps ahead variance for the model 16.3 is: \\[\\begin{equation} \\sigma^2_h = \\mu_{o,t+h|t} (1-\\mu_{o,t+h|t}) \\sigma^2_{z,h} + \\mu_{o,t+h|t}^2 \\sigma^2_{z,h} + \\mu_{o,t+h|t} (1-\\mu_{o,t+h|t}) \\mu_{z,t+h|t}^2 , \\tag{16.36} \\end{equation}\\] or after some manipulations: \\[\\begin{equation} \\sigma^2_h = \\mu_{o,t+h|t} \\left(\\sigma^2_{z,h} + (1 - \\mu_{o,t+h|t}) \\mu_{z,t+h|t}^2 \\right). \\tag{16.37} \\end{equation}\\] All the elements of the formula (16.37) are available and have been discussed in the previous Chapters (Sections 8.3, 9.2 and 16.1). References "],["examples-of-application-1.html", "16.4 Examples of application", " 16.4 Examples of application In order to see how ADAM works on intermittent data, we consider the same example from the Section 16.1. We remember that in that example both demand occurrence and demand sizes increase over time, meaning that we can try the model with trend for both parts: plot(y) This can be done using adam() function from smooth package, defining the type of occurrence to use. We will try several options and select the one that has the lowest AICc: adamModelsiETS &lt;- vector(&quot;list&quot;,4) adamModelsiETS[[1]] &lt;- adam(y, &quot;MMdN&quot;, occurrence=&quot;odds-ratio&quot;, h=10, holdout=TRUE) adamModelsiETS[[2]] &lt;- adam(y, &quot;MMdN&quot;, occurrence=&quot;inverse-odds-ratio&quot;, h=10, holdout=TRUE) adamModelsiETS[[3]] &lt;- adam(y, &quot;MMdN&quot;, occurrence=&quot;direct&quot;, h=10, holdout=TRUE) adamModelsiETS[[4]] &lt;- adam(y, &quot;MMdN&quot;, occurrence=&quot;general&quot;, h=10, holdout=TRUE) adamModelsiETSAICcs &lt;- setNames(sapply(adamModelsiETS,AICc), c(&quot;odds-ratio&quot;,&quot;inverse-odds-ratio&quot;,&quot;direct&quot;,&quot;general&quot;)) adamModelsiETSAICcs ## odds-ratio inverse-odds-ratio direct general ## 364.8040 510.7244 361.9342 379.8016 Based on this, we can see that the model with direct has the lowest AICc. We can see how the model has approximated the data and produced forecasts for the holdout: i &lt;- which.min(adamModelsiETSAICcs) plot(adamModelsiETS[[i]],7) We can explore the demand occurrence part of this model the following way: adamModelsiETS[[i]]$occurrence ## Occurrence state space model estimated: Direct probability ## Underlying ETS model: oETS[D](MMdN) ## Smoothing parameters: ## level trend ## 0 0 ## Vector of initials: ## level trend ## 0.0525 1.1080 ## ## Error standard deviation: 1.2915 ## Sample size: 110 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 105 ## Information criteria: ## AIC AICc BIC BICc ## 99.3525 99.9294 112.8549 114.2108 plot(adamModelsiETS[[i]]$occurrence) Depending on the generated data, there might be issues in the ETS(M,Md,N) model for demand sizes, if the smoothing parameters are large. So, we can try out the ADAM logARIMA(1,1,2) to see how it compares with this model. Given that ARIMA is not yet implemented for the occurrence part of the model, we need to construct it separately and then use in adam(): oETSModel &lt;- oes(y, &quot;MMdN&quot;, occurrence=names(adamModelsiETSAICcs)[i], h=10, holdout=TRUE) adamModeliARIMA &lt;- adam(y, &quot;NNN&quot;, occurrence=oETSModel, orders=c(1,1,2), distribution=&quot;dlnorm&quot;, h=10, holdout=TRUE) adamModeliARIMA ## Time elapsed: 0.13 seconds ## Model estimated using adam() function: iARIMA(1,1,2)[D] ## Occurrence model type: Direct ## Distribution assumed in the model: Mixture of Bernoulli and Log Normal ## Loss function type: likelihood; Loss function value: 138.7611 ## ARMA parameters of the model: ## AR: ## phi1[1] ## -0.293 ## MA: ## theta1[1] theta2[1] ## -0.6346 -0.0926 ## ## Sample size: 110 ## Number of estimated parameters: 6 ## Number of degrees of freedom: 104 ## Number of provided parameters: 5 ## Information criteria: ## AIC AICc BIC BICc ## 378.8746 379.6902 395.0775 396.9942 ## ## Forecast errors: ## Asymmetry: 64.664%; sMSE: 25.22%; rRMSE: 0.906; sPIS: -2322.912%; sCE: 302.47% plot(adamModeliARIMA,7) Comparing the iARIMA model with the previous iETS based on AIC would not be fair, because as soon as the occurrence model is provided to adam(), he does not count the parameters estimated in that part towards the overal number of estimated parameters. In order to make the comparison fair, we need to estimate ADAM iETS in the following way: adamModelsiETS[[i]] &lt;- adam(y, &quot;MMdN&quot;, occurrence=oETSModel, h=10, holdout=TRUE) adamModelsiETS[[i]] ## Time elapsed: 0.05 seconds ## Model estimated using adam() function: iETS(MMdN)[D] ## Occurrence model type: Direct ## Distribution assumed in the model: Mixture of Bernoulli and Gamma ## Loss function type: likelihood; Loss function value: 124.8831 ## Persistence vector g: ## alpha beta ## 0.0553 0.0553 ## Damping parameter: 0.7536 ## Sample size: 110 ## Number of estimated parameters: 6 ## Number of degrees of freedom: 104 ## Number of provided parameters: 5 ## Information criteria: ## AIC AICc BIC BICc ## 351.1186 351.9342 367.3215 369.2382 ## ## Forecast errors: ## Asymmetry: -13.719%; sMSE: 18.356%; rRMSE: 0.773; sPIS: -909.162%; sCE: 24.572% plot(adamModelsiETS[[i]],7) Comparing information criteria, the iETS model is more appropriate for this data. But this might be due to a different distributional assumptions and difficulties estimating ARIMA model. If you want to experiment more with ADAM iARIMA, you might try fine tuning it for the data either by increasing the maxeval or changing the initialisation, for example: adamModeliARIMA &lt;- adam(y, &quot;NNN&quot;, occurrence=oETSModel, orders=c(1,1,2), distribution=&quot;dgamma&quot;, h=10, holdout=TRUE, initial=&quot;back&quot;) adamModeliARIMA ## Time elapsed: 0.08 seconds ## Model estimated using adam() function: iARIMA(1,1,2)[D] ## Occurrence model type: Direct ## Distribution assumed in the model: Mixture of Bernoulli and Gamma ## Loss function type: likelihood; Loss function value: 144.4077 ## ARMA parameters of the model: ## AR: ## phi1[1] ## -0.1681 ## MA: ## theta1[1] theta2[1] ## -0.6663 -0.0404 ## ## Sample size: 110 ## Number of estimated parameters: 4 ## Number of degrees of freedom: 106 ## Number of provided parameters: 5 ## Information criteria: ## AIC AICc BIC BICc ## 386.1679 386.5488 396.9698 397.8651 ## ## Forecast errors: ## Asymmetry: 53.386%; sMSE: 21.714%; rRMSE: 0.84; sPIS: -1987.206%; sCE: 243.109% plot(adamModeliARIMA,7) Finally, we can produce point and interval forecasts from either of the model via the forecast() method. Here is an example: adamiETSForecasts &lt;- forecast(adamModelsiETS[[i]],h=10,interval=&quot;prediction&quot;,nsim=10000) plot(adamiETSForecasts) The prediction intervals produced from multiplicative ETS models will typically be simulated, so in order to make them smoother you might need to increase nsim parameter, for example to nsim=100000. "],["intermittent-demand-challenges.html", "16.5 Intermittent demand challenges", " 16.5 Intermittent demand challenges Intermittent demand is complicated and is difficult to work with. As a result, there are several challenges that are related to ADAM model specifically and to the intermittent demand in general that are worth discussing. First, given the presence of zeroes, the decomposition of intermittent time series does not make sense. The classical time series model assumes that the demand happens on every observation, while the intermittent demand happens irregularly. This makes all the conventional models inapplicable to the problem, although some of them might still work well in some cases (for example, SES in case of slightly intermittent data). Second, follows directly from the previous point: while in theory it is possible to use any ETS / ARIMA model for both demand occurrence and demand sizes of the ADAM model, some of the specific model types are either impossible or very difficult to estimate. For example, seasonality on intermittent data is not very well pronounced, so estimating the initial values of components of seasonal models (such as ETS(M,N,M)) is not a trivial task. In some cases, if we have several products in a group that exhibit the same seasonal patterns, we can aggregate them to the group level in order to get a better estimate of seasonal indices, and then use them on the lower level. adam() function allows doing that via initial=list(seasonal=seasonalIndices). Third, in some cases you might know, when specifically demand will happen (for example, kiwis stop growing in New Zealand from May till September, so the crop will go down around that time). In this case, you do not need a proper intermittent demand model, you just need to deal with demand sizes via ADAM ETS / ARIMA and provide zeroes and ones in the demand occurrence part for the variable \\(o_t\\). This can be done in adam() via occurrence=ot, where ot contains zeroes and ones for each observation. Fourth, more specialised models, such as iETS, will produce positively biased estimates of the smoothing parameters, whatever the estimator is used (see explanation in Svetunkov and Boylan, 2019). This is caused by the assumption that the potential demand might change between the observed sales. In this situation the components would evolve over time slowly, while we would only see their values before the set of zeroes and afterwards, which will make the applied model catch up to the data, increasing the values of smoothing parameters. This also implies that such forecasting methods as Croston (Croston, 1972) and TSB (Teunter et al., 2011) would also result in positively biased estimates of parameters, if we assume that demand might change between the non-zero observations. Practically speaking, this means that the smoothing parameters will be higher than needed, implying more rapid changes in components and higher uncertainty in final forecasts. Finally, summarising this chapter, intermittent demand forecasting is a difficult problem. Differences between various forecasting models and methods in this case might be insignificant, and it would be challenging to select the appropriate one. Furthermore, point forecasts on intermittent demand are difficult to grasp and make actionable (unless you are interested in lead time forecasts, to get an idea about the expected demand over a period of time). All of this means that intermittent demand should be avoided if possible. Yes, you can have fancy models for it, but do you need to? For example, do you need to look at daily demand on products if decisions are made on weekly basis (e.g. how many units of pasta should a supermarket order for the next week)? In many cases thinking about the problem carefully would allow avoiding intermittent demand, making life of analysts easier. But if it is not possible to do, then ADAM iETS and iARIMA models can be considered as possible solutions in some situations. References "],["diagnostics.html", "Chapter 17 Model diagnostics", " Chapter 17 Model diagnostics In this chapter we investigate how ADAM can be diagnosed and improved. The majority of topics will build upon the typical model assumptions discussed in Section 3.6. Some of the assumptions cannot be diagnosed properly, but for the others there are some existing and well established instruments. We will consider the following assumptions and discuss how to check whether they are violated or not: Model is correctly specified: No omitted variables; No redundant variables; The necessary transformation of the variables are applied; No outliers in the model. Residuals are i.i.d.: They are not autocorrelated; They are homoscedastic; The expectation of residuals is zero, no matter what; The residuals follow the specified distribution; The distribution of residuals does not change over time. The explanatory variables are not correlated with anything but the response variable; All the model diagnostics is aimed at spotting patterns in residuals. If there are some, then something is probably missing in the model. In this chapter we will discuss, which instruments can be used to diagnose different types of assumptions In order to make this more actionable, we will consider a conventional regression model on Seatbelts data. This can be estimated equally well either with adam() from smooth or alm() from greybox. In general, I recommend using alm(), when no dynamic elements are present in the model, but for illustrative purposes we will do this with adam(): adamModelSeat01 &lt;- adam(Seatbelts,&quot;NNN&quot;,formula=drivers~PetrolPrice+kms) plot(adamModelSeat01,7) This model has several issues, and in this chapter we will discuss how to diagnose and fix them. "],["diagnosticsOmitted.html", "17.1 Model specification: Omitted variables", " 17.1 Model specification: Omitted variables We start with one of the most important assumptions for models: model has not omitted important variables. In general this is difficult to diagnose, because typically it is not possible what is missing if we do not have it in front of us. The best thing one can do is a mental experiment, trying to comprise a list of all theoretically possible variables that would impact the variable of interest. If you manage to come up with such a list and realise that some of variables are missing, the next step would be to either collect the variables themselves or their proxies. One way or another, we would need to add the missing information in the model. In some cases we might be able to diagnose this. For example, with our regression model from the previous section, we have a set of variables that are not included in the model. A simple thing to do would be to see if the residuals of our model are correlated with any of the omitted variables. We can either produce scatterplots or calculate measures of association to see if there is some relation in the residuals that is not explained by the existing structure. I will use assoc() and spread() functions from greybox for this: # Create a new matrix, removing the variables that are already in the model SeatbeltsWithResiduals &lt;- cbind(as.data.frame(residuals(adamModelSeat01)), Seatbelts[,-c(2,5,6)]) colnames(SeatbeltsWithResiduals)[1] &lt;- &quot;residuals&quot; # Spread plot greybox::spread(SeatbeltsWithResiduals) spread() function automatically detects the type of variable and produces scatterplot / boxplot() / tableplot() between them, making the final plot more readable. The plot above tells us that residuals are correlated with DriversKilled, front, rear and law, so some of these variables can be added to the model to improve it. VanKilled might have a weak relation with drivers, but judging by description does not make sense in the model (this is a part of the drivers variable). In our case, it is safe to add these variables, because they make sense in explaining the number of injured drivers. However, I would not add DriversKilled as it seems not to drive the number of deaths and injuries, but is just correlated with it for obvious reasons (DriversKilled is included in drivers). We can also calculate measures of association between variables: greybox::assoc(SeatbeltsWithResiduals) ## Associations: ## values: ## residuals DriversKilled front rear VanKilled law ## residuals 1.0000 0.7826 0.6121 0.4811 0.2751 0.1892 ## DriversKilled 0.7826 1.0000 0.7068 0.3534 0.4070 0.3285 ## front 0.6121 0.7068 1.0000 0.6202 0.4724 0.5624 ## rear 0.4811 0.3534 0.6202 1.0000 0.1218 0.0291 ## VanKilled 0.2751 0.4070 0.4724 0.1218 1.0000 0.3949 ## law 0.1892 0.3285 0.5624 0.0291 0.3949 1.0000 ## ## p-values: ## residuals DriversKilled front rear VanKilled law ## residuals 0.0000 0 0 0.0000 0.0001 0.0086 ## DriversKilled 0.0000 0 0 0.0000 0.0000 0.0000 ## front 0.0000 0 0 0.0000 0.0000 0.0000 ## rear 0.0000 0 0 0.0000 0.0925 0.6890 ## VanKilled 0.0001 0 0 0.0925 0.0000 0.0000 ## law 0.0086 0 0 0.6890 0.0000 0.0000 ## ## types: ## residuals DriversKilled front rear VanKilled law ## residuals &quot;none&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;mcor&quot; ## DriversKilled &quot;pearson&quot; &quot;none&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;mcor&quot; ## front &quot;pearson&quot; &quot;pearson&quot; &quot;none&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;mcor&quot; ## rear &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;none&quot; &quot;pearson&quot; &quot;mcor&quot; ## VanKilled &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;none&quot; &quot;mcor&quot; ## law &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;none&quot; Technically speaking, the output of this function tells us that all variables are correlated with residuals and can be considered in the model. I would still prefer not to add DriversKilled in the model for the reasons explained earlier. We can construct a new model in the following way: adamModelSeat02 &lt;- adam(Seatbelts,&quot;NNN&quot;,formula=drivers~PetrolPrice+kms+front+rear+law) plot(adamModelSeat02,7) How can we know that we have not omitted any important variables in our new model? Unfortunately, there is no good way of knowing that. In general, we should use judgment in order to decide whether anything else is needed or not. But given that we deal with time series, we can analyse residuals over time and see if there is any structure left: plot(adamModelSeat02,8) This plot shows that the model has not captured seasonality and that there is stil some structure left in the residuals. In order to address this, we will add ETS(A,N,A) element to the model: adamModelSeat03 &lt;- adam(Seatbelts,&quot;ANA&quot;,formula=drivers~PetrolPrice+kms+front+rear+law) par(mfcol=c(1,2)) plot(adamModelSeat03,7:8) This is much better. There is no apparent missing structure in the data and no apparent omitted variables. We can now move to the next steps of diagnostics. "],["model-specification-redundant-variables.html", "17.2 Model specification: Redundant variables", " 17.2 Model specification: Redundant variables While there are some ways of testing for omitted variables, the redundant ones are very difficult to diagnose. Yes, we could look at the significance of variables or compare models with and without some variables based on information criteria, but even if our approaches say that a variable is not significant, this does not mean that it is not needed in the model. There can be many reasons, why a test would fail to reject H\\(_0\\) and AIC would prefer a model without the variable under consideration. So, it comes to using judgment, trying to figure out whether a variable is needed in the model or not. In the example with Seatbelt data, DriversKilled would be a redundant variable. Let’s see what happens with the model in this case: adamModelSeat04 &lt;- adam(Seatbelts,&quot;NNN&quot;,formula=drivers~PetrolPrice+kms+front+rear+law+DriversKilled) par(mfcol=c(1,2)) plot(adamModelSeat04,7:8) The residuals from this model look adequate, with only issue being the first 45 observations lying below the zero line. The summary of this model is: summary(adamModelSeat04) ## ## Model estimated using alm() function: Regression ## Response variable: drivers ## Distribution used in the estimation: Normal ## Loss function type: likelihood; Loss function value: 1159.417 ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## (Intercept) 320.2844 127.4014 68.9379 571.5145 * ## PetrolPrice 741.7600 769.1811 -775.7343 2258.5517 ## kms -0.0039 0.0042 -0.0122 0.0044 ## front 0.9302 0.1375 0.6589 1.2014 * ## rear -0.6859 0.2122 -1.1044 -0.2675 * ## law 67.9625 35.8203 -2.7064 138.5986 ## DriversKilled 6.6785 0.4377 5.8150 7.5416 * ## ## Sample size: 192 ## Number of estimated parameters: 7 ## Number of degrees of freedom: 185 ## Information criteria: ## AIC AICc BIC BICc ## 2332.834 2333.443 2355.637 2357.237 The uncertainty around the parameter DriversKilled is narrow, showing that the variable has a positive impact on the drivers. However the issue here is not statistical, but rather fundamental: we have included the variable that is a part of our response variable. It does not explain why drivers get injured and killed, it just reflects a specific part of this relation. So it explains part of the variance, which should have been explained by other variables (e.g. kms and law), making them statistically not significant. So, based on technical analysis we would be inclined to keep the variable, but based on our understanding of the problem we should not. If we have redundant variables in the model, then the model might overfit the data, leading to narrower prediction intervals and biased forecasts. The parameters of such model are typically unbiased, but inefficient. "],["diagnosticsTransformations.html", "17.3 Model specification: Transformations", " 17.3 Model specification: Transformations The question of appropriate transformations for variables in the model is challenging, because it is difficult to decide, what sort of transformation is needed, if needed at all. In many cases, this comes to selecting between additive linear model and a multiplicative one. This implies that we compare the model: \\[\\begin{equation} y_t = a_0 + a_1 x_{1,t} + \\dots + a_n x_{n,t} + \\epsilon_t, \\tag{17.1} \\end{equation}\\] and \\[\\begin{equation} y_t = \\exp\\left(a_0 + a_1 x_{1,t} + \\dots + a_n x_{n,t} + \\epsilon_t\\right) . \\tag{17.2} \\end{equation}\\] The latter model is equivalent to the so called “log-linear” model, but can also include logarithms of explanatory variables instead of the variables themselves. There are different ways to diagnose the problem with wrong transformations, which sometimes help in detecting it. The first one is the actuals vs fitted plot: plot(adamModelSeat03,1) The grey dashed line on the plot corresponds to the situation, when actuals and fitted coincide (100% fit). The red line on the plot above is LOESS line, produced by lowess() function in R, smoothing the scatterplot to reflect the potential tendencies in the data. In the ideal situation this red line should coinside with the grey line. In addition the variability around the line should not change with the increase of fitted values. In our case there is a slight U-shape in the red line and an insignificant increase in variability around the middle of the data. This could either be due to pure randomness and thus should be ignored, or could indicate a slight non-linearity in the data. After all, we have constructed pure additive model on the data that exhibits seasonality with multiplicative characteristics, which becomes especially apparent at the end of the series, where the drop in level is accompanied by the decrease of variability of the data: plot(adamModelSeat03,7) In order to diagnose this properly, we might use other instruments. One of these is the analysis of standardised residuals. The formula for the standardised residuals will differ depending on the assumed distribution and for some of them comes to the value inside the “\\(\\exp\\)” part of the probability density function: Normal, \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\): \\(u_t = \\frac{e_t - \\bar{e}}{\\hat{\\sigma}}\\); Laplace, \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\): \\(u_t = \\frac{e_t - \\bar{e}}{\\hat{s}}\\); S, \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\): \\(u_t = \\frac{e_t - \\bar{e}}{\\hat{s}^2}\\); Generalised Normal, \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\): \\(u_t = \\frac{e_t - \\bar{e}}{\\hat{s}^{\\frac{1}{\\beta}}}\\); Inverse Gaussian, \\(1+\\epsilon_t \\sim \\mathcal{IG}(1, s)\\): \\(u_t = \\frac{1+e_t}{\\bar{e}}\\); Gamma, \\(1+\\epsilon_t \\sim \\mathcal{\\Gamma}(s^{-1}, s)\\): \\(u_t = \\frac{1+e_t}{\\bar{e}}\\); Log Normal, \\(1+\\epsilon_t \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\): \\(u_t = \\frac{e_t - \\bar{e} +\\frac{\\hat{\\sigma}^2}{2}}{\\hat{\\sigma}}\\). where \\(\\bar{e}\\) is the mean of residuals, which is typically assumed to be zero and \\(u_t\\) is the value of standardised residuals. Note that the scales in the formulae above should be calculated via the formula with the bias correction, i.e. with the division by degrees of freedom, not the number of observations. Also, note that in case of \\(\\mathcal{IG}\\), \\(\\Gamma\\) and \\(\\mathrm{log}\\mathcal{N}\\) and additive error models, the formulae for the standardised residuals will be the same, only the assumptions will change (see Section 8.5). Here is an example of a plot of fitted vs standardised residuals in R: plot(adamModelSeat03,2) Figure 17.1: Diagnostics of pure additive ETSX model. Given that the scale of the original variable is now removed in the standardised residuals, it might be easier to spot the non-linearity. In our case it is still not apparent, but there is a slight U-shape in LOESS line and a slight change in variance. Another plot that we have already used before is standardised residuals over time: plot(adamModelSeat03,8) This plot shows that there is a slight decline in the residuals around year 1977. Still, there is no prominent non-linearity in the residuals, so it is not clear whether any transformations are needed or not. However, based on my judgment and understanding of the problem, I would expect for the number of injuries and deaths to change proportionally to the change of the level of the data: if after some external interventions the overal level of injuries and deaths would increase, we would expect a percentage decline, not a unit decline with a change of already existing variables in the model. This is why I will try a multiplicative model next: adamModelSeat05 &lt;- adam(Seatbelts,&quot;MNM&quot;,formula=drivers~PetrolPrice+kms+front+rear+law) plot(adamModelSeat05,2) Figure 17.2: Diagnostics of pure multiplicative ETSX model. The plot shows that the variability is now slightly more uniform across all fitted values, but the difference between Figures 17.1 and 17.2 is not very prominent. One of potential solutions in this situation is to compare the models in terms of information criteria: setNames(c(AICc(adamModelSeat03),AICc(adamModelSeat05)), c(&quot;Additive model&quot;, &quot;Multiplicative model&quot;)) ## Additive model Multiplicative model ## 2233.966 2228.807 Based on this, we would be inclined to select the multiplicative model. My personal judgment in this specific case agrees with the information criterion. "],["diagnosticsOutliers.html", "17.4 Model specification: Outliers", " 17.4 Model specification: Outliers As we discussed in Section 3.6.1, one of the important assumptions in forecasting and analytics is the correct specification of the model, which also includes “no outliers in the model” element. Outliers might appear for many different reasons: We missed some important information (e.g. promotion) and did not include a respective variable in the model; There was an error in recordings of the data, i.e. a value of 2000 was recorded as 200; We did not miss anything predictable, we just deal with a distribution with fat tails. In any of these cases, outliers will impact estimates of parameters of our models. In case of ETS, this will lead to higher than needed smoothing parameters, which leads to wider prediction intervals and potentially biased forecasts. In case of ARIMA, the mechanism is more complicated, but also leads to widened intervals and biased forecasts. So, it is important to identify outliers and deal with them. 17.4.1 Outliers detection One of the simplest ways for identifying outliers is based on distributional assumptions. For example, if we assume that our data follows normal distribution, then we would expect 95% of observations lie inside the bounds with approximately \\(\\pm 1.96\\sigma\\) and 99.8% of them to lie inside the \\(\\pm3.09 \\sigma\\). Sometimes these values are substituted by heuristic “values lying inside 2 / 3 sigmas,” which is not precise and works only for Normal distribution. Still, based on this, we could flag the values outside these bounds and investigate them in order to see if any of them are indeed outliers. Given that ADAM framework supports different distributions, the heuristics mentioned above is not appropriate. We need to get proper quantiles for each of the assumed distributions. Luckily, this is not difficult to do, because the quantile functions for all the distributions supported by ADAM either have analytical forms or can be obtained numerically. Here is an example in R with the same multiplicative ETSX model and the standardised residuals vs fitted values with the 95% bounds: plot(adamModelSeat05, 2, level=0.95) Note that in case of \\(\\mathcal{IG}\\), \\(\\Gamma\\) and \\(\\mathrm{log}\\mathcal{N}\\), the function will plot \\(\\log u_t\\) in order to make the plot more readable. The plot demonstrates that there are outliers, some of which are further away from the bounds. Although the amount of outliers is not big, it would make sense investigating why they happened. Well, we know why - we constructed an incorrect model. Given that we deal with time series, plotting residuals vs time is also sometimes helpful: plot(adamModelSeat05, 8) We see that there is no specific pattern in the outliers, they happen randomly, so they appear not because of the omitted variables or wrong transformations. We have 5 observations lying outside the bounds, which given that the sample size of 192 observations, means that the 95% interval contains \\(\\frac{192-9}{192} \\times 100 \\mathrm{\\%} \\approx 95.3\\mathrm{\\%}\\) of observations, which is close to the nominal value. In some cases, the outliers might impact the scale of distribution and will lead to wrong standardised residuals, distorting the picture. This is where studentised residuals come into play. They are calculated similarly to the standardised ones, but the scale of distribution is recalculated for each observation by considering errors on all but the current observation. So, in a general case, this is an iterative procedure which involves looking through \\(t=\\{1,\\dots,T\\}\\) and which should in theory guarantee that the real outliers do not impact the scale of distribution. Here how they can be analysed in R: par(mfcol=c(1,2)) plot(adamModelSeat05, c(3,9)) In many cases (ours included) the standardised and studentised residuals will look very similar, but in some cases of extreme outliers they might differ and the latter might show outliers better than the former. Given the situation with outliers in our case, we could investigate when they happen in the original data to better understand whether they need to be taken care of. But instead of manually recording, which of the observations lie beyond the bounds, we can get their ids via the outlierdummy method from the package greybox, which extracts either standardised or studentised residuals and flags those observations that lie outside the constructed interval, automatically creating dummy variables for these observations. Here how it works: adamModelSeat05Outliers &lt;- outlierdummy(adamModelSeat05, level=0.95, type=&quot;rstandard&quot;) The method returns several objects (see documentation for details), including the ids of outliers: adamModelSeat05Outliers$id ## [1] 25 33 49 66 85 86 143 156 170 These ids can be used to produce additional plots. For example: plot(actuals(adamModelSeat05)) points(time(Seatbelts[,&quot;drivers&quot;])[adamModelSeat05Outliers$id], Seatbelts[adamModelSeat05Outliers$id,&quot;drivers&quot;], col=&quot;red&quot;, pch=16) text(time(Seatbelts[,&quot;drivers&quot;])[adamModelSeat05Outliers$id], Seatbelts[adamModelSeat05Outliers$id,&quot;drivers&quot;], adamModelSeat05Outliers$id, col=&quot;red&quot;, pos=2) Among all these points, there is one special that happens on observation 170. This is when the law for seatbelts was introduced and the model cannot capture the change in injuries and deaths correctly. Remark. As a side note, in R, there are several methods for extracting residuals: resid() or residuals() will extract either \\(e_t\\) or \\(1+e_t\\), depending on the distributional assumptions of the model; rstandard() will extract the standardised residuals \\(u_t\\); rstudent() will do the same for the studentised ones. smooth package also introduces rmultistep which extracts multiple steps ahead in sample forecast errors. We do not discuss this method here, but we might come back to it later in this textbook. 17.4.2 Dealing with outliers Based on the output of outlierdummy() method from the previous example, we can construct a model with explanatory variables to interpolate the outliers and neglect their impact on the model: SeatbeltsWithOutliers &lt;- cbind(as.data.frame(Seatbelts[,-c(1,7)]),adamModelSeat05Outliers$outliers) SeatbeltsWithOutliers$drivers &lt;- ts(SeatbeltsWithOutliers$drivers, start=start(Seatbelts), frequency=frequency(Seatbelts)) adamModelSeat06 &lt;- adam(SeatbeltsWithOutliers,&quot;MNM&quot;,lags=12,formula=drivers~.) In order to decide, whether the dummy variables help or not, we can use information criteria, comparing the two models: setNames(c(AICc(adamModelSeat05), AICc(adamModelSeat06)), c(&quot;ETSX&quot;,&quot;ETSXOutliers&quot;)) ## ETSX ETSXOutliers ## 2228.807 2199.470 Comparing the two values above, I would conclude that adding dummies improves the model. But instead of including all of them, we could try the model with the outlier for the suspicious observation 170, which corresponds to the ninth outlier: adamModelSeat07 &lt;- adam(SeatbeltsWithOutliers,&quot;MNM&quot;,lags=12, formula=drivers~PetrolPrice+kms+front+rear+law+outlier9) plot(adamModelSeat07,2) AICc(adamModelSeat07) ## [1] 2219.569 This model is slightly worse than both the one with all outliers in terms of AICc, so there are some other dummy variables that improve the fit that might be considered as well, along with the outlier for the observation 170. We could continue the exploration introducing other dummy variables, but in general we should not do that unless we have good reason for that (e.g. we know that something happened that was not captured by the model). 17.4.3 An automatic mechanism A similar automated mechanism is implemented in adam() function, which has outliers parameter, defining what to do with outliers if there are any with the following three options: “ignore” - do nothing; “use” - create the model with explanatory variables as shown in the previous subsection and see if it is better than the simpler model in terms of an information criterion; “select” - create lags and leads of dummies from outlierdummy() and then select the dummies based on the explanatory variables selection mechanism. Lags and leads are needed for cases, when the effect of outlier is carried over to neighbouring observations. Here how this works for our case: adamModelSeat08 &lt;- adam(Seatbelts,&quot;MNM&quot;,lags=12, formula=drivers~PetrolPrice+kms+front+rear+law, outliers=&quot;select&quot;,level=0.95) AICc(adamModelSeat08) ## [1] 3089.447 This automatic procedure will form a matrix that will include original variables together with the outliers, their lags and leads and then select those of them that minimise AICc in a stepwise procedure (discussed in Section 18.3). In our case, the function throws away some of the important variables and sticks with some of outliers. This might also happen because it could not converge to the optimum on each iteration, so increasing maxeval might help. Still, given that this is an automated approach, it is prone to potential mistakes and needs to be treated with care as it might select unnecessary dummy variables and lead to overfitting. I would recommend exploring the outliers manually, when possible and not to rely too much on the automated procedures. 17.4.4 Final remarks Koehler et al. (2012) explored the question of the impact of outliers on ETS performance in terms of forecasting accuracy. They found that if outliers happen at the end of the time series then it is important to take them into account in a model. If they happen much earlier, then their impact on the final forecast will be negligible. Unfortunately, the authors did not explore the impact of outliers on the prediction intervals, and based on my experience I can tell that the main impact of outliers is on the width of the interval. References "],["diagnosticsResidualsIIDAuto.html", "17.5 Residuals are i.i.d.: autocorrelation", " 17.5 Residuals are i.i.d.: autocorrelation One of the typical characteristics of time series models is the dynamic relation between variables. Even if fundamentally the sales of ice cream on Monday do not impact sales of the same ice cream on Tuesday, they might impact advertising expenses or sales of a competing product on Tuesday, Wednesday or next week. Missing this sort of structure might lead to autocorrelation of residuals, which then would impact the estimates of parameters and final forecasts. Autocorrelations might also arise due to wrong transformations of variables, where the model would systematically underforecast the actuals, producing autocorrelated residuals. In this section, we will see one of the potential ways for the regression diagnostics and try to improve the model in a stepwise fashion, trying out different order of ARIMA model. As an example, we continue with the same seatbelts data, dropping the dynamic part to see what would happen in this case: adamModelSeat09 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~PetrolPrice+kms+front+rear+law) AICc(adamModelSeat09) ## [1] 2487.714 There are different ways to diagnose this model. We start with a basic plot of residuals over time: plot(adamModelSeat09,8) We see that on one hand the residuals still contain seasonality and on the other that they do not look stationary. We could conduct ADF and / or KPSS test to get a formal answer to the stationarity question: tseries::kpss.test(resid(adamModelSeat09)) ## ## KPSS Test for Level Stationarity ## ## data: resid(adamModelSeat09) ## KPSS Level = 0.69641, Truncation lag parameter = 4, p-value = 0.01387 tseries::adf.test(resid(adamModelSeat09)) ## Warning in tseries::adf.test(resid(adamModelSeat09)): p-value smaller than ## printed p-value ## ## Augmented Dickey-Fuller Test ## ## data: resid(adamModelSeat09) ## Dickey-Fuller = -5.6449, Lag order = 5, p-value = 0.01 ## alternative hypothesis: stationary The tests have opposite null hypothesis, and in our case we would reject H\\(_0\\) for both of them on 5% significance level. This means that they contradict each other and we need to use our judgment. First I will see what happens with the model, when we do take differences: # ARIMAX(0,1,0)(0,0,0)_12 adamModelSeat10 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~PetrolPrice+kms+front+rear+law, orders=list(i=1)) AICc(adamModelSeat10) ## [1] 2390.028 This leads to an improvement in AICc in comparison with the previous model. The residuals of the model are now also much better behaved: plot(adamModelSeat10,8) In order to see whether there are any other dynamic elements left, we will plt ACF and PACF of residuals: par(mfcol=c(1,2)) plot(adamModelSeat10,10:11) In case of adam() objects, these plots will always have the range for y-axis from -1 to 1 and will start from lag 1 on x-axis. The red horizontal lines represent the “non-rejection” region: if the point lie inside the region, then they are not statistically different from zero on the selected level (the uncertainty around them is so high that it covers zero). The points with numbers are those that are statistically significantly different from zero. So, the ACF / PACF analysis might show the lags that are statistically significant on the selected level (the default one is 0.95). Given that this is a statistical instrument, we would expect for approximately (1-level)% (e.g. 5%) of lags lie outside these bounds, so it is fine if we don’t see all point lying inside them. However, we should not see any patterns there and we might need to investigate the suspicious lags (low orders of up to 3 - 5 and the seasonal lags if they appear). In our example we see that there is a suspicious lag 1 on ACF and a suspicious lag 2 on the PACF, which might indicate that some dynamic elements are missing (e.g. MA(1) or AR(2)). Furthermore, there are spikes on lag 12 for both ACF and PACF. While it is not clear, what specifically is needed here, we can try out several models and see which one of them is better in order to deremine the appropriate element: # ARIMAX(0,1,0)(1,0,0)_12 adamModelSeat11 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~PetrolPrice+kms+front+rear+law, orders=list(ar=c(0,1),i=1)) AICc(adamModelSeat11) ## [1] 2389.865 # ARIMAX(0,1,0)(0,0,1)_12 adamModelSeat12 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~PetrolPrice+kms+front+rear+law, orders=list(i=1,ma=c(0,1))) AICc(adamModelSeat12) ## [1] 2405.051 # ARIMAX(0,1,0)(1,0,1)_12 adamModelSeat13 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~PetrolPrice+kms+front+rear+law, orders=list(ar=c(0,1),i=1,ma=c(0,1))) AICc(adamModelSeat13) ## [1] 2346.304 Based on this analysis, we would be inclined to include both seasonal AR(1) and seasonal MA(1) in the model. Next step in our iterative process - another ACF / PACF plot of the residuals: par(mfcol=c(1,2)) plot(adamModelSeat13,10:11) In this case, there is a big spike on ACF for lag 1, so we can try adding MA(1) component in the model: # ARIMAX(0,1,1)(1,0,1)_12 adamModelSeat14 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~PetrolPrice+kms+front+rear+law, orders=list(ar=c(0,1),i=1,ma=c(1,1))) AICc(adamModelSeat14) ## [1] 2319.997 Which leads to further improvement in AICc. We could continue our investigations in order to find the most suitable ARIMAX model for the data using this iterative procedure, but this example should suffice in providing the general idea of how it can be done. What we could do else to simplify the process is to use the automated ARIMA selection algorithm in adam(), which is built on the principles discussed in this section: adamModelSeat15 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~PetrolPrice+kms+front+rear+law, orders=list(ar=c(3,2),i=c(2,1),ma=c(3,2),select=TRUE)) AICc(adamModelSeat15) ## [1] 2195.784 This new constructed SARIMAX(0,1,1)(0,1,1)\\(_{12}\\) model has lower AICc than the previous one and should be used instead. In fact, it is even better than the ETSX(MNM) (model 5) from the previous section in terms of AICc, and its residuals are much better behaved than the ones of model 5 (we might need to analyse the residuals for the potential outliers in this model though): par(mfcol=c(1,3)) plot(adamModelSeat15,c(8,10:11)) So for the purposes of analytics and forecasting, we would be inclined to use SARIMAX(0,1,1)(0,1,1)\\(_{12}\\) rather than ETSX(MNM). As a final word for this section, we have focused our discussion on the visual analysis of time series, ignoring the statistical tests (we only used ADF and KPSS). Yes, there is Durbin-Watson (Wikipedia, 2021c) test for AR(1) in residuals, and yes there are Ljung-Box (Wikipedia, 2021d), Box-Pierce and Breusch–Godfrey (Wikipedia, 2021e) tests for multiple AR elements. But visual inspection of time series is not less powerful than hypothesis testing. In fact, it makes you think and analyse the model and its assumptions, while the tests are the lazy way out that might lead to wrong conclusions because they have the standard limitations of any hypothesis tests. After all, if you fail to reject H\\(_0\\) it does not mean that the effect does not exist. Having said that, the statistical tests become extremely useful when you need to process many time series at the same time and cannot physically do visual inspection of them. So, if you are in that situation, I would recommend reading more about them, but I do not aim to retell the content of Wikipedia in this textbook. References "],["diagnosticsResidualsIIDHetero.html", "17.6 Residuals are i.i.d.: heteroscedasticity", " 17.6 Residuals are i.i.d.: heteroscedasticity Another important assumption for conventional models is that the residuals are homoscedastic, meaning that the variance of the residuals stays the same (no matter what). Typically, this assumption will be violated if the model is not specified correctly. The classical example is the income versus expenditure on meals for different families. If the income is low, then there is not many options what to buy and the variability of expenditures would be low. However, with the increase of the income, the mean expenditures and their variability would increase as well, because there are more options of what to buy, including both cheap and expensive products. If we constructed a basic linear model on such data, then it would violate the assumption of homoscedasticity and as a result will have issues discussed in section 3.6.2. But arguably this would typically appear because of the misspecification of the model. For example, taking logarithms might resolve the issue in many cases, implying that the effect of one variable on the other should be multiplicative rather than the additive. Alternatively, dividing variables by some other variable might (e.g. working with expenses per family member, not per family) resolve the problem as well. Unfortunately, the transformations are not the panacea, so in some cases analyst would need to construct a model, taking the changing variance into account (e.g. GARCH or GAMLSS models). While in forecasting we are more interested in the holdout performance of models, in econometrics, the parameters of models are typically of the main interest. And, as we discussed earlier, in case of correctly specified model with heteroscedastic residuals, the estimates of parameters will be unbiased, but inefficient. So, econometricians would use different approaches to diminish the heteroscedasticity effect on parameters: either a different estimator for a model (such as Weighted Least Squares), or a different method for calculation of standard errors of parameters (e.g. Heteroskedasticity-Consistent Standard Errors). This does not resolve the problem, but rather corrects the parameters of the model (i.e. does not heal the illness, but treats the symptoms). Although these approaches typically suffice for the analytical purposes, they do not fix the issues in forecasting. In this section we will see how the issue can be resolved in some cases. 17.6.1 Detecting heteroscedasticity Building upon our previous example. We will use the ETSX(A,N,A) model, which as we remember has some issues. In order to see if the residuals of the model are homoscedastic, we can plot their values against the fitted: par(mfcol=c(1,2)) plot(adamModelSeat03,4:5) These two plots allow detecting specific type of heteroscedasticity, when with the increase of fitted values the residuals variability changes. The plot of absolute residuals vs fitted is more appropriate for models, where the scale parameters is calculated based on absolute values of residuals (e.g. the model with Laplace distribution), while the squared residuals vs fitted shows whether the variance of residuals is stable or not (thus making it more suitable for models with Normal and related distributions). Furthermore, the squared residuals plot might be difficult to read due to outliers, so the first one might help in detecting the heteroscedasticity even, when the scale is supposed to rely on squared errors. What we want to see on these plots, is for all the points to lie in the same corridor for lower and for the higher fitted values and for the red line to be constant. In our case There is a slight increase of the line and the variability of residuals around 1000 is lower than the one around 2000, which might indicate that we have heteroscedasticity in residuals. In our case this is cause by the wrong transformations in the model (see Section 17.3), so the fix of the issue is to use the multiplicative model. Another diagnostics tool that might become useful in some situations are the plot of absolute and squared standardised residuals versus fitted values. They have a similar idea to the previous plots, but they might change a little because of the standardisation (mean is equal to 0 and scale is equal to 1). These plots become especially useful if the changing variance is modelled explicitly (e.g. via a regression model or a GARCH-type of model. This feature is not yet supported in ADAM): par(mfcol=c(1,2)) plot(adamModelSeat03,13:14) In our case, these plots do not give additional message, we already know that there is a slight heteroscedasticity and that we need to transform the response variable some how (build multiplicative model). If we suspect that there are some specific variables that might cause heteroscedasticity, we can plot absolute or squared residuals vs these variables to see if they are indeed to blame for that. For example, here how we can produce a basic plot of residuals vs all explanatory variables included in the model: spread(cbind(as.data.frame(abs(resid(adamModelSeat03))), adamModelSeat03$data[,all.vars(formula(adamModelSeat03))[-1]]), lowess=TRUE) The plot above can be read similarly to the plots discussed above: if we notice a change in variability of residuals or a change (increase of decrease) in the lowess lines with the change of a variable, then this might indicate that the respective variable causes multicollinearity in the model. In our example, it looks like the variable law causes the largest issue - all the other variables do not cause as a strong change in variance. We already know that we need to use multiplicative model instead of the additive one in our example, so we will see how the residuals look for the correctly specified model: par(mfcol=c(1,2)) plot(adamModelSeat05,4:5) The plots above do not demonstrate any substantial issues: the residuals look more homoscedastic and given the scale of residuals the change of lowess line does not reflect significant changes in residuals. Additional plot of absolute residuals vs explanatory variables does not show anys serious issues either: spread(cbind(as.data.frame(abs(log(resid(adamModelSeat05)))), adamModelSeat05$data[,all.vars(formula(adamModelSeat05))[-1]]), lowess=TRUE) So, we can conclude that the multiplicative model resolves the issue with heteroscedasticity. If a variable would still cause an issue with it, it would make sense to construct a model for the variance (e.g. GARCH) in order to address the issue and improve the performance of the model in terms of prediction interval. There are formal statistical tests for heteroscedasticity, such as White (Wikipedia, 2021f), Breusch-Pagan (Wikipedia, 2021g) and Bartlett’s (Wikipedia, 2021h) tests. We do not discuss them here for the reason outlined in Section 17.5. References "],["residuals-are-i-i-d-zero-expectation.html", "17.7 Residuals are i.i.d.: zero expectation", " 17.7 Residuals are i.i.d.: zero expectation In ADAM framework, this assumption only works for the additive error models. In case of the multiplicative error models, it is changed to “expectation of the error term is equal to one.” It does not make sense to check this assumption unconditionally, because it does not mean anything in sample. Yes, it will hold automatically in sample in case of OLS estimation, and the observed mean of the residuals might not be equal to zero in other cases, but this does not give any useful information. In fact, when we work with exponential smoothing models, the in sample residuals being equal to zero might imply for some of them that the final values of components are identical to the initial ones. For example, in case of ETS(A,N,N), we can use the measurement equation from (7.6) to express the final value of level via the previous values up until \\(t=0\\): \\[\\begin{equation} \\begin{aligned} \\hat{l}_t &amp;= \\hat{l}_{t-1} + \\hat{\\alpha} e_t = \\hat{l}_{t-2} + \\hat{\\alpha} e_{t-1} + \\hat{\\alpha} e_t = \\\\ &amp; \\hat{l}_0 + \\hat{\\alpha} \\sum_{j=1}^t e_{t-j} . \\end{aligned} \\tag{17.3} \\end{equation}\\] If the mean of the residuals in sample is indeed equal to zero then the equation (17.3) reduces to \\(\\hat{l}_t=\\hat{l}_0\\). So, this assumption cannot be checked in sample, meaning that it is all about the true model and the asymptotic behaviour rather than the model applied to the data. The only part of this assumption that can be checked is whether the expectation of the residuals conditional on some variables is equal to zero (or one). In a way, this comes to making sure that there are no patterns in the residuals and thus no consecutive parts of the data, where residuals have systematically non-zero expectation. There are different ways to diagnose the issue. The first one is the already discussed plot of standardised (or studentised) residuals vs fitted values from Section 17.3. The other one is the plot of residuals over time, something that we have already discussed in Section 17.5. In addition, you can also plot residuals vs some of variables in order to see if they cause the change in mean. But in a way all these methods might also mean that the residuals are autocorrelated and / or some transformations of variables are needed. There is also an effect related to this, which is called “endogeneity.” According to econometrics literature it implies that the residuals are correlated with some variables. This becomes equivalent to the situation when the expectation of residuals changes with the change of a variable. The most prominent cause of this is the omitted variables (discussed in Section 17.1), which can be sometimes diagnosed by looking at correlations between the residuals and omitted variables. While econometricians propose using other estimation methods (such as Instrumental Variables) in order to diminish the effect of endogeneity, the forecasters cannot allow themselves doing that, because we need to fix the problem in order to get more adequate forecasts. Unfortunately, there is no universal recipe for the solution of this problem, but in some cases transforming variables, adding the omitted ones or substituting them by proxies (if we the variables are unavailable) might resolve the problem. "],["residuals-are-i-i-d-distributional-assumptions.html", "17.8 Residuals are i.i.d.: distributional assumptions", " 17.8 Residuals are i.i.d.: distributional assumptions Finally, we come to the distributional assumptions of ADAM. As discussed earlier (for example, in Section 14.1), ADAM framework supports several distributions, and the specific parts of assumptions will change depending on the type of the error term in the model. Given that, it is relatively straightforward to see if the residuals of the model follow the assumed distribution or not. There exist several tools for that. The simplest one is called Quantile-Quantile (QQ) plot. It produces a figure with theoretical vs actual quantiles and shows, whether they are close to each other or not. Here is, for example, how the QQ plot will look for one of the previous models, assuming Normal distribution: plot(adamModelSeat03,6) If the residuals do not contradict the assumed distribution, then all the points should lie either very close to or on the line. In our case, the majority of points are close to the line, but the tails are slightly off. In ADAM, this might mean that we should either use a different error type or a different distribution. Just for the sake of argument, we can try ETSX(M,N,M) model, with the same set of explanatory variables as in the model adamModelSeat03, and with the same Normal distribution: adamModelSeat16 &lt;- adam(Seatbelts,&quot;MNM&quot;,formula=drivers~PetrolPrice+kms+front+rear+law, distribution=&quot;dnorm&quot;) plot(adamModelSeat16,6) According to the new QQ plot, the residuals of the new model are much closer to the theoretical ones, there is now only the right tail that is wrong - the actual values are a bit further away than expected. This can be addressed by using a skewed distribution, for example, Inverse Gaussian: adamModelSeat17 &lt;- adam(Seatbelts,&quot;MNM&quot;,formula=drivers~PetrolPrice+kms+front+rear+law, distribution=&quot;dinvgauss&quot;) plot(adamModelSeat17,6) The new QQ plot demonstrates that the empirical residuals follow the assumed distribution much closer than in the previous cases: there are just few observations that lie slightly away from the line, but they could happen at random. So, based on this simple analysis we could conclude that Inverse Gaussian distribution is more suitable for this situation than the Normal one. Interestingly, this is supported by the AIC values, which very roughly reflect the same thing: setNames(c(AIC(adamModelSeat03),AIC(adamModelSeat16),AIC(adamModelSeat17)), c(&quot;Additive Normal&quot;,&quot;Multiplicative Normal&quot;,&quot;Multiplicative IG&quot;)) ## Additive Normal Multiplicative Normal Multiplicative IG ## 2229.054 2219.542 2218.411 Another way to analyse the distribution of residuals is to plot histogram together with the theoretical density function. Here is an example: hist(residuals(adamModelSeat03), probability=TRUE) lines(seq(-250,250,1), dnorm(seq(-250,250,1), 0, sd(residuals(adamModelSeat03))), col=&quot;red&quot;) However, this plot is much more difficult to analyse than QQ plot, because of the bars, which average out the quantiles. So, in general I would recommend using QQ plots instead. There are also formal tests for the distribution of residuals, such as Shapiro-Wilk (Wikipedia, 2021i) and Kolmogorov-Smirnov (Wikipedia, 2021j). The former tests the hypothesis that residuals follow Normal distribution, while the latter one is much more flexible and allows comparing the empirical distribution with any other (theoretical or empirical). However, I prefer to use visual inspection, when possible instead of doing these tests because, as we discussed earlier in Section 2.5, the null hypothesis is always wrong, and it will inevitably be rejected with the increase of the sample size. Besides, if you fail to reject H\\(_0\\), it does not mean that your variable follows the assumed distribution, it only means that you have not found enough evidence to reject it. References "],["multicollinearity.html", "17.9 Multicollinearity", " 17.9 Multicollinearity One of the classical issues in econometrics and in statistics in regression context is the issue of multicollinearity. In a way, this has nothing to do with classical assumptions of linear regression, because it is unreasonable to assume that the explanatory variables have some specific relation between them - they are what they are, and multicollinearity mainly causes issues with estimation of the parameters of model, not with its structure. But it is an issue nonetheless, so it is worth exploring. Multicollinearity appears, when either some of explanatory variables are correlated with each other (see Section 2.6.3), or their linear combination explains another explanatory variable included in the model. Depending on the strength of this relation and the estimation method used for model construction, the multicollinearity might cause issues of varying severity. For example, in the case, when two variables are perfectly correlated (correlation coefficient is equal to 1 or -1), the model will have perfect multicollinearity and it would not be possible to estimate its parameters. Another example is a case, when an explanatory variable can be perfectly explained by a set of other explanatory variables (resulting in \\(R^2\\) being close to one), which will cause exactly the same issue. The classical example of this situation is the dummy variables trap, when all values of categorical variable are used in regression together with the constant resulting in the linear relation \\(\\sum_{j=1}^k d_j = 1\\). Given that the square root of \\(R^2\\) of linear regression is equal to multiple correlation coefficient, these two situations are equivalent and just come to “absolute value of correlation coefficient is equal to 1.” Finally, as discussed briefly in Section 3.6.3, if correlation coefficient is high, but not equal to one, the effect of multicollinearity will lead to less efficient estimates of parameters. The loss of efficiency is in this case proportional to the absolute value of correlation coefficient. All of this tells us how this problem can be diagnosed and that this diagnoses should be carried out before constructing regression model. First, we can calculate correlation matrix for the available variables. If they are all numeric, then cor() function from stats should do the trick (we remove the response variable from consideration): cor(Seatbelts[,-2]) ## DriversKilled front rear kms PetrolPrice ## DriversKilled 1.0000000 0.7067596 0.35335102 -0.3211016 -0.3866061 ## front 0.7067596 1.0000000 0.62022476 -0.3573823 -0.5392394 ## rear 0.3533510 0.6202248 1.00000000 0.3330069 -0.1326272 ## kms -0.3211016 -0.3573823 0.33300689 1.0000000 0.3839004 ## PetrolPrice -0.3866061 -0.5392394 -0.13262721 0.3839004 1.0000000 ## VanKilled 0.4070412 0.4724207 0.12175808 -0.4980356 -0.2885584 ## law -0.3285051 -0.5624455 0.02906753 0.4905494 0.3906932 ## VanKilled law ## DriversKilled 0.4070412 -0.32850510 ## front 0.4724207 -0.56244554 ## rear 0.1217581 0.02906753 ## kms -0.4980356 0.49054938 ## PetrolPrice -0.2885584 0.39069323 ## VanKilled 1.0000000 -0.39494121 ## law -0.3949412 1.00000000 This matrix tells us that there are some variables that are highly correlated and might reduce efficiency of estimates of parameters of regression model if included in the model together. This applies to DriversKilled and front and also to front and rear. However, the values are not very high, so this might mean that there won’t be a serious issue in the model. If we have a mix of numerical and categorical variables, then assoc() (aka association()) function from greybox will help. It will detect the categorical variables automatically and will select between correlation and other appropriate measures of association (such as multiple correlation and Cramer’s V). In our case, all the variables are numeric, so it would produce a similar matrix. If you want to see this function in action, see what it produces for the mtcars data from datasets: assoc(mtcars) In order to cover the second situation with linear combination of variables, we can use the determ() (aka determination()) function from greybox: determ(Seatbelts[,-2]) ## DriversKilled front rear kms PetrolPrice ## 0.5533460 0.9035455 0.8236052 0.6796848 0.3627678 ## VanKilled law ## 0.3683910 0.5942838 This function will construct linear regression models for each variable from all the other variables and report the \\(R^2\\) from these models. If there are coefficients of determination close to one, then this might indicate that the variables would cause multicollinearity in the model. In our case, we see that front is linearly related to other variables, and we can expect it to cause the reduction of efficiency of estimate of parameters. If we remove the DriversKilled from consideration (we do not want to include it in our model anyway), then the picture will change: determ(Seatbelts[,-c(1,2)]) ## front rear kms PetrolPrice VanKilled law ## 0.8664929 0.8116719 0.6769670 0.3608227 0.3589600 0.5607605 In this case, the effect multicollinearity will be lower than in the previous situation, but it will still impact the estimates of parameters, making them less efficient than in the ideal situation. Instead of calculating the coefficients of determination, econometricians prefer to calculate Variance Inflation Factor (VIF), which shows by how many times the estimates of parameters will loose efficiency. Its formula is based on the \\(R^2\\) calculated above: \\[\\begin{equation*} \\mathrm{VIF}_j = \\frac{1}{1-R_j^2} \\end{equation*}\\] for each model \\(j\\). Which in our case can be calculated as: 1/(1-determ(Seatbelts[,-c(1,2)])) ## front rear kms PetrolPrice VanKilled law ## 7.490237 5.309882 3.095659 1.564511 1.559965 2.276662 This is useful when you want to see the specific impact on the variance of parameters, but is difficult to work with, when it comes to model diagnostics, because the value of VIF lies between zero and infinity. So, I prefer using the determination coefficients instead, which is always bounded by \\((0, 1)\\) region and thus easier to interpret. Furthermore, as you might have noticed, the discussion here was mainly focused on regression models. When it comes to dynamic models, then the situation might differ. In case of conventional ARIMA model, multicollinearity is inevitable by construct, because of the autocorrelations between actual values. This is why sometimes Heteroskedasticity- and autocorrelation-consistent (HAC) estimators of the covariance matrix (see Section 15.4 of Hanck et al., 2020) of parameters are used instead of the standard ones. They are designed to fix the issue and produce standard errors of parameters that are close to those without the issue. Finally, in case of state space models, and specifically in case of ETS, the multicollinearity might not cause as serious issues as in the case of regression. For example, it is possible to use all the values of categorical variable, avoiding the trap of dummy variables. The values of categorical variable in this case are considered as changes relative to the baseline. The classical example of this is the seasonal model, for example, ETS(A,A,A), where the seasonal components can be considered as a set of parameters for dummy variables, expanded from the seasonal categorical variable (e.g. months of year variable). If we set \\(\\gamma=0\\), thus making the seasonality deterministic, the ETS can still be estimated even though all values of the variable are used. This becomes apparent with the conventional ETS model, for example, from forecast package for R: etsModel &lt;- forecast::ets(AirPassengers, &quot;AAA&quot;) # Calculate determination coefficients for seasonal states determ(etsModel$states[,-c(1:2)]) ## s1 s2 s3 s4 s5 s6 s7 s8 ## 0.9999992 0.9999992 0.9999991 0.9999991 0.9999992 0.9999992 0.9999992 0.9999991 ## s9 s10 s11 s12 ## 0.9999991 0.9999991 0.9999992 0.9999992 As we see, the states of the model are almost perfectly correlated, but still the model works and does not have issue that the classical linear regression would have. References "],["ADAMSelection.html", "Chapter 18 Model selection and combinations in ADAM", " Chapter 18 Model selection and combinations in ADAM When it comes to time series analysis and to forecasting a specific time series, there are several ways to decide, which model to use, and there are several dimensions, in which a decision needs to be made: Which of the models to use: ETS / ARIMA / ETS+ARIMA / Regression / ETSX / ARIMAX / ETSX+ARIMA? What components of the ETS model to select? What order of ARIMA model to select? Which of the explanatory variables to use? What distribution to use? Should we select model or combine forecasts from different ones? Do we need all models in the pool? How should we do all the above? What about the demand occurrence part of the model? (this question has already been answered in Section 16.1.6). In this chapter, we discuss all aspects, related to model selection and combinations in ADAM. We will start the discussion with principles based on information criteria, we will then move to more complicated topics, related to pooling and then we will finish with selection and combinations based on rolling origin. Before we do that, we need to recall the distributional assumptions in ADAM, which play an important role if the model is estimated via the maximisation of likelihood function. In this case an information criterion (IC) can be calculated and used for the selection of the most appropriate model. Based on this, we can fit several ADAM models with different distributions and then select the one that leads to the lowest IC. Here is the list of the supported distributions in ADAM: Normal; Laplace; S; Generalised Normal; Log Normal; Inverse Gaussian; Gamma. The function auto.adam() implements this automatic selection of distribution based on IC for the provided vector of distribution by user. This selection procedure can be combined together with other selection techniques for different elements of ADAM model discussed in the following sections of the textbook. Here is an example of selection of distribution for a specific model, ETS(M,M,N) on Box-Jenkins data using auto.adam(): adamModel &lt;- auto.adam(BJsales, model=&quot;MMN&quot;, silent=FALSE, h=10, holdout=TRUE) ## Evaluating models with different distributions... dnorm \b, dlaplace \b, ds \b, dgnorm \b, dlnorm \b, dinvgauss \b, dgamma \b, Done! adamModel ## Time elapsed: 0.26 seconds ## Model estimated using auto.adam() function: ETS(MMN) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 245.3795 ## Persistence vector g: ## alpha beta ## 0.9986 0.2431 ## ## Sample size: 140 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 135 ## Information criteria: ## AIC AICc BIC BICc ## 500.7590 501.2068 515.4672 516.5736 ## ## Forecast errors: ## ME: 3.211; MAE: 3.325; RMSE: 3.778 ## sCE: 14.1%; Asymmetry: 91.6%; sMAE: 1.46%; sMSE: 0.028% ## MASE: 2.813; RMSSE: 2.478; rMAE: 0.924; rRMSE: 0.92 In this case the function has applied one and the same model but with different distributions, estimated it using likelihood and selected the one that has the lowest AICc value. "],["ETSSelection.html", "18.1 ADAM ETS components selection", " 18.1 ADAM ETS components selection Having 30 ETS models to choose from, the task of selecting the most appropriate one becomes challenging. Petropoulos et al. (2018) show that human experts can do this task successfully if they need to choose, which components to include in time series. However, when you face the problem of fitting ETS to thousands of time series, the judgmental selection becomes infeasible. Using some sort of automatic selection becomes critically important. The basic idea underlying the components selection in ETS is based on information criteria: we define a pool of models, we fit those models and we select the one that has the lowest information criterion. Using this approach in ETS context was first proposed by Hyndman et al. (2002). Based on this, we can construct some pool of models (e.g. based on our understanding of the problem) and then select the one that is the most appropriate to our data. adam() function in smooth package supports the following options for the pools: Pool of all 30 models, model=\"FFF\"; Pool of pure additive models, model=\"XXX\". As an option, “X” can also be used to tell function to only try additive component on the selected place. e.g. model=\"MXM\" will tell function to only test ETS(M,N,M), ETS(M,A,M) and ETS(M,Ad,M) models; Pool of pure multiplicative models, model=\"YYY\". Similarly to (2), we can tell adam() to only consider multiplicative component in a specific place. e.g. model=\"YNY\" will consider only ETS(M,N,N) and ETS(M,N,M); Pool of pure models only, model=\"PPP\" - this is a shortcut for doing (2) and (3) and then selecting the best between the two pools; Manual pool of models, which can be provided as a vector of models, for example: model=c(\"ANN\",\"MNN\",\"ANA\",\"AAN\"); model=\"ZZZ\", which triggers the selection among all possible models based on branch-and-bound algorithm (see below). In the cases explained above, adam() will try different models and select the most appropriate one from the predefined pool. There is a trade-off, when deciding which pool to use: if you provide the bigger one, it will take more time to find the appropriate one and there is a risk of overfitting the data; if you provide the smaller pool, then the optimal model might be outside of the pool, giving you the sub-optimal one. Furthermore, in some situations you might not need to go through all 30 models, because, for example, the seasonal component is not needed in the data. Trying out all the models would be just a waste of time. So, in order to address this issue, I have developed a branch-and-bound algorithm for the selection of the most appropriate ETS model, which is triggered via model=\"ZZZ\" (the same mechanism is used in es() function). The idea of the algorithm is to drop the components that do not improve the model. Here how it works: Apply ETS(A,N,N) to the data, calculate an information criterion (IC); Apply ETS(A,N,A) to the data, calculate IC. If it is lower than (1), then this means that there is some sort of seasonal component in the data, move to step (3). Otherwise go to (4); If (2) is lower than (1), then apply ETS(M,N,M) model and calculate IC. If it is lower than it means that the data exhibits multiplicative seasonality. Go to (4); Fit the model with the additive trend component and the seasonal component selected from previous steps, which can be either “N,” “A” or “M,” depending on the IC value. Calculate IC for the new model and compare it with the best IC so far. If it is lower, then there is some trend component in the data. If it is not, then the trend component is not needed. Based on these 4 steps, we can kick off the unneeded components and reduce the pool of models to test. For example, if the algorithm shows that seasonality is not needed, but there is a trend, then we only have 10 models to test overall instead of 30: ETS(A,N,N), ETS(A,A,N), ETS(A,Ad,N), ETS(M,N,N), ETS(M,M,N), ETS(M,Md,N), ETS(A,M,N), ETS(A,Md,N), ETS(M,A,N), ETS(M,Ad,N). Also, in steps (2) and (3), if there is a trend in the data, then the model will have higher than needed smoothing parameter \\(\\alpha\\), but the seasonality will play an important role in reducing the value of IC. This is why the algorithm is in general efficient. It might not guarantee that the optimal model will be selected all the time, but it reduces the computational time. The branch-and-bound algorithm can be combined with different types of models and is in fact also supported in model=\"XXX\" and model=\"YYY, where the pool of models for steps (1) - (4) is restricted by the pure models only. Finally, while the branch-and-bound algorithm is quite efficient, it might end up providing a mixed model, which might not be very suitable for the data. So, it is recommended to think of the possible pool of models prior to applying it to the data. For example, in some cases you might realise that additive seasonality is not needed, and that the data can be either non-seasonal or with multiplicative seasonality. In this case, you can explore the model=\"YZY\" option, aligning the error term with the seasonal component. Here is an example with automatically selected ETS model using the branch-and-bound algorithm described above: adamETSModel &lt;- adam(AirPassengers, model=&quot;ZZZ&quot;, silent=FALSE, h=12, holdout=TRUE) ## Forming the pool of models based on... ANN \b, ANA \b, MNM \b, MAM \b, Estimation progress: \b\b\b45 \b%\b\b\b55 \b%\b\b\b64 \b%\b\b\b73 \b%\b\b\b82 \b%\b\b\b91 \b%\b\b\b100 \b%... Done! adamETSModel ## Time elapsed: 1.44 seconds ## Model estimated using adam() function: ETS(MAM) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 466.9124 ## Persistence vector g: ## alpha beta gamma ## 0.7694 0.0058 0.0001 ## ## Sample size: 132 ## Number of estimated parameters: 17 ## Number of degrees of freedom: 115 ## Information criteria: ## AIC AICc BIC BICc ## 967.8249 973.1933 1016.8325 1029.9390 ## ## Forecast errors: ## ME: 7.913; MAE: 19.331; RMSE: 25.012 ## sCE: 36.175%; Asymmetry: 61.7%; sMAE: 7.364%; sMSE: 0.908% ## MASE: 0.803; RMSSE: 0.798; rMAE: 0.254; rRMSE: 0.243 In this specific example, the optimal model will coincide with the one selected via model=\"FFF\" and model=\"ZXZ\", although this does not necessarily is the case universally. References "],["ARIMASelection.html", "18.2 ADAM ARIMA order selection", " 18.2 ADAM ARIMA order selection While ETS has 30 models to choose from, ARIMA has many more options. For example, selecting the non-seasonal ARIMA with / without constant restricting the orders with \\(p \\leq 3\\), \\(d \\leq 2\\) and \\(q \\leq 3\\) leads to the combination of \\(3 \\times 2 \\times 3 \\times 2 = 36\\) possible ARIMA models. If we increase the possible orders to 5 or even more, we will need to go through hundreds of models. Adding the seasonal part increases this number by an order of magnitude. This means that we cannot just test all possible ARIMA models and select the most appropriate one, we need to be smart in the selection proces. Hyndman and Khandakar (2008) developed an efficient mechanism of ARIMA order selection based on statistical tests (for stationarity and seasonality), reducing the number of models to test to reasonable ammount. Svetunkov and Boylan (2020b) developed an alternative mechanism, relying purely on information criteria, which works especially well on seasonal data, but potentially may lead to models that overfit the data (this is implemented in auto.ssarima() and auto.msarima() functions in smooth package). We also have the Box-Jenkins approach for ARIMA orders selection, which relies on the analysis of ACF and PACF, but we should not forget the limitations of that approach. Finally, Sagaert and Svetunkov (2021) proposed the stepwise trace forward approach, which relies on partial correlations and uses the information criteria to test model on each iteration. Building upon all of that, I have developed the following algorithm for order selection of ADAM ARIMA: Determine the order of differences by fitting all possible combinations of ARIMA models with \\(P_j=0\\) and \\(Q_j=0\\) for all lags \\(j\\). This includes trying the models with and without the constant term. The order \\(D_j\\) is then determined via the model with the lowest IC; Then iteratively, starting from the highest seasonal lag and moving to the lag of 1 do for every lag \\(m_j\\): Calculate ACF of residuals of the model; Find the highest value of autocorrelation coefficient that corresponds to the multiple of the respective seasonal lag \\(m_j\\); Define, what should be the order of MA based on the lag of the autocorrelation coefficient on the previous step and include it in the ARIMA model; Calculate IC, and if it is lower than for the previous best model, leave the new MA order; Repeat (a) - (d) while there is an improvement in IC; Do steps (a) - (e) for AR order, substituting ACF with PACF of the residuals of the best model; Move to the next seasonal lag; Try out several restricted ARIMA models of the order \\(q=d\\) (this is based on (1) and the restrictions provide by the user). The motivation for this comes from the idea of relation between ARIMA and ETS. As you can see, this algorithm relies on the idea of Box-Jenkins methodology, but takes it with a pinch of salt, checking every time if the proposed order is improving the model or not. The motivation for doing MA orders before AR is based on the understanding of what AR model implies for forecasting. In a way, it is safer to have ARIMA(0,d,q) model than ARIMA(p,d,0), because the former is less prone to overfitting than the latter. Finally, the proposed algorithm is faster than the algorithm of Svetunkov and Boylan (2020b) and is more modest in the number of selected orders of the model. In order to start the algorithm, you would need to provide a parameter select=TRUE in the orders. Here is an example with Box-Jenkins data: adamARIMAModel &lt;- adam(BJsales, model=&quot;NNN&quot;, orders=list(ar=3,i=2,ma=3,select=TRUE), silent=FALSE, h=10, holdout=TRUE) ## Evaluating models with different distributions... default \b: Selecting ARIMA orders... ## Selecting differences... ## Selecting ARMA... |\b-\b\\\b|\b/\b- ## The best ARIMA is selected. Done! adamARIMAModel ## Time elapsed: 0.72 seconds ## Model estimated using auto.adam() function: ARIMA(0,2,2) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 243.2818 ## ARMA parameters of the model: ## MA: ## theta1[1] theta2[1] ## -0.7514 -0.0109 ## ## Sample size: 140 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 135 ## Information criteria: ## AIC AICc BIC BICc ## 496.5637 497.0114 511.2719 512.3782 ## ## Forecast errors: ## ME: 3.224; MAE: 3.339; RMSE: 3.794 ## sCE: 14.156%; Asymmetry: 91.6%; sMAE: 1.466%; sMSE: 0.028% ## MASE: 2.824; RMSSE: 2.488; rMAE: 0.927; rRMSE: 0.923 In this example, orders=list(ar=3,i=2,ma=3,select=TRUE) tells function that the maximum orders to check are \\(p\\leq 3\\), \\(d\\leq 2\\) \\(q\\leq 3\\). References "],["ETSXSelection.html", "18.3 Explanatory variables selection", " 18.3 Explanatory variables selection There are different approaches for automatic variables selection, but not all of them are efficient in the context of dynamic models. For example, backward stepwise might be either not feasible in case of small samples or may take too much time to converge to an optimal solution (it has polynomial computational time). This is because the ADAMX model needs to be refitted and reestimated over and over again using recursive relations based, for example, on the state space model (13.3). The classical stepwise forward might also be too slow, because it has polynomial computational time. So, there need to be some simplifications, which will make variables selection in ADAMX doable in a reasonable time. In order to make the mechanism doable in a limitted time, we rely on Sagaert and Svetunkov (2021) approach of stepwise trace forward selection of variables. It is the approach that uses the partial correlations between variables in order to identify, which of the variables to include on each iteration, and has because of that linear computational time. Still, doing that in the proper ADAMX would take more time than needed, so one of the possibles solutions is to do variables selection in ADAMX in the following steps: Estimate and fit the ETS model; Extract the residuals of the ETS model; Select the most suitable variables, explaining the residuals, based on an information criterion; Estimate the ADAMX model with the selected explanatory variables. The residuals in step (2) might vary from model to model, depending on the type of the error term and the selected distribution: Normal, Laplace, S, Generalised Normal or Asymmetric Laplace: \\(e_t\\); Additive error and Log Normal, Inverse Gaussian or Gamma: \\(\\left(1+\\frac{e_t}{\\hat{y}_t} \\right)\\); Multiplicative error and Log Normal, Inverse Gaussian or Gamma: \\(1+e_t\\). So, the extracted residuals should be formulated based on the distributional assumptions of each model. In R, step (3) is done using the stepwise() function from greybox package, which supports all the distributions discussed in the previous chapters. While the suggested approach has obvious limitations (e.g. smoothing parameters can be higher than needed, explaining the variability otherwise explained by variables), it is efficient in terms of computational time. In order to see how it works, we use SeatBelt data: SeatbeltsData &lt;- Seatbelts[,c(&quot;drivers&quot;,&quot;kms&quot;,&quot;PetrolPrice&quot;,&quot;law&quot;)] We have already had a look at this data earlier, so we can move directly to the selection part: adamModelETSXMNMSelect &lt;- adam(SeatbeltsData,&quot;MNM&quot;,h=12,holdout=TRUE,regressors=&quot;select&quot;) plot(forecast(adamModelETSXMNMSelect,h=12,interval=&quot;prediction&quot;)) summary(adamModelETSXMNMSelect) ## Warning: Observed Fisher Information is not positive semi-definite, which means ## that the likelihood was not maximised properly. Consider reestimating the model, ## tuning the optimiser or using bootstrap via bootstrap=TRUE. ## ## Model estimated using adam() function: ETSX(MNM) ## Response variable: drivers ## Distribution used in the estimation: Gamma ## Loss function type: likelihood; Loss function value: 1117.167 ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## alpha 0.2877 0.0852 0.1194 0.4559 * ## gamma 0.0000 0.0433 0.0000 0.0854 ## level 1655.3309 91.0434 1475.5626 1834.8813 * ## seasonal_1 1.0093 0.0150 0.9802 1.0454 * ## seasonal_2 0.9039 0.0154 0.8747 0.9400 * ## seasonal_3 0.9350 0.0155 0.9059 0.9711 * ## seasonal_4 0.8687 0.0148 0.8396 0.9048 * ## seasonal_5 0.9464 0.0161 0.9173 0.9825 * ## seasonal_6 0.9157 0.0155 0.8866 0.9518 * ## seasonal_7 0.9616 0.0161 0.9324 0.9977 * ## seasonal_8 0.9701 0.0159 0.9410 1.0062 * ## seasonal_9 1.0024 0.0169 0.9732 1.0385 * ## seasonal_10 1.0848 0.0182 1.0556 1.1209 * ## seasonal_11 1.2022 0.0183 1.1731 1.2383 * ## law 0.0200 0.1047 -0.1867 0.2264 ## ## Sample size: 180 ## Number of estimated parameters: 16 ## Number of degrees of freedom: 164 ## Information criteria: ## AIC AICc BIC BICc ## 2266.334 2269.672 2317.422 2326.087 Note that the function might complain about the observed Fisher Information. This only means that the estimated variances of parameters might be lower than they should be in reality. Based on the summary from the model, we can see that neither kms, nor PetrolPrice improve the model in terms of AICc. We could check them manually in order to see if the selection worked out well in our case (construct sink regression as a benchmark): adamModelETSXMNMSink &lt;- adam(SeatbeltsData,&quot;MNM&quot;,h=12,holdout=TRUE) summary(adamModelETSXMNMSink) ## Warning: Observed Fisher Information is not positive semi-definite, which means ## that the likelihood was not maximised properly. Consider reestimating the model, ## tuning the optimiser or using bootstrap via bootstrap=TRUE. ## ## Model estimated using adam() function: ETSX(MNM) ## Response variable: drivers ## Distribution used in the estimation: Gamma ## Loss function type: likelihood; Loss function value: 1139.57 ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## alpha 0.2818 0.0301 0.2223 0.3412 * ## gamma 0.0001 0.0049 0.0000 0.0097 ## level 693.6015 88.5288 518.7822 868.1819 * ## seasonal_1 1.2636 0.0584 1.2271 1.3889 * ## seasonal_2 1.1487 0.0472 1.1122 1.2740 * ## seasonal_3 0.9877 0.0421 0.9512 1.1130 * ## seasonal_4 0.8797 0.0368 0.8431 1.0050 * ## seasonal_5 0.8672 0.0322 0.8306 0.9925 * ## seasonal_6 0.8316 0.0321 0.7950 0.9569 * ## seasonal_7 0.7758 0.0239 0.7392 0.9011 * ## seasonal_8 0.7530 0.0185 0.7164 0.8783 * ## seasonal_9 0.9065 0.0362 0.8699 1.0318 * ## seasonal_10 1.0493 0.0450 1.0127 1.1746 * ## seasonal_11 1.3348 0.0635 1.2983 1.4601 * ## kms 0.0000 0.0000 0.0000 0.0000 ## PetrolPrice 0.0602 1.2620 -2.4318 2.5489 ## law 0.0204 0.1631 -0.3017 0.3420 ## ## Sample size: 180 ## Number of estimated parameters: 18 ## Number of degrees of freedom: 162 ## Information criteria: ## AIC AICc BIC BICc ## 2315.140 2319.389 2372.613 2383.644 We can see that the sink regression model has a higher AICc value than the model with the selected variables, which means that the latter is closer to the “true model.” While adamModelETSXMNMSelect might not be the best possible model in terms of information criteria, it is still a reasonable one and allows making different decisions. References "],["references.html", "References", " References "]]
