[["index.html", "Forecasting and Analytics with ADAM Preface", " Forecasting and Analytics with ADAM Ivan Svetunkov 2022-02-25 Preface What is ADAM? ADAM stands for “Augmented Dynamic Adaptive Model.” The term “adaptive model” means that the parameters of the model change over time according to some assumed process. The word “dynamic” reflects the idea that the model has time series related components (ETS, ARIMA). Finally, the word “augmented” is included because ADAM is the model that supports additional features not included in the conventional ETS / ARIMA. ADAM is a unified framework for constructing ETS / ARIMA / regression, based on more advanced statistical instruments. For example, classical ARIMA is built on the assumption of normality of the error term, but ADAM lifts this assumption and allows using other distributions as well (e.g. Generalised Normal, Inverse Gaussian etc). Another example, typically the conventional models are estimated either via the maximisation of the likelihood function or using basic losses like MSE or MAE, but ADAM has a broader spectrum of losses and allows using custom ones. There is much more, and we will discuss different aspects of ADAM in detail later in this textbook. The ADAM model includes: ETS; ARIMA; Regression; TVP regression; Combination of (1), (2) and either (3), or (4); Automatic selection / combination of states for ETS; Automatic orders selection for ARIMA; Variables selection for regression part; Normal and non-normal distributions; Automatic selection of most suitable distributions; Multiple seasonality; Occurrence part of the model to handle zeroes in data (intermittent demand); Handling uncertainty of estimates of parameters. All these extensions are needed to solve specific real life problems, so we will have examples and case studies later in the book to see how all of this can be used. The adam() function from smooth package implements ADAM and supports the following features: Model diagnostics using plot() and other methods; Confidence intervals for parameters of models; Automatic outliers detection; Handling missing data; Fine-tuning of persistence vector (smoothing parameters); Fine-tuning of initial values of the state vector (e.g. level / trend / seasonality); Two initialisation options (optimal / backcasting); Advanced and custom loss functions; Provided ETS, ARMA and regression parameters; Fine-tuning of the optimiser (selection of optimisation algorithm and convergence criteria); This textbook uses two packages from R, namely greybox, which focuses on forecasting using regression models, and smooth, which implements Single Source of Error (SSOE) state space models for time series analysis and forecasting. The textbook focuses on explaining how ADAM (“ADAM is Dynamic Adaptive Model” - recursive acronym), one of the smooth functions (introduced in v3.0.0) works, also showing how it can be used in practice with examples from R. If you want to run examples from the textbook, two packages are needed (Svetunkov, 2022a, 2022b): install.packages(&quot;greybox&quot;) install.packages(&quot;smooth&quot;) Some explanations of functions from the packages are given in my blog: Package greybox for R, Package smooth for R. An important thing to note is that this textbook does not use tidyverse packages. I like base R, and, to be honest, I am sure that tidyverse packages are great, but I have never needed them in my research. So, I will not use pipeline operators, tibble or tsibble objects and ggplot2. I assume throughout the textbook that you can do all those nice tricks on your own if you want to. You can use the following to cite the online version of this book: Svetunkov, I. (2022) Forecasting and Analytics with ADAM: Lancaster, UK. openforecast.org/adam. Accessed on [current date]. If you use LaTeX, the following can be used instead: @MISC{SvetunkovAdam, title = {Forecasting and Analytics with ADAM}, author = {Ivan Svetunkov}, howpublished = {OpenForecast}, note = {(version: [current date])}, url = {https://openforecast.org/adam/}, year = {2022} } License This textbook is licensed under Creative Common License by-nc-sa 4.0, which means that you can share, copy, redistribute and remix the content of the textbook for non-commercial purposes as long as you give appropriate credit to the author and provide the link to the original license. If you remix, transform, or build upon the material, you must distribute your contributions under the same CC-BY-NC-SA 4.0 license. See the explanation on the Creative Commons website. Acknowledgments I would like to thank Tobias Schmidt for his help in refining earlier parts of the textbook and correcting grammatical mistakes. References "],["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction I started writing this book in 2020 during the COVID-19 pandemic, having figured out that it had been more than ten years since publication of the fundamental textbook of Hyndman et al. (2008). Their original textbook discussed ETS (Error-Trend-Seasonality) model in the Single Source of Error (SSOE) form and that the topic has not been updated substantially since then. If you are interested in learning more about exponential smoothing, then this is a must-read material on the topic. However, there has been some progress in the area since 2008, and I have developed some models and functions based on SSOE, making the framework more flexible and general. Given that publication of all aspects of these models in peer-reviewed journals would be very time consuming, I have decided to summarise all progress in this book, showing what happens inside the models and how to use the functions in different cases, so that there is a source to refer to. Many parts of this textbook rely on such topics as a model, scales of information, model uncertainty, likelihood, information criteria and model building. All these topics are discussed in detail in the online textbook of Svetunkov (2021). You are recommended to familiarise yourself with them before moving to ADAM’s more advanced modelling topics. In this chapter, we explain what forecasting is, how it is different from planning and analytics and the main forecasting principles one should follow in order not to fail in trying to predict the future. References "],["forecastingPlanningAnalytics.html", "1.1 Forecasting, planning and analytics", " 1.1 Forecasting, planning and analytics While there are many definitions of what forecast is, I like the following one, proposed by Sergey Svetunkov (Svetunkov and Svetunkov, 2014): Forecast is a scientifically justified assertion about possible states of an object in future. This definition does not have the word “probability” in it, because in some cases, forecasts do not rely on rigorous statistical methods and theory of probabilities. For example, the Delphi method allows obtaining judgmental forecasts, typically focusing on what to expect, not on the probability side. An essential word in the definition is “scientific.” If a prediction is made based on coffee grounds, then it is not a forecast. Judgmental predictions, on the other hand, can be considered as forecasts if a person has a reason behind them. If they do not, this should be called “guesses,” not forecasts. Finally, the word “future” is important as well, as it shows the focus of the discipline: without the future, there is no forecasting, only overfitting. As for the definition of forecasting, it is a process of producing forecasts – as simple as that. Forecasting is a vital activity carried by many companies, some of which do it unconsciously or label it as “demand planning” or “predictive analytics.” However, there is a difference between the terms “forecasting” and “planning.” The latter relies on the former and implies the company’s actions to adjust its decisions. For example, if we forecast that the sales will go down, a company should make some marketing decisions to increase the demand on the product. The first part relates to forecasting, while the second relates to planning. If a company does not like a forecast, it should change something in its activities, not in the forecast itself. It is important not to confuse these terms in practice. Another crucial thing to keep in mind is that any forecasting activity should be done to inform decisions. Forecasting for the sake of forecasting is pointless. Yes, we can forecast the overall number of hospitalisations due to SARS-CoV-2 virus in the world for the next decade, but what decisions can be made based on that? If there are some decisions, then this exercise is worthwhile. If not, then this is just a waste of time. Example 1.1 Retailers typically need to order some amount of milk that they will sell over the next week. They do not know how much they will sell, so they usually order, hoping to satisfy, let us say, 95% of demand. This situation tells us that the forecasts need to be made a week ahead, they should be cumulative (considering the overall demand during a week before the following order) and that they should focus on an upper bound of a 95% prediction interval. Producing only point forecasts would not be helpful in this situation. Related to this is the question of forecasts accuracy. In reality, accurate forecasts do not always translate to good decisions. This is because many different aspects of reality need to be taken into account, and forecasting focuses only on one of them. Capturing the variability of demand correctly is sometimes more useful than producing very accurate point forecasts – this is because many decisions are based on distributions of values rather than on point forecasts. The classical example of this situation is inventory management, where the ordering decisions are made based on quantiles of distribution to form safety stock. Furthermore, the orders are typically done in pallets, so it is not important whether the expected demand is 99 or 95 units if a pallet includes 100 units of a product. This means that whenever we produce forecasts, we need to consider how they will be used and by whom. In some cases, accurate forecasts might be wasted if people make decisions differently and/or do not trust what they see. For example, a demand planner might decide that a straight line is not a good point forecast and would start changing the values, introducing noise. This might happen due to a lack of experience, expertise or trust in models, and this means that it is crucial to understand who will use the forecasts and how. Finally, in practice, not everything can be solved with forecasting. In some cases, companies can make decisions based on other reasons. For example, promotional decisions can be dictated by the existing stock of the product that needs to be moved out. In another case, if the holding costs for a product are low, then there is no need to spend time forecasting the demand on it – a company can implement a simple replenishment policy, ordering, when the stock reaches some threshold. And in times of crisis, some decisions are dictated by the company’s financial situation, not by forecasts: you do not need to predict demand on products that are sold out of prestige if they are not profitable, and a company needs to cut costs. Summarising all the above it makes sense to determine what decisions will be made based on forecast, by whom and how. There is no need to waste time and effort on improving the forecasting accuracy if the process in the company is flawed and forecasts are then ignored, not needed or amended inadequately. As for analytics, this is a relatively new term, which implies a set of activities based on analysis, forecasting and optimisation to support informed managerial decisions. The term is broad and relies on many research areas, including forecasting, simulations, optimisation etc. In this textbook, we will focus on the forecasting side, occasionally discussing how to analyse the existing processes (thus touching the analytics part) and how various models could help make good practical decisions. References "],["forecastingPrinciples.html", "1.2 Forecasting principles", " 1.2 Forecasting principles If you have decided that you need to forecast something, it makes sense to keep several important forecasting principles in mind. First, as discussed earlier, you need to understand why the forecast is required, how it will be used and by whom. Answers to these questions will guide you in deciding what technique to use, how specifically to forecast and what should be reported. For example, if a client does not know machine learning, it might be unwise to use Neural Networks for forecasting – the client will not trust the technique and thus will not trust the forecasts, switching to simpler methods. If the final decision is to order a number of units, it would be more reasonable to produce cumulative forecasts over the lead time (time between the order and product delivery) and form safety stock based on the model and assumed distribution. When you understand what to forecast and how the second principle comes into play: select the relevant error measure. You need to decide how to measure the accuracy of forecasting methods, keeping in mind that accuracy needs to be as close to the final decision as possible. For example, if you need to decide the number of nurses for a specific day in the A&amp;E department based on the patients’ attendance, then it would be more reasonable to compare models in terms of their quantile performance (see Section 2.2) rather than expectation or median. Thus, it would be more appropriate to calculate pinball loss instead of MAE or RMSE (see details in Section 2). Third, you should always test your models on a sample of data not seen by them. Train your model on one part of a sample (train set or in-sample) and test it on another one (test set or holdout sample). This way, you can have some guarantees that the model will not overfit the data and that it will be reasonable when you need to produce a final forecast. Yes, there are cases when you do not have enough data to do that. All you can do in these situations is use simpler, robust models (for example, damped trend exponential smoothing by Roberts, 1982; and Gardner and McKenzie, 1985; or Theta by Assimakopoulos and Nikolopoulos, 2000) and to use judgment in deciding whether the final forecasts are reasonable or not. But in all the other cases, you should test the model on the data it is unaware of. The recommended approach, in this case, is rolling origin, discussed in more detail in Section 2.4. Fourth, the forecast horizon should be aligned with specific decisions in practice. If you need predictions for a week ahead, there is no need to produce forecasts for the next 52 weeks. On the one hand, this is costly and excessive; on the other hand, the accuracy measurement will not align with the company’s needs. The related issue is the test set (or holdout) size selection. There is no unique guideline for this, but it should not be shorter than the forecasting horizon. Fifth, the time series aggregation level should be as close to the specific decisions as possible. There is no need to produce forecasts on an hourly level for the next week (168 hours ahead) if the decision is based on the order of a product for the whole week. We would not need such a granularity of data for the decision; aggregating the actual values to the weekly level will do the trick. Still, we would waste a lot of time making complicated models work on an hourly level. Sixth, you need to have benchmark models. Always compare forecasts from your favourite approach with those from Naïve, global average and/or regression – depending on what you deal with specifically. If your fancy Neural Network performs worse than Naïve, it does not bring value and should not be used in practice. Comparing one Neural Network with another is also not a good idea because Simple Exponential Smoothing (see Section 4.1), being a much simpler model, might beat both networks, and you would never find out about that. If possible, also compare forecasts from the proposed approach with forecasts of other well-established benchmarks, such as ETS (Hyndman et al., 2008), ARIMA (Box and Jenkins, 1976) and Theta (Assimakopoulos and Nikolopoulos, 2000). Finally, when comparing forecasts from different models, you might end up with several very similar performing approaches. If the difference between them is not significant, then the general recommendation is to select the faster and simpler one. This is because simpler models are more difficult to break, and those that work faster are more attractive in practice due to reduced energy consumption (save the planet and stop global warming! Dhar, 1999). These principles do not guarantee that you will end up with the most accurate forecasts, but at least you will not end up with unreasonable ones. References "],["typesOfForecasts.html", "1.3 Types of forecasts", " 1.3 Types of forecasts Depending on circumstances, we might require different types of forecasts with different characteristics. It is essential to understand what your model produces to measure its performance correctly (see Section 2.1) and make correct decisions in practice. Several things are typically produced for forecasting purposes. We start with the most popular one. 1.3.1 Point forecasts The classical and most often produced thing is the point forecast, which corresponds to some trajectory from a model. This, however, might align with different types of statistics depending on the model and its assumptions. In the case of a pure additive model (such as linear regression), the point forecasts correspond to the conditional expectation (mean) from the model. The conventional interpretation of this value is that it shows what to expect on average if the situation would repeat itself many times (e.g. if we have the day with similar conditions, then the average temperature will be 10 degrees Celsius). In the case of time series, this interpretation is difficult to digest, given that time does not repeat itself, but this is the best we can have. I will discuss the technicalities of producing conditional expectations from ADAM in Section 18.1. Another type of point forecast is the (conditional) geometric expectation (geometric mean). It typically arises, when the model is applied to the data in logarithms and the final forecast is only exponentiated. This becomes apparent from the following definition of geometric mean: \\[\\begin{equation} \\check{y} = \\sqrt[T]{\\prod_{t=1}^T y_t} = \\exp \\left(\\frac{1}{T} \\sum_{t=1}^T \\log(y_t) \\right) , \\tag{1.1} \\end{equation}\\] where \\(y_t\\) is the actual value, and \\(T\\) is the sample size. To use the geometric mean, we need to assume that the actual values can only be positive. Otherwise, the root in c might produce imaginary units (for example, taking a square root out of a negative number) or be equal to zero (if one of the values is zero). In general, the arithmetic and geometric means are related via the following inequality: \\[\\begin{equation} \\check{y} \\leq \\mu , \\tag{1.2} \\end{equation}\\] where \\(\\check{y}\\) is the geometric mean and \\(\\mu\\) is the arithmetic one. Although geometric mean makes sense in many contexts, it is more difficult to explain than the arithmetic one to decision makers. Finally, sometimes medians are used in place of point forecasts. In this case, the point forecast splits the sample into two halves and shows the level below which 50% of observations will lie in the future. Note, the specific type of point forecast will differ from the model used in construction. For example, in the case of the pure additive model, assuming some symmetric distribution (e.g. Normal one), the arithmetic mean, geometric mean and median will coincide. In this case, there is nothing to choose from. On the other hand, a model constructed in logarithms will assume an asymmetric distribution for the original data, leading to the following relation between the means and the median (in case of positively skewed distribution): \\[\\begin{equation} \\check{y} \\leq \\tilde{y}\\leq \\mu , \\tag{1.3} \\end{equation}\\] where \\(\\tilde{y}\\) is the median of distribution. 1.3.2 Quantiles and prediction intervals As some forecasters say, all point forecasts are wrong. They will never correspond to the actual values because they only capture the model’s mean (or median) performance, as discussed in the previous subsection. Everything that is not included in the point forecast can be considered as the uncertainty of demand. For example, we never will be able to say precisely how many cups of coffee we will sell following Monday, but we can at least capture the main tendencies and the uncertainty around our point forecast. Figure 1.1: An example of a well behaved data, point forecast and a 95% prediction interval. Figure 1.1 shows an example with a well-behaved demand, for which the best point forecast is the straight line. To capture the uncertainty of demand, we can construct the prediction interval, which will tell which bound the demand will lie in \\(1-\\alpha\\) per cent of cases. The interval in Figure 1.1 has the width of 95% (\\(\\alpha=0.05\\)) and shows that if the situation is repeated many times, the actual demand will be between 78.34 and 119.85. Capturing the uncertainty correctly is important because real-life decisions need to be made based on the full information, not only on the point forecasts. We will discuss how to produce prediction intervals in more detail in Section 18.2. For a more detailed discussion on the concepts of prediction and confidence intervals, see Chapter 5 of Svetunkov (2021). Another way to capture the uncertainty (related to the prediction interval) is via specific quantiles of distribution. The prediction interval typically has two sides, leaving \\(\\frac{\\alpha}{2}\\) values on the left and the same on the right, outside the bounds of the interval. Instead of producing the interval, in some cases, we might need just a specific quantile, essentially creating the one-sided prediction interval (see Section 18.3.2 for technicalities). The bound in this case will show the particular value below which the pre-selected percentage of cases would lie. This becomes especially useful in such contexts as safety stock calculation (because we are not interested in knowing the lower bound, we want products to satisfy some proportion of demand). 1.3.3 Forecast horizon Finally, an important aspect in forecasting is the horizon, for which we need to produce forecasts. Depending on the context, we might need: Only a specific value h steps ahead, e.g., the temperature following Monday. All values from 1 to h steps ahead, e.g. how many patients we will have each day next week. Cumulative values for the period from 1 to h steps ahead, e.g. what the cumulative demand over the lead time (the time between the order and product delivery) will be (see discussion in Section 18.3.3). It is essential to understand how decisions are made in practice and align them with the forecast horizon. In combination with the point forecasts and prediction intervals discussed above, this will give us an understanding of what to produce from the model and how. For example, in the case of safety stock calculation, it would be more reasonable to produce quantile of the cumulative over the lead time demand than to produce point forecasts from the model. References "],["modelsMethods.html", "1.4 Models, methods and typical assumptions", " 1.4 Models, methods and typical assumptions While we do not aim to fully cover the topic of models, methods, and typical assumptions of statistical models, we need to make several important definitions to clarify what we will discuss in this textbook. For a more detailed discussion, see Chapters 1 and 12 of Svetunkov (2021). Cambridge dictionary (Dictionary, 2021) defines method as a particular way of doing something. So, the method does not necessarily explain how the structure appears or how the error term interacts with it; it only describes how a value is produced. In our context, the forecasting method would be a formula that generates point forecasts based on some parameters and available data. It would not explain how what underlies the data. Statistical model on the other hand, is a ‘mathematical representation of a real phenomenon with a complete specification of distribution and parameters’ (Svetunkov and Boylan, 2019). It explains what happens inside the data, reveals the structure and shows how the error term interacts with the structure. While discussing statistical models, we should also define true model. It is “the idealistic statistical model that is correctly specified (has all the necessary components in the correct form), and applied to the data in population” (Svetunkov, 2021). Some statisticians also use the term Data Generating Process (DGP) when discussing the true model. Still, we need to distinguish between the two terms, as DGP implies that the data is somehow generated using a mathematical formula. In real life, the data is not generated from any function; it comes from a measurement of a complex process, influenced by many factors (e.g. behaviour of a group of customers based on their individual preferences and mental states). The DGP is useful when we want to conduct experiments on simulated data in a controlled environment, but it is not helpful when applying models to the data. Finally, the true model is an abstract notion because it is never known or reachable. But it is still a useful one, as it allows us to see what would happen if we knew the model and, more importantly, what would happen if the model we used was wrong (which is always the case in real life). The related to this definition is the estimated or applied model, which is the statistical model that is applied to the available sample of data. This model will almost always be wrong because even if we know the specification of the true model for some mysterious reason, we would still need to estimate it on our data. In this case, the estimates of parameters would differ from those in the population, and thus the model will still be wrong. Mathematically, in the simplest case the true model can be written as: \\[\\begin{equation} y_t = \\mu_{y,t} + \\epsilon_t, \\tag{1.4} \\end{equation}\\] where \\(y_t\\) is the actual value, \\(\\mu_{y,t}\\) is the structure and \\(\\epsilon_t\\) is the true noise. If we manage to capture the structure correctly, the model applied to the sample of data would be written as: \\[\\begin{equation} y_t = \\hat{\\mu}_{y,t} + e_t, \\tag{1.5} \\end{equation}\\] where \\(\\hat{\\mu}_{y,t}\\) is the estimate of the structure \\(\\mu_{y,t}\\) and \\(e_t\\) is the estimate of the noise \\(\\epsilon_t\\) (also known as “residuals”). If the structure is captured correctly, there would still be a difference between (1.4) and (1.5) because the latter is estimated on the data. However, if the sample size increases and we use an adequate estimation procedure, then due to Central Limit Theorem (see Chapter 4 of Svetunkov, 2021), the distance between the two models will decrease and asymptotically (with the increase of sample size) \\(e_t\\) would converge to \\(\\epsilon_t\\). This does not happen automatically, and some assumptions should hold for this to happen. 1.4.1 Assumptions of statistical models Very roughly, the typical assumptions of statistical models can be split into the following categories (Svetunkov, 2021): Model is correctly specified: We have not omitted important variables in the model (underfitting the data); We do not have redundant variables in the model (overfitting the data); The necessary transformations of the variables are applied; We do not have outliers in the model; Residuals are independent and identically distributed (i.i.d.): There is no autocorrelation in the residuals; The residuals are homoscedastic; The expectation of residuals is zero, no matter what; The variable follows the assumed distribution; More, generally speaking, the distribution of residuals does not change over time; The explanatory variables are not correlated with anything but the response variable: No multicollinearity; No endogeneity. Many of these assumptions come to the idea that we have correctly captured the structure, meaning that we have not omitted any essential variables, we have not included the redundant ones, and we transformed all the variables correctly (e.g. took logarithms, where needed). If all these assumptions hold, then we would expect the applied model to converge to the true one with the increase of the sample size. If some of them do not hold, then the point forecasts from our model might be biased, or we might end up producing wider (or narrower) prediction intervals than needed. These assumptions with their implications on an example of multiple regression are discussed in detail in Chapter 12 of Svetunkov (2021). The diagnostics of dynamic models based on these assumptions is discussed in Chapter 15. References "],["forecastsEvaluation.html", "Chapter 2 Forecasts evaluation", " Chapter 2 Forecasts evaluation As discussed in Section 1.1, forecasts should serve a specific purpose. They should not be made “just because” but help make decisions. The decision then dictates the kind of forecast that should be made – its form and its time horizon(s). It also dictates how the forecast should be evaluated – a forecast only being as good as the quality of the decisions it enables. When you understand how your system works and what sort of forecasts you should produce, you can start an evaluation process, measuring the performance of different forecasting models/methods and selecting the most appropriate for your data. There are various ways to measure and compare their performance. This chapter discusses the most common approaches, focusing on evaluating point forecasts, then moving towards prediction intervals and quantile forecasts. After that, we discuss how to choose the appropriate error measure and, finally, ensure that the model performs consistently on the available data via rolling origin evaluation and statistical tests. "],["errorMeasures.html", "2.1 Measuring accuracy of point forecasts", " 2.1 Measuring accuracy of point forecasts We start with a setting in which we are interested in point forecasts only. In this case, we typically begin by splitting the available data into train and test sets, applying the models under consideration to the former, and producing forecasts on the latter, not showing that part to the models. This is called the “fixed origin” approach: we fix the point in time from which to produce forecasts, produce them, calculate some error measure and compare the models. Different error measures can be used in this case. Which measure to use depends on the specific need. Here we briefly discuss the most important measures and refer to (Davydenko and Fildes, 2013; Svetunkov, 2019, 2017) for the gory details. The majority of point forecast measures relies on the following two popular metrics: Root Mean Squared Error (RMSE): \\[\\begin{equation} \\mathrm{RMSE} = \\sqrt{\\frac{1}{h} \\sum_{j=1}^h \\left( y_{t+j} -\\hat{y}_{t+j} \\right)^2 }, \\tag{2.1} \\end{equation}\\] and Mean Absolute Error (MAE): \\[\\begin{equation} \\mathrm{MAE} = \\frac{1}{h} \\sum_{j=1}^h \\left| y_{t+j} -\\hat{y}_{t+j} \\right| , \\tag{2.2} \\end{equation}\\] where \\(y_{t+j}\\) is the actual value \\(j\\) steps ahead from the holdout, \\(\\hat{y}_{t+j}\\) is the \\(j\\) steps ahead point forecast, and \\(h\\) is the forecast horizon. As you see, these error measures aggregate the performance of competing forecasting methods across the forecasting horizon, averaging out the specific performances on each \\(j\\). If this information needs to be retained, the summation can be dropped to obtain “SE” and “AE” values. It is well-known (see, for example, Kolassa, 2016) that the mean value of distribution minimises RMSE, and the median value minimises MAE. So, when selecting between the two, you should consider this property. It also implies, for example, that MAE-based error measures should not be used for the evaluation of models on intermittent demand because zero forecast will minimise MAE, when the sample contains more than 50% of zeroes (see, for example, Wallström and Segerstedt, 2010). Another error measure that has been used in some cases is Root Mean Squared Logarithmic Error (RMSLE, see discussion in Tofallis, 2015): \\[\\begin{equation} \\mathrm{RMSLE} = \\exp\\left(\\sqrt{\\frac{1}{h} \\sum_{j=1}^h \\left( \\log y_{t+j} -\\log \\hat{y}_{t+j} \\right)^2} \\right). \\tag{2.3} \\end{equation}\\] It assumes that the actual values and the forecasts are positive and is minimised by geometric mean. I have added the exponentiation in the formula (2.3), which is sometimes omitted, bringing the metric to the original scale to have the same units as the actual values \\(y_t\\). The main difference in the three measures arises when the data we deal with is not symmetric – in that case, the arithmetic, geometric means, and median will be different. Thus, the error measures might recommend different approaches depending on what specifically is produced as a point forecast from the model (see discussion in Section 1.3.1). 2.1.1 An example in R In order to see how the error measures work, we consider the following example based on a couple of forecasting functions from smooth package for R Svetunkov and Kourentzes (2015) and measures from greybox: y &lt;- rnorm(100,100,10) model1 &lt;- es(y,h=10,holdout=TRUE) model2 &lt;- ces(y,h=10,holdout=TRUE) # RMSE setNames(sqrt(c(MSE(model1$holdout, model1$forecast), MSE(model2$holdout, model2$forecast))), c(&quot;ETS&quot;,&quot;CES&quot;)) # MAE setNames(c(MAE(model1$holdout, model1$forecast), MAE(model2$holdout, model2$forecast)), c(&quot;ETS&quot;,&quot;CES&quot;)) # RMSLE setNames(exp(sqrt(c(MSE(log(model1$holdout), log(model1$forecast)), MSE(log(model2$holdout), log(model2$forecast))))), c(&quot;ETS&quot;,&quot;CES&quot;)) ## ETS CES ## 9.492744 9.494683 ## ETS CES ## 7.678865 7.678846 ## ETS CES ## 1.095623 1.095626 Given that the distribution of the original data is symmetric, all three error measures should generally recommend the same model. But also, given that the data we generated for the example is stationary, the two models will produce very similar forecasts. The values above demonstrate the latter point – the accuracy between the two models is roughly the same. Note that we have evaluated the same point forecasts from the models using different error measures, which would be wrong if the distribution of the data was skewed. In our case, the model relies on normal distribution so that the point forecast would coincide with arithmetic mean, geometric mean and median. 2.1.2 Aggregating error measures The main advantage of the error measures discussed in the previous subsection is that they are straightforward and have a clear interpretation: they reflect the “average” distances between the point forecasts and the observed values. They are perfect for the work with only one time series. However, they are not suitable when a set of time series is under consideration, and a forecasting method needs to be selected across them. This is because they are scale-dependent and contain specific units: if you measure sales of apples in units, then MAE, RMSE and RMSLE (defined in equation (2.3)) will show the error in units as well. And, as we know, you should not add up apples with oranges – the result might not make sense. To tackle this issue, different error scaling techniques have been proposed, resulting in a zoo of error measures: MAPE – Mean Absolute Percentage Error: \\[\\begin{equation} \\mathrm{MAPE} = \\frac{1}{h} \\sum_{j=1}^h \\frac{|y_{t+j} -\\hat{y}_{t+j}|}{y_{t+j}}, \\tag{2.4} \\end{equation}\\] MASE – Mean Absolute Scaled Error (Hyndman and Koehler, 2006): \\[\\begin{equation} \\mathrm{MASE} = \\frac{1}{h} \\sum_{j=1}^h \\frac{|y_{t+j} -\\hat{y}_{t+j}|}{\\bar{\\Delta}_y}, \\tag{2.5} \\end{equation}\\] where \\(\\bar{\\Delta}_y = \\frac{1}{t-1}\\sum_{j=2}^t |\\Delta y_{j}|\\) is the mean absolute value of the first differences \\(\\Delta y_{j}=y_j-y_{j-1}\\) of the in-sample data; rMAE – Relative Mean Absolute Error (Davydenko and Fildes, 2013): \\[\\begin{equation} \\mathrm{rMAE} = \\frac{\\mathrm{MAE}_a}{\\mathrm{MAE}_b}, \\tag{2.6} \\end{equation}\\] where \\(\\mathrm{MAE}_a\\) is the mean absolute error of the model under consideration and \\(\\mathrm{MAE}_b\\) is the MAE of the benchmark model; sMAE – scaled Mean Absolute Error (Petropoulos and Kourentzes, 2015): \\[\\begin{equation} \\mathrm{sMAE} = \\frac{\\mathrm{MAE}}{\\bar{y}}, \\tag{2.7} \\end{equation}\\] where \\(\\bar{y}\\) is the mean of the in-sample data. and others. There is no “best” error measure. All have advantages and disadvantages, but some are more suitable in some circumstances than others. For example: MAPE is scale sensitive (if the actual values are measured in thousands of units, the resulting error will be much lower than in the case of hundreds of units) and cannot be estimated on data with zeroes. Furthermore, this error measure is biased, preferring when models underforecast the data (see, for example, Makridakis, 1993) and is not minimised by either mean or median, but by an unknown quantity. Accidentally, in the case of Log-Normal distribution, it is minimised by the mode (see discussion in Kolassa, 2016). Despite all the limitations, MAPE has a simple interpretation as it shows the percentage error (as the name suggests); MASE avoids the disadvantages of MAPE but does so at the cost of a simple interpretation. This is because of the division by the first differences of the data (some interpret this as an in-sample one-step-ahead Naïve forecast, which does not simplify the interpretation); rMAE avoids the disadvantages of MAPE, has a simple interpretation (it shows by how much one model is better than the other), but fails, when either \\(\\mathrm{MAE}_a\\) or \\(\\mathrm{MAE}_b\\) for a specific time series is equal to zero. In practice, this happens more often than desired and can be considered a severe error measure limitation. Furthermore, the increase of rMAE (for example, with the increase of sample size) might mean that either the method A is performing better than before or that the method B is performing worse than before – it is not possible to tell the difference unless the denominator in the formula (2.6) is fixed; sMAE avoids the disadvantages of MAPE has an interpretation close to it but breaks down when the data has a trend. When comparing different forecasting methods, it might make sense to calculate several error measures for comparison. The choice of metric might depend on the specific needs of the forecaster. Here are a few rules of thumb, however: If you want a robust measure that works consistently, but you do not care about the interpretation, then go with MASE. If you want an interpretation, go with rMAE or sMAE. Just keep in mind that if you decide to use rMAE or any other relative measure, you might get attacked by its creator, Andrey Davydenko, who might blame you for stealing his creation, even if you put a reference to his work. If the data does not exhibit trends (stationary), you can use sMAE. You should typically avoid MAPE and other percentage error measures because the actual values highly influence them in the holdout. Furthermore, similarly to the measures above, there have been proposed RMSE-based scaled and relative error metrics, which would measure the performance of methods in terms of means rather than medians. Here is a brief list of some of them: RMSSE – Root Mean Squared Scaled Error (Makridakis et al., 2020): \\[\\begin{equation} \\mathrm{RMSSE} = \\sqrt{\\frac{1}{h} \\sum_{j=1}^h \\frac{(y_{t+j} -\\hat{y}_{t+j})^2}{\\bar{\\Delta}_y^2}} ; \\tag{2.8} \\end{equation}\\] rRMSE – Relative Root Mean Squared Error (Stock and Watson, 2004): \\[\\begin{equation} \\mathrm{rRMSE} = \\frac{\\mathrm{RMSE}_a}{\\mathrm{RMSE}_b} ; \\tag{2.9} \\end{equation}\\] sRMSE – scaled Root Mean Squared Error (Petropoulos and Kourentzes, 2015): \\[\\begin{equation} \\mathrm{sRMSE} = \\frac{\\mathrm{RMSE}}{\\bar{y}} . \\tag{2.10} \\end{equation}\\] Similarly, RMSSLE, rRMSLE and sRMSLE can be proposed, using the same principles as in (2.8), (2.9) and (2.10) to assess performance of models in terms of geometric means across time series. Finally, when aggregating the performance of forecasting methods across several time series, sometimes it makes sense to look at the distribution of errors – this way, you will know which of the methods fails seriously and which does a consistently good job. If an aggregate measure is needed, then use mean and median of the chosen metric. The mean might be non-finite for some error measures, especially when a method performs exceptionally poorly on a time series (an outlier). Still, it will give you information about the average performance of the method and might flag extreme cases. The median at the same time is robust to outliers and is always calculable, no matter what the distribution of the error term is. Furthermore, comparing mean and median might provide additional information about the tail of distribution without reverting to histograms or the calculation of quantiles. Davydenko and Fildes (2013) argue for the use of geometric mean for relative and scaled measures. Still, as discussed earlier, it might become equal to zero or infinity if the data contains outliers (e.g. two cases, when one of the methods produced a perfect forecast, or the benchmark in rMAE produced a perfect forecast). At the same time, if the distribution of errors in logarithms is symmetric (which is the main argument of Davydenko and Fildes, 2013), then geometric mean will coincide with median, so there is no point in calculating the geometric mean at all. 2.1.3 Demonstration in R In R, there is a variety of functions that calculate the error measures discussed above, including the accuracy() function from forecast package and measures() from greybox. Here is an example of how the measures can be calculated based on a couple of forecasting functions from smooth package for R and a set of generated time series: # Apply a model to a test data to get names of error measures y &lt;- rnorm(100,100,10) test &lt;- es(y,h=10,holdout=TRUE) # Define number of iterations nsim &lt;- 100 # Create an array for nsim time series, # 2 models and a set of error measures errorMeasures &lt;- array(NA, c(nsim,2,length(test$accuracy)), dimnames=list(NULL,c(&quot;ETS&quot;,&quot;CES&quot;), names(test$accuracy))) # Start a loop for nsim iterations for(i in 1:nsim){ # Generate a time series y &lt;- rnorm(100,100,10) # Apply ETS testModel1 &lt;- es(y,&quot;ANN&quot;,h=10,holdout=TRUE) errorMeasures[i,1,] &lt;- measures(testModel1$holdout, testModel1$forecast, actuals(testModel1)) # Apply CES testModel2 &lt;- ces(y,h=10,holdout=TRUE) errorMeasures[i,2,] &lt;- measures(testModel2$holdout, testModel2$forecast, actuals(testModel2)) } The default benchmark method for relative measures above is Naïve. To see how the distribution of error measures would look like, we can produce violinplots via the vioplot() function from the vioplot package. We will focus on rRMSE measure (see Figure 2.2). vioplot::vioplot(errorMeasures[,,&quot;rRMSE&quot;]) Figure 2.1: Distribution of rRMSE on the original scale. The distributions in Figure 2.2 look similar, and it is hard to tell which one performs better. Besides, they do not look symmetric, so we will take logarithms to see if this fixes the issue with the skewness (Figure 2.2). vioplot::vioplot(log(errorMeasures[,,&quot;rRMSE&quot;])) Figure 2.2: Distribution of rRMSE on the log scale. Figure 2.2 demonstrates that the distribution in logarithms is skewed, so the geometric mean in this case would not be suitable and might provide a misleading information. So, we calculate mean and median rRMSE to check the overall performance of the two models: # Calculate mean rRMSE apply(errorMeasures[,,&quot;rRMSE&quot;],2,mean) ## ETS CES ## 0.8163452 0.8135303 # Calculate median rRMSE apply(errorMeasures[,,&quot;rRMSE&quot;],2,median) ## ETS CES ## 0.8796325 0.8725286 Based on the values above, we cannot make any solid conclusion about the performance of the two models: in terms of both mean and median rRMSE, CES is doing slightly better, but the difference between the two models is not substantial, so we can probably choose the one that is easier to work with. References "],["uncertainty.html", "2.2 Measuring uncertainty", " 2.2 Measuring uncertainty As discussed in Section 1.3.2, point forecasts are not sufficient for adequate decision making – prediction intervals and quantiles are needed to capture the uncertainty of demand around the point forecast. As with point forecasts, multiple measures can be used to evaluate them. There are several useful measures for the evaluation of intervals. We start with the simplest of them, coverage. Coverage shows the percentage of observations lying inside the interval: \\[\\begin{equation} \\mathrm{coverage} = \\frac{1}{h} \\sum_{j=1}^h \\left( \\mathbb{1}(y_{t+j} &lt; l_{t+j}) \\times \\mathbb{1}(y_{t+j} &gt; u_{t+j}) \\right), \\tag{2.11} \\end{equation}\\] where \\(l_{t+j}\\) is the lower bound and \\(u_{t+j}\\) is the upper bound of the interval and \\(\\mathbb{1}(\\cdot)\\) is the indicator function, returning one, when the condition is true and zero otherwise. Ideally, the coverage should be equal to the confidence level of the interval, but in reality, this can only be observed asymptotically (with the increase of the sample size), as the sample size increases due to the inheritted randomness of any sample estimates of parameters; Range shows the width of the prediction interval: \\[\\begin{equation} \\mathrm{range} = \\frac{1}{h} \\sum_{j=1}^h (u_{t+j} -l_{t+j}); \\tag{2.12} \\end{equation}\\] If the range of interval from one model is lower than the range of the other one, then the uncertainty about the future values is lower for the first one. However, the narrower interval might not include as many actual values in the holdout sample, leading to lower coverage. So, there is a natural trade-off between the two measures. Mean Interval Score (Gneiting and Raftery, 2007) combines the properties of the previous two measures: \\[\\begin{equation} \\begin{aligned} \\mathrm{MIS} = &amp; \\frac{1}{h} \\sum_{j=1}^h \\left( (u_{t+j} -l_{t+j}) + \\frac{2}{\\alpha} (l_{t+j} -y_{t+j}) \\mathbb{1}(y_{t+j} &lt; l_{t+j}) +\\right. \\\\ &amp; \\left. \\frac{2}{\\alpha} (y_{t+j} -u_{t+j}) \\mathbb{1}(y_{t+j} &gt; u_{t+j}) \\right) , \\end{aligned} \\tag{2.13} \\end{equation}\\] where \\(\\alpha\\) is the significance level. If the actual values lie outside of the interval, they get penalised with a ratio of \\(\\frac{2}{\\alpha}\\), proportional to the distance from the interval bound. At the same time the width of the interval positively influences the value of the measure: the wider the interval, the higher the score. The ideal model with \\(\\mathrm{MIS}=0\\) should have all the actual values in the holdout lying on the bounds of the interval and \\(u_{t+j}=l_{t+j}\\), implying that the bounds coincide with each other and that there is no uncertainty about the future (which is not possible in real life). Pinball Score (Koenker and Bassett, 1978) measures the accuracy of models in terms of specific quantiles (this is usually applied to different quantiles produced from the model, not just to the lower and upper bounds of 95% interval): \\[\\begin{equation} \\mathrm{PS} = (1 -\\alpha) \\sum_{y_{t+j} &lt; q_{t+j}, j=1,\\dots,h } |y_{t+j} -q_{t+j}| + \\alpha \\sum_{y_{t+j} \\geq q_{t+j} , j=1,\\dots,h } |y_{t+j} -q_{t+j}|, \\tag{2.14} \\end{equation}\\] where \\(q_{t+j}\\) is the value of the specific quantile of the distribution. PS shows how well we capture the specific quantile in the data. The lower the value of pinball is, the closer the bound is to the specific quantile of the holdout distribution. If the PS is equal to zero, then we have done the perfect job in hitting that specific quantile. The main issue with PS is that it is very difficult to assess the quantiles correctly on small samples. For example, in order to get a better idea of how the 0.975 quantile performs, we would need to have at least 40 observations, so that 39 of them would be expected to lie below this bound \\(\\left(\\frac{39}{40} = 0.975\\right)\\). In fact, quantiles are not always uniquely defined (see, for example, Taylor, 2020), which makes the measurement difficult. Similar to the pinball function, it is possible to propose the expectile-based score, but while it has good statistical properties (Taylor, 2020), it is more difficult to interpret. Range, MIS and PS are unit-dependent. To aggregate them over several time series, they need to be scaled either via division by either the in-sample mean or in-sample mean absolute differences to obtain the scaled counterparts of the measures or via division by the values from the benchmark model to get the relative one. The idea here would be similar to what we discussed for MAE and RMSE in Section 2.1. If you are interested in the model’s overall performance, then MIS provides this information. However, it does not show what happens specifically inside and is difficult to interpret. Coverage and range are easier to interpret but only give information about the specific prediction interval. They typically must be traded off against each other (i.e. one can either cover more or have a narrower interval). Academics prefer pinball for uncertainty assessment, as it shows more detailed information about the predictive distribution from each model. However, while it is easier to interpret than MIS, it is still not as straightforward as coverage and range. So, the selection of the measure depends on your specific situation and the understanding of statistics by decision-makers. 2.2.1 Example in R Continuing the example from Section 2.1, we could produce prediction intervals from the two models and compare them using MIS and pinball: model1Forecast &lt;- forecast(model1,h=10,interval=&quot;p&quot;,level=0.95) model2Forecast &lt;- forecast(model2,h=10,interval=&quot;p&quot;,level=0.95) # Mean Interval Score setNames(c(MIS(model1$holdout, model1Forecast$lower, model1Forecast$upper, 0.95), MIS(model2$holdout, model2Forecast$lower, model2Forecast$upper, 0.95)), c(&quot;Model 1&quot;, &quot;Model 2&quot;)) ## Model 1 Model 2 ## 39.68038 46.91441 # Pinball for the upper bound setNames(c(pinball(model1$holdout, model1Forecast$upper, 0.975), pinball(model2$holdout, model2Forecast$upper, 0.975)), c(&quot;Model 1&quot;, &quot;Model 2&quot;)) ## Model 1 Model 2 ## 5.402511 6.703046 # Pinball for the lower bound setNames(c(pinball(model1$holdout, model1Forecast$lower, 0.025), pinball(model2$holdout, model2Forecast$lower, 0.025)), c(&quot;Model 1&quot;, &quot;Model 2&quot;)) ## Model 1 Model 2 ## 4.517584 5.025557 # Coverage setNames(c(mean(model1$holdout &gt; model1Forecast$lower &amp; model1$holdout &lt; model1Forecast$upper), mean(model2$holdout &gt; model2Forecast$lower &amp; model2$holdout &lt; model2Forecast$upper)), c(&quot;Model 1&quot;, &quot;Model 2&quot;)) ## Model 1 Model 2 ## 0.9 0.9 The values above imply that the first model (ETS) performed better than the second one in terms of MIS and pinball loss (the interval was narrower). However, these measures do not tell much in terms of the performance of models when only applied to one time series. To see more solid results, we need to apply models to a set of time series, produce prediction intervals, calculate measures and then look at their aggregate performance, e.g. via mean / median or quantiles. The loop and the analysis would be similar to the one discussed in Section @ref(#errorMeasuresExampleBig), so we do not repeat it here. References "],["errorMeasuresSelection.html", "2.3 How to choose appropriate error measure", " 2.3 How to choose appropriate error measure While, in general, the selection of error measures should be dictated by the specific problem at hand, some guidelines might be helpful in the process. I have summarised them in the flowchart in Figure 2.3. Figure 2.3: Error measures selection flowchart. The flowchart does not provide excessive options and simplifies the possible process. It does not discuss the quantile and interval measures in detail, as there are many options for them in this direction, and the idea of the flowchart is to list the most important ones. The aim of this is to provide a guideline for selection based on: Number of time series under consideration. If there are several of them and you need to aggregate the error measure, you need to use either scaled or relative ones. In the case of just one time series, you do not need to scale the error measure; What specifically you want to measure: point forecasts, quantiles, prediction interval or something else; Whether the interpretability of the error measure is essential or not. If not, then scaled measures similar to Hyndman and Koehler (2006) can be used. If yes, then the choice is between relative and scaled using mean measures; Whether the data is stationary or not. If it is, then it is safe to use scaled measures similar to Petropoulos and Kourentzes (2015) because the division by in-sample mean would be meaningful. Otherwise, you should either use Hyndman and Koehler (2006) scaling or relative measures; Whether the data is intermittent or not. If it is and you are interested in point forecasts, then you should use RMSE based measures – other measures might recommend zero forecast as the best one; Symmetry of distribution of demand. If it is symmetric (which does not happen very often), then the median will coincide with the mean and geometric mean, and it would not be important whether to use RMSE-, MAE- or RMSLE- based measure. In that case, just use a MAE-based one; What you need (denoted as “What do you like?” in the flowchart). If you are interested in mean performance, then use RMSE based measures. The median minimises MAE, and the geometric mean minimises RMSLE. This relates to the discussion in Section 1.3. The point forecast related error measures have been discussed in Section 2.1, while the interval and quantile ones – in Section 2.2. You can also download this flowchart in pdf format via this link. References "],["rollingOrigin.html", "2.4 Rolling origin", " 2.4 Rolling origin Remark. The text in this section is based on the vignette for the greybox package, written by the author of this textbook. When there is a need to select the most appropriate forecasting model or method for the data, the forecaster usually splits the sample into two parts: in-sample (aka “training set”) and holdout sample (aka out-sample or “test set”). The model is estimated on the in-sample, and its forecasting performance is evaluated using some error measure on the holdout sample. Using this procedure only once is known as “fixed origin” evaluation. However, this might give a misleading impression of the accuracy of forecasting methods. If, for example, the time series contains outliers or level shifts, a poor model might perform better in fixed origin evaluation than a more appropriate one. Besides, a good performance might happen by chance. So it makes sense to have a more robust evaluation technique. An alternative procedure known as “rolling origin” evaluation is one such technique. In rolling origin evaluation, the forecasting origin is repeatedly moved forward, and forecasts are produced from each origin (Tashman, 2000). This technique allows obtaining several forecast errors for time series, which gives a better understanding of how the models perform. This can be considered a time series analogue to cross-validation techniques (Wikipedia, 2020). Here is a simple graphical representation, courtesy of Nikos Kourentzes. Figure 2.4: Rolling origin illustrated, by Nikos Kourentzes There are different options of how this can be done. 2.4.1 Principles of Rolling origin Figure 2.5 (Svetunkov and Petropoulos, 2018) illustrates the basic idea behind rolling origin. White cells correspond to the in-sample data, while the light grey cells correspond to the three-steps-ahead forecasts. The time series in the figure has 25 observations, and forecasts are produced for eight origins starting from observation 15. The model is estimated on the first in-sample set, and forecasts are created for the holdout. Next, another observation is added to the end of the in-sample set, the test set is advanced, and the procedure is repeated. The process stops when there is no more data left. This is a rolling origin with a constant holdout sample size. As a result of this procedure, eight one to three steps ahead forecasts are produced. Based on them, we can calculate the preferred error measures and choose the best performing model (see Section 2.1.2). Figure 2.5: Rolling origin with constant holdout size Another option for producing forecasts via rolling origin would be to continue with rolling origin even when the test sample is smaller than the forecast horizon, as shown in Figure 2.6. In this case, the procedure continues until origin 22, when the last complete set of three-steps-ahead forecasts can be produced but continues with a decreasing forecasting horizon. So the two-steps-ahead forecast is produced from origin 23, and only a one-step-ahead forecast is produced from origin 24. As a result, we obtain ten one-step-ahead forecasts, nine two-steps-ahead forecasts and eight three-steps-ahead forecasts. This is a rolling origin with a non-constant holdout sample size, which can be helpful with small samples when we don’t have enough observations. Figure 2.6: Rolling origin with non-constant holdout size Finally, in both cases above, we had the increasing in-sample size. However, we might need a constant in-sample for some research purposes. Figure 2.7 demonstrates such a setup. In this case, in each iteration, we add an observation to the end of the in-sample series and remove one from the beginning (dark grey cells). Figure 2.7: Rolling origin with constant in-sample size 2.4.2 Rolling origin in R The function ro() from the greybox package (written by Yves Sagaert and Ivan Svetunkov in 2016 on the way to the International Symposium on Forecasting) implements the rolling origin evaluation for any function you like with a predefined call and returns the desired value. It heavily relies on the two variables: call and value – so it is pretty important to understand how to formulate them to get the desired results. ro() is a very flexible function, but as a result, it is not very simple. In this subsection, we will see how it works on a couple of examples. We start with a simple example, generating a series from normal distribution: y &lt;- rnorm(100,100,10) We use an ARIMA(0,1,1) model implemented in the stats package (this model is discussed in Section 8): ourCall &lt;- &quot;predict(arima(x=data,order=c(0,1,1)),n.ahead=h)&quot; The call that we specify includes two important elements: data and h. data specifies where the in-sample values are located in the function that we want to use, and it needs to be called “data” in the call; h will tell our function, where the forecasting horizon is specified in the selected function. Note that in this example we use arima(x=data,order=c(0,1,1)), which produces a desired ARIMA(0,1,1) model and then we use predict(..., n.ahead=h), which produces an h steps ahead forecast from that model. Having the call, we also need to specify what the function should return. This can be the conditional mean (point forecasts), prediction intervals, the parameters of a model, or, in fact, anything that the model returns (e.g. name of the fitted model and its likelihood). However, there are some differences in what ro() returns depending on what the function returns. If it is a vector, then ro() will produce a matrix (with values for each origin in columns). If it is a matrix, then an array is returned. Finally, if it is a list, then a list of lists is returned. In order not to overcomplicate things, we start from collecting the conditional mean from the predict() function: ourValue &lt;- c(&quot;pred&quot;) NOTE: If you do not specify the value to return, the function will try to return everything, but it might fail, especially if many values are returned. So, to be on the safe side, always provide the value, when possible. Now that we have specified ourCall and ourValue, we can produce forecasts from the model using rolling origin. Let’s say that we want three-steps-ahead forecasts and eight origins with the default values of all the other parameters: returnedValues1 &lt;- ro(y, h=3, origins=8, call=ourCall, value=ourValue) The same can be achieved using the following loop: obs &lt;- 100 roh &lt;- 8 h &lt;- 3 data &lt;- y returnedValues1 &lt;- setNames(vector(&quot;list&quot;,3), c(&quot;actuals&quot;,&quot;holdout&quot;,&quot;pred&quot;)) returnedValues1$actuals &lt;- y returnedValues1$holdout &lt;- returnedValues1$pred &lt;- matrix(NA,h,roh, dimnames=list(paste0(&quot;h&quot;,1:h), paste0(&quot;origin&quot;,1:roh))) for(i in 1:roh){ testModel &lt;- arima(x=data[1:(obs-roh+i-h)],order=c(0,1,1)) returnedValues1$holdout[,i] &lt;- data[-c(1:(obs-roh+i-h))] returnedValues1$pred[,i] &lt;- predict(testModel, n.ahead=h)$pred } The function returns a list with all the values that we asked for plus the actual values from the holdout sample. We can calculate some basic error measure based on those values, for example, scaled Absolute Error (Petropoulos and Kourentzes, 2015): apply(abs(returnedValues1$holdout - returnedValues1$pred), 1, mean, na.rm=TRUE) / mean(returnedValues1$actuals) ## h1 h2 h3 ## 0.09373759 0.11074337 0.12212627 In this example, we use the apply() function to distinguish between the different forecasting horizons and have an idea of how the model performs for each of them. These numbers do not tell us much on their own, but if we compare the performance of this model with another one, we could infer if one model is more appropriate for the data than the other one. For example, applying ARIMA(1,1,2) to the same data, we will get: ourCall &lt;- &quot;predict(arima(x=data,order=c(1,1,2)),n.ahead=h)&quot; returnedValues2 &lt;- ro(y, h=3, origins=8, call=ourCall, value=ourValue) apply(abs(returnedValues2$holdout - returnedValues2$pred), 1, mean, na.rm=TRUE) / mean(returnedValues2$actuals) ## h1 h2 h3 ## 0.09776787 0.11248970 0.12081304 Comparing these errors with the ones from the previous model, we can conclude which of the approaches is more suitable for the data. We can also plot the forecasts from the rolling origin, which shows how the models behave: par(mfcol=c(2,1), mar=c(4,4,1,1)) plot(returnedValues1) plot(returnedValues2) Figure 2.8: Rolling origin performance of two forecasting methods In Figure 2.8, the forecasts from different origins are close to each other. This is because the data is stationary, and both models produce flat lines as forecasts. The rolling origin function from the greybox package also allows working with explanatory variables and returning prediction intervals if needed. Some further examples are discussed in the vignette of the package: vignette(\"ro\",\"greybox\"). Practically speaking, if we have a set of forecasts from different models we can analyse the distribution of error measures and come to conclusions about performance of models. Here is an example with analysis of performance for \\(h=1\\) based on absolute errors: aeValuesh1 &lt;- cbind(abs(returnedValues1$holdout - returnedValues1$pred)[1,], abs(returnedValues1$holdout - returnedValues2$pred)[1,]) colnames(aeValuesh1) &lt;- c(&quot;ARIMA(0,1,1)&quot;,&quot;ARIMA(1,1,2)&quot;) boxplot(aeValuesh1) points(apply(aeValuesh1,2,mean),pch=16,col=&quot;red&quot;) Figure 2.9: Boxplots of error measures of two methods. The boxplots in Figure 2.9 can be interpreted as any other boxplots applied to random variables (see, for example, discussion in Section 2.2 of Svetunkov, 2021). References "],["statisticalTests.html", "2.5 Statistical comparison of forecasts", " 2.5 Statistical comparison of forecasts After applying several competing models to the data and obtaining a distribution of error terms, we might find that some performed very similarly. In this case, there might be a question, whether the difference is significant and which of the forecasting models we should select. Consider the following artificial example, where we have four competing models and measure their performance in terms of RMSSE: smallCompetition &lt;- matrix(NA, 100, 4, dimnames=list(NULL, paste0(&quot;Method&quot;,c(1:4)))) smallCompetition[,1] &lt;- rnorm(100,1,0.35) smallCompetition[,2] &lt;- rnorm(100,1.2,0.2) smallCompetition[,3] &lt;- runif(100,0.5,1.5) smallCompetition[,4] &lt;- rlnorm(100,0,0.3) We can check the mean and median error measures in this example in order to see, how the methods perform overall: overalResults &lt;- matrix(c(colMeans(smallCompetition), apply(smallCompetition, 2, median)), 4, 2, dimnames=list(colnames(smallCompetition), c(&quot;Mean&quot;,&quot;Median&quot;))) round(overalResults,5) ## Mean Median ## Method1 1.04148 0.97523 ## Method2 1.23750 1.23572 ## Method3 1.00213 1.02843 ## Method4 0.97789 0.92591 In this artificial example, it looks like the most accurate method in terms of mean and median RMSSE is Method 4, and the least accurate one is Method 2. However, the difference in terms of accuracy between methods 1, 3 and 4 does not look substantial. So, should we conclude that Method 4 is the best? Let’s first look at the distribution of errors using vioplot() function from vioplot package (Figure 2.10). vioplot::vioplot(smallCompetition) points(colMeans(smallCompetition), col=&quot;red&quot;, pch=16) Figure 2.10: Boxplot of RMSE for the artificial example The violin plots in Figure 2.10 show that the distribution of errors for Method 2 is shifted higher than the distributions of other methods. It also looks like Method 2 is working more consistently, meaning that the variability of the errors is lower (the size of the box on the graph). It is difficult to tell whether Method 1 is better than Methods 3 and 4 or not – their boxes intersect and roughly look similar, with Method 4 having a slightly shorter box and Method 3 having the box slightly lower positioned. This is all the basics of descriptive statistics, which allows concluding that in general, Methods 1, 3 and 4 do a better job than Method 2. This is also reflected in the mean and median error measures discussed above. So, what should we conclude? We should not make hasty decisions, and we should remember that we are dealing with a sample of data (100 time series), so inevitably, the performance of methods will change if we try them on different data sets. If we had a population of all the time series in the world, we could run our methods and make a more solid conclusion about their performances. But here, we deal with a sample. So it might make sense to see whether the difference in the performance of methods is significant. How should we do that? First, we can compare means of distributions of errors using a parametric statistical test. We can try F-test (Wikipedia, 2021a), which will tell us whether the mean performance of methods is similar or not. Unfortunately, this will not tell us how the methods compare. But t-test (Wikipedia, 2021b) could be used to do that instead for pairwise comparison. One could also use a regression model with dummy variables for methods, giving us parameters and their confidence intervals (based on t-statistics), telling us how the means of methods compare. However, F-test, t-test and t-statistics from regression rely on strong assumptions related to the distribution of the means of error measures (normality). If we had a large sample (e.g. a thousand of series) and well-behaved distribution, we could try it, hoping that the central limit theorem would work and might get something relatively meaningful. However, on 100 observations, this still could be an issue, especially given that the distribution of error measures is typically asymmetric (the estimate of mean might be biased, which leads to many issues). Second, we could compare medians of distributions of errors. They are robust to outliers, so their estimates should not be too biased in case of skewed distributions on smaller samples. To have a general understanding of performance (is everything the same or is there at least one method that performs differently), we could try the Friedman test (Wikipedia, 2021c), which could be considered a nonparametric alternative of F-test. This should work in our case but won’t tell us how specifically the methods compare. We could try the Wilcoxon signed-ranks test (Wikipedia, 2021d), which could be considered a nonparametric counterpart of the t-test. However, it only applies to two variables, while we want to compare four. Luckily, there is the Nemenyi test (Demšar, 2006), which is equivalent to the MCB test (Koning et al., 2005; Kourentzes, 2012). What the test does, is it ranks the performance of methods for each time series and then takes the mean of those ranks and produces confidence bounds for those means. The means of ranks correspond to medians, so by using this test, we compare medians of errors of different methods. If the confidence bounds for different methods intersect, we can conclude that the medians are not different from a statistical point of view. Otherwise, we can see which of the methods has a higher rank and which has the lower one. There are different ways to present the test results, and there are several R functions that implement it, including nemenyi() from the tsutils package. However, we will use the function rmcb() from the greybox, which has more flexible plotting capabilities, supporting all the default parameters for the plot() method. smallCompetitionTest &lt;- rmcb(smallCompetition, plottype=&quot;none&quot;) smallCompetitionTest plot(smallCompetitionTest, &quot;mcb&quot;, main=&quot;&quot;) ## Regression for Multiple Comparison with the Best ## The significance level is 5% ## The number of observations is 100, the number of methods is 4 ## Significance test p-value: 0 Figure 2.11: MCB test results for small competition. Figure 2.11 shows that Methods 1, 3 and 4 are not statistically different – their intervals intersect, so we cannot tell the difference between them, even though the mean rank of Method 4 is lower than for the other methods. Method 2, on the other hand, is significantly worse than the other methods: it has the highest mean rank of all, and its interval does not intersect intervals of other methods. Note that while this is a good way of presenting the results, all the MCB test does is a comparison of mean ranks. It does not tell much about the distribution of errors and neglects the distances between values (i.e. 0.1 is lower than 0.11, so the first method has a lower rank, which is precisely the same result as with comparing 0.1 and 100). This happens because by doing the test, we move from a numerical scale to the ordinal one (see Section 1.2 of Svetunkov, 2021). Finally, like any other statistical test, it will get its power when the sample increases. We know that the null hypothesis “variables are equal to each other” in reality is always wrong (see Section 5.3 of Svetunkov, 2021), so the increase of sample size will lead at some point to the correct conclusion: methods are statistically different. Here is a demonstration of this assertion: largeCompetition &lt;- matrix(NA, 100000, 4, dimnames=list(NULL, paste0(&quot;Method&quot;,c(1:4)))) # Generate data largeCompetition[,1] &lt;- rnorm(100000,1,0.35) largeCompetition[,2] &lt;- rnorm(100000,1.2,0.2) largeCompetition[,3] &lt;- runif(100000,0.5,1.5) largeCompetition[,4] &lt;- rlnorm(100000,0,0.3) # Run the test largeCompetitionTest &lt;- rmcb(largeCompetition, plottype=&quot;none&quot;) plot(largeCompetitionTest, &quot;mcb&quot;, main=&quot;&quot;) Figure 2.12: MCB test results for large competition. In the plot in Figure 2.12, Method 4 has become significantly worse than Methods 1 and 3 in terms of mean ranks (note that it was winning in the small competition). The difference between Methods 1 and 3 is still not significant, but it would become if we continue increasing the sample size. This example tells us that we need to be careful when selecting the best method, as this might change under different circumstances. At least we knew from the start that Method 2 was not suitable. References "],["tsDecomposition.html", "Chapter 3 From time series components to ETS", " Chapter 3 From time series components to ETS Before we turn to the state space framework, ETS, ARIMA and other models, we need to discuss time series decomposition and the ETS taxonomy. These topics lie at the heart of ETS models and are essential for understanding further material. In this chapter, we start with a discussion of time series components, then move to the idea of decomposing time series into distinct components and then to the conventional ETS taxonomy, as formulated by Hyndman et al. (2008), demonstrating its connection with the previous topics. References "],["tsComponents.html", "3.1 Time series components", " 3.1 Time series components The main idea behind many forecasting techniques is that any time series can contain several unobservable components, such as: Level of the series – the average value for a specific time period, Growth of the series – the average increase or decrease of the value over a period of time, Seasonality – a pattern that repeats itself with a fixed periodicity. Error – unexplainable white noise. The level is the fundamental component that is present in any time series. In the simplest form (without variability), when plotted on its own without other components, it will look like a straight line, shown, for example, in Figure 3.1. level &lt;- rep(100,40) plot(ts(level, frequency=4), type=&quot;l&quot;, xlab=&quot;Time&quot;, ylab=&quot;Sales&quot;, ylim=c(80,160)) Figure 3.1: Level of time series without any variability. If the time series exhibits growth, the level will change depending on the observation. For example, if the growth is positive and constant, we can update the level in Figure 3.1 to have a straight line with a non-zero slope as shown in Figure 3.2. growth &lt;- c(1:40) plot(ts(level+growth, frequency=4), type=&quot;l&quot;, xlab=&quot;Time&quot;, ylab=&quot;Sales&quot;, ylim=c(80,160)) Figure 3.2: Time series with a positive trend and no variability. The seasonal pattern will introduce some similarities from one period to another. This pattern does not have to literally be seasonal, like beer sales being higher in Summer than in Winter (season of the year). Any pattern with a fixed periodicity works: the number of hospital visitors is higher on Mondays than on Saturdays or Sundays because people tend to stay at home over the weekend. This can be considered as the day of week seasonality. Furthermore, if we deal with hourly data, sales are higher during the daytime than at night (hour of the day seasonality). Adding a deterministic seasonal component to the example above will result in fluctuations around the straight line, as shown in Figure 3.3. seasonal &lt;- rep(c(10,15,-20,-5),10) plot(ts(level+growth+seasonal, frequency=4), type=&quot;l&quot;, xlab=&quot;Time&quot;, ylab=&quot;Sales&quot;, ylim=c(80,160)) Figure 3.3: Time series with a positive trend, seasonal pattern and no variability. Finally, we can introduce the random error to the plots above to have a more realistic time series as shown in Figure 3.4. Figure 3.4: Time series with random errors. Figure 3.4 shows artificial time series with the above components. The level, growth, and seasonal components in those plots are deterministic, they are fixed and do not evolve over time (growth is positive and equal to 1 from year to year). However, in real life, typically, these components will have more complex dynamics, changing over time and thus demonstrating their stochastic nature. For example, in the case of stochastic seasonality, the seasonal shape might change, and instead of having peaks in sales in January, the data would exhibit peaks in May due to the change in consumers’ behaviour. Note that each textbook and paper might use slightly different names to refer to the abovementioned components. For example, in classical decomposition (Warren M. Persons, 1919), it is assumed that (1) and (2) jointly represent a “trend” component so that a model will contain error, trend and seasonality. This decomposition has modifications, which include cyclical component(s). When it comes to ETS, the growth component (2) is called “trend,” so the model consists of the four components: level, trend, seasonal and the error term. We will use the ETS formulation in this textbook. According to this formulation, the components can interact in one of two ways: additively or multiplicatively. The pure additive model, in this case, can be summarised as: \\[\\begin{equation} y_t = l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t , \\tag{3.1} \\end{equation}\\] where \\(l_{t-1}\\) is the level, \\(b_{t-1}\\) is the trend, \\(s_{t-m}\\) is the seasonal component with periodicity \\(m\\) (e.g. 12 for months of year data, implying that something is repeated every 12 months) – all these components are produced on the previous observations and are used on the current one. Finally, \\(\\epsilon_t\\) is the error term, which follows some distribution and has zero mean. The pure additive models were plotted in Figure 3.4. Similarly, the pure multiplicative model is: \\[\\begin{equation} y_t = l_{t-1} b_{t-1} s_{t-m} \\varepsilon_t , \\tag{3.2} \\end{equation}\\] where \\(\\varepsilon_t\\) is the error term with a mean of one. The interpretation of the model (3.1) is that the different components add up to each other, so, for example, the sales increase over time by the value \\(b_{t-1}\\), each January they typically change by the amount \\(s_{t-m}\\), and that there is still some randomness in the model. The pure additive models can be applied to data with positive, negative and zero values. In the case of the multiplicative model (3.2), the interpretation is different, showing by how many times the sales change over time and from one season to another. The sales, in this case, will change every January by \\((s_{t-m}-1)\\)% from the baseline. The model @ref(eq: PureMultiplicative) only works on strictly positive data (data with purely negative values are also possible but rare in practice). It is also possible to define mixed models in which, for example, the trend is additive but the other components are multiplicative: \\[\\begin{equation} y_t = (l_{t-1} + b_{t-1}) s_{t-m} \\varepsilon_t \\tag{3.3} \\end{equation}\\] These models work well in practice when the data has large values far from zero. In other cases, however, they might break and produce strange results (e.g. negative values on positive data), so the conventional decomposition techniques only consider the pure models. References "],["ClassicalDecomposition.html", "3.2 Classical Seasonal Decomposition", " 3.2 Classical Seasonal Decomposition 3.2.1 How to do? One of the classical textbook methods for decomposing the time series into unobservable components is “Classical Seasonal Decomposition” (Warren M. Persons, 1919). It assumes either a pure additive or pure multiplicative model, is done using centred moving averages and is focused on approximation, not forecasting. The idea of the method can be summarised in the following steps: Decide which of the models to use based on the type of seasonality in the data: additive (3.1) or multiplicative (3.2) Smooth the data using a centred moving average (CMA) of order equal to the periodicity of the data \\(m\\). If \\(m\\) is the an number then the formula is: \\[\\begin{equation} d_t = \\frac{1}{m}\\sum_{i=-(m-1)/2}^{(m-1)/2} y_{t+i}, \\tag{3.4} \\end{equation}\\] which means that, for example, the value on Thursday is the average of values from Monday to Sunday. If \\(m\\) is an even number then a different weighting scheme is typically used, involving the inclusion of additional an value: \\[\\begin{equation} d_t = \\frac{1}{m}\\left(\\frac{1}{2}\\left(y_{t+(m-1)/2}+y_{t-(m-1)/2}\\right) + \\sum_{i=-(m-2)/2}^{(m-2)/2} y_{t+i}\\right), \\tag{3.5} \\end{equation}\\] which means that we use half of the December of the previous year and half of the December of the current year to calculate the centred moving average in June. The values \\(d_t\\) are placed in the middle of the window going through the series (e.g. on Thursday, the average will contain values from Monday to Sunday). The resulting series is deseasonalised. When we average, e.g. sales in a year, we automatically remove the potential seasonality, which can be observed each month individually. A drawback of using CMA is that we inevitably lose \\(\\frac{m}{2}\\) observations at the beginning and the end of the series. In R, the ma() function from the forecast package implements CMA. De-trend the data: For the additive decomposition this is done using: \\({y^\\prime}_t = y_t -d_t\\); For the multiplicative decomposition, it is: \\({y^\\prime}_t = \\frac{y_t}{d_t}\\); If the data is seasonal, the average value for each period is calculated based on the de-trended series. e.g. we produce average seasonal indices for each January, February, etc. This will give us the set of seasonal indices \\(s_t\\); Calculate the residuals based on what you assume in the model: additive seasonality: \\(e_t = y_t -d_t -s_t\\); multiplicative seasonality: \\(e_t = \\frac{y_t}{d_t s_t}\\); no seasonality: \\(e_t = {y^\\prime}_t\\). Note that the functions in R typically allow selecting between additive and multiplicative seasonality. There is no option for “none,” and so even if the data is not seasonal, you will nonetheless get values for \\(s_t\\) in the output. Also, notice that the classical decomposition assumes that there is a deseasonalised series \\(d_t\\) but does not make any further split of this variable into level \\(l_t\\) and trend \\(b_t\\). 3.2.2 A couple of examples An example of the classical decomposition in R is the decompose() function from stats package. Here is an example with pure multiplicative model and AirPassengers data: ourDecomposition &lt;- decompose(AirPassengers, type=&quot;multiplicative&quot;) plot(ourDecomposition) We can see that the function has smoothed the original series and produced the seasonal indices. Note that the trend component has gaps at the beginning and the end. This is because the method relies on CMA (see above). Note also that the error term still contains some seasonal elements, which is a downside of such a simple decomposition procedure. However, the lack of precision in this method is compensated by the simplicity and speed of calculation. Note again that the trend component in decompose() function is in fact \\(d_t = l_{t}+b_{t}\\). Here is an example of decomposition of the non-seasonal data (we assume pure additive model in this example): y &lt;- ts(c(1:100)+rnorm(100,0,10),frequency=12) ourDecomposition &lt;- decompose(y, type=&quot;additive&quot;) plot(ourDecomposition) As you can see, the original data is not seasonal, but the decomposition assumes that it is and proceeds with the default approach returning a seasonal component. You get what you ask for. 3.2.3 Other techniques There are other techniques that decompose series into error, trend and seasonal components but make different assumptions about each component. The general procedure, however, always remains the same: (1) smooth the original series, (2) extract the seasonal components, (3) smooth them out. The methods differ in the smoother they use (LOESS, e.g., uses a bisquare function instead of CMA), and in some cases, multiple rounds of smoothing are performed to make sure that the components are split correctly. There are many functions in R that implement seasonal decomposition. Here is a small selection: decomp() from the tsutils package does classical decomposition and fills in the tail and head of the smoothed trend with forecasts from exponential smoothing; stl() from the stats package uses a different approach – seasonal decomposition via LOESS. It is an iterative algorithm that smoothes the states and allows them to evolve over time. So, for example, the seasonal component in STL can change; mstl() from the forecast package does the STL for data with several seasonalities; msdecompose() from the smooth package does a classical decomposition for multiple seasonal series. 3.2.4 “Why bother?” “Why to decompose?” you may wonder at this point. Understanding the idea behind decompositions and how to perform them helps understand ETS, which relies on it. From a practical point of view, it can be helpful if you want to see if there is a trend in the data and whether the residuals contain outliers or not. It will not show you if the data is seasonal as the seasonality is assumed in the decomposition (I stress this because many students think otherwise). Additionally, when seasonality cannot be added to the model under consideration decomposing the series, predicting the trend and then reseasonalising can be a viable solution. Finally, the values from the decomposition can be used as starting points for the estimation of components in ETS or other dynamic models relying on the error-trend-seasonality. References "],["simpleForecastingMethods.html", "3.3 Simple forecasting methods", " 3.3 Simple forecasting methods Now that we understand that time series might contain different components and that there are approaches for their decomposition, we can introduce several simple forecasting methods that can be used in practice, at least as benchmarks. Their usage aligns with the idea of forecasting principles discussed in Section 1.2. 3.3.1 Naïve Naïve is one of the simplest forecasting methods. According to each, the one-step-ahead forecast is equal to the most recent actual value: \\[\\begin{equation} \\hat{y}_t = y_{t-1} . \\tag{3.6} \\end{equation}\\] Using this approach might sound naïve indeed, but there are cases where it is very hard to outperform the method. Consider an example with temperature forecasting. If we want to know what the temperature outside will be in 5 minutes, then Naïve would be typically very accurate: the temperature in 5 minutes will be the same as it is right now. The statistical model underlying Naïve is called “Random Walk” and is written as: \\[\\begin{equation} y_t = y_{t-1} + \\epsilon_t. \\tag{3.7} \\end{equation}\\] The variability of \\(\\epsilon_t\\) will impact the speed of change of the data: the higher it is, the more rapid the values will change. Random Walk and Naïve can be represented in Figure 3.5. In the example below, we use a simple moving average (discussed later in Section 3.3.3) of order 1 to generate the data from Random Walk and then produce forecasts using Naïve. y &lt;- sim.sma(1, 120) testModel &lt;- sma(y$data, 1, h=10, holdout=TRUE) plot(testModel, 7, main=&quot;&quot;) Figure 3.5: A Random Walk example. As shown from the plot in Figure 3.5, Naïve lags behind the actual time series by one observation because of how it is constructed via equation (3.6). The point forecast corresponds to the straight line parallel to the x-axis. Given that the data was generated from Random Walk, the point forecast shown in Figure 3.5 is the best possible forecast for the time series, even though it exhibits rapid changes in the level. Note that if the time series exhibits level shifts or other types of unexpected changes in dynamics, Naïve will update rapidly and reach the new level instantaneously. However, because it only has a memory of one (last) observation, it will not filter out the noise in the data but rather copy it into the future. So, it has limited usefulness in practice. However, being the simplest possible forecasting method, it is considered one of the basic forecasting benchmarks. If your model cannot beat it, it is not worth using. 3.3.2 Global Mean While Naïve considered only one observation (the most recent one), global mean (aka “global average”) relies on all the observations in the data: \\[\\begin{equation} \\hat{y}_t = \\bar{y} = \\frac{1}{T} \\sum_{t=1}^T y_{t} , \\tag{3.8} \\end{equation}\\] where \\(T\\) is the sample size. The model underlying this forecasting method is called “global level” and is written as: \\[\\begin{equation} y_t = \\mu + \\epsilon_t, \\tag{3.9} \\end{equation}\\] so that the \\(\\bar{y}\\) is an estimate of the fixed expectation \\(\\mu\\). Graphically, this is represented with a straight line going through the series as shown in Figure 3.6. y &lt;- rnorm(120, 100, 10) testModel &lt;- es(y, &quot;ANN&quot;, persistence=0, h=10, holdout=TRUE) plot(testModel, 7, main=&quot;&quot;) Figure 3.6: A global level example. The series shown in Figure 3.6 is generated from the global level model, and the point forecast corresponds to the forecast from the global mean method. Note that the method assumes that the weights between the in-sample observation are equal, i.e. the first observation has precisely the exact weight of \\(\\frac{1}{T}\\) as the last one. Suppose the series exhibits some changes in level over time. In that case, the global mean will not be suitable because it will produce the averaged out forecast, considering values for parts before and after the change. 3.3.3 Simple Moving Average Naïve and Global Mean can be considered as opposite points in the spectrum of possible level time series (although there are series beyond Naïve, see for example ARIMA(0,1,1) with \\(\\theta_1&gt;0\\), discussed in Section 8). The series between them exhibit slow changes in level and can be modelled using different forecasting approaches. One of those is the Simple Moving Average (SMA), which uses the mechanism of the mean for a small part of the time series. It relies on the formula: \\[\\begin{equation} \\hat{y}_t = \\frac{1}{m}\\sum_{j=1}^{m} y_{t-j}, \\tag{3.10} \\end{equation}\\] which implies going through time series with something like a “window” of \\(m\\) observations and using their average for forecasting. The order \\(m\\) determines the length of the memory of the method: if it is equal to 1, then (3.10) turns into Naïve, while in the case of \\(m=T\\) it transforms into Global Mean. The order \\(m\\) is typically decided by a forecaster, keeping in mind that the lower \\(m\\) corresponds to the shorter memory method, while the higher one corresponds to the longer one. Svetunkov and Petropoulos (2018) have shown that SMA has an underlying non-stationary AR(m) model with \\(\\phi_j=\\frac{1}{m}\\) for all \\(j=1, \\dots, m\\). While the conventional approach to forecasting from SMA is to produce the straight line, equal to the last obtained observation, Svetunkov and Petropoulos (2018) demonstrate that, in general, the point forecast does not correspond to the straight line. y &lt;- sim.sma(10,120) par(mfcol=c(2,2), mar=c(2,2,2,1)) # SMA(1) testModel &lt;- sma(y, order=1, h=10, holdout=TRUE) plot(testModel, 7, main=testModel$model) # SMA(10) testModel &lt;- sma(y, order=10, h=10, holdout=TRUE) plot(testModel, 7, main=testModel$model) # SMA(20) testModel &lt;- sma(y, order=20, h=10, holdout=TRUE) plot(testModel, 7, main=testModel$model) # SMA(110) testModel &lt;- sma(y, order=110, h=10, holdout=TRUE) plot(testModel, 7, main=testModel$model) Figure 3.7: Examples of SMA time series and several SMA models of different orders applied to it. Figure 3.7 demonstrates the time series generated from SMA(10) and several SMA models applied to it. We can see that the higher orders of SMA lead to smoother fitted lines and calmer point forecasts. On the other hand, the SMA of a very high order, such as SMA(110), does not follow the changes in time series efficiently, ignoring the potential changes in level. Given the difficulty with selecting the order \\(m\\), Svetunkov and Petropoulos (2018) proposed using information criteria for the order selection of SMA in practice. Finally, an attentive reader has already spotted that the formula for SMA corresponds to the procedure of CMA of an odd order from equation (3.4). They are similar, but they have a different purpose: CMA is needed to smooth out the series, the calculated values are inserted in the middle of the average, while SMA is used for forecasting, and the point forecasts are inserted at the end of the average. 3.3.4 Random Walk with drift So far, we have discussed the methods used for level time series. But as mentioned in Section 3.1, there are other components in the time series. In the case of the series with a trend, Naïve, Global Mean and SMA will be inappropriate because they would be missing the essential component. The simplest model that can be used in this case is called “Random Walk with drift,” which is formulated as: \\[\\begin{equation} y_t = y_{t-1} + a_0 + \\epsilon_t, \\tag{3.11} \\end{equation}\\] where \\(a_0\\) is a constant term, the introduction of which leads to increasing or decreasing trajectories, depending on the value of \\(a_0\\). The point forecast from this model is calculated as: \\[\\begin{equation} \\hat{y}_{t+h} = y_{t} + a_0 h, \\tag{3.12} \\end{equation}\\] implying that the forecast from the model is a straight line with the slope parameter \\(a_0\\). Figure 3.8 shows how the data generated from Random Walk with drift and \\(a_0=10\\) looks like. This model is discussed in Section 8. y &lt;- sim.ssarima(orders=list(i=1), lags=1, obs=120, constant=10) testModel &lt;- msarima(y, orders=list(i=1), constant=TRUE, h=10, holdout=TRUE) plot(testModel, 7, main=&quot;&quot;) Figure 3.8: Random Walk with drift data and the model applied to that data. The data in Figure 3.8 demonstrates a positive trend (because \\(a_0&gt;0\\)) and randomness from one observation to another. The model is helpful as a benchmark and a special case for several other models because it is simple and requires the estimation of only one parameter. 3.3.5 Seasonal Naïve Finally, in the case of seasonal data, there is a simple forecasting method that can be considered as a good benchmark in many situations. Similar to Naïve, Seasonal Naïve relies only on one observation, but instead of taking the most recent value, it uses the value from the same period a season ago. For example, for producing a forecast for January 1984, we would use January 1983. Mathematically this is written as: \\[\\begin{equation} \\hat{y}_t = y_{t-m} , \\tag{3.13} \\end{equation}\\] where \\(m\\) is the seasonal frequency. This method has an underlying model, Seasonal Random Walk: \\[\\begin{equation} \\hat{y}_t = y_{t-m} + \\epsilon_t. \\tag{3.14} \\end{equation}\\] Similar to Naïve, the higher variability of the error term \\(\\epsilon_t\\) in (3.14) is, the faster the data exhibit changes. Seasonal Naïve does not require estimation of any parameters and thus is considered one of the popular benchmarks in seasonal data. Figure 3.9 demonstrates how the data generated from seasonal Random Walk looks and how the point forecast from the seasonal Naïve applied to this data performs. y &lt;- sim.ssarima(orders=list(i=1), lags=4, obs=120, sd=50) testModel &lt;- msarima(y, orders=list(i=1), lags=4, h=10, holdout=TRUE) plot(testModel, 7, main=&quot;&quot;) Figure 3.9: Seasonal Random Walk and Seasonal Naïve. Similarly to the previous methods, if other approaches cannot outperform seasonal Naïve, it is not worth spending time on those approaches. References "],["ETSTaxonomy.html", "3.4 ETS taxonomy", " 3.4 ETS taxonomy Building on the idea of time series components (from Section 3.1), we can move to the ETS taxonomy. ETS stands for “Error-Trend-Seasonality” and defines how specifically the components interact with each other. Based on the type of error, trend and seasonality, Pegels (1969) proposed a taxonomy, which was then developed further by Hyndman et al. (2002) and refined by Hyndman et al. (2008). According to this taxonomy, error, trend and seasonality can be: Error: “Additive” (A), or “Multiplicative” (M); Trend: “None” (N), or “Additive” (A), or “Additive damped” (Ad), or “Multiplicative” (M), or “Multiplicative damped” (Md); Seasonality: “None” (N), or “Additive” (A), or “Multiplicative” (M). According to this taxonomy, the model (3.1) is denoted as ETS(A,A,A) while the model (3.2) is denoted as ETS(M,M,M), and (3.3) is ETS(M,A,M). The components in the ETS taxonomy have clear interpretations. Furthermore, ETS supports 30 models with different types of error, trend and seasonality. Figure 3.10 shows examples of different time series with deterministic (they do not change over time) level, trend, seasonality and with the additive error term. Figure 3.10: Time series corresponding to the additive error ETS models Things to note from the plots in Figure 3.10: When seasonality is multiplicative, its amplitude increases with the increase of the level of the data, while with additive seasonality, the amplitude is constant. Compare, for example, ETS(A,A,A) with ETS(A,A,M): for the former, the distance between the highest and the lowest points in the first year is roughly the same as in the last year. In the case of ETS(A,A,M) the distance increases with the increase in the level of series. When the trend is multiplicative, data exhibits exponential growth/decay result. The damped trend models slow down both additive and multiplicative trends. It is practically impossible to distinguish additive and multiplicative seasonality if the level of series does not change because the amplitude of seasonality will be constant in both cases (compare ETS(A,N,A) and ETS(A,N,M)). Figure 3.11 demonstrates a similar plot for the multiplicative error models. Figure 3.11: Time series corresponding to the multiplicative error ETS models The plots in Figure 3.11 show roughly the same idea as the additive case, the main difference being that the variance of the error increases with the increase of the level of the data – this becomes clearer on ETS(M,A,N) and ETS(M,M,N) data. This property is called heteroscedasticity in statistics, and Hyndman et al. (2008) argue that the main benefit of the multiplicative error models is in capturing this feature. We will discuss the most important ETS family members in the following chapters. Not all the models in this taxonomy are sensible, and some are typically ignored entirely. Although ADAM implements the entire taxonomy, we will discuss potential issues and what to expect from them. References "],["ETSTaxonomyMaths.html", "3.5 Mathematical models in the ETS taxonomy", " 3.5 Mathematical models in the ETS taxonomy I hope that it becomes more apparent to the reader how the ETS framework is built upon the idea of time series decomposition (from Section 3.1). By introducing different components, defining their types, and adding the equations for their update, we can construct models that would work better on the time series. The equations discussed in Section 3.1 represent so-called “measurement” or “observation” equations of the ETS models. But we should also consider the potential change in components over time. The “transition” or “state” equation is supposed to reflect this change: they explain how the level, trend or seasonal components evolve. As discussed in Section 3.4, given different types of components and their interactions, we end up with 30 models in the taxonomy. Tables 3.1 and 3.2 summarise mathematically all 30 ETS models shown graphically on Figures 3.10 and 3.11, presenting formulae for measurement and transition equations. Table 3.1: Additive error ETS models Nonseasonal Additive Multiplicative No trend \\(\\begin{aligned} &amp;y_{t} = l_{t-1} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\alpha \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = l_{t-1} + s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\alpha \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = l_{t-1} s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1}} \\end{aligned}\\) Additive \\(\\begin{aligned} &amp;y_{t} = l_{t-1} + b_{t-1} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\ &amp;b_t = b_{t-1} + \\beta \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\ &amp;b_t = b_{t-1} + \\beta \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = (l_{t-1} + b_{t-1}) s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + b_{t-1}} \\end{aligned}\\) Additive damped \\(\\begin{aligned} &amp;y_{t} = l_{t-1} + \\phi b_{t-1} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\ &amp;b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\ &amp;b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;b_t = \\phi b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + \\phi b_{t-1}} \\end{aligned}\\) Multiplicative \\(\\begin{aligned} &amp;y_{t} = l_{t-1} b_{t-1} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\ &amp;b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = l_{t-1} b_{t-1} + s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\ &amp;b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = l_{t-1} b_{t-1} s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}} \\end{aligned}\\) Multiplicative damped \\(\\begin{aligned} &amp;y_{t} = l_{t-1} b_{t-1}^\\phi + \\epsilon_t \\\\ &amp;l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\ &amp;b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\ &amp;b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp;b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\ &amp;s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}} \\end{aligned}\\) Table 3.2: Multiplicative error ETS models Nonseasonal Additive Multiplicative No trend \\(\\begin{aligned} &amp;y_{t} = l_{t-1}(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = (l_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = l_{t-1} s_{t-m}(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\ &amp;s_t = s_{t-m}(1 + \\gamma \\epsilon_t) \\end{aligned}\\) Additive \\(\\begin{aligned} &amp;y_{t} = (l_{t-1} + b_{t-1})(1 + \\epsilon_t) \\\\ &amp;l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = (l_{t-1} + b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} + b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &amp;b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = (l_{t-1} + b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\ &amp;l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = b_{t-1} + \\beta (l_{t-1} + b_{t-1}) \\epsilon_t \\\\ &amp;s_t = s_{t-m} (1 + \\gamma \\epsilon_t) \\end{aligned}\\) Additive damped \\(\\begin{aligned} &amp;y_{t} = (l_{t-1} + \\phi b_{t-1})(1 + \\epsilon_t) \\\\ &amp;l_t = (l_{t-1} + \\phi b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = (l_{t-1} + \\phi b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &amp;b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} + \\phi b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = \\phi b_{t-1} + \\beta (l_{t-1} + \\phi b_{t-1}) \\epsilon_t \\\\ &amp;s_t = s_{t-m}(1 + \\gamma \\epsilon_t) \\end{aligned}\\) Multiplicative \\(\\begin{aligned} &amp;y_{t} = l_{t-1} b_{t-1} (1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = (l_{t-1} b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &amp;b_t = b_{t-1} + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = l_{t-1} b_{t-1} s_{t-m} (1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\\\ &amp;s_t = s_{t-m} (1 + \\gamma \\epsilon_t) \\end{aligned}\\) Multiplicative damped \\(\\begin{aligned} &amp;y_{t} = l_{t-1} b_{t-1}^\\phi (1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} b_{t-1}^\\phi (1 + \\alpha \\epsilon_t) \\\\ &amp;b_t = b_{t-1}^\\phi (1 + \\beta \\epsilon_t) \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = (l_{t-1} b_{t-1}^\\phi + s_{t-m})(1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\mu_{y,t} \\epsilon_t \\\\ &amp;b_t = b_{t-1}^\\phi + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\end{aligned}\\) \\(\\begin{aligned} &amp;y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} (1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1} b_{t-1}^\\phi \\left(1 + \\alpha \\epsilon_t\\right) \\\\ &amp;b_t = b_{t-1}^\\phi \\left(1 + \\beta \\epsilon_t\\right) \\\\ &amp;s_t = s_{t-m} \\left(1 + \\gamma \\epsilon_t\\right) \\end{aligned}\\) From a statistical point of view, formulae in Tables 3.1 and 3.2 correspond to the “true models” (see Section 1.2 of Svetunkov, 2021), they explain the models underlying potential data, but when it comes to their construction and estimation, the \\(\\epsilon_t\\) is substituted by the estimated \\(e_t\\) (which is calculated differently depending on the error type), and time series components and smoothing parameters are also replaced by their estimated analogues (e.g. \\(\\hat{\\alpha}\\) instead of \\(\\alpha\\)). However, if the values of these models’ parameters were known, it would be possible to produce point forecasts and conditional h steps ahead expectations from these models. Table 3.3 summarises: Conditional one step ahead expectation \\(\\mu_{y,t} = \\mu_{y,t|t-1}\\); Multiple steps ahead point forecast \\(\\hat{y}_{t+h}\\); Conditional multiple steps ahead expectation \\(\\mu_{y,t+h|t}\\); In the case of the additive error models, the point forecasts correspond to the expectations only when the expectation of the error term is zero, i.e. \\(\\text{E}(\\epsilon_t)=0\\). In contrast, in the case of the multiplicative one, the condition is changed to \\(\\text{E}(1+\\epsilon_t)=1\\). Remark. Not all the point forecasts of ETS models correspond to conditional expectations. This issue applies to the models with multiplicative trend and/or multiplicative seasonality. This is because the ETS model assumes that different states are correlated (they have the same source of error), and as a result, multiple steps ahead values (when h&gt;1) of states introduce products of error terms. So, the conditional expectations in these cases might not have analytical forms (“n.c.f.” in Table 3.3 stands for “no closed-form”), and when working with these models, simulations might be required. This does not apply to the one-step-ahead forecasts, for which all the classical formulae work. Table 3.3: Point forecasts and expectations of ETS models. n.c.f. stands for “No Closed Form.” Nonseasonal Additive Multiplicative No trend \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} \\\\ &amp;\\hat{y}_{t+h} = l_{t} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{aligned}\\) \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{aligned}\\) \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m \\end{aligned}\\) Additive \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} + b_{t-1} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + h b_t \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{aligned}\\) \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} + b_{t-1} + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + h b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{aligned}\\) \\(\\begin{aligned} &amp;\\mu_{y,t} = (l_{t-1} + b_{t-1}) s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = \\left(l_{t} + h b_{t-1}\\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m \\end{aligned}\\) Additive damped \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_t \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{aligned}\\) \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\end{aligned}\\) \\(\\begin{aligned} &amp;\\mu_{y,t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = \\left(l_{t} + \\sum_{j=1}^h \\phi^j b_t \\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m \\end{aligned}\\) Multiplicative \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} b_{t-1} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_t^h \\\\ &amp;\\mu_{y,t+h|t} \\text{ -- n.c.f. for } h&gt;1 \\end{aligned}\\) \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} b_{t-1} + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_{t-1}^h + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} \\text{ -- n.c.f. for } h&gt;1 \\end{aligned}\\) \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} b_{t-1} s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_{t-1}^h s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} \\text{ -- n.c.f. for } h&gt;1 \\end{aligned}\\) Multiplicative damped \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_t^{\\sum_{j=1}^h \\phi^j} \\\\ &amp;\\mu_{y,t+h|t} \\text{ -- n.c.f. for } h&gt;1 \\end{aligned}\\) \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} \\text{ -- n.c.f. for } h&gt;1 \\end{aligned}\\) \\(\\begin{aligned} &amp;\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi s_{t-m} \\\\ &amp;\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\ &amp;\\mu_{y,t+h|t} \\text{ -- n.c.f. for } h&gt;1 \\end{aligned}\\) The multiplicative error models have the same one step ahead expectations and point forecasts as the additive error ones. However, due to the multiplication by the error term, the multiple steps ahead conditional expectations between the two types of models might differ, specifically for the multiplicative trend and multiplicative seasonal models. These values do not have closed forms and can only be obtained via simulations. Although there are 30 potential ETS models, not all of them are stable. So, Rob Hyndman has reduced the pool of models under consideration in the ets() function of forecast package to the following 19: ANN, AAN, AAdN, ANA, AAA, AAdA, MNN, MAN, MAdN, MNA, MAA, MAdA, MNM, MAM, MAdM, MMN, MMdN, MMM, MMdM. In addition, the multiplicative trend models are unstable in data with outliers, so they are switched off in the ets() function by default, which reduces the pool of models further to the first 15. References "],["ETSConventional.html", "Chapter 4 Conventional Exponential Smoothing", " Chapter 4 Conventional Exponential Smoothing Now that we know how time series can be decomposed into components, we can discuss the exponential smoothing methods, focusing on the most popular ones. We do not detail how the methods were originally derived and how to work with them. Instead, we focus on their connection with ETS and the main ideas behind the conventional ETS. The reader interested in the history of exponential smoothing, how it was developed, and what papers contributed to the field can refer to the reviews of Gardner (1985) and Gardner (2006). They summarise all the progress in exponential smoothing up until 1985 and until 2006 respectively. References "],["SES.html", "4.1 Simple Exponential Smoothing", " 4.1 Simple Exponential Smoothing We start our discussion of exponential smoothing with the original Simple Exponential Smoothing (SES) forecasting method, which was formulated by (Brown, 1956): \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{\\alpha} {y}_{t} + (1 -\\hat{\\alpha}) \\hat{y}_{t}, \\tag{4.1} \\end{equation}\\] where \\(\\hat{\\alpha}\\) is the smoothing parameter defined by analyst and which is typically restricted with (0, 1) region (this region is arbitrary, and we will see in Section 4.6 what is the correct one). This is one of the simplest forecasting methods. The smoothing parameter is typically interpreted as a weight between the actual value and the predicted one-step-ahead. If the smoothing parameter is close to zero, then more weight is given to the previous fitted value \\(\\hat{y}_{t}\\) and the new information is neglected. If \\(\\hat{\\alpha}=0\\), then the method becomes equivalent to the Global Mean method, discussed in Section 3.3.2. When it is close to one, then mainly the actual value \\({y}_{t}\\) is taken into account. If \\(\\hat{\\alpha}=1\\), then the method transforms into Naïve, discussed in Section 3.3.1. By changing the smoothing parameter value, the forecaster can decide how to approximate the data and filter out the noise. Also, notice that this is a recursive method, meaning that there needs to be some starting point \\(\\hat{y}_1\\) to apply (4.1) to the existing data. Different initialisation and estimation methods for SES have been discussed in the literature. Still, the state of the art one is to estimate \\(\\hat{\\alpha}\\) and \\(\\hat{y}_{1}\\) together by minimising some loss function (Hyndman et al., 2002). Typically MSE (see Section 2.1) is used as one, minimising the squares of one step ahead forecast errors. 4.1.1 Examples of application Here is an example of how this method works on different time series. We start with generating a stationary series and using es() function from smooth package. Although it implements the ETS model, we will see later the connection between SES and ETS(A,N,N). We start with the stationary time series and \\(\\hat{\\alpha}=0\\): y &lt;- rnorm(100,100,10) ourModel &lt;- es(y, model=&quot;ANN&quot;, h=10, persistence=0) plot(ourModel, 7, main=&quot;&quot;) Figure 4.1: An example with a time series and SES forecast. \\(\\hat{\\alpha}=0\\) As we see from Figure 4.1, the SES works well in this case, capturing the deterministic level of the series and filtering out the noise. In this case, it works like a global average applied to the data. As mentioned before, the method is flexible, so if we have a level shift in the data and increase the smoothing parameter, it will adapt and get to the new level. Figure 4.2 shows an example with a level shift in the data. y &lt;- c(rnorm(50,100,10),rnorm(50,130,10)) ourModel &lt;- es(y, model=&quot;ANN&quot;, h=10, persistence=0.1) plot(ourModel, 7, main=&quot;&quot;) Figure 4.2: An example with a time series and SES forecast. \\(\\hat{\\alpha}=0.1\\) With \\(\\hat{\\alpha}=0.1\\), SES manages to get to the new level, but now the method starts adapting to noise a little bit – it follows the peaks and troughs and repeats them with a lag, but with a much smaller magnitude (see Figure 4.2). Increasing the smoothing parameter, the model will react to the changes much faster, at the cost of responding more to noise. This is shown in Figure 4.3 with different smoothing parameters values. Figure 4.3: SES with different smoothing parameters applied to the same data. If we set \\(\\hat{\\alpha}=1\\), we will end up with Naive forecasting method (see Section 3.3.1), which is not appropriate for our example (see Figure 4.4). Figure 4.4: SES with \\(\\hat{\\alpha}=1\\). So, when working with SES, we need to make sure that the reasonable smoothing parameter is selected. This can be done automatically via minimising the MSE (see Figure 4.5): ourModel &lt;- es(y, model=&quot;ANN&quot;, h=10, loss=&quot;MSE&quot;) plot(ourModel, 7, main=paste0(&quot;SES with alpha=&quot;, round(ourModel$persistence,3))) Figure 4.5: SES with optimal smoothing parameter. This approach won’t guarantee that we will get the most appropriate \\(\\hat{\\alpha}\\). Still, it has been shown in the literature that the optimisation of smoothing parameters on average leads to improvements in terms of forecasting accuracy (see, for example, Fildes, 1992). 4.1.2 Why “exponential?” Now, why is it called “exponential”? Because the same method can be represented in a different form, if we substitute \\(\\hat{y}_{t}\\) in right hand side of (4.1) by the formula for the previous step: \\[\\begin{equation} \\begin{aligned} \\hat{y}_{t} = &amp;\\hat{\\alpha} {y}_{t-1} + (1 -\\hat{\\alpha}) \\hat{y}_{t-1}, \\\\ \\hat{y}_{t+1} = &amp;\\hat{\\alpha} {y}_{t} + (1 -\\hat{\\alpha}) \\hat{y}_{t} = \\\\ &amp; \\hat{\\alpha} {y}_{t} + (1 -\\hat{\\alpha}) \\left( \\hat{\\alpha} {y}_{t-1} + (1 -\\hat{\\alpha}) \\hat{y}_{t-1} \\right). \\end{aligned} \\tag{4.2} \\end{equation}\\] By repeating this procedure for each \\(\\hat{y}_{t-1}\\), \\(\\hat{y}_{t-2}\\) etc, we will obtain a different form of the method: \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{\\alpha} {y}_{t} + \\hat{\\alpha} (1 -\\hat{\\alpha}) {y}_{t-1} + \\hat{\\alpha} (1 -\\hat{\\alpha})^2 {y}_{t-2} + \\dots + \\hat{\\alpha} (1 -\\hat{\\alpha})^{t-1} {y}_{1} + (1 -\\hat{\\alpha})^t \\hat{y}_1 \\tag{4.3} \\end{equation}\\] or equivalently: \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{\\alpha} \\sum_{j=0}^{t-1} (1 -\\hat{\\alpha})^j {y}_{t-j} + (1 -\\hat{\\alpha})^t \\hat{y}_1 . \\tag{4.4} \\end{equation}\\] In the form (4.4), each actual observation has a weight infront of it. For the most recent observation it is equal to \\(\\hat{\\alpha}\\), for the previous one it is \\(\\hat{\\alpha} (1 -\\hat{\\alpha})\\), then \\(\\hat{\\alpha} (1 -\\hat{\\alpha})^2\\) etc. These form the geometric series or an exponential curve. Figure 4.6 shows an example with \\(\\hat{\\alpha} =0.25\\) for a sample of 30 observations: plot(0.25*(1-0.25)^c(0:30), type=&quot;b&quot;, xlab=&quot;Time lags&quot;, ylab=&quot;Weights&quot;) Figure 4.6: Example of weights distribution for \\(\\hat{\\alpha}=0.25\\) This explains the name “exponential.” The term “smoothing” comes from the idea that the parameter \\(\\hat{\\alpha}\\) should be selected so that the method smooths the original time series and does not react to noise. 4.1.3 Error correction form of SES Finally, there is an alternative form of SES, known as error correction form, which can be obtained after some simple permutations. Taking that \\(e_t=y_t-\\hat{y}_t\\) is the one step ahead forecast error, formula (4.1) can be written as: \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{y}_{t} + \\hat{\\alpha} e_{t}. \\tag{4.5} \\end{equation}\\] In this form, the smoothing parameter \\(\\hat{\\alpha}\\) has a different meaning: it regulates how much the model reacts to the forecast error. In this interpretation, it no longer needs to be restricted with (0, 1) region, but we would still typically want it to be closer to zero to filter out the noise, not to adapt to it. As you see, SES is a straightforward method. It is easy to explain to practitioners, and it is very easy to implement in practice. However, this is just a forecasting method (see Section 1.4), so it provides a way of generating point forecasts but does not explain where the error comes from and how to create prediction intervals. References "],["SESandETS.html", "4.2 SES and ETS", " 4.2 SES and ETS 4.2.1 ETS(A,N,N) There have been several tries to develop statistical models underlying SES, and we know now that the model has underlying ARIMA(0,1,1), local level MSOE (Multiple Source of Error, Muth, 1960) and SSOE (Single Source of Error, Snyder, 1985) models. According to Hyndman et al. (2002), the ETS(A,N,N) model also underlies the SES method. To see the connection and to get to it from SES, we need to recall two things: how in general, the actual value relates to the forecast error and the fitted value, and the error correction form of SES from Subsection 4.1.3: \\[\\begin{equation} \\begin{aligned} &amp; y_t = \\hat{y}_{t} + e_t \\\\ &amp; \\hat{y}_{t+1} = \\hat{y}_{t} + \\hat{\\alpha} e_{t} \\end{aligned} . \\tag{4.6} \\end{equation}\\] In order to get to the SSOE state space model for SES, we need to substitute \\(\\hat{y}_t=\\hat{l}_{t-1}\\), implying that the fitted value is equal to the level of the series: \\[\\begin{equation} \\begin{aligned} &amp; y_t = \\hat{l}_{t-1} + e_t \\\\ &amp; \\hat{l}_{t} = \\hat{l}_{t-1} + \\hat{\\alpha} e_{t} \\end{aligned} . \\tag{4.7} \\end{equation}\\] If we now substitute the sample estimates of level, smoothing parameter and forecast error by their population values, we will get the ETS(A,N,N), which was discussed in Section 3.5: \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = l_{t-1} + \\epsilon_t \\\\ &amp; l_t = l_{t-1} + \\alpha \\epsilon_t \\end{aligned} , \\tag{4.7} \\end{equation}\\] where, as we know from Section 3.1, \\(l_t\\) is the level of the data, \\(\\epsilon_t\\) is the error term, and \\(\\alpha\\) is the smoothing parameter. Note that we use \\(\\alpha\\) without the “hat” symbol, which implies that there is a “true” value of the parameter (which could be obtained if we had all the data in the world or just knew it for some reason). The main benefit of having the model (4.7) instead of just the method (4.5) is in having a flexible framework, which allows adding other components, selecting the most appropriate ones, consistently estimating parameters (see Section 4.3 of Svetunkov, 2021), producing prediction intervals etc. In order to see the data that corresponds to the ETS(A,N,N) we can use sim.es() function from smooth package. Here are several examples with different smoothing parameters values: # list with generated data y &lt;- vector(&quot;list&quot;,6) # Parameters for DGP initial &lt;- 1000 meanValue &lt;- 0 sdValue &lt;- 20 alphas &lt;- c(0.1,0.3,0.5,0.75,1,1.5) # Go through all alphas and generate respective data for(i in 1:length(alphas)){ y[[i]] &lt;- sim.es(&quot;ANN&quot;, 120, 1, 12, persistence=alphas[i], initial=initial, mean=meanValue, sd=sdValue) } The generated data can be plotted the following way: par(mfrow=c(3,2), mar=c(2,2,2,1)) for(i in 1:6){ plot(y[[i]], main=paste0(&quot;alpha=&quot;,y[[i]]$persistence), ylim=initial+c(-500,500)) } Figure 4.7: Local level data corresponding to ETS(A,N,N) model with different smoothing parameters. This simple simulation shows that the smoothing parameter in ETS(A,N,N) controls the variability in the data (Figure 4.7): the higher \\(\\alpha\\) is, the higher variability is and less predictable the data becomes. With the higher values of \\(\\alpha\\), the level changes faster, leading to increased uncertainty about the future values of the level in the data. When it comes to the application of this model to the data, the conditional h steps ahead mean corresponds to the point forecast and is equal to the last observed level: \\[\\begin{equation} \\mu_{y,t+h|t} = \\hat{y}_{t+h} = l_{t} , \\tag{4.8} \\end{equation}\\] this holds because it is assumed (see Section 1.4.1) that \\(\\mathrm{E}(\\epsilon_t)=0\\), which implies that the conditional h steps ahead expectation of the level in the model is (from the second equation in (4.7)): \\[\\begin{equation} \\mathrm{E}(l_{t+h}|t) = l_t + \\mathrm{E}(\\alpha\\sum_{j=1}^{h-1}\\epsilon_{t+j}|t) = l_t . \\tag{4.9} \\end{equation}\\] Here is an example of a forecast from ETS(A,N,N) with automatic parameter estimation using es() function from smooth package: # Generate the data y &lt;- sim.es(&quot;ANN&quot;, 120, 1, 12, persistence=0.3, initial=1000) # Apply ETS(A,N,N) model es(y$data, &quot;ANN&quot;, h=12, interval=TRUE, holdout=TRUE, silent=FALSE) Figure 4.8: An example of ETS(A,N,N) applied to the data generated from the same model. ## Time elapsed: 0.03 seconds ## Model estimated: ETS(ANN) ## Persistence vector g: ## alpha ## 0.4133 ## Initial values were optimised. ## ## Loss function type: likelihood; Loss function value: 533.7822 ## Error standard deviation: 34.3834 ## Sample size: 108 ## Number of estimated parameters: 3 ## Number of degrees of freedom: 105 ## Information criteria: ## AIC AICc BIC BICc ## 1073.564 1073.795 1081.611 1082.151 ## ## 95% parametric prediction interval was constructed ## 100% of values are in the prediction interval ## Forecast errors: ## MPE: -2.5%; sCE: -29.4%; Asymmetry: -71.3%; MAPE: 3.1% ## MASE: 0.808; sMAE: 3%; sMSE: 0.1%; rMAE: 0.836; rRMSE: 0.823 As we see from Figure 4.8, the true smoothing parameter is 0.3, but the estimated one is not exactly 0.3, which is expected because we deal with an in-sample estimation. Also, notice that with such a smoothing parameter, the prediction interval widens with the increase of the forecast horizon. If the smoothing parameter were lower, the bounds would not increase, but this might not reflect the uncertainty about the level correctly. Here is an example with \\(\\alpha=0.01\\) on the same data (Figure @ref(fig:ETSANNExamplealpha0.1)) ourModel &lt;- es(y$data, &quot;ANN&quot;, h=12, interval=TRUE, holdout=TRUE, silent=FALSE, persistence=0.01) (#fig:ETSANNExamplealpha0.1)ETS(A,N,N) with \\(\\hat{\\alpha}=0.01\\) applied to the data generated from the same model with \\(\\alpha=0.3\\). Figure @ref(fig:ETSANNExamplealpha0.1) shows that the prediction interval does not expand, but at the same time is wider than needed, and the forecast is biased – the model does not keep up to the fast-changing time series. So, it is essential to correctly estimate the smoothing parameters not only to approximate the data but also to produce a less biased point forecast and a more appropriate prediction interval. 4.2.2 ETS(M,N,N) Hyndman et al. (2008) demonstrate that there is another ETS model, underlying SES. It is the model with multiplicative error, which is formulated in the following way, as mentioned in Chapter 3.5: \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = l_{t-1}(1 + \\epsilon_t) \\\\ &amp; l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\end{aligned} , \\tag{4.10} \\end{equation}\\] where \\((1+\\epsilon_t)\\) corresponds to the \\(\\varepsilon_t\\) discussed in Section 3.1. In order to see the connection of this model with SES, we need to revert to the estimation of the model on the data again: \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = \\hat{l}_{t-1}(1 + e_t) \\\\ &amp; \\hat{l}_t = \\hat{l}_{t-1}(1 + \\hat{\\alpha} e_t) \\end{aligned} . \\tag{4.11} \\end{equation}\\] where one step ahead forecast is (Section 3.5) \\(\\hat{y}_t = \\hat{l}_{t-1}\\) and \\(e_t=\\frac{y_t -\\hat{y}_t}{\\hat{y}_t}\\). Substituting these values in second equation of (4.11) we obtain: \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{y}_t \\left(1 + \\hat{\\alpha} \\frac{y_t -\\hat{y}_t}{\\hat{y}_t} \\right) \\tag{4.12} \\end{equation}\\] Finally, opening the brackets, we get the SES in the form similar to (4.5): \\[\\begin{equation} \\hat{y}_{t+1} = \\hat{y}_t + \\hat{\\alpha} (y_t -\\hat{y}_t). \\tag{4.13} \\end{equation}\\] This example again demonstrates the difference between a forecasting method and a model. When we use SES, we ignore the distributional assumptions, which restricts the usage of the method. When we use a model, we assume a specific structure, which on the one hand, makes it more restrictive, but on the other hand, gives it additional features. The main ones in the case of ETS(M,N,N) in comparison with ETS(A,N,N) are: The variance of the actual values in ETS(M,N,N) increases with the increase of the level \\(l_{t}\\). This allows modelling heteroscedasticity situation in the data; If \\((1+\\epsilon_t)\\) is always positive, then the ETS(M,N,N) model will always produce only positive forecasts (both point and interval). This makes this model applicable in principle to the data with low levels. An alternative to (4.10) would be the model (4.7) applied to the data in logarithms (assuming that the data we work with is always positive), implying that: \\[\\begin{equation} \\begin{aligned} &amp; \\log y_{t} = l_{t-1} + \\epsilon_t \\\\ &amp; l_t = l_{t-1} + \\alpha \\epsilon_t \\end{aligned} . \\tag{4.14} \\end{equation}\\] However, to produce forecasts from (4.14), exponentiation is needed, making the application of the model more difficult than needed. The ETS(M,N,N), on the other hand, does not rely on exponentiation, making it safe in cases when the model produces very high values (e.g. exp(1000) returns infinity in R). Finally, the conditional h steps ahead mean of ETS(M,N,N) corresponds to the point forecast and is equal to the last observed level, but only if \\(\\mathrm{E}(1+\\epsilon_t)=1\\): \\[\\begin{equation} \\mu_{y,t+h|t} = \\hat{y}_{t+h} = l_{t} . \\tag{4.15} \\end{equation}\\] And here is an example with the ETS(M,N,N) data (Figure 4.9): y &lt;- sim.es(&quot;MNN&quot;, 120, 1, 12, persistence=0.3, initial=1000) ourModel &lt;- es(y$data, &quot;MNN&quot;, h=12, holdout=TRUE, interval=TRUE, silent=FALSE) Figure 4.9: ETS(M,N,N) model applied to the data generated from the same model. ourModel ## Time elapsed: 0.02 seconds ## Model estimated: ETS(MNN) ## Persistence vector g: ## alpha ## 0.2712 ## Initial values were optimised. ## ## Loss function type: likelihood; Loss function value: 593.1825 ## Error standard deviation: 0.0841 ## Sample size: 108 ## Number of estimated parameters: 3 ## Number of degrees of freedom: 105 ## Information criteria: ## AIC AICc BIC BICc ## 1192.365 1192.596 1200.411 1200.952 ## ## 95% parametric prediction interval was constructed ## 83% of values are in the prediction interval ## Forecast errors: ## MPE: -2.1%; sCE: -6%; Asymmetry: -23.1%; MAPE: 10.9% ## MASE: 1.178; sMAE: 9.2%; sMSE: 1.1%; rMAE: 1.086; rRMSE: 0.877 Conceptually, the data in Figure 4.9 looks very similar to the one from ETS(A,N,N) (Figure 4.8), but demonstrating the changing variance of the error term with the change of the level. References "],["ETSExamples.html", "4.3 Several examples of ETS and related exponential smoothing methods", " 4.3 Several examples of ETS and related exponential smoothing methods There are other exponential smoothing methods, which include more components, as discussed in Section 3.1. This includes but is not limited to: Holt’s (Holt, 2004, originally proposed in 1957), Holt-Winter’s (Winters, 1960), multiplicative trend (Pegels, 1969), Damped trend Gardner and McKenzie (1985), Damped trend Holt-Winters (Gardner and McKenzie, 1989) and damped multiplicative trend (James W. Taylor, 2003a) methods. We will not discuss them here one by one, as we will not use them further in this textbook. Instead, we will focus on the ETS models underlying them. We already understand that there can be different components in time series and that they can interact either in an additive or a multiplicative way, which gives us the taxonomy above, discussed in Section 3.4. This section considers several examples of ETS models and their relations to the conventional exponential smoothing methods. 4.3.1 ETS(A,A,N) This is also sometimes known as local trend model and is formulated similar to ETS(A,N,N), but with addition of the trend equation. It underlies Holt’s method (Ord et al., 1997): \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = l_{t-1} + b_{t-1} + \\epsilon_t \\\\ &amp; l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\ &amp; b_t = b_{t-1} + \\beta \\epsilon_t \\end{aligned} , \\tag{4.16} \\end{equation}\\] where \\(\\beta\\) is the smoothing parameter for the trend component. It has a similar idea as ETS(A,N,N): the states evolve, and the speed of their change depends on the values of \\(\\alpha\\) and \\(\\beta\\). The trend is not deterministic in this model: both the intercept and the slope change over time. The higher the smoothing parameters are, the more uncertain the level and the slope will be; thus, the higher the uncertainty about the future values is. Here is an example of the data that corresponds to the ETS(A,A,N) model: y &lt;- sim.es(&quot;AAN&quot;, 120, 1, 12, persistence=c(0.3,0.1), initial=c(1000,20), mean=0, sd=20) plot(y) Figure 4.10: Data generated from ETS(A,A,N) model. The series in Figure 4.10 demonstrates a trend that changes over time. If we needed to produce forecasts for this data, we would capture the dynamics of the trend component and then use the last values for the several steps ahead prediction. The point forecast h steps ahead from this model is a straight line with a slope \\(b_t\\) (as shown in Table 3.1 from Section 3.5): \\[\\begin{equation} \\mu_{y,t+h|t} = \\hat{y}_{t+h} = l_{t} + h b_t. \\tag{4.17} \\end{equation}\\] This becomes apparent if one takes the conditional expectations E\\((l_{t+h}|t)\\) and E\\((b_{t+h}|t)\\) in the second and third equations of (4.16) and then inserts them in the measurement equation. Graphically it will look as shown in Figure 4.11: esModel &lt;- es(y, h=10, silent=FALSE) Figure 4.11: ETS(A,A,N) and a point forecast produced from it. If you want to experiment with the model and see how its parameters influence the fit and forecast, you can use the following R code: esModel &lt;- es(y$data, h=10, silent=FALSE, persistence=c(0.2,0.1)) where persistence is the vector of smoothing parameters (first \\(\\hat\\alpha\\), then \\(\\hat\\beta\\)). By changing their values, we will make the model less/more responsive to the changes in the data. 4.3.2 ETS(A,Ad,N) This is the model that underlies Damped trend method (Roberts, 1982): \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = l_{t-1} + \\phi b_{t-1} + \\epsilon_t \\\\ &amp; l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\ &amp; b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\end{aligned} , \\tag{4.18} \\end{equation}\\] where \\(\\phi\\) is the dampening parameter, typically lying between 0 and 1. If it is equal to zero, the model reduces to ETS(A,N,N), (4.7). If it is equal to one, it becomes equivalent to ETS(A,A,N), (4.16). The dampening parameter slows down the trend, making it non-linear. An example of data that corresponds to ETS(A,Ad,N) is provided in Figure 4.12. y &lt;- sim.es(&quot;AAdN&quot;, 120, 1, 12, persistence=c(0.3,0.1), initial=c(1000,20), phi=0.95, mean=0, sd=20) plot(y) Figure 4.12: An example of ETS(A,Ad,N) data. Visually it is typically challenging to distinguish ETS(A,A,N) from ETS(A,Ad,N) data. So, some other model selection techniques are recommended (see Section 16.1). The point forecast from this model is a bit more complicated (see Section 3.5): \\[\\begin{equation} \\mu_{y,t+h|t} = \\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_t. \\tag{4.19} \\end{equation}\\] It corresponds to the slowing down trajectory, as shown in Figure 4.13. Figure 4.13: A point forecast from ETS(A,Ad,N). As can be seen in Figure 4.13, the forecast trajectory from the ETS(A,Ad,N) has a slowing down element in it. This is because of the \\(\\phi=0.95\\) in our example. 4.3.3 ETS(A,A,M) Finaly, this is an exotic model with additive error and trend, but multiplicative seasonality. Still, we list it here, because it underlies the Holt-Winters method (Winters, 1960): \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = (l_{t-1} + b_{t-1}) s_{t-m} + \\epsilon_t \\\\ &amp; l_t = l_{t-1} + b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp; b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\ &amp; s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1}+b_{t-1}} \\end{aligned} , \\tag{4.20} \\end{equation}\\] where \\(s_t\\) is the seasonal component and \\(\\gamma\\) is its smoothing parameter. This is one of the potentially unstable models, which due to the mix of components, might produce unreasonable forecasts because the seasonal component might become negative, while in the multiplicative model, it should always be positive. Still, it might work on the strictly positive high-level data. Figure 4.14 shows how the data for this model can look like. y &lt;- sim.es(&quot;AAM&quot;, 120, 1, 4, persistence=c(0.3,0.05,0.2), initial=c(1000,20), initialSeason=c(0.9,1.1,0.8,1.2), mean=0, sd=20) plot(y) Figure 4.14: An example of ETS(A,A,M) data. The data in Figure 4.14 exhibits an additive trend with increasing seasonal amplitude, which are the two characteristics of the model. Finally, the point forecast from this model build upon the ETS(A,A,N), introducing seasonal component: \\[\\begin{equation} \\hat{y}_{t+h} = (l_{t} + h b_t) s_{t+h-m\\lceil\\frac{h}{m}\\rceil}, \\tag{4.21} \\end{equation}\\] where \\(\\lceil\\frac{h}{m}\\rceil\\) is the rounded up value of the fraction in the brackets. The point forecast from this model is shown in Figure 4.15. Figure 4.15: A point forecast from ETS(A,A,M). Remark. The point forecasts produced from this model do not correspond to the conditional expectations. This was discussed in Section 3.5. Hyndman et al. (2008) argue that in ETS models, the error term should be aligned with the seasonal component because it is difficult to motivate why the amplitude of seasonality increases with the increase of level, while the variability of the error term stays the same. So, they recommend using ETS(M,A,M) instead of ETS(A,A,M) if you deal with positive high volume data. This is a reasonable recommendation, but keep in mind that both models might break if you deal with low volume data. References "],["ets-assumptions-estimation-and-selection.html", "4.4 ETS assumptions, estimation and selection", " 4.4 ETS assumptions, estimation and selection Several assumptions need to hold for the conventional ETS models to work properly. Some of them have already been discussed in Section 1.4.1, and we will not discuss them here again. What is important in our context is that the conventional ETS assumes that the error term \\(\\epsilon_t\\) follows the normal distribution with zero mean and variance \\(\\sigma^2\\). The normal distribution is defined for positive, negative and zero values. This is not a big deal for additive models, which assume that the actual value can be anything. And it is not an issue for the multiplicative models when we deal with high-level positive data (e.g. thousands of units): the variance of the error term will be small enough for the \\(\\epsilon_t\\) not to become less than minus one. However, if the level of the data is low, then the variance of the error term can be large enough for the normally distributed error to cover negative values, less than minus one. This implies that the error term \\(1+\\epsilon_t\\) can become negative, and the model will break. This is a potential flaw in the conventional ETS model with the multiplicative error term. So, what the standard multiplicative error ETS model assumes, in fact, is that the data we work with is strictly positive and has high-level values. Based on the assumption of normality of error term, the ETS model can be estimated via the maximisation of likelihood (see Chapter 13 of Svetunkov, 2021), which is equivalent to the minimisation of the mean squared one step ahead forecast error \\(e_t\\). Note that in order to apply the ETS models to the data, we also need to know the initial values of components, \\(\\hat{l}_0, \\hat{b}_0, \\hat{s}_{-m+2}, \\hat{s}_{-m+3}, \\dots, \\hat{s}_{0}\\). The conventional approach is to estimate these values together with the smoothing parameters during likelihood maximisation. As a result, the optimisation might involve a large number of parameters. In addition, the variance of the error term is considered as an additional parameter in the maximum likelihood estimation, so the number of parameters for different models is (here \"*\" stands for any type): ETS(*,N,N) – 3 parameters: \\(\\hat{l}_0\\), \\(\\hat{\\alpha}\\) and \\(\\hat{\\sigma}^2\\); ETS(*,*,N) – 5 parameters: \\(\\hat{l}_0\\), \\(\\hat{b}_0\\), \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\); ETS(*,*d,N) – 6 parameters: \\(\\hat{l}_0\\), \\(\\hat{b}_0\\), \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}\\), \\(\\hat{\\phi}\\) and \\(\\hat{\\sigma}^2\\); ETS(*,N,*) – 4+m-1 parameters: \\(\\hat{l}_0\\), \\(\\hat{s}_{-m+2}, \\hat{s}_{-m+3}, \\dots, \\hat{s}_{0}\\), \\(\\hat{\\alpha}\\), \\(\\hat{\\gamma}\\) and \\(\\hat{\\sigma}^2\\); ETS(*,*,*) – 6+m-1 parameters: \\(\\hat{l}_0\\), \\(\\hat{b}_0\\), \\(\\hat{s}_{-m+2}, \\hat{s}_{-m+3}, \\dots, \\hat{s}_{0}\\), \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}\\), \\(\\hat{\\gamma}\\) and \\(\\hat{\\sigma}^2\\); ETS(*,*d,*) – 7+m-1 parameters: \\(\\hat{l}_0\\), \\(\\hat{b}_0\\), \\(\\hat{s}_{-m+2}, \\hat{s}_{-m+3}, \\dots, \\hat{s}_{0}\\), \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}\\), \\(\\hat{\\gamma}\\), \\(\\hat{\\phi}\\) and \\(\\hat{\\sigma}^2\\). Note that in the case of seasonal models, we typically make sure that the initial seasonal indices are normalised, so we only need to estimate \\(m-1\\) of them, the last one is calculated based on the linear combination of the others. For example, for the additive seasonality, it is equal to \\(-\\sum_{j=1}^{m-1} s_j\\) because the sum of all the indices should be equal to zero. When it comes to selecting the most appropriate model, the conventional approach involves the application of all models to the data and then selecting the most appropriate of them based on an information criterion (see Section 13.4 of Svetunkov, 2021). This was first proposed by Hyndman et al. (2002). In the case of the conventional ETS model, this relies on the likelihood value of normal distribution used in the estimation of the model. Finally, the assumption of normality is used to generate prediction intervals from the model. There are typically two ways of doing that: Calculating the variance of multiple steps ahead forecast error and then using it for the intervals construction (see Chapter 6 of Hyndman et al., 2008); Generating thousands of possible paths for the components of the series and the actual values and then taking the necessary quantiles for the prediction intervals; Typically, (1) is applied for pure additive models, where the closed forms for the variances are known, and the assumption of normality holds for several steps ahead. In some special cases of mixed models, approximations for variances work on short horizons (see Section 6.4 of Hyndman et al., 2008). But in all the other cases, (2) should be used, despite being typically slower than (1) and producing bounds that differ slightly from run to run due to randomness. References "],["ETSConventionalModel.html", "4.5 State space form of ETS", " 4.5 State space form of ETS One of the main advantages of the ETS model is its state space form, which gives it the flexibility. Hyndman et al. (2008) use the following general formulation of the model with the first equation called “measurement equation” and the second one “transition equation”: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = w(\\mathbf{v}_{t-1}) + r(\\mathbf{v}_{t-1}) \\epsilon_t \\\\ &amp; \\mathbf{v}_{t} = f(\\mathbf{v}_{t-1}) + g(\\mathbf{v}_{t-1}) \\epsilon_t \\end{aligned}, \\tag{4.22} \\end{equation}\\] where \\(\\mathbf{v}_t\\) is the state vector, containing the components of series (level, trend and seasonal), \\(w(\\cdot)\\) is the measurement, \\(r(\\cdot)\\) is the error, \\(f(\\cdot)\\) is the transition and \\(g(\\cdot)\\) is the persistence functions. Depending on the types of components these functions can have different values: Depending on the types of trend and seasonality \\(w(\\mathbf{v}_{t-1})\\) will be equal either to the addition or multiplication of components. The special cases were presented in tables 3.1 and 3.2 in the Section 3.5. For example, in case of ETS(M,M,M) it is: \\(w(\\mathbf{v}_{t-1}) = l_{t-1} b_{t-1} s_{t-m}\\); If the error is additive, then \\(r(\\mathbf{v}_{t-1})=1\\), otherwise (in case of multiplicative error) it is \\(r(\\mathbf{v}_{t-1})=w(\\mathbf{v}_{t-1})\\). For example, for ETS(M,M,M) it will be \\(r(\\mathbf{v}_{t-1}) = l_{t-1} b_{t-1} s_{t-m}\\); The transition function \\(f(\\cdot)\\) will produce values depending on the types of trend and seasonality and will correspond to the first parts in the Tables 3.1 and 3.2 of the transition equations (dropping the error term). This function records how components interact with each other and how they change from one observation to another (thus the term “transition”). An example is the ETS(M,M,M) model, for which the transition function will produce three values: \\(l_{t-1}b_{t-1}\\), \\(b_{t-1}\\) and \\(s_{t-m}\\) respectively for the level, trend and seasonal components. So, if we drop the persistence function \\(g(\\cdot)\\) and the error term \\(\\epsilon_t\\) for a moment, the second equation in (4.22) will be: \\[\\begin{equation} \\begin{aligned} &amp; {l}_{t} = l_{t-1} b_{t-1} \\\\ &amp; b_t = b_{t-1} \\\\ &amp; s_t = s_{t-m} \\end{aligned}, \\tag{4.23} \\end{equation}\\] Finally, the persistence function will differ from one model to another, but in some special cases it can either be: \\(g(\\mathbf{v}_{t-1})=\\mathbf{g}\\) if all components are additive, or \\(g(\\mathbf{v}_{t-1})=f(\\mathbf{v}_{t-1})\\mathbf{g}\\) if they are all multiplicative. \\(\\mathbf{g}\\) is the vector of smoothing parameters, called in the ETS context the “persistence vector.” An example of persistence function is the ETS(M,M,M) model, for which it is: \\(l_{t-1}b_{t-1}\\alpha\\), \\(b_{t-1}\\beta\\) and \\(s_{t-m}\\gamma\\) respectively for the level, trend and seasonal components. Uniting this with the transition function (4.23) we get the equation from the table 3.2: \\[\\begin{equation} \\begin{aligned} &amp; {l}_{t} = l_{t-1} b_{t-1} + l_{t-1} b_{t-1} \\alpha\\epsilon_t \\\\ &amp; b_t = b_{t-1} + b_{t-1} \\beta\\epsilon_t \\\\ &amp; s_t = s_{t-m} + s_{t-m} \\gamma\\epsilon_t \\end{aligned}, \\tag{4.24} \\end{equation}\\] which can be simplified to: \\[\\begin{equation} \\begin{aligned} &amp; {l}_{t} = l_{t-1}b_{t-1} (1+\\alpha\\epsilon_t)\\\\ &amp; b_t = b_{t-1} (1+\\beta\\epsilon_t)\\\\ &amp; s_t = s_{t-m} (1+\\gamma\\epsilon_t) \\end{aligned} . \\tag{4.25} \\end{equation}\\] Some of mixed models have more complicated persistence function values. For example, for ETS(A,A,M) it is: \\[\\begin{equation} g(\\mathbf{v}_{t-1}) = \\begin{pmatrix} \\alpha \\frac{1}{s_{t-m}} \\\\ \\beta \\frac{1}{s_{t-m}} \\\\ \\gamma \\frac{1}{l_{t-1} + b_{t-1}} \\end{pmatrix} , \\end{equation}\\] which results in the state space model discussed in subsection 4.3.3. The compact form (4.22) is thus convenient, it underlies all the 30 ETS models discussed in the Sections 3.4 and 3.5. Unfortunately, they cannot be used directly for deriving conditional values, so they are needed just for the general understanding of ETS and can be used in coding. 4.5.1 Pure additive state space model The more useful state space model in ETS framework is the pure additive one, which, based on the discussion above, is formulated as: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = \\mathbf{w}&#39; \\mathbf{v}_{t-1} + \\epsilon_t \\\\ &amp; \\mathbf{v}_{t} = \\mathbf{F} \\mathbf{v}_{t-1} + \\mathbf{g} \\epsilon_t \\end{aligned}, \\tag{4.26} \\end{equation}\\] where \\(\\mathbf{w}\\) is the measurement vector, showing how the components form the structure, \\(\\mathbf{F}\\) is the transition matrix, showing how components interact with each other and change over time (e.g. level is equal to the previous level plus trend) and \\(\\mathbf{g}\\) is the persistence vector, containing smoothing parameters. The conditional expectation and variance can be derived based on (4.26), together with bounds on the smoothing parameters for any model that can be formulated in this way. And, as mentioned above, any pure additive ETS model can be written in the form (4.26), which means that all of them have relatively simple analytical formulae for the statistics mentioned above. For example, the h steps ahead conditional expectation and variance of the model (4.26) are (Hyndman et al., 2008, Chapter 6): \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = \\mathrm{E}(y_{t+h}|t) = &amp; \\mathbf{w}^\\prime \\mathbf{F}^{h-1} \\mathbf{v}_{t} \\\\ \\sigma^2_{h} = \\mathrm{V}(y_{t+h}|t) = &amp; \\left(\\mathbf{w}^\\prime \\mathbf{F}^{j-1} \\mathbf{g} \\mathbf{g}^\\prime \\mathbf{F}^\\prime \\mathbf{w} + 1 \\right) \\sigma^2 \\end{aligned}, \\tag{4.27} \\end{equation}\\] where \\(\\sigma^2\\) is the variance of the error term. Formulae (4.27) can be used for generation of respective moments from any pure additive ETS model. The conditional expectation can also be used for some mixed models as an approximation for the true conditional mean. References "],["ETSParametersBounds.html", "4.6 Parameters bounds", " 4.6 Parameters bounds While many practitioners and academics accept that the smoothing parameters of ETS models should lie between zero and one, this is not entirely true for the models. There are, in fact, several possible restrictions on smoothing parameters, and it is worth discussing them separately: Classical or conventional bounds are \\(\\alpha, \\beta, \\gamma \\in (0,1)\\). The idea behind them originates from the simple exponential smoothing method (Section 4.1), where it is logical to restrict the bounds with this region because then the smoothing parameters regulate what weight the actual value \\(y_t\\) will have and what weight will be assigned to the predicted one \\(\\hat{y}_t\\). Hyndman et al. (2008) showed that this condition is sometimes too loose and, in other cases, is too restrictive to some ETS models. Brenner et al. (1968) was one of the first to show that the bounds are wider than this region for many exponential smoothing methods. Still, the conventional restriction is the most often used in practice, just because it is nice to work with. Usual or traditional bounds are those that satisfy the set of the following equations: \\[\\begin{equation} \\begin{aligned} &amp;\\alpha \\in [0, 1)\\\\ &amp;\\beta \\in [0, \\alpha) \\\\ &amp;\\gamma \\in [0, 1-\\alpha) \\end{aligned}, \\tag{4.28} \\end{equation}\\] This set of restrictions guarantees that the weights decline over time exponentially (see Section 4.1.2), and the ETS models have the property of “averaging” the values over time. In the lower boundary condition, the model’s components become deterministic, and we can say that they are calculated as the global averages of the values over time. Admissible bounds, satisfying stability condition. The idea here is that the most recent observation should have a higher weight than the older ones, which is regulated via the smoothing parameters. However, in this case, we do not impose the restriction of exponential decay of weights on the models, so they can oscillate or decay harmonically as long as their absolute values decrease over time. The condition is more complicated mathematically than the previous two. It will be discussed later in the textbook for the pure additive models (see Section 5.1), but here are several examples for bounds, satisfying this condition (from Chapter 10 of Hyndman et al., 2008): ETS(A,N,N): \\(\\alpha \\in (0, 2)\\); ETS(A,A,N): \\(\\alpha \\in (0, 2); \\beta \\in (0, 4-2\\alpha)\\); ETS(A,N,A): \\(\\alpha \\in \\left(\\frac{-2}{m-1}, 2-\\gamma\\right); \\gamma \\in (\\max(-m\\alpha, 0), 2-\\alpha)\\); As you see, the admissible bounds are much wider than the conventional and usual ones. In fact, smoothing parameters can become either negative or greater than one in some cases for some models, which is hard to interpret but might indicate that the data is difficult to predict. Furthermore, the admissible bounds correspond to the restrictions of the parameters for ARIMA models, underlying some of pure additive ETS models. In a way, they are more natural for the ETS models than the other two because they follow the formulation and arise naturally. However, their usage in practice has been met with mixed success, with only a handful of papers using them instead of (1) or (2) (e.g. Gardner and Diaz-Saiz, 2008; mention that they appear in some cases and Snyder et al., 2017 use them in their model). The admissible bounds are calculated based on the discount matrix in the R code, which will be discussed in the context of pure additive ADAM ETS models in the chapter 5.1. References "],["ADAMETSIntroduction.html", "Chapter 5 Pure additive ADAM ETS", " Chapter 5 Pure additive ADAM ETS Now that we are familiar with the conventional ETS, we can move to the discussion of ADAM implementation, which has several important differences from the classical one. We start the discussion with the pure additive models, which are much easier to work with than other ETS models. This chapter focuses on technical details of the model, discussing general formulation in algebraic form, then moving to recursive relations, which are needed to understand how to produce forecasts from the model and how to estimate it correctly (i.e. impose restrictions on the parameters). Finally, we discuss the distributional assumptions for ADAM ETS, introducing not only the Normal distribution but also showing how to use Laplace, S, Generalised Normal, Log-Normal, Gamma and Inverse Gaussian distributions in the context. "],["ADAMETSPureAdditive.html", "5.1 Model formulation", " 5.1 Model formulation The pure additive case is interesting, because this is the group of models that have closed forms for both conditional mean and variance. In order to understand how we can get to the general model form, we consider an example of ETS(A,A,A) model, which, as discussed in Section 3.5, is formulated as: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; l_{t-1} &amp; + &amp; b_{t-1} &amp; + &amp; s_{t-m} &amp; + &amp; \\epsilon_t \\\\ l_t = &amp; l_{t-1} &amp; + &amp; b_{t-1} &amp; &amp; &amp; + &amp; \\alpha \\epsilon_t \\\\ b_t = &amp; &amp; &amp; b_{t-1} &amp; &amp; &amp; + &amp; \\beta \\epsilon_t \\\\ s_t = &amp; &amp; &amp; &amp; &amp; s_{t-m} &amp; + &amp; \\gamma \\epsilon_t \\end{aligned}. \\tag{5.1} \\end{equation}\\] The same model can be represented in the matrix form based on (5.1): \\[\\begin{equation} \\begin{aligned} y_t &amp; = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} l_{t-1} \\\\ b_{t-1} \\\\ s_{t-m} \\end{pmatrix} + \\epsilon_t \\\\ \\begin{pmatrix} l_t \\\\ b_t \\\\ s_t \\end{pmatrix} &amp; = \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} l_{t-1} \\\\ b_{t-1} \\\\ s_{t-m} \\end{pmatrix} + \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix} \\epsilon_t \\end{aligned}. \\tag{5.2} \\end{equation}\\] I use spaces in equation (5.1) to show how the matrix form is related to the general one. The positions of \\(l_{t-1}\\), \\(b_{t-1}\\) and \\(s_{t-m}\\) correspond to the non-zero values in the matrices in (5.2). Now we can give names to each matrix and vector, for example: \\[\\begin{equation} \\begin{aligned} \\mathbf{w} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, &amp; \\mathbf{F} = \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}, &amp; \\mathbf{g} = \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix}, \\\\ \\mathbf{v}_{t} = \\begin{pmatrix} l_t \\\\ b_t \\\\ s_t \\end{pmatrix}, &amp; \\mathbf{v}_{t-\\mathbf{l}} = \\begin{pmatrix} l_{t-1} \\\\ b_{t-1} \\\\ s_{t-m} \\end{pmatrix}, &amp; \\mathbf{l} = \\begin{pmatrix} 1 \\\\ 1 \\\\ m \\end{pmatrix} \\end{aligned}. \\tag{5.3} \\end{equation}\\] Substituting (5.3) into (5.2), we get the general pure additive ADAM ETS model: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp;\\mathbf{w}^\\prime \\mathbf{v}_{t-\\mathbf{l}} + \\epsilon_t \\\\ \\mathbf{v}_{t} = &amp;\\mathbf{F} \\mathbf{v}_{t-\\mathbf{l}} + \\mathbf{g} \\epsilon_t \\end{aligned}, \\tag{5.4} \\end{equation}\\] where \\(\\mathbf{w}\\) is the measurement vector, \\(\\mathbf{F}\\) is the transition matrix, \\(\\mathbf{g}\\) is the persistence vector and \\(\\mathbf{v}_{t-\\mathbf{l}}\\) is the vector of lagged components and \\(\\mathbf{l}\\) is the vector of lags. The important thing to note is that the ADAM model is based on the model discussed in Section 4.5.1. It is formulated using lags of components rather than their transition over time. This comes to the elements of the vector \\(\\mathbf{l}\\). Just for the comparison, the conventional ETS(A,A,A), formulated according to (4.22) would have the following transition matrix (instead of (5.3)): \\[\\begin{equation} \\mathbf{F} = \\begin{pmatrix} 1 &amp; 1 &amp; \\mathbf{0}^\\prime_{m-1} &amp; 0 \\\\ 0 &amp; 1 &amp; \\mathbf{0}^\\prime_{m-1} &amp; 0 \\\\ 0 &amp; 0 &amp; \\mathbf{0}^\\prime_{m-1} &amp; 1 \\\\ \\mathbf{0}_{m-1} &amp; \\mathbf{0}_{m-1} &amp; \\mathbf{I}_{m-1} &amp; \\mathbf{0}_{m-1} \\end{pmatrix}, \\tag{5.5} \\end{equation}\\] where \\(\\mathbf{I}_{m-1}\\) is the identity matrix of the size \\((m-1) \\times (m-1)\\) and \\(\\mathbf{0}_{m-1}\\) is the vector of zeroes of size \\(m-1\\). The main benefit of using the vector of lags \\(\\mathbf{l}\\) instead of the conventional mechanism in the transition equation is in the reduction of dimensions of matrices (the transition matrix contains \\(3\\times 3\\) elements in case of ETS(A,A,A) instead of \\((2+m)\\times (2+m)\\) as for the conventional ETS model). The model (5.4) is more parsimonious than the conventional one and simplifies some of the calculations, making it realistic, for example, to apply models to data with large frequency \\(m\\) (e.g. 24, 48, 52, 365). The main disadvantage of this approach is in the complications arising in the derivation of conditional expectation and variance, which still have closed forms, but are more cumbersome. They are discussed later in this chapter in Section 5.3. "],["adamETSPureAdditiveRecursive.html", "5.2 Recursive relation", " 5.2 Recursive relation A useful thing that can be derived from the pure additive model (5.4) is the recursive value, which can be used for further inference. First, when we produce forecast for \\(h\\) steps ahead, it is important to understand what the actual value \\(h\\) steps ahead might be, given all the information we have on the observation \\(t\\): \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t+h} = \\mathbf{w}^\\prime \\mathbf{v}_{t+h-\\mathbf{l}} + \\epsilon_{t+h} \\\\ &amp; \\mathbf{v}_{t+h} = \\mathbf{F} \\mathbf{v}_{t+h-\\mathbf{l}} + \\mathbf{g} \\epsilon_{t+h} \\end{aligned}, \\tag{5.6} \\end{equation}\\] where \\(\\mathbf{v}_{t+h-\\mathbf{l}}\\) is the vector of previous states, given the lagged values \\(\\mathbf{l}\\). In order to obtain the recursion, we need to split the measurement and persisitence vectors together with the transition matrix into parts for the same lags of components, leading to the following transition equation in (5.6): \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t+h} = (\\mathbf{w}_{m_1}^\\prime + \\mathbf{w}_{m_2}^\\prime + \\dots + \\mathbf{w}_{m_d}^\\prime) \\mathbf{v}_{t-h{l}} + \\epsilon_{t+h} \\\\ &amp; \\mathbf{v}_{t+h} = (\\mathbf{F}_{m_1} + \\mathbf{F}_{m_2} + \\dots + \\mathbf{F}_{m_d}) \\mathbf{v}_{t-h{l}} + (\\mathbf{g}_{m_1} + \\mathbf{g}_{m_2} + \\dots \\mathbf{g}_{m_d}) \\epsilon_{t+h} \\end{aligned}, \\tag{5.7} \\end{equation}\\] where \\(m_1, m_2, \\dots, m_d\\) are the distinct seasonal frequencies. So, for example, in case of ETS(A,A,A) model on quarterly data (periodicity is equal to four), \\(m_1=1\\), \\(m_2=4\\), leading to \\(\\mathbf{F}_{1} = \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}\\) and \\(\\mathbf{F}_{4} = \\begin{pmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\), where the split of the transition matrix is done column-wise. This split of matrices and vectors into distinct sub matrices and subvectors is needed in order to get the correct recursion and obtain the correct conditional expectation and variance. By substituting the values in the transition equation of (5.7) with their previous values until we reach \\(t\\), we get: \\[\\begin{equation} \\begin{aligned} \\mathbf{v}_{t-h{l}} = &amp; \\mathbf{F}_{m_1}^{\\lceil\\frac{h}{m_1}\\rceil-1} \\mathbf{v}_{t} + \\sum_{j=1}^{\\lceil\\frac{h}{m_1}\\rceil-1} \\mathbf{F}_{m_1}^{j-1} \\mathbf{g}_{m_1} \\epsilon_{t+m_1\\lceil\\frac{h}{m_1}\\rceil-j} + \\\\ &amp; \\mathbf{F}_{m_2}^{\\lceil\\frac{h}{m_2}\\rceil-1} \\mathbf{v}_{t} + \\sum_{j=1}^{\\lceil\\frac{h}{m_2}\\rceil-1} \\mathbf{F}_{m_2}^{j-1} \\mathbf{g}_{m_2} \\epsilon_{t+m_2\\lceil\\frac{h}{m_2}\\rceil-j} + \\\\ &amp; \\dots \\\\ &amp; \\mathbf{F}_{m_d}^{\\lceil\\frac{h}{m_d}\\rceil-1} \\mathbf{v}_{t} + \\sum_{j=1}^{\\lceil\\frac{h}{m_d}\\rceil-1} \\mathbf{F}_{m_d}^{j-1} \\mathbf{g}_{m_d} \\epsilon_{t+m_d\\lceil\\frac{h}{m_d}\\rceil-j} \\end{aligned}. \\tag{5.8} \\end{equation}\\] Inserting (5.8) in the measurement equation of (5.7), we get: \\[\\begin{equation} \\begin{aligned} y_{t+h} = &amp; \\mathbf{w}_{m_1}^\\prime \\mathbf{F}_{m_1}^{\\lceil\\frac{h}{m_1}\\rceil-1} \\mathbf{v}_{t} + \\mathbf{w}_{m_1}^\\prime \\sum_{j=1}^{\\lceil\\frac{h}{m_1}\\rceil-1} \\mathbf{F}_{m_1}^{j-1} \\mathbf{g}_{m_1} \\epsilon_{t+m_1\\lceil\\frac{h}{m_1}\\rceil-j} + \\\\ &amp; \\mathbf{w}_{m_2}^\\prime \\mathbf{F}_{m_2}^{\\lceil\\frac{h}{m_2}\\rceil-1} \\mathbf{v}_{t} + \\mathbf{w}_{m_2}^\\prime \\sum_{j=1}^{\\lceil\\frac{h}{m_2}\\rceil-1} \\mathbf{F}_{m_2}^{j-1} \\mathbf{g}_{m_2} \\epsilon_{t+m_2\\lceil\\frac{h}{m_2}\\rceil-j} + \\\\ &amp; \\dots + \\\\ &amp; \\mathbf{w}_{m_d}^\\prime \\mathbf{F}_{m_d}^{\\lceil\\frac{h}{m_d}\\rceil-1} \\mathbf{v}_{t} + \\mathbf{w}_{m_d}^\\prime \\sum_{j=1}^{\\lceil\\frac{h}{m_d}\\rceil-1} \\mathbf{F}_{m_d}^{j-1} \\mathbf{g}_{m_d} \\epsilon_{t+m_d\\lceil\\frac{h}{m_d}\\rceil-j} + \\\\ &amp; \\epsilon_{t+h} \\end{aligned}. \\tag{5.9} \\end{equation}\\] Substituting the specific values of \\(m_1, m_2, \\dots, m_d\\) in (5.9) will simplify the equation and make it easier to understand. For example, for ETS(A,N,N), \\(m_1=1\\) and all the other frequencies are equal to zero, so the recursion (5.9) simplifies to: \\[\\begin{equation} y_{t+h} = \\mathbf{w}_{1}^\\prime \\mathbf{F}_{1}^{h-1} \\mathbf{v}_{t} + \\mathbf{w}_{1}^\\prime \\sum_{j=1}^{h-1} \\mathbf{F}_{1}^{j-1} \\mathbf{g}_{1} \\epsilon_{t+h-j} + \\epsilon_{t+h} , \\tag{5.10} \\end{equation}\\] which is the recursion obtained by Hyndman et al. (2008), page 103. References "],["pureAdditiveExpectationAndVariance.html", "5.3 Conditional expectation and variance", " 5.3 Conditional expectation and variance Now, why is the recursion (5.9) important? This is because we can take the expectation and variance of (5.9) conditional on the values of the state vector \\(\\mathbf{v}_{t}\\) on the observation \\(t\\) (assuming that the error term is homoscedastic, uncorrelated and has the expectation of zero) in order to get: \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = \\text{E}(y_{t+h}|t) = &amp; \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i}^\\prime \\mathbf{F}_{m_i}^{\\lceil\\frac{h}{m_i}\\rceil-1} \\right) \\mathbf{v}_{t} \\\\ \\sigma^2_{h} = \\text{V}(y_{t+h}|t) = &amp; \\left( \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i}^\\prime \\sum_{j=1}^{\\lceil\\frac{h}{m_i}\\rceil-1} \\mathbf{F}_{m_i}^{j-1} \\mathbf{g}_{m_i} \\mathbf{g}^\\prime_{m_i} (\\mathbf{F}_{m_i}^\\prime)^{j-1} \\mathbf{w}_{m_i} \\right) + 1 \\right) \\sigma^2 \\end{aligned}, \\tag{5.11} \\end{equation}\\] where \\(\\sigma^2\\) is the variance of the error term. These two formulae are cumbersome, but they give the analytical solutions to the two moments. Having obtained both of them, we can construct prediction intervals, assuming, for example, that the error term follows normal distribution (see Section 18.2 for details): \\[\\begin{equation} y_{t+h} \\in \\left( \\text{E}(y_{t+h}|t) + z_{\\frac{\\alpha}{2}} \\sqrt{\\text{V}(y_{t+h}|t)}, \\text{E}(y_{t+h}|t) + z_{1-\\frac{\\alpha}{2}} \\sqrt{\\text{V}(y_{t+h}|t)} \\right), \\tag{5.12} \\end{equation}\\] where \\(z_{\\frac{\\alpha}{2}}\\) is quantile of standardised normal distribution for the level \\(\\alpha\\). When it comes to other distributions (see Section 5.5), in order to get the conditional h steps ahead scale parameter, we can first calculate the variance using (5.11) and then using the relation between the scale and the variance for the specific distribution (see discussion in Chapter 3 of Svetunkov, 2021) to get the necessary value. 5.3.1 Example with ETS(A,N,N) For example, for the ETS(A,N,N) model, discussed above, we get: \\[\\begin{equation} \\begin{aligned} \\text{E}(y_{t+h}|t) = &amp; \\mathbf{w}_{1}^\\prime \\mathbf{F}_{1}^{h-1} \\mathbf{v}_{t} \\\\ \\text{V}(y_{t+h}|t) = &amp; \\left(\\mathbf{w}_{1}^\\prime \\sum_{j=1}^{h-1} \\mathbf{F}_{1}^{j-1} \\mathbf{g}_{1} \\mathbf{g}^\\prime_{1} (\\mathbf{F}_{1}^\\prime)^{j-1} \\mathbf{w}_{1} + 1 \\right) \\sigma^2 \\end{aligned}, \\tag{5.13} \\end{equation}\\] or by substituting \\(\\mathbf{F}=1\\), \\(\\mathbf{w}=1\\), \\(\\mathbf{g}=\\alpha\\) and \\(\\mathbf{v}_t=l_t\\): \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = &amp; l_{t} \\\\ \\sigma^2_{h} = &amp; \\left((h-1) \\alpha^2 + 1 \\right) \\sigma^2 \\end{aligned}, \\tag{5.14} \\end{equation}\\] which is the same conditional expectation and variance as in the Section 3.5 and in the Hyndman et al. (2008) textbook. References "],["stabilityConditionAdditiveError.html", "5.4 Stability and forecastability conditions", " 5.4 Stability and forecastability conditions Another important aspect of the pure additive model (5.4) is the restriction on the smoothing parameters. This is related to the stability and forecastability conditions of the model. The stability implies that the weights for observations decay over time (Section 4.1.2), guaranteeing that the newer ones will have higher weights than the older ones. If this condition holds, the model behaves “steadily,” forgetting the past values eventually. The forecastability does not guarantee that the weights decay, but it guarantees that the initial value of the state vector will have a constant impact on forecasts, i.e. will not increase in weight with the increase of the forecast horizon. An example of the non-stable, but forecastable model is ETS(A,N,N) with \\(\\alpha=0\\). In this case, it reverts to the global level model (Section 3.3.2), where the initial value impacts the final forecast in the same way as it does for the first observation. In order to obtain both conditions, we need to use a reduced form of ETS by inserting the measurement equation in the transition equation via \\(\\epsilon_t= {y}_{t} -\\mathbf{w}^\\prime \\mathbf{v}_{t-\\mathbf{l}}\\): \\[\\begin{equation} \\begin{aligned} \\mathbf{v}_{t} = &amp;\\mathbf{F} \\mathbf{v}_{t-\\mathbf{l}} + \\mathbf{g} \\left({y}_{t} -\\mathbf{w}^\\prime \\mathbf{v}_{t-\\mathbf{l}} \\right)\\\\ = &amp; \\left(\\mathbf{F} -\\mathbf{g}\\mathbf{w}^\\prime \\right) \\mathbf{v}_{t-\\mathbf{l}} + \\mathbf{g} {y}_{t} \\\\ \\end{aligned}. \\tag{5.15} \\end{equation}\\] The matrix \\(\\mathbf{D}=\\mathbf{F} -\\mathbf{g}\\mathbf{w}^\\prime\\) is called the discount matrix and it shows how the weights diminish over time. It is the main part of the model that determines, whether the model will be stable / forecastable or not. 5.4.1 Example with ETS(A,N,N) In order to better understand what we plan to discuss in this section, consider an example of ETS(A,N,N) model, for which \\(\\mathbf{F}=1\\), \\(\\mathbf{w}=1\\), \\(\\mathbf{g}=\\alpha\\), \\(\\mathbf{v}_t=l_t\\) and \\(\\mathbf{l}=1\\). Inserting these values in (5.15), we get: \\[\\begin{equation} \\begin{aligned} l_{t} = &amp; \\left(1 -\\alpha \\right) {l}_{t-1} + \\alpha {y}_{t}, \\end{aligned}. \\tag{5.16} \\end{equation}\\] which corresponds to the formula of Simple Exponential Smoothing from Section 4.1. The discount matrix, in this case, is \\(\\mathbf{D}=1-\\alpha\\). If we now substitute the values for the level on the right-hand side of the equation (5.16) by the previous values of the level, we will obtain the recursion that we have already discussed in Section 4.1.2, but now in terms of the “true” components and parameters: \\[\\begin{equation} \\begin{aligned} l_{t} = &amp; {\\alpha} \\sum_{j=0}^{t-1} (1 -{\\alpha})^j {y}_{t-j} + (1 -{\\alpha})^t l_0, \\end{aligned}. \\tag{5.17} \\end{equation}\\] The stability condition for ETS(A,N,N) is that the discount value \\(1-\\alpha\\) is less than one by absolute value. This way, the weights will decay in time because of the exponentiation in (5.17) to the power of \\(j\\). This condition is satisfied when \\(\\alpha \\in(0, 2)\\), which is admissible bound discussed in Section 4.6. As for the forecastability condition, in this case it implies that \\(\\lim\\limits_{t\\rightarrow\\infty}(1 -{\\alpha})^t = \\text{const}\\), which means that the effect of the initial state on future values stays the same. This is achievable, for example, when \\(\\alpha=0\\), but is violated, when \\(\\alpha&lt;0\\) or \\(\\alpha\\geq 2\\). So, the bounds for the smoothing parameters in the ETS(A,N,N) model, guaranteeing the forecastability of the model (i.e. making it useful), are: \\[\\begin{equation} \\alpha \\in [0, 2) . \\tag{5.18} \\end{equation}\\] 5.4.2 Comming back to the general case In the general case, the logic is the same as with ETS(A,N,N), but it implies the usage of linear algebra. Due to our lagged formulation, the recursion becomes more complicated: \\[\\begin{equation} \\begin{aligned} \\mathbf{v}_{t} = &amp; \\mathbf{D}_{m_1}^{\\lceil\\frac{t}{m_1}\\rceil} \\mathbf{v}_{0} + \\sum_{j=0}^{\\lceil\\frac{t}{m_1}\\rceil-1} \\mathbf{D}_{m_1}^{j} y_{t -j m_1} + \\\\ &amp; \\mathbf{D}_{m_2}^{\\lceil\\frac{t}{m_2}\\rceil} \\mathbf{v}_{0} + \\sum_{j=0}^{\\lceil\\frac{t}{m_2}\\rceil-1} \\mathbf{D}_{m_2}^j y_{t -j m_2} + \\\\ &amp; \\dots + \\\\ &amp; \\mathbf{D}_{m_d}^{\\lceil\\frac{t}{m_d}\\rceil} \\mathbf{v}_{0} + \\sum_{j=0}^{\\lceil\\frac{t}{m_d}\\rceil-1} \\mathbf{D}_{m_d}^j y_{t -j m_d} \\end{aligned}, \\tag{5.19} \\end{equation}\\] where \\(\\mathbf{D}_{m_i} = \\mathbf{F}_{m_i} -\\mathbf{g}_{m_i} \\mathbf{w}_{m_i}^\\prime\\) is the discount matrix for each lagged part of the model. The stability condition in this case is that the absolute values of all the non-zero eigenvalues of the discount matrices \\(\\mathbf{D}_{m_i}\\) are lower than one. This condition can be checked at the model construction stage, ensuring that the selected parameters guarantee the stability of the model. As for the forecastability, the idea is that the initial value of the state vector should not have an increasing impact on the last observed value, which is obtained by inserting (5.19) in the measurement equation: \\[\\begin{equation} \\begin{aligned} y_t = &amp; \\mathbf{w}_{m_1}^\\prime \\mathbf{D}_{m_1}^{\\lceil\\frac{t-1}{m_1}\\rceil} \\mathbf{v}_{0} + \\mathbf{w}_{m_1}^\\prime \\sum_{j=0}^{\\lceil\\frac{t-1}{m_1}\\rceil-1} \\mathbf{D}_{m_1}^{j} y_{t-1 -j m_1} + \\\\ &amp; \\mathbf{w}_{m_2}^\\prime \\mathbf{D}_{m_2}^{\\lceil\\frac{t-1}{m_2}\\rceil} \\mathbf{v}_{0} + \\mathbf{w}_{m_2}^\\prime \\sum_{j=0}^{\\lceil\\frac{t-1}{m_2}\\rceil-1} \\mathbf{D}_{m_2}^j y_{t-1 -j m_2} + \\\\ &amp; \\dots + \\\\ &amp; \\mathbf{w}_{m_d}^\\prime \\mathbf{D}_{m_d}^{\\lceil\\frac{t-1}{m_d}\\rceil} \\mathbf{v}_{0} + \\mathbf{w}_{m_d}^\\prime \\sum_{j=0}^{\\lceil\\frac{t-1}{m_d}\\rceil-1} \\mathbf{D}_{m_d}^j y_{t-1 -j m_d} \\end{aligned}, \\tag{5.20} \\end{equation}\\] and analysing the impact of \\(\\mathbf{v}_0\\) on the actual value \\(y_t\\). In our case forecastability condition implies that: \\[\\begin{equation} \\lim\\limits_{t\\rightarrow\\infty}\\left(\\mathbf{w}_{m_i}^\\prime\\mathbf{D}_{m_i}^{\\lceil\\frac{t-1}{m_i}\\rceil} \\mathbf{v}_{0}\\right) = \\text{const for all } i=1, \\dots, d. \\tag{5.21} \\end{equation}\\] "],["ADAMETSAdditiveDistributions.html", "5.5 Distributional assumptions in pure additive ETS", " 5.5 Distributional assumptions in pure additive ETS While the conventional ETS assumes that the error term follows Normal distribution, ADAM ETS proposes some flexibility, implementing the following options for the error term distribution in the additive error models: Normal: \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\), meaning that \\(y_t = \\mu_{y,t} + \\epsilon_t \\sim \\mathcal{N}(\\mu_{y,t}, \\sigma^2)\\); Laplace: \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\), so that \\(y_t = \\mu_{y,t} + \\epsilon_t \\sim \\mathcal{Laplace}(\\mu_{y,t}, s)\\); S: \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\), implying that \\(y_t = \\mu_{y,t} + \\epsilon_t \\sim \\mathcal{S}(\\mu_{y,t}, s)\\); Generalised Normal: \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\), leading to \\(y_t = \\mu_{y,t} + \\epsilon_t \\sim \\mathcal{GN}(\\mu_{y,t}, s, \\beta)\\). The conditional moments and stability / forecastability conditions do not change for the model with these new assumptions. The main element that changes is the scale and the width of prediction intervals. Given that scales of these distributions are linearly related to the variance, one can calculate the conditional variance as discussed in Section 5.3 and then use it in order to obtain the respective scales. Having the scales it becomes straightforward to calculate the needed quantiles for the prediction intervals. Here are the formulae for the scales of distributions mentioned above: Normal: scale is \\(\\sigma^2_h\\); Laplace: \\(s_h = \\sigma_h \\sqrt{\\frac{1}{2}}\\); S: \\(s_h = \\sqrt{\\sigma_h}\\sqrt[4]{\\frac{1}{120}}\\); Generalised Normal: \\(s_h = \\sigma_h \\sqrt{\\frac{\\Gamma(1/\\beta)}{\\Gamma(3/\\beta)}}\\). The estimation of pure additive ETS models can be done via the maximisation of the likelihood of the assumed distribution (see Chapter 13 of Svetunkov, 2021), which in some cases coincide with the popular losses (e.g. Normal and MSE, or Laplace and MAE). In addition, the following more exotic options for the additive error models are available in ADAM ETS: Log Normal: \\(\\left(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\right) \\sim \\text{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\), implying that \\(y_t = \\mu_{y,t} \\left(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\right) \\sim \\text{log}\\mathcal{N}\\left(\\log\\mu_{y,t} -\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\). Here, \\(\\sigma^2\\) is the variance of the error term in logarithms and the \\(-\\frac{\\sigma^2}{2}\\) appears due to the restriction \\(\\text{E}(\\epsilon_t)=0\\). Inverse Gaussian: \\(\\left(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\right) \\sim \\mathcal{IG}(1, \\sigma^2)\\) with \\(y_t=\\mu_{y,t} \\left(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\right) \\sim \\mathcal{IG}\\left(\\mu_{y,t}, \\frac{\\sigma^2}{\\mu_{y,t}}\\right)\\); Gamma: \\(\\left(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\right) \\sim \\mathcal{\\Gamma}(\\sigma^{-2}, \\sigma^2)\\), so that \\(y_t = \\mu_{y,t} \\left(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\right) \\sim \\mathcal{\\Gamma}(\\sigma^{-2}, \\sigma^2 \\mu_{y,t})\\); where \\(\\mu_{y,t} = \\mathbf{w}^\\prime \\mathbf{v}_{t-\\mathbf{l}}\\) is one step ahead point forecast, The possibility of application of these distributions arises from the reformulation of the original pure additive model (5.4) into: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp;\\mathbf{w}^\\prime \\mathbf{v}_{t-\\mathbf{l}}\\left(1 + \\frac{\\epsilon_t}{\\mathbf{w}^\\prime \\mathbf{v}_{t-\\mathbf{l}}}\\right) \\\\ \\mathbf{v}_{t} = &amp;\\mathbf{F} \\mathbf{v}_{t-\\mathbf{l}} + \\mathbf{g} \\epsilon_t \\end{aligned}. \\tag{5.22} \\end{equation}\\] The connection between the two formulations becomes apparent when opening the brackets in the measurement equation of (5.22). Note that in this case, the model assumes that the data is strictly positive, and while it might be possible to fit the model on the data with negative values, the calculation of the scale and the likelihood might become impossible. Using alternative losses (e.g. MSE) is a possible solution in this case. References "],["ADAMETSPureAdditiveExamples.html", "5.6 Examples of application", " 5.6 Examples of application 5.6.1 Non-seasonal data To see how the pure additive ADAM ETS works, we will try it out using the adam() function from the smooth package for R on Box-Jenkins sales data. We start with plotting the data: plot(BJsales) Figure 5.1: Box-Jenkins sales data. The series in Figure 5.1 seem to exhibit trend, so we will apply ETS(A,A,N) model: adamModel &lt;- adam(BJsales, &quot;AAN&quot;) adamModel ## Time elapsed: 0.02 seconds ## Model estimated using adam() function: ETS(AAN) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 258.8098 ## Persistence vector g: ## alpha beta ## 1.0000 0.2438 ## ## Sample size: 150 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 145 ## Information criteria: ## AIC AICc BIC BICc ## 527.6196 528.0362 542.6728 543.7166 The model’s output summarises which specific model was constructed, assuming what distribution, how it was estimated, and the values of smoothing parameters. It also reports the sample size, the number of parameters, degrees of freedom and produces information criteria (see Section 13.4 of Svetunkov, 2021). We can compare this model with the ETS(A,N,N) to see which of them performs better in terms of information criteria (e.g. in terms of AICc): adam(BJsales, &quot;ANN&quot;) ## Time elapsed: 0.01 seconds ## Model estimated using adam() function: ETS(ANN) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 273.2898 ## Persistence vector g: ## alpha ## 1 ## ## Sample size: 150 ## Number of estimated parameters: 3 ## Number of degrees of freedom: 147 ## Information criteria: ## AIC AICc BIC BICc ## 552.5795 552.7439 561.6114 562.0233 In this situation the AICc for ETS(A,N,N) is higher than for ETS(A,A,N), so we should use the latter for forecasting purposes. We can produce point forecasts and prediction interval (in this example we will construct 90% and 95% ones) and plot them (Figure 5.2): plot(forecast(adamModel, h=10, interval=&quot;prediction&quot;, level=c(0.9,0.95))) Figure 5.2: Forecast for Box-Jenkins sales data from ETS(A,A,N) model. Notice that the smoothing parameters of ETS(A,A,N) are very high, with \\(\\alpha=1\\). This might mean that the maximum likelihood is achieved in the admissible bounds. We can try it out and see what happens: adamModel &lt;- adam(BJsales, &quot;AAN&quot;, bounds=&quot;admissible&quot;) adamModel ## Time elapsed: 0.04 seconds ## Model estimated using adam() function: ETS(AAN) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 258.5358 ## Persistence vector g: ## alpha beta ## 1.0541 0.2185 ## ## Sample size: 150 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 145 ## Information criteria: ## AIC AICc BIC BICc ## 527.0716 527.4883 542.1248 543.1687 Both smoothing parameters are now higher, which implies that the uncertainty about the future values of states is higher as well, which is then reflected in the slightly wider prediction interval (Figure 5.3): plot(forecast(adamModel, h=10, interval=&quot;prediction&quot;, level=c(0.9,0.95))) Figure 5.3: Forecast for Box-Jenkins sales data from ETS(A,A,N) model with admissible bounds. Although the values of smoothing parameters are larger than one, the model is still stable. In order to see that, we can calculate the discount matrix \\(\\mathbf{D}\\) using the objects returned by the function: discountMatrix &lt;- adamModel$transition - adamModel$persistence %*% adamModel$measurement[nobs(adamModel),,drop=FALSE] eigen(discountMatrix)$values ## [1] 0.79538429 -0.06800887 Notice that the absolute values of both eigenvalues in the matrix are less than one, which means that the newer observations have higher weights than the older ones and that the absolute values of weights decrease over time, making the model stable. If we want to test ADAM ETS with another distribution, it can be done using the respective parameter (here we use Generalised Normal, estimating the shape together with the other parameters): adamModel &lt;- adam(BJsales, &quot;AAN&quot;, distribution=&quot;dgnorm&quot;) print(adamModel, digits=3) ## Time elapsed: 0.03 seconds ## Model estimated using adam() function: ETS(AAN) ## Distribution assumed in the model: Generalised Normal with shape=1.741 ## Loss function type: likelihood; Loss function value: 258.456 ## Persistence vector g: ## alpha beta ## 1.000 0.217 ## ## Sample size: 150 ## Number of estimated parameters: 6 ## Number of degrees of freedom: 144 ## Information criteria: ## AIC AICc BIC BICc ## 528.913 529.500 546.977 548.448 Similar to the previous cases, we can plot the forecasts from the model: plot(forecast(adamModel, h=10, interval=&quot;prediction&quot;)) Figure 5.4: Forecast for Box-Jenkins sales data from ETS(A,A,N) model with Generalised Normal distribution. The prediction interval in this case is slightly wider than in the previous one, because \\(\\mathcal{GN}\\) distribution with \\(\\beta=\\) 1.74 has fatter tails than the normal distribution (Figure 5.4). 5.6.2 Seasonal data Figure 5.5: Air passengers data from Box-Jenkins textbook. Now we will check what happens in the case of seasonal data. We use AirPassengers data, plotted in Figure 5.5, which has multiplicative seasonality. But for demonstration purposes, we will see what happens when we use the wrong model with additive seasonality. We will withhold 12 observations to look closer at the performance of the ETS(A,A,A) model in this case: adamModel &lt;- adam(AirPassengers, &quot;AAA&quot;, lags=12, h=12, holdout=TRUE) Remark. In this specific case, the lags parameter is not necessary because the function will automatically get the frequency from the ts object. If we were to provide a vector of values instead of the ts object, we would need to specify the correct lag. Note that 1 (lag for level and trend) is unnecessary; the function will always use it anyway. Remark. In some cases, the optimiser might converge to the local minimum, so if you find the results unsatisfactory, it might make sense to reestimate the model tuning the parameters of the optimiser (see Section 11.4 for details). Here is an example (we increase the number of iterations in the optimisation and set new starting values for the smoothing parameters): adamModel$B[1:3] &lt;- c(0.2,0.1,0.3) adamModel &lt;- adam(AirPassengers, &quot;AAA&quot;, lags=12, h=12, holdout=TRUE, B=adamModel$B, maxeval=1000) adamModel ## Time elapsed: 0.2 seconds ## Model estimated using adam() function: ETS(AAA) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 513.0026 ## Persistence vector g: ## alpha beta gamma ## 0.1928 0.0000 0.8072 ## ## Sample size: 132 ## Number of estimated parameters: 17 ## Number of degrees of freedom: 115 ## Information criteria: ## AIC AICc BIC BICc ## 1060.005 1065.374 1109.013 1122.119 ## ## Forecast errors: ## ME: 6.216; MAE: 14.162; RMSE: 17.75 ## sCE: 28.418%; Asymmetry: 51.9%; sMAE: 5.395%; sMSE: 0.457% ## MASE: 0.588; RMSSE: 0.567; rMAE: 0.186; rRMSE: 0.172 Notice that because we fit the seasonal additive model to the data with multiplicative seasonality, the smoothing parameter \\(\\gamma\\) has become large – the seasonal component needs to be updated to keep up with the changing seasonal profile. In addition, because we use the holdout parameter, the function also reports the error measures for the point forecasts on that part of the data. This can be useful when comparing the performance of several models on a time series. Here is how the forecast from ETS(A,A,A) looks on this data: Figure 5.6: Forecast for air passengers data using ETS(A,A,A) model. Figure 5.6 demonstrates that while the fit to the data is far from perfect, due to a pure coincidence, the point forecast from this model is decent. In order to see how the ADAM ETS decomposes the data into components, we can plot it via the plot() method with which parameter: Figure 5.7: Decomposition of air passengers data using ETS(A,A,A) model. We can see on the graph in Figure 5.7 that the residuals still contain some seasonality, so there is room for improvement. This probably happened because the data exhibits multiplicative seasonality rather than the additive one. For now, we do not aim to fix this issue. References "],["ADAMETSPureMultiplicativeChapter.html", "Chapter 6 Pure multiplicative ADAM ETS", " Chapter 6 Pure multiplicative ADAM ETS There is a reason why we discuss pure multiplicative ADAM ETS models separately: they are suitable for the positive data, especially when the level is low, yet they do not rely on prior data transformations (such as taking logarithms or applying a power transform). However, the models discussed in this chapter are not easy to work with – they typically do not have closed forms for the conditional h steps ahead mean and variance and do not have well-defined parameters space. Furthermore, they make more sense in conjunction with positive-valued distributions, although they also work with the normal one. All these aspects are discussed in this chapter. "],["ADAMETSPureMultiplicative.html", "6.1 Model formulation", " 6.1 Model formulation The pure multiplicative ETS implemented in ADAM framework can be formulated using logarithms, similar to how the pure additive ADAM ETS is formulated in (5.4): \\[\\begin{equation} \\begin{aligned} \\log y_t = &amp; \\mathbf{w}^\\prime \\log(\\mathbf{v}_{t-\\mathbf{l}}) + \\log(1 + \\epsilon_{t}) \\\\ \\log \\mathbf{v}_{t} = &amp; \\mathbf{F} \\log \\mathbf{v}_{t-\\mathbf{l}} + \\log(\\mathbf{1}_k + \\mathbf{g} \\epsilon_t) \\end{aligned}, \\tag{6.1} \\end{equation}\\] where \\(\\mathbf{1}_k\\) is the vector of ones, containing \\(k\\) elements (number of components in the model), \\(\\log\\) is the natural logarithm, applied element-wise to the vectors, and all the other values have been discussed in the previous sections. An example of a pure multiplicative model is ETS(M,M,M), for which we have the following values: \\[\\begin{equation} \\begin{aligned} \\mathbf{w} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, &amp; \\mathbf{F} = \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}, &amp; \\mathbf{g} = \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix}, \\\\ \\mathbf{v}_{t} = \\begin{pmatrix} l_t \\\\ b_t \\\\ s_t \\end{pmatrix}, &amp; \\mathbf{l} = \\begin{pmatrix} 1 \\\\ 1 \\\\ m \\end{pmatrix}, &amp; \\mathbf{1}_k = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\end{aligned}. \\tag{6.2} \\end{equation}\\] By inserting these values in the equation (6.1), we obtain model in logarithms: \\[\\begin{equation} \\begin{aligned} \\log y_t = &amp; \\log l_{t-1} + \\log b_{t-1} + \\log s_{t-m} + \\log \\left(1 + \\epsilon_{t} \\right) \\\\ \\log l_{t} = &amp; \\log l_{t-1} + \\log b_{t-1} + \\log( 1 + \\alpha \\epsilon_{t}) \\\\ \\log b_{t} = &amp; \\log b_{t-1} + \\log( 1 + \\beta \\epsilon_{t}) \\\\ \\log s_{t} = &amp; \\log s_{t-m} + \\log( 1 + \\gamma \\epsilon_{t}) \\\\ \\end{aligned} , \\tag{6.3} \\end{equation}\\] which after exponentiation becomes equal to the one, discussed in Section 3.5: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; l_{t-1} b_{t-1} s_{t-m} (1 + \\epsilon_t) \\\\ l_t = &amp; l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\ b_t = &amp; b_{t-1} (1 + \\beta \\epsilon_t) \\\\ s_t = &amp; s_{t-m} (1 + \\gamma \\epsilon_t) \\end{aligned}. \\tag{6.4} \\end{equation}\\] An interesting observation is that the model (6.3) will produce values similar to the model ETS(A,A,A) applied to the data in logarithms, when the values of smoothing parameters are close to zero. This becomes apparent, when recalling the limit: \\[\\begin{equation} \\lim\\limits_{x \\to 0}\\log(1+x) = x . \\tag{6.5} \\end{equation}\\] Based on that, the model will become close to the following one in cases of small values of smoothing parameters: \\[\\begin{equation} \\begin{aligned} \\log y_t = &amp; \\log l_{t-1} + \\log b_{t-1} + \\log s_{t-m} + \\epsilon_{t} \\\\ \\log l_{t} = &amp; \\log l_{t-1} + \\log b_{t-1} + \\alpha \\epsilon_{t} \\\\ \\log b_{t} = &amp; \\log b_{t-1} + \\beta \\epsilon_{t} \\\\ \\log s_{t} = &amp; \\log s_{t-m} + \\gamma \\epsilon_{t} \\\\ \\end{aligned} , \\tag{6.6} \\end{equation}\\] which is the ETS(A,A,A) applied to the data in the logarithms. In many cases, the smoothing parameters will be small enough for the limit (6.5) to hold, so the two models will produce similar forecasts. The main benefit of (6.6) is that it has closed forms for the conditional mean and variance, so the model (6.6) can be used instead of (6.3) when the smoothing parameters are close to zero, and the variance of the error term is small to get conditional moments and quantiles of distribution. However, the form (6.6) does not permit mixed components – it only supports the multiplicative ones, making it detached from the other ETS models. "],["adamETSPuremultiplicativeRecursive.html", "6.2 Recursive relation", " 6.2 Recursive relation Similarly to how it was done for the pure additive model in Section 5.2, we can show what the recursive relation will look like for the pure multiplicative one (the logic here is the same, the main difference is in working with logarithms instead of the original values): \\[\\begin{equation} \\begin{aligned} \\log y_{t+h} = &amp; \\mathbf{w}_{m_1}^\\prime \\mathbf{F}_{m_1}^{\\lceil\\frac{h}{m_1}\\rceil-1} \\log \\mathbf{v}_{t} + \\mathbf{w}_{m_1}^\\prime \\sum_{j=1}^{\\lceil\\frac{h}{m_1}\\rceil-1} \\mathbf{F}_{m_1}^{j-1} \\log \\left(\\mathbf{1}_k + \\mathbf{g}_{m_1} \\epsilon_{t+m_1\\lceil\\frac{h}{m_1}\\rceil-j}\\right) + \\\\ &amp; \\mathbf{w}_{m_2}^\\prime \\mathbf{F}_{m_2}^{\\lceil\\frac{h}{m_2}\\rceil-1} \\log \\mathbf{v}_{t} + \\mathbf{w}_{m_2}^\\prime \\sum_{j=1}^{\\lceil\\frac{h}{m_2}\\rceil-1} \\mathbf{F}_{m_2}^{j-1} \\log \\left(\\mathbf{1}_k + \\mathbf{g}_{m_2} \\epsilon_{t+m_2\\lceil\\frac{h}{m_2}\\rceil-j}\\right) + \\\\ &amp; \\dots \\\\ &amp; \\mathbf{w}_{m_d}^\\prime \\mathbf{F}_{m_d}^{\\lceil\\frac{h}{m_d}\\rceil-1} \\log \\mathbf{v}_{t} + \\mathbf{w}_{m_d}^\\prime \\sum_{j=1}^{\\lceil\\frac{h}{m_d}\\rceil-1} \\mathbf{F}_{m_d}^{j-1} \\log \\left(\\mathbf{1}_k + \\mathbf{g}_{m_d} \\epsilon_{t+m_d\\lceil\\frac{h}{m_d}\\rceil-j}\\right) + \\\\ &amp; \\log \\left(1 + \\epsilon_{t+h}\\right) \\end{aligned}. \\tag{6.7} \\end{equation}\\] In order to see how this recursion works, we can take the example of ETS(M,N,N), for which \\(m_1=1\\) and all the other frequencies are equal to zero: \\[\\begin{equation} y_{t+h} = \\exp\\left(\\mathbf{w}_{1}^\\prime \\mathbf{F}_{1}^{h-1} \\log\\mathbf{v}_{t} + \\mathbf{w}_{1}^\\prime \\sum_{j=1}^{h-1} \\mathbf{F}_{1}^{j-1} \\log \\left(\\mathbf{1}_k + \\mathbf{g}_{1} \\epsilon_{t+h-j}\\right) +\\log \\left(1 + \\epsilon_{t+h}\\right)\\right) , \\tag{6.8} \\end{equation}\\] or after inserting \\(\\mathbf{w}_{1}=1\\), \\(\\mathbf{F}_{1}=1\\), \\(\\mathbf{v}_{t}=l_t\\), \\(\\mathbf{g}_{1}=\\alpha\\) and \\(\\mathbf{1}_k=1\\): \\[\\begin{equation} y_{t+h} = l_t \\prod_{j=1}^{h-1} \\left(1 + \\alpha \\epsilon_{t+h-j}\\right) \\left(1 + \\epsilon_{t+h}\\right) . \\tag{6.9} \\end{equation}\\] This recursion is useful to understand how the states evolve, and in the case of ETS(M,N,N), it allows obtaining the conditional expectation and variance. But in general, for models with trend and/or seasonality, it cannot be used to calculate moments, like the one for the pure additive ADAM ETS. This is discussed in Section 6.3). "],["pureMultiplicativeExpectationAndVariance.html", "6.3 The problem with moments in pure multiplicative ETS", " 6.3 The problem with moments in pure multiplicative ETS The recursion (6.7) obtained in the previous subsection shows how the previous values influence the logarithms of states. While it is possible to calculate the expectation of the logarithm of the variable \\(y_{t+h}\\), in general, this does not allow deriving the expectation of the variable in the original scale. This is because of the convolution of terms \\(\\log(\\mathbf{1}_k + \\mathbf{g}_{m_i} \\epsilon_{t+j})\\) for different \\(j\\). To better understand this issue, we consider this persistence part of the equation for the ETS(M,N,N) model: \\[\\begin{equation} \\log(1+\\alpha\\epsilon_t) = \\log(1-\\alpha + \\alpha(1+\\epsilon_t)). \\tag{6.10} \\end{equation}\\] Whatever we assume about the distribution of the variable \\((1+\\epsilon_t)\\), the distribution of (6.10) will be more complicated. For example, if we assume that \\((1+\\epsilon_t)\\sim\\mathrm{log}\\mathcal{N}(0,\\sigma^2)\\), then the distribution of (6.10) is something like exp three-parameter log normal distribution (Sangal and Biswas, 1970). The convolution of (6.10) for different \\(t\\) does not follow a known distribution, so it is not possible to calculate the conditional expectation and variance based on (6.7). Similar issues arrise if we assume any other distribution. The problem is worsened in case of multiplicative trend and / or multiplicative seasonality models, because then the recursion (6.7) contains several errors on the same observation (e.g. \\(\\log(1+\\alpha\\epsilon_t)\\) and \\(\\log(1+\\beta\\epsilon_t)\\)). The only way to derive the conditional expectation and variance for the pure multiplicative models is to use the formulae from tables 3.1 and 3.2 in Section 3.5 and manually derive the values in the original scale. This works well only for the ETS(M,N,N) model, for which it is possible to take conditional expectation and variance of the recursion (6.9) to obtain: \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = \\mathrm{E}(y_{t+h}|t) = &amp; l_{t} \\\\ \\mathrm{V}(y_{t+h}|t) = &amp; l_{t}^2 \\left( \\left(1+ \\alpha^2 \\sigma^2 \\right)^{h-1} (1 + \\sigma^2) -1 \\right), \\end{aligned} \\tag{6.11} \\end{equation}\\] where \\(\\sigma^2\\) is the variance of the error term. For the other models, the conditional moments do not have a general closed forms because of the product of \\(\\log(1+\\alpha\\epsilon_t)\\), \\(\\log(1+\\beta\\epsilon_t)\\) and \\(\\log(1+\\gamma\\epsilon_t)\\). It is still possible to derive the moments for special cases of \\(h\\), but this is a tedious process. In order to see that, we demonstrate here how the recursion looks for ETS(M,Md,M) model: \\[\\begin{equation} \\begin{aligned} &amp; y_{t+h} = l_{t+h-1} b_{t+h-1}^\\phi s_{t+h-m} \\left(1 + \\epsilon_{t+h} \\right) = \\\\ &amp; l_{t} b_{t}^{\\sum_{j=1}^h{\\phi^j}} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\prod_{j=1}^{h-1} \\left( (1 + \\alpha \\epsilon_{t+j}) \\prod_{i=1}^{j} (1 + \\beta \\epsilon_{t+i})^{\\phi^{j-i}} \\right) \\prod_{j=1}^{\\lceil\\frac{h}{m}\\rceil} \\left(1 + \\gamma \\epsilon_{t+j}\\right) \\left(1 + \\epsilon_{t+h} \\right) . \\end{aligned} \\tag{6.12} \\end{equation}\\] The conditional expectation of the recursion (6.12) does not have a simple form, because of the difficulties in calculating the expectation of \\((1 + \\alpha \\epsilon_{t+j})(1 + \\beta \\epsilon_{t+i})^{\\phi^{j-i}}(1 + \\gamma \\epsilon_{t+j})\\). In a simple example of \\(h=2\\) and \\(m&gt;h\\) the conditional expectation based on (6.12) can be simplified to: \\[\\begin{equation} \\mu_{y,t+2} = l_{t} b_{t}^{\\phi+\\phi^2} \\left(1 + \\alpha \\beta \\sigma^2 \\right), \\tag{6.13} \\end{equation}\\] introducing the second moment, the variance of the error term \\(\\sigma^2\\). The case of \\(h=3\\) implies the appearance of the third moment, the \\(h=4\\) – the fourth etc. This is why there are no closed forms for the conditional moments for the pure multiplicative models with trend and/or seasonality. In some special cases, when smoothing parameters and the variance of error term are all low, it is possible to use approximate formulae for some of the multiplicative models. These are discussed in Chapter 6 of Hyndman et al. (2008). In a special case, when all smoothing parameters are equal to zero or when \\(h=1\\), the conditional expectation will coincide with the point forecast from the Tables 3.1 and 3.2 in Section 3.5. But in general, the best thing that can be done in this case is the simulation of possible paths (using the formulae from the tables mentioned above) and then the calculation of mean and variance based on them. Finally, it can be shown for pure multiplicative models that: \\[\\begin{equation} \\hat{y}_{t+h} \\leq \\check{y}_{t+h} \\leq \\mu_{y,t+h} , \\tag{6.14} \\end{equation}\\] where \\(\\mu_{y,t+h}\\) is the conditional h steps ahead expectation, \\(\\check{y}_{t+h}\\) is the conditional h steps ahead geometric expectation (expectation in logarithms) and \\(\\hat{y}_{t+h}\\) is the point forecast (Svetunkov and Boylan, 2020a). References "],["stabilityConditionMultiplicativeError.html", "6.4 Smoothing parameters bounds", " 6.4 Smoothing parameters bounds Similar to the pure additive ADAM ETS, it is possible to have different restrictions on smoothing parameters for pure multiplicative models. However, in this case, the classical and the usual restrictions become more reasonable from the model’s point of view. In contrast, the derivation of admissible bounds becomes a challenging task. Consider the ETS(M,N,N) model, for which the level is updated using the following relation: \\[\\begin{equation} l_t = l_{t-1} (1 + \\alpha\\epsilon_t) = l_{t-1} (1-\\alpha + \\alpha(1+\\epsilon_t)). \\tag{6.15} \\end{equation}\\] As discussed previously, the main benefit of pure multiplicative models is in dealing with positive data. So, it is reasonable to assume that \\((1 + \\epsilon_t)&gt;0\\), which implies that the actual values will always be positive and that each model component should also be positive. This means that \\(\\alpha(1 + \\epsilon_t)&gt;0\\), which implies that \\((1-\\alpha + \\alpha(1+\\epsilon_t))&gt;1-\\alpha\\) or equivalently based on (6.10) \\((1 + \\alpha\\epsilon_t)&gt;1-\\alpha\\) should always hold. In order for the model to make sense, the condition \\((1 + \\alpha\\epsilon_t)&gt;0\\) should hold as well, ensuring that the level is always positive. Connecting the two inequalities, this can be achieved when \\(1-\\alpha \\geq 0\\), meaning that \\(\\alpha \\leq 1\\). Furthermore, for the level to be positive irrespective of the specific error on observation \\(t\\), the smoothing parameter should be non-negative. So, in general, the bounds \\([0, 1]\\) guarantee that the model ETS(M,N,N) will produce positive values only. The two special cases \\(\\alpha=0\\) and \\(\\alpha=1\\) make sense because the level in (6.15) will be positive in both of them, implying that for the former, the model becomes equivalent to the global level, while for the latter the model is equivalent to Random Walk. Using similar logic, it can be shown that the classical restriction \\(\\alpha, \\beta, \\gamma \\in [0, 1]\\) guarantees that the model will always produce positive values. The more restrictive condition of the usual bounds, discussed in Section 4.6, makes sense as well, although it might be more restrictive than needed. But it has a different idea: guaranteeing that the model exhibits averaging properties. Finally, the admissible bounds might still make sense for the pure multiplicative models, but the condition for parameters bounds becomes more complicated and implies that the distribution of the error term becomes trimmed from below to satisfy the classical restrictions discussed above. Very crudely, the conventional restriction from pure additive models can be used to approximate the proper admissible bounds, given the limit (6.5), but this should be used with care, given the discussion above. From the practical point of view, the pure multiplicative models typically have low smoothing parameters, close to zero, because they rely on multiplication of components rather than on addition, so even the classical restriction might seem broad in many situations. "],["ADAMETSMultiplicativeDistributions.html", "6.5 Distributional assumptions in pure multiplicative ETS", " 6.5 Distributional assumptions in pure multiplicative ETS The conventional assumption for the error term in ETS is that \\(\\epsilon_t\\sim\\mathcal{N}(0,\\sigma^2)\\), which guarantees that the conditional expectation of the model will be equal to the point forecasts when the trend and seasonal components are not multiplicative. In general, ETS works well in many cases with this assumption, mainly when the data is strictly positive, and the level of series is high (e.g. thousands of units). However, this assumption might become unhelpful when dealing with lower-level data because the models may start generating non-positive values, which contradicts the idea of pure multiplicative ETS models. Akram et al. (2009) studied the ETS models with multiplicative error and suggested that applying ETS on data in logarithms is a better approach than just using ETS(M,Y,Y) models (here “Y” stands for non-additive components). However, this approach sidesteps the ETS taxonomy, creating a new group of models. An alternative (also discussed in Akram et al. (2009)) is to assume that the error term \\(1+\\epsilon_t\\) follows some distribution for positive data. The authors mentioned log Normal, truncated and Gamma distributions but never explored them further. Svetunkov and Boylan (2020a) discussed several options for the distribution of \\(1+\\epsilon_t\\) in ETS, including log Normal, Gamma and Inverse Gaussian. Other distributions for positive data can be applied as well, but their usage might become complicated, because they need to meet condition \\(\\mathrm{E}(1+\\epsilon_t)=1\\) in order for the expectation to coincide with the point forecasts for models with non-multiplicative trend and seasonality. For example, if the error term follows log Normal distribution, then this restriction implies that the location of the distribution should be non-zero: \\(1+\\epsilon_t\\sim\\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2},\\sigma^2\\right)\\). Using this principle the following distributions can be used for ADAM ETS: Inverse Gaussian: \\(\\left(1+\\epsilon_t \\right) \\sim \\mathcal{IG}(1, \\sigma^2)\\), so that \\(y_t = \\mu_{y,t} (1+\\epsilon_t) \\sim \\mathcal{IG}(\\mu_{y,t}, \\sigma^2)\\); Gamma: \\(\\left(1+\\epsilon_t \\right) \\sim \\Gamma (\\sigma^{-2}, \\sigma^2)\\), so that \\(y_t = \\mu_{y,t} (1+\\epsilon_t) \\sim \\Gamma (\\sigma^{-2}, \\sigma^2 \\mu_{y,t})\\); Log Normal: \\(\\left(1+\\epsilon_t \\right) \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\) so that \\(y_t = \\mu_{y,t} (1+\\epsilon_t) \\sim \\mathrm{log}\\mathcal{N}(\\log \\mu_{y,t} -\\frac{\\sigma^2}{2}, \\sigma^2)\\). The MLE of \\(s\\) in \\(\\mathcal{IG}\\) is straightforward and is: \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{e_{t}^2}{1+e_t} , \\tag{6.16} \\end{equation}\\] where \\(e_t\\) is the estimate of the error term \\(\\epsilon_t\\). However, when it comes to the MLE of scale parameter for the log Normal distribution with the aforementioned restrictions, it is more complicated and is (Svetunkov and Boylan, 2020a): \\[\\begin{equation} \\hat{\\sigma}^2 = 2\\left(1-\\sqrt{ 1-\\frac{1}{T} \\sum_{t=1}^{T} \\log^2(1+e_{t})}\\right). \\tag{6.17} \\end{equation}\\] Finally, MLE of \\(s\\) in \\(\\mathcal{\\Gamma}\\) does not have a closed form. Luckily, method of moments can be used to obtain its value (Svetunkov and Boylan, 2020a): \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^{T} e_{t}^2 . \\tag{6.18} \\end{equation}\\] This value will coincide with the variance of the error term, given the imposed restrictions on the \\(\\Gamma\\) distribution. Even if we deal with strictly positive high level data, it is not necessary to limit the distribution with Normal only. The following distributions can be applied as well: Normal: \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\), implying that \\(y_t = \\mu_{y,t} (1+\\epsilon_t) \\sim \\mathcal{N}(\\mu_{y,t}, \\mu_{y,t}^2 \\sigma^2)\\); Laplace: \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\), meaning that \\(y_t = \\mu_{y,t} (1+\\epsilon_t) \\sim \\mathcal{Laplace}(\\mu_{y,t}, \\mu_{y,t} s)\\); S: \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\), so that \\(y_t = \\mu_{y,t} (1+\\epsilon_t) \\sim \\mathcal{S}(\\mu_{y,t}, \\sqrt{\\mu_{y,t}} s)\\); Generalised Normal: \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\) and \\(y_t = \\mu_{y,t} (1+\\epsilon_t) \\sim \\mathcal{GN}(\\mu_{y,t}, \\mu_{y,t}^\\beta s)\\); Note that the MLE of scale parameters for these distributions will be calculated differently than in the case of pure additive models. For example, for the normal distribution it is: \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{1}{T}\\sum_{t=1}^T \\frac{y_t-\\hat{\\mu}_{y,t}}{\\hat{\\mu}_{y,t}} , \\tag{6.19} \\end{equation}\\] where the main difference from the additive error case arises from the measurement equation of the multiplicative error models: \\[\\begin{equation} y_t = \\mu_{y,t} (1+\\epsilon_t), \\tag{6.20} \\end{equation}\\] implying that \\[\\begin{equation} e_t = \\frac{y_t-\\hat{\\mu}_{y,t}}{\\hat{\\mu}_{y,t}}. \\tag{6.21} \\end{equation}\\] The estimates of scale can then be used in the next phase, when parameters are optimised via the maximisation of respective log-likelihood function. The maximum likelihood approach is in case of ADAM models is discussed in detail in Section 11.1. The distributional assumptions impact both the estimation of models and the prediction intervals. In the case of asymmetric distributions (such as log Normal, Gamma and Inverse Gaussian), the intervals will typically be asymmetric, with the upper bound being further away from the point forecast than the lower one. Furthermore, even with the comparable estimates of scales of distributions, Inverse Gaussian distribution will typically produce wider bounds than Log-Normal and Gamma. The width of intervals relates to the kurtosis of distributions, which is discussed in Chapter 3 of Svetunkov (2021). References "],["ADAMETSMultiplicativeExamples.html", "6.6 Examples of application", " 6.6 Examples of application 6.6.1 Non-seasonal data We continue our examples with the same Box-Jenkins sales data by fitting the ETS(M,M,N) model, but this time with a holdout of 10 observations: adamModel &lt;- adam(BJsales, &quot;MMN&quot;, h=10, holdout=TRUE) adamModel ## Time elapsed: 0.02 seconds ## Model estimated using adam() function: ETS(MMN) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 245.3759 ## Persistence vector g: ## alpha beta ## 1.0000 0.2412 ## ## Sample size: 140 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 135 ## Information criteria: ## AIC AICc BIC BICc ## 500.7518 501.1996 515.4600 516.5664 ## ## Forecast errors: ## ME: 3.217; MAE: 3.33; RMSE: 3.784 ## sCE: 14.124%; Asymmetry: 91.6%; sMAE: 1.462%; sMSE: 0.028% ## MASE: 2.817; RMSSE: 2.482; rMAE: 0.925; rRMSE: 0.921 The output above is similar to the one we discussed in Section 5.6, so we can compare the two models using various criteria and select the most appropriate. Even though the default distribution for the multiplicative error models in ADAM is \\(\\Gamma\\), we can compare this model with the ETS(A,A,N) via information criteria. For example, here are the AICc for the two models: # ETS(M,M,N) AICc(adamModel) ## [1] 501.1996 # ETS(A,A,N) AICc(adam(BJsales, &quot;AAN&quot;, h=10, holdout=TRUE)) ## [1] 497.2624 The comparison is fair because both models were estimated via likelihood, and both likelihoods are formulated correctly, without omitting any terms (e.g. the ets() function from the forecast package omits the \\(-\\frac{T}{2} \\log\\left(2\\pi e \\frac{1}{T}\\right)\\) for convenience, which makes it incomparable with other models). In this example, the pure additive model is more suitable for the data than the pure multiplicative one. Figure 6.1 shows how the model fits the data and what forecast it produces. Note that the function produces the point forecast in this case, which is not equivalent to the conditional expectation! The point forecast undershoots the actual values in the holdout. Figure 6.1: Model fit for Box-Jenkins Sales data from ETS(M,M,N). If we want to produce the forecasts (conditional expectation and prediction interval) from the model, we can do it, using the same command as in Section 5.6: Figure 6.2: Forecast for Box-Jenkins Sales data from ETS(M,M,N). Note that, when we ask for “prediction” interval, the forecast() function will automatically decide what to use based on the estimated model: in case of pure additive one, it will use analytical solutions, while in the other cases, it will use simulations (see Section 18.2). The point forecast obtained from the forecast function corresponds to the conditional expectation and is calculated based on the simulations. This also means that it will differ slightly from one run of the function to another (reflecting the uncertainty in the error term). Still, the difference, in general, should be negligible for a large number of simulation paths. The forecast with prediction interval is shown in Figure 6.2. The conditional expectation is not very different from the point forecast in this example. This is because the variance of the error term is close to zero, thus bringing the two close to each other: sigma(adamModel)^2 ## [1] 3.928668e-05 We can also compare the performance of ETS(M,M,N) with \\(\\Gamma\\) distribution and the conventional ETS(M,M,N), assuming normality: adamModelNormal &lt;- adam(BJsales, &quot;MMN&quot;, h=10, holdout=TRUE, distribution=&quot;dnorm&quot;) adamModelNormal ## Time elapsed: 0.02 seconds ## Model estimated using adam() function: ETS(MMN) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 245.3872 ## Persistence vector g: ## alpha beta ## 1.000 0.241 ## ## Sample size: 140 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 135 ## Information criteria: ## AIC AICc BIC BICc ## 500.7745 501.2222 515.4827 516.5890 ## ## Forecast errors: ## ME: 3.217; MAE: 3.33; RMSE: 3.785 ## sCE: 14.126%; Asymmetry: 91.6%; sMAE: 1.462%; sMSE: 0.028% ## MASE: 2.817; RMSSE: 2.483; rMAE: 0.925; rRMSE: 0.921 In this specific example, the two distributions produce very similar results with almost indistinguishable estimates of parameters. 6.6.2 Seasonal data The AirPassengers data used in Section 5.6 has (as we discussed) multiplicative seasonality. So, the ETS(M,M,M) model might be more suitable than the pure additive one that we used previously: adamModel &lt;- adam(AirPassengers, &quot;MMM&quot;, h=12, holdout=TRUE) adamForecast &lt;- forecast(adamModel, h=12, interval=&quot;prediction&quot;) adamModel ## Time elapsed: 0.1 seconds ## Model estimated using adam() function: ETS(MMM) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 468.5176 ## Persistence vector g: ## alpha beta gamma ## 0.7684 0.0206 0.0000 ## ## Sample size: 132 ## Number of estimated parameters: 17 ## Number of degrees of freedom: 115 ## Information criteria: ## AIC AICc BIC BICc ## 971.0351 976.4036 1020.0428 1033.1492 ## ## Forecast errors: ## ME: -5.617; MAE: 15.496; RMSE: 21.938 ## sCE: -25.677%; Asymmetry: -23.1%; sMAE: 5.903%; sMSE: 0.698% ## MASE: 0.643; RMSSE: 0.7; rMAE: 0.204; rRMSE: 0.213 Notice that the smoothing parameter \\(\\gamma=0\\), which implies that we deal with the data with deterministic multiplicative seasonality. Comparing the information criteria (e.g. AICc) with the ETS(A,A,A) (discussed in Section 5.6.2), the pure multiplicative model does a better job at fitting the data than the additive one: adamModelAdditive &lt;- adam(AirPassengers, &quot;AAA&quot;, lags=12, h=12, holdout=TRUE) AICc(adamModelAdditive) ## [1] 1130.756 The conditional expectation and prediction interval from this model are mroe adequate as well (Figure 6.3): Figure 6.3: Forecast for air passengers data using ETS(M,M,M) model. If we want to calculate the error measures based on the conditional expectation, we can use the measures() function from greybox package the following way: measures(adamModel$holdout,adamForecast$mean,actuals(adamModel)) ## ME MAE MSE MPE MAPE ## -6.042133695 15.521342185 482.266691939 -0.017199678 0.033928733 ## sCE sMAE sMSE MASE RMSSE ## -0.276219798 0.059130629 0.006999286 0.644467774 0.700896243 ## rMAE rRMSE rAME asymmetry sPIS ## 0.204228187 0.213258015 0.084901176 -0.254845300 2.232688542 These can be compared with the measures from the ETS(A,A,A) model: measures(adamModel$holdout,adamModelAdditive$forecast,actuals(adamModel)) ## ME MAE MSE MPE MAPE ## 28.36910729 37.11462699 2442.64739763 0.04881281 0.07053896 ## sCE sMAE sMSE MASE RMSSE ## 1.29691091 0.14139314 0.03545090 1.54105107 1.57739510 ## rMAE rRMSE rAME asymmetry sPIS ## 0.48835036 0.47994571 0.39862914 0.69383499 -7.31044007 Comparing, for example, MSE from the two models, we can conclude that the pure multiplicative model is more accurate than the pure additive one. We can also produce the plot of the time series decomposition according to ETS(M,M,M) (see Figure 6.4). Figure 6.4: Decomposition of air passengers data using ETS(M,M,M) model. The plot in Figure 6.4 shows that the residuals are more random for the pure multiplicative model than for the ETS(A,A,A), but there still might be some structure left. The autocorrelation and partial autocorrelation functions (discussed in Section 8.3) might help in understanding this better: par(mfcol=c(2,1), mar=c(2,4,2,1)) plot(adamModel,10:11) Figure 6.5: ACF and PACF of residuals of ETS(M,M,M) model. The plot in Figure 6.4 shows that there is still some correlation left in the residuals, which could be either due to pure randomness or imperfect estimation of the model. Tuning the parameters of the optimiser or selecting a different model might solve the problem. "],["ADAMETSOther.html", "Chapter 7 General ADAM ETS model", " Chapter 7 General ADAM ETS model Now that we have discussed two special cases of ADAM ETS models (namely pure additive from Chapter 5.1 and pure multiplicative from Chapter 6.1 ADAM ETS), we can move to the discussion of the general model and special cases of it, including some of the mixed models. We will consider two important groups of mixed ADAM ETS models: with non-multiplicative and with multiplicative trends. They have very different properties that are worth mentioning. Finally, we will discuss the question of the normalisation of seasonal indices. This topic was studied in the literature back in the 80s when we still did not have a proper ETS model, but it has not been thoroughly discussed since then. In Section 7.5, we take a more critical view towards the topic. "],["ADAMETSGeneral.html", "7.1 Model formulation", " 7.1 Model formulation Based on the discussion in previous chapters, we can summarise the general ADAM ETS model. It is built upon the conventional model discussed in Section 4.5 but has several significant differences, the most important of which is that it is formulated using lags of components rather than the transition of them over time (this was discussed in Chapter 5.1 for the pure additive model). The general ADAM ETS model is formulated in the following way: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp;w(\\mathbf{v}_{t-\\mathbf{l}}) + r(\\mathbf{v}_{t-\\mathbf{l}}) \\epsilon_t \\\\ \\mathbf{v}_{t} = &amp;f(\\mathbf{v}_{t-\\mathbf{l}}) + g(\\mathbf{v}_{t-\\mathbf{l}}) \\epsilon_t \\end{aligned}, \\tag{7.1} \\end{equation}\\] where \\(\\mathbf{v}_{t-\\mathbf{l}}\\) is the vector of lagged components and \\(\\mathbf{l}\\) is the vector of lags, while all the other functions correspond to the ones used in (4.22). This model form is mainly useful for the formulation, rather than for the derivations, as discussed in Section 4.5. Not only it encompasses any pure models, it also allows formulating any of the mixed ones. For example, the ETS(M,A,M) will have the following values: \\[\\begin{equation*} \\begin{aligned} w(\\mathbf{v}_{t-\\mathbf{l}}) = (l_{t-1}+b_{t-1}) s_{t-m}\\text{, } &amp; r(\\mathbf{v}_{t-\\mathbf{l}}) = w(\\mathbf{v}_{t-\\mathbf{l}}), \\\\ f(\\mathbf{v}_{t-\\mathbf{l}}) = \\begin{pmatrix} l_{t-1} + b_{t-1} \\\\ b_{t-1} \\\\ s_{t-m} \\end{pmatrix}\\text{, } &amp; g(\\mathbf{v}_{t-\\mathbf{l}}) = \\begin{pmatrix} \\alpha (l_{t-1} + b_{t-1}) \\\\ \\beta (l_{t-1} + b_{t-1}) \\\\ \\gamma s_{t-m} \\end{pmatrix}, \\\\ \\mathbf{v}_{t} = \\begin{pmatrix} l_t \\\\ b_t \\\\ s_t \\end{pmatrix}\\text{, } &amp; \\mathbf{l} = \\begin{pmatrix} 1 \\\\ 1 \\\\ m \\end{pmatrix}, \\\\ \\mathbf{v}_{t-\\mathbf{l}} = \\begin{pmatrix} l_{t-1} \\\\ b_{t-1} \\\\ s_{t-m} \\end{pmatrix} \\end{aligned}. \\end{equation*}\\] By inserting these values in (7.1) we will get the classical ETS(M,A,M) model, mentioned in Section 3.5: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; (l_{t-1} + b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\ l_t = &amp; (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\ b_t = &amp; b_{t-1} + (l_{t-1} + b_{t-1}) \\beta \\epsilon_t \\\\ s_t = &amp; s_{t-m} (1 + \\gamma \\epsilon_t) \\end{aligned}. \\tag{7.2} \\end{equation}\\] The model (7.1) with different values for the functions is the basis of adam() function from smooth package. "],["ADAMETSMixedModels.html", "7.2 Mixed ADAM ETS models", " 7.2 Mixed ADAM ETS models Hyndman et al. (2008) proposed five classes of ETS models, based on the types of their components: ANN; AAN; AAdN; ANA; AAA; AAdA; MNN; MAN; MAdN; MNA; MAA; MAdA; MNM; MAM; MAdM; MMN; MMdN; MMM; MMdM; ANM; AAM; AAdM; MMA; MMdA; AMN; AMdN; AMA; AMdA; AMM; AMdM The idea behind this split is to distinguish the models by their complexity and the availability of analytical expressions for conditional moments. Class 1 models were discussed in Chapter 5.1. They have analytical expressions for conditional mean and variance; they can be applied to any data; they have simple formulae for prediction intervals. Hyndman et al. (2008) demonstrate that models from Class 2 have closed forms for conditional expectation and variance, with the former corresponding to the point forecasts. However, the conditional distribution from these models is not Gaussian, so there are no formulae for the prediction intervals from these models. Yes, in some cases, Normal distribution might be used as a satisfactory approximation for the real one, but simulations should generally be preferred. Class 3 models suffer from similar issues, but the situation worsens: there are no analytical solutions for the conditional mean and variance, and there are only approximations to these statistics. Class 4 models were discussed in Chapter 6.1. They do not have analytical expressions for the moments, their conditional h steps ahead distributions represent a complex convolution of products of the basic ones, but they are appropriate for the positive data and become more valuable when the level of series is low, as already discussed in Chapter 6.1. Finally, Class 5 models might have infinite variances, specifically on long horizons and when the data has low values. Indeed, when the level in one of these models becomes close to zero, there is an increased chance of breaking the model due to the appearance of negative values. Consider an example of the ETS(A,A,M) model, which might have a negative trend, leading to negative values, which are then multiplied by the positive seasonal indices. This would lead to unreasonable values of states in the model. That is why in practice, these models should only be used when the level of the series is high. Furthermore, some Class 5 models are very difficult to estimate and are very sensitive to the smoothing parameter values. The ets() function from the forecast package supports only Classes 1 – 4 for the reasons explained above. To be fair, any mixed model can potentially break when the level of the series is close to zero. For example, ETS(M,A,N) can have a negative trend, which might lead to the negative level and, as a result, to the multiplication of pure positive error term by the negative components. Estimating such a model on real data becomes a non-trivial task. In addition, as discussed above, simulations are typically needed to get prediction intervals for models of Classes 2 – 5 and conditional mean and variance for models for Classes 4 – 5. All of this, in my opinion, means that the more useful classification of ETS models is the following (this classification was first proposed by Akram et al., 2009): A. Pure additive models (Chapter 5.1): ANN; AAN; AAdN; ANA; AAA; AAdA; B. Pure multiplicative models (Chapter 6.1): MNN; MMN; MMdN; MNM; MMM; MMdM; C. Mixed models with non-multiplicative trend (Section 7.3): MAN; MAdN; MNA; MAA; MAdA; MAM; MAdM; ANM; AAM; AAdM; D. Mixed models with multiplicative trend (Section 7.4: MMA; MMdA; AMN; AMdN; AMA; AMdA; AMM; AMdM; The main idea behind the split to (C) and (D) is that the multiplicative trend makes it almost impossible to derive the formulae for the conditional moments of the distribution. So this class of models can be considered as the most challenging one. adam() function supports all 30 ETS models, but you should keep in mind the limitations of some of them discussed in this section. References "],["ADAMETSMixedModelsGroup3.html", "7.3 Mixed models with non-multiplicative trend", " 7.3 Mixed models with non-multiplicative trend There are two subclasses in this class of models: one with a non-multiplicative seasonal component (MAN, MAdN, MNA, MAA, MAdA) and another one with the multiplicative one (MAM; MAdM; ANM; AAM; AAdM). The conditional mean for the former models coincides with the point forecasts, while the conditional variance can be calculated using the following recursive formula (Hyndman et al., 2008, p. 84): \\[\\begin{equation} \\begin{aligned} &amp; \\text{V}(y_{t+h}|t) = (1+\\sigma^2) \\xi_h -\\mu_{t+h|t}^2 \\\\ &amp; \\text{where } \\xi_{1} = \\mu_{t+1|t}^2 \\text{ and } \\xi_h = \\mu_{t+h|t}^2 + \\sigma^2 \\sum_{j=1}^{h-1} c_{j}^2 \\xi_{h-j} \\end{aligned} , \\tag{7.3} \\end{equation}\\] where \\(\\sigma^2\\) is the variance of the error term. As for the second subgroup, the conditional mean corresponds to the point forecasts, when the forecast horizon is less than or equal to the seasonal frequency of the component (i.e. \\(h\\leq m\\)), and there is a cumbersome formula for calculating the conditional mean to some of models in this subgroup for the \\(h&gt;m\\). When it comes to the conditional variance, there exists a formula for some of models in the second subgroup, but they are cumbersome as well. The interested reader is directed to Hyndman et al. (2008), page 85. When it comes to the parameters bounds for the models in this group, the first subgroup of models should have similar bounds to the ones for the respective additive error (Section 5.4) models because they both underlie the same exponential smoothing methods, but with additional restrictions, coming from the multiplicative error (Section 6.4). The traditional bounds (aka “usual”) should work fine for these models for the same reasons they work for the pure additive ones, although they might be too restrictive in some cases; The admissible bounds for smoothing parameters for the models in this group might be too wide and violate the condition of \\((1+ \\alpha \\epsilon_t)&gt;0, (1+ \\beta \\epsilon_t)&gt;0, (1+ \\gamma \\epsilon_t)&gt;0\\), which is important in order not to break the models. The second subgroup is more challenging in terms of parameters bounds because of the multiplication of states by the seasonal components. References "],["ADAMETSMixedModelsGroup4.html", "7.4 Mixed models with multiplicative trend", " 7.4 Mixed models with multiplicative trend This is the most challenging class of models (MMA; MMdA; AMN; AMdN; AMA; AMdA; AMM; AMdM). These do not have analytical formulae for conditional moments, and they are very sensitive to smoothing parameter values and may lead to explosive forecasting trajectories. So, to obtain conditional expectation, variance and prediction interval from the models of these classes, simulations should be used. One of the issues encountered when using these models is the explosive trajectories because of the multiplicative trend. As a result, when these models are estimated, it makes sense to set the initial value of the trend to 1 or a lower value so that the optimiser does not encounter difficulties in the calculations. Furthermore, the combination of components for the models in this class makes even less sense than the combination for Class C (discussed in Section (ADAMETSMixedModelsGroup3)). For example, the multiplicative trend assumes either explosive growth or decay, as shown in Figure 7.1. Figure 7.1: Plots of exponential increase and exponential decline. However, assuming that either seasonal component, the error term, or both will have precisely the same impact on the final value irrespective of the point of time is unrealistic for this situation. The more reasonable would be for the amplitude of seasonality to decrease together with the exponential decay of the trend and for the variance of the error term to do the same. But this means that we are talking about the pure multiplicative models (Chapter 6.1), not the mixed ones. There is only one situation where such mixed models could make sense: when the speed of change of the exponential trajectory is close to zero, meaning that the level of the series does not change rapidly and when the volume of the data is high. In this case, the mixed models might perform well and even produce more accurate forecasts than the models from the other classes. When it comes to the bounds of the parameters, this is a mystery for the mixed models of this class. This is because the recursive relations are complicated, and calculating the discount matrix or anything like that becomes challenging. The usual bounds should still be okay, but keep in mind that these mixed models are typically not very stable. So from my experience, the smoothing parameters should be as low as possible, assuming that the initial values guarantee a working model (not breaking at some of the observations). "],["ADAMETSSeasonalNormalisation.html", "7.5 Normalisation of seasonal indices in ETS models", " 7.5 Normalisation of seasonal indices in ETS models One of the ideas arising from time series decomposition (Section 3.2), inherited by the conventional ETS, is the renormalisation of seasonal indices. It comes to one of the two principles, depending on the type of seasonality: If the model has additive seasonality, then the seasonal indices should add up to zero in a specific period of time, e.g. monthly indices should add up to 0 over the yearly period; If the model has multiplicative seasonality, then the geometric mean of seasonal indices over a specific period should be equal to one. Condition (2) in the conventional ETS is substituted by “the arithmetic mean of multiplicative indices should be equal to one,” which does not have reasonable grounds behind it. If we deal with the multiplicative effect, the geometric mean should be used, not the arithmetic one. Otherwise, we introduce bias in the model by multiplying components by indices. While the normalisation is a natural element of the time series decomposition and works fine for the initial seasonal indices, renormalising the seasonal indices over time might not be natural for the ETS. Hyndman et al. (2008) discuss different mechanisms for the renormalisation of seasonal indices, which, as the authors claim, are needed to make the principles (1) and (2) hold from period to period in the data. However, I argue that this is an unnatural idea for the ETS models. The indices should only be normalised during the initialisation of the model (at the moment \\(t=0\\)), and that they should vary independently for the rest of the sample. The rationale for this comes from the model itself. To illustrate it, I will use ETS(A,N,A), but the idea can be easily applied to any other ETS model with any types of components and any number of seasonal frequencies. Just a reminder, this model is formulated as: \\[\\begin{equation} \\begin{aligned} y_t = &amp;l_{t-1} + s_{t-m} + \\epsilon_t \\\\ {l}_{t} = &amp;l_{t-1} + \\alpha\\epsilon_t \\\\ s_t = &amp;s_{t-m} + \\gamma\\epsilon_t \\end{aligned}. \\tag{7.4} \\end{equation}\\] Let’s assume that this is the “true model” (as discussed in Section 1.4) for whatever data we have for whatever reason. In this case, the set of equations (7.4) tells us that the seasonal indices change over time, depending on the smoothing parameter \\(\\gamma\\) values and each specific value of \\(\\epsilon_t\\), which is assumed to be i.i.d. All seasonal indices \\(s_{t+i}\\) in a particular period (e.g. monthly indices in a year) can be written down explicitly based on (7.4): \\[\\begin{equation} \\begin{aligned} s_{t+1} = &amp;s_{t+1-m} + \\gamma\\epsilon_{t+1} \\\\ s_{t+2} = &amp;s_{t+2-m} + \\gamma\\epsilon_{t+2} \\\\ \\vdots \\\\ s_{t+m} = &amp;s_{t} + \\gamma\\epsilon_{t+m} \\end{aligned}. \\tag{7.5} \\end{equation}\\] If this is how the data is “generated” and the seasonality evolves over time, then there is only one possibility, for the indices \\(s_{t+1}, s_{t+2}, \\dots, s_{t+m}\\) to add up to zero: \\[\\begin{equation} s_{t+1}+ s_{t+2}+ \\dots+ s_{t+m} = 0 \\tag{7.6} \\end{equation}\\] or after inserting in (7.5): \\[\\begin{equation} s_{t+1-m}+ s_{t+2-m}+ \\dots+ s_{t} + \\gamma \\left(\\epsilon_{t+1}+ \\epsilon_{t+2}+ \\dots+ \\epsilon_{t+m}\\right) = 0 \\tag{7.7} \\end{equation}\\] meaning that: the previous indices \\(s_{t+1-m}, s_{t+2-m}, \\dots, s_{t}\\) add up to zero and either \\(\\gamma=0\\), or the sum of error terms \\(\\epsilon_{t+1}, \\epsilon_{t+2}, \\dots, \\epsilon_{t+m}\\) is zero. Note that we do not consider the situation \\(s_{t+1-m}+ \\dots+ s_{t} = -\\gamma \\left(\\epsilon_{t+1}+ \\dots+ \\epsilon_{t+m}\\right)\\) as it does not make sense. The condition (a) can be considered reasonable if the previous indices are normalised. (b) means that the seasonal indices do not evolve over time. However, (c) implies that the error term is not independent, because \\(\\epsilon_{t+m} = -\\epsilon_{t+1}- \\epsilon_{t+2}- \\dots- \\epsilon_{t+m-1}\\), which violates one of the basic assumptions of the model from Section 1.4.1, meaning that (7.5) cannot be considered as the “true” model any more, as it omits some important elements. Thus renormalisation is unnatural for ETS from the “true” model point of view. Alternatively each seasonal index could be updated on each observation \\(t\\) (to make sure that the indices are renormalised). In this situation we have: \\[\\begin{equation*} \\begin{aligned} &amp;s_t = s_{t-m} + \\gamma\\epsilon_t \\\\ &amp;s_{t-m+1}+ s_{t-m+2}+ \\dots+ s_{t-1} + s_{t} = 0 \\end{aligned}, \\end{equation*}\\] which can be rewritten as \\(s_{t-m} + \\gamma\\epsilon_t = -s_{t+1-m}- s_{t+2-m}- \\dots- s_{t-1}\\), meaning that: \\[\\begin{equation*} \\begin{aligned} s_{t-m}+ s_{t+1-m}+ s_{t+2-m}+ \\dots+ s_{t-1} = -\\gamma\\epsilon_t \\end{aligned}, \\end{equation*}\\] But due to the renormalisation, the sum on the left-hand side should be equal to zero, implying that either \\(\\gamma=0\\) or \\(\\epsilon_t=0\\). While the former might hold in some cases (deterministic seasonality), the latter cannot hold for all \\(t=1,\\dots,T\\) and violates the model’s assumptions. The renormalisation is thus impossible without changing the structure of the model. Hyndman et al. (2008) acknowledge that and propose in Chapter 8 some modifications for the seasonal ETS model (i.e. introducing new models), which we do not aim to discuss in this Chapter. The discussion in this section demonstrates that the renormalisation of seasonal indices is unnatural for the ETS model and should not be used. This does not imply that the initial seasonal indices (corresponding to the observation \\(t=0\\)) cannot be normalised. On the contrary, this is the desired approach as it reduces the number of estimated parameters in the model. References "],["ARIMA.html", "Chapter 8 Conventional ARIMA", " Chapter 8 Conventional ARIMA Another important dynamic element in ADAM is the ARIMA model (developed originally by Box and Jenkins, 1976). ARIMA stands for “AutoRegressive Integrated Moving Average,” although the name does not tell much on its own and needs additional explanation, which will be provided in this chapter. The main idea of the model is that the data might have dynamic relations over time, where the new values depend on the values on the previous observations. This becomes more obvious in the case of engineering systems and modelling physical processes. For example, Box and Jenkins (1976) use an example of a series of CO\\(_2\\) output of a furnace when the input gas rate changes. In this case, the elements of the ARIMA process are natural, as the CO\\(_2\\) cannot just drop to zero when the gas is switched off – it will leave the furnace, in reducing quantity over time (i.e. leaving \\(\\phi_1\\times100\\%\\) of CO\\(_2\\) in the next minute, where \\(\\phi_1\\) is a parameter in the model). Another example where AR processes are natural is the temperature in the room, measured with 5 minutes intervals. In this case, the temperature at 5:30 pm will depend on the one at 5:25 pm (if the temperature outside the room is lower, then the one in the room will go down slightly due to the loss of heat). So, in these examples, the ARIMA model can be considered a “true model” (see discussion in Section 1.4). Unfortunately, when it comes to time series from the social or business domains, it becomes very difficult to motivate ARIMA usage from the modelling point of view. For example, the demand on products does not reproduce itself and in real life does not depend on the demand on previous observations unless we are talking about repetitive purchases by the same group of consumers. So, if we construct ARIMA for such a process, we turn a blind eye to the fact that the observed time series relations in the data are most probably spurious. At best, in this case, ARIMA can be considered a very crude approximation of a complex process (demand is typically influenced by price changes, consumer behaviour, and promotional activities). Thus, whenever we work with ARIMA models in social or business domains, we should keep in mind that they are wrong even from the philosophical point of view. Nevertheless, they still can be useful, which is why we discuss them in this chapter. We focus our discussion on forecasting with ARIMA. A reader interested in time series analysis is directed to Box and Jenkins (1976). This chapter will discuss the main theoretical properties of ARIMA processes (i.e. what would happen if the data indeed followed the specified model), moving to more practical aspects in the next chapter. We start the discussion with the non-seasonal ARIMA models, explaining how the forecasts from those models would look like, then move to the seasonal and multi-seasonal ARIMA, then discuss the classical Box-Jenkins approach for ARIMA order selection and its limitations. Finally, we explain the connection between ARIMA and ETS models. References "],["ARIMAIntro.html", "8.1 Introduction to ARIMA", " 8.1 Introduction to ARIMA ARIMA contains several elements: AR(p) – the AutoRegressive part, showing how the variable is impacted by its values on the previous observations. It contains \\(p\\) lags. For example, the quantity of the liquid in a vessel with an opened tap on some observation will depend on the quantity on the previous steps; I(d) – the number of differences \\(d\\) taken in the model (I stands for “Integrated”). Working with differences rather than with the original data means that we deal with changes and rates of changes, not with just values. Technically, differences are needed to make data stationary (i.e. with fixed expectation and variance, although there are different definitions of the term stationarity, see below); MA(q) – the Moving Average component, explaining how the previous white noise impacts the variable. It contains \\(q\\) lags. Once again, in technical systems, the idea that random error can affect the value has a relatively simple explanation. For example, when the liquid drips out of a vessel, we might not be able to predict the air fluctuations, which would impact the flow and could be perceived as elements of random noise. This randomness might, in turn, influence the quantity of liquid in a vessel on the following observation, thus introducing the MA elements in the model. I intentionally do not provide ARIMA examples from the demand forecasting area, as these are much more difficult to motivate and explain than the examples from the more technical areas. Before continuing our discussion, we should define the term stationarity. There are two definitions in the literature: one refers to “strict stationarity,” while the other refers to the “weak stationarity”: Time series is said to be weak stationary when its conditional expectation and variance are constant, and the variance is finite on all observations; Time series is said to be strong stationary when its unconditional joint probability distribution does not change over time. This automatically implies that all its moments are constant (i.e. the process is also weak stationary). The stationarity is essential in the ARIMA context and plays an important role in regression analysis. If the series is not stationary, it might be challenging to estimate its moments correctly using conventional methods. In some cases, it might be impossible to get the correct parameters (e.g. there is an infinite combination of parameters that would produce the minimum of the selected loss function). To avoid this issue, the series is transformed to ensure that the moments are finite and constant (e.g. take logarithms or do Box-Cox transform, take differences or detrend the series). After that, the model becomes easier to identify. In contrast with ARIMA, the ETS models are almost always non-stationary and do not require the series to be stationary. We will see the connection between the two approaches later in this chapter. 8.1.1 AR(p) We start with a simple AR(1) model, which is written as: \\[\\begin{equation} {y}_{t} = \\phi_1 y_{t-1} + \\epsilon_t , \\tag{8.1} \\end{equation}\\] where \\(\\phi_1\\) is the parameter of the model. This formula tells us that the value on the previous observation is carried out to the new one in the proportion of \\(\\phi_1\\). Typically, the parameter \\(\\phi_1\\) is restricted with the region (-1, 1) to make the model stationary, but very often in real life, \\(\\phi_1\\) lies in the (0, 1) region. If the parameter is equal to 1, the model becomes equivalent to Random Walk (Section 3.3.1). The forecast trajectory (conditional expectation several steps ahead) of this model would typically correspond to the exponentially declining curve. Here is a simple example in R of a very basic forecast from AR(1) with \\(\\phi_1=0.9\\) (see Figure 8.1): y &lt;- vector(&quot;numeric&quot;, 20) y[1] &lt;- 100 phi &lt;- 0.9 for(i in 2:length(y)){ y[i] &lt;- phi * y[i-1] } plot(y, type=&quot;l&quot;, xlab=&quot;horizon&quot;, ylab=&quot;Forecast&quot;) Figure 8.1: Forecast trajectory for AR(1) with \\(\\phi_1=0.9\\). If, for some reason, we get \\(\\phi_1&gt;1\\), then the trajectory corresponds to an exponential increase, becoming explosive, implying non-stationary behaviour. The model, in this case, becomes very difficult to work with, even if the parameter is close to one. So it is typically advised to restrict the parameter with the stationarity region (we will discuss this in more detail later in this chapter). In general, it is possible to imagine the situation, when the value at the moment of time \\(t\\) would depend on several previous values, so the model AR(p) would be needed: \\[\\begin{equation} {y}_{t} = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t , \\tag{8.2} \\end{equation}\\] where \\(\\phi_i\\) is the parameter for the \\(i\\)-th lag of the model. So, the model assumes that the data on the recent observation is influenced by the \\(p\\) previous observations. The more lags we introduce in the model, the more complicated the forecasting trajectory becomes, potentially demonstrating harmonic behaviour. Here is an example of a point forecast from AR(3) model \\({y}_{t} = 0.9 y_{t-1} -0.7 y_{t-2} + 0.6 y_{t-3} + \\epsilon_t\\) (Figure 8.2): y &lt;- vector(&quot;numeric&quot;, 30) y[1:3] &lt;- c(100, 75, 30) phi &lt;- c(0.9,-0.7,0.6) for(i in 4:30){ y[i] &lt;- phi[1] * y[i-1] + phi[2] * y[i-2] + phi[3] * y[i-3] } plot(y, type=&quot;l&quot;, xlab=&quot;horizon&quot;, ylab=&quot;Forecast&quot;) Figure 8.2: Forecast trajectory for an AR(3) model. No matter what the forecast trajectory of the AR model is, it will asymptotically converge to zero as long as the model is stationary. 8.1.2 MA(q) Before discussing the “Moving Averages” model, we should acknowledge that the name is quite misleading and that the model has nothing to do with Centred Moving Averages used in time series decomposition (Section 3.2) or Simple Moving Averages (discussed in Section 3.3.3). The idea of the simplest MA(1) model can be summarised in the following mathematical way: \\[\\begin{equation} {y}_{t} = \\theta_1 \\epsilon_{t-1} + \\epsilon_t , \\tag{8.3} \\end{equation}\\] where \\(\\theta_1\\) is the parameter of the model, typically lying between (-1, 1), showing what part of the error is carried out to the next observation. Because of the conventional assumption that the error term has a zero mean (\\(\\mathrm{E}(\\epsilon_{t})=0\\)), the forecast trajectory of this model is just a straight line coinciding with zero starting from the \\(h=2\\). But for the one step ahead forecast we have: \\[\\begin{equation} \\mathrm{E}({y}_{t+1}|t) = \\theta_1 \\mathrm{E}(\\epsilon_{t}|t) + \\mathrm{E}(\\epsilon_{t+1}|t) = \\theta_1 \\epsilon_{t}. \\tag{8.4} \\end{equation}\\] Starting from \\(h=2\\) there are no observable error terms \\(\\epsilon_t\\), so all the values past that are equal to zero: \\[\\begin{equation} \\mathrm{E}({y}_{t+2}|t) = \\theta_1 \\mathrm{E}(\\epsilon_{t+1}|t) + \\mathrm{E}(\\epsilon_{t+2}|t) = 0. \\tag{8.5} \\end{equation}\\] So, the forecast trajectory for MA(1) model converges to zero, when \\(h&gt;1\\). More generally, MA(q) model is written as: \\[\\begin{equation} {y}_{t} = \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_q \\epsilon_{t-q} + \\epsilon_t , \\tag{8.6} \\end{equation}\\] where \\(\\theta_i\\) is the parameters for the \\(i\\)-th lag of the error term, typically restricted with the so-called invertibility region (discussed in the next section). In this case, the model implies that the recent observation is influenced by several errors on previous observations (your mistakes in the past will haunt you in the future). The more lags we introduce, the more complicated the model becomes. As for the forecast trajectory, it will reach zero when \\(h&gt;q\\). 8.1.3 ARMA(p,q) Connecting the models (8.2) and (8.6), we get the more complicated model, ARMA(p,q): \\[\\begin{equation} {y}_{t} = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_q \\epsilon_{t-q} + \\epsilon_t , \\tag{8.7} \\end{equation}\\] which has the properties of the two models discussed above. The forecast trajectory from this model will have a combination of trajectories for AR and MA for \\(h \\leq q\\) and then will correspond to AR(p) for \\(h&gt;q\\). To simplify the work with ARMA models, the equation (8.7) is typically rewritten by moving all terms with \\(y_t\\) to the left-hand side: \\[\\begin{equation} {y}_{t} -\\phi_1 y_{t-1} -\\phi_2 y_{t-2} -\\dots -\\phi_p y_{t-p} = \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_q \\epsilon_{t-q} + \\epsilon_t . \\tag{8.8} \\end{equation}\\] Furthermore, in order to make this even more compact, the backshift operator B can be introduced. It just shows by how much the subscript of the variable is shifted back in time: \\[\\begin{equation} {y}_{t} B^i = {y}_{t-i}. \\tag{8.9} \\end{equation}\\] Using (8.9), the ARMA model can be written as: \\[\\begin{equation} {y}_{t} (1 -\\phi_1 B -\\phi_2 B^2 -\\dots -\\phi_p B^p) = \\epsilon_t (1 + \\theta_1 B + \\theta_2 B^2 + \\dots + \\theta_q B^q) . \\tag{8.10} \\end{equation}\\] Finally, we can also introduce the AR and MA polynomial functions to make the model even more compact: \\[\\begin{equation} \\begin{aligned} &amp; \\varphi^p(B) = 1 -\\phi_1 B -\\phi_2 B^2 -\\dots -\\phi_p B^p \\\\ &amp; \\vartheta^q(B) = 1 + \\theta_1 B + \\theta_2 B^2 + \\dots + \\theta_q B^q . \\end{aligned} \\tag{8.11} \\end{equation}\\] Inserting the functions (8.11) in (8.10) leads to the compact presentation of ARMA model: \\[\\begin{equation} {y}_{t} \\varphi^p(B) = \\epsilon_t \\vartheta^q(B) . \\tag{8.12} \\end{equation}\\] The model (8.12) can be considered a compact form of (8.7). It is more difficult to understand and interpret but easier to work with from a mathematical point of view. In addition, this form permits introducing additional elements, which will be discussed later in this chapter. Coming back to the ARMA model (8.7), as discussed earlier, it assumes convergence of forecast trajectory to zero, the speed of which is regulated via the parameters. This implies that the data has the mean of zero, and ARMA should be applied to somehow pre-processed data so that it is stationary and varies around zero. This means that if you work with non-stationary and/or with non-zero mean data, the pure AR/MA or ARMA will be inappropriate – some prior transformations are in order. 8.1.4 ARMA with constant One of the simpler ways to deal with the issue with zero forecasts is to introduce the constant (or intercept) in ARMA: \\[\\begin{equation} {y}_{t} = a_0 + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_p \\epsilon_{t-p} + \\epsilon_t \\tag{8.13} \\end{equation}\\] or \\[\\begin{equation} {y}_{t} \\varphi^p(B) = a_0 + \\epsilon_t \\vartheta^q(B) , \\tag{8.14} \\end{equation}\\] where \\(a_0\\) is the constant parameter, which in this case also works as the unconditional mean of the series. The forecast trajectory in this case would converge to \\(a_0\\) instead of zero, but with some minor distinctions from the ARMA without constant. For example, in case of ARMA(1,1) with constant we will have: \\[\\begin{equation} {y}_{t} = a_0 + \\phi_1 y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\epsilon_t . \\tag{8.15} \\end{equation}\\] The conditional expectation of \\(y_{t+h}\\) for \\(h=1\\) and \\(h=2\\) can be written as (based on the discussions in previous sections): \\[\\begin{equation} \\begin{aligned} &amp; \\mathrm{E}({y}_{t+1}|t) = a_0 + \\phi_1 y_{t} + \\theta_1 \\epsilon_{t} \\\\ &amp; \\mathrm{E}({y}_{t+2}|t) = a_0 + \\phi_1 \\mathrm{E}(y_{t+1}|t) = a_0 + \\phi_1 a_0 + \\phi_1^2 y_{t} + \\phi_1 \\theta_1 \\epsilon_t \\end{aligned} , \\tag{8.16} \\end{equation}\\] or in general for the horizon \\(h\\): \\[\\begin{equation} \\mathrm{E}({y}_{t+h}|t) = \\sum_{j=1}^h a_0\\phi_1^{j-1} + \\phi_1^h y_{t} + \\phi_1^{h-1} \\theta_1 \\epsilon_{t} . \\tag{8.17} \\end{equation}\\] So, the forecast trajectory from this model dampens out, similar to the ETS(A,Ad,N) model (Subsection 4.3.2), and the rate of dampening is regulated by the value of \\(\\phi_1\\). The following simple example demonstrates this point (see Figure 8.3; I drop the MA(1) part because it does not change the shape of the curve): y &lt;- vector(&quot;numeric&quot;, 20) y[1] &lt;- 100 phi &lt;- 0.9 for(i in 2:length(y)){ y[i] &lt;- 100 + phi * y[i-1] } plot(y, type=&quot;l&quot;, xlab=&quot;horizon&quot;, ylab=&quot;Forecast&quot;) Figure 8.3: Forecast trajectory for an ARMA(1,1) model with constant. The more complicated ARMA(p,q) models with p&gt;1 will have more complex trajectories with potential harmonics, but the idea of dampening in AR(p) part of the model stays. Finally, as alternative to adding \\(a_0\\), each actual value of \\(y_t\\) can be centred via \\(y^\\prime_t = y_t -\\bar{y}\\), making sure that the mean of \\(y^\\prime_t\\) is zero and ARMA can be applied to the \\(y^\\prime_t\\) data instead of \\(y_t\\). However, this approach introduces additional steps, but the result on stationary data is typically the same as adding the constant. 8.1.5 I(d) Based on the previous discussion, we can conclude that ARMA cannot be applied to non-stationary data. So, if we deal with one, we need to make it stationary somehow. The conventional way of doing that is by taking differences in the data. The logic behind this is straightforward: if the data is not stationary, the mean somehow changes over time. This can be, for example, due to a trend in the data. In this case, we should be talking about the change of variable \\(y_t\\) rather than the variable itself. So we should work on the following data instead: \\[\\begin{equation} \\Delta y_t = y_t -y_{t-1} = y_t (1 -B), \\tag{8.18} \\end{equation}\\] if the differences have constant mean. The simplest model with differences is I(1), which is also known as the Random walk (see Section 3.3.1): \\[\\begin{equation} \\Delta y_t = \\epsilon_t, \\tag{8.19} \\end{equation}\\] which can be reformulated in a simpler, more interpretable form by inserting (8.18) in (8.19) and regrouping elements: \\[\\begin{equation} y_t = y_{t-1} + \\epsilon_t. \\tag{8.20} \\end{equation}\\] The model (8.20) can also be perceived as AR(1) with \\(\\phi_1=1\\). This is a non-stationary model, meaning that the unconditional mean of \\(y_t\\) is not constant. The forecast from this model corresponds to the Naïve method (see Section 3.3.1) with a straight line equal to the last observed actual value (again, assuming that \\(\\mathrm{E}(\\epsilon_{t})=0\\) and that other basic assumptions from Section 1.4.1 hold): \\[\\begin{equation} \\mathrm{E}(y_{t+h}|t) = \\mathrm{E}(y_{t+h-1}|t) + \\mathrm{E}(\\epsilon_{t+h}|t) = y_{t} . \\tag{8.21} \\end{equation}\\] Another simple model that relies on differences of the data is called Random Walk with drift (which was discussed in Section 3.3.4) and is formulated by adding constant \\(a_0\\) to the right hand side of equation (8.19): \\[\\begin{equation} \\Delta y_t = a_0 + \\epsilon_t. \\tag{8.22} \\end{equation}\\] This model has some similarities with the global level model, which is formulated via the actual value rather than differences (see Section 3.3.2): \\[\\begin{equation} {y}_{t} = a_0 + \\epsilon_t. \\tag{8.23} \\end{equation}\\] Using a similar regrouping as with the Random Walk, we can obtain a simpler form of (8.22): \\[\\begin{equation} y_t = a_0 + y_{t-1} + \\epsilon_t. \\tag{8.24} \\end{equation}\\] which is, again, equivalent to AR(1) model with \\(\\phi_1=1\\), but this time with a constant. The term “drift” appears because \\(a_0\\) acts as an additional element, showing the tendency in the data: if it is positive, the model will exhibit a positive trend; if it is negative, the trend will be negative. This can be seen for the conditional mean, for example, for the case of \\(h=2\\): \\[\\begin{equation} \\mathrm{E}(y_{t+2}|t) = \\mathrm{E}(a_0) + \\mathrm{E}(y_{t+1}|t) + \\mathrm{E}(\\epsilon_{t+2}|t) = a_0 + \\mathrm{E}(a_0 + y_t + \\epsilon_t|t) = 2 a_0 + y_t , \\tag{8.25} \\end{equation}\\] or in general for the horizon \\(h\\): \\[\\begin{equation} \\mathrm{E}(y_{t+h}|t) = h a_0 + y_t . \\tag{8.26} \\end{equation}\\] In a manner similar to (8.18), we can also introduce second differences of the data (differences of differences) if we suspect that the change of variable over time is not stationary, which would be written as: \\[\\begin{equation} \\Delta^2 y_t = \\Delta y_t -\\Delta y_{t-1} = y_t -y_{t-1} -y_{t-1} + y_{t-2}, \\tag{8.27} \\end{equation}\\] which can also be written using the backshift operator: \\[\\begin{equation} \\Delta^2 y_t = y_t(1 -2B + B^2) = y_t (1 -B)^2. \\tag{8.28} \\end{equation}\\] In fact, we can introduce higher level differences if we want (but typically we should not) based on the idea of (8.28): \\[\\begin{equation} \\Delta^d = (1-B)^d. \\tag{8.29} \\end{equation}\\] Using (8.29), the I(d) model is formulated as: \\[\\begin{equation} \\Delta^d y_t = \\epsilon_t. \\tag{8.30} \\end{equation}\\] 8.1.6 ARIMA(p,d,q) Finally, having made the data stationary via the differences, we can introduce ARMA elements (8.12) to it which would be done on the differenced data, instead of the original \\(y_t\\): \\[\\begin{equation} y_t \\Delta^d(B) \\varphi^p(B) = \\epsilon_t \\vartheta^q(B) , \\tag{8.31} \\end{equation}\\] or in a more general form (8.10) with (8.28): \\[\\begin{equation} y_t (1-B)^d (1 -\\phi_1 B -\\dots -\\phi_p B^p) = \\epsilon_t (1 + \\theta_1 B + \\dots + \\theta_q B^q), \\tag{8.32} \\end{equation}\\] which is ARIMA(p,d,q) model. This model allows producing trends with some values of differences and also inherits the trajectories from both AR(p) and MA(q). This implies that the point forecasts from the model can exhibit complicated trajectories, depending on the values of the model’s parameters. The model (8.32) is difficult to interpret in a general form, but opening the brackets and moving all elements but \\(y_t\\) to the right-hand side helps understand each specific model. 8.1.7 Parameters bounds ARMA models have two conditions that need to be satisfied for them to be useful and to work appropriately: Stationarity, Invertibility. Condition (1) has already been discussed in Section 8.1. It is imposed on the model’s AR parameters, ensuring that the forecast trajectories do not exhibit explosive behaviour (in terms of both mean and variance). (2) is equivalent to the stability condition in ETS (Section 5.4) and refers to the MA parameters: it guarantees that the old observations do not have an increasing impact on the recent ones. The term “invertibility” comes from the idea that any MA process can be represented as an infinite AR process via the inversion of the parameters. For example, MA(1) model, which is written as: \\[\\begin{equation} y_t = \\epsilon_t + \\theta_1 \\epsilon_{t-1} = \\epsilon_t (1 + \\theta_1 B) , \\tag{8.33} \\end{equation}\\] can be rewritten as: \\[\\begin{equation} y_t (1 + \\theta_1 B)^{-1} = \\epsilon_t, \\tag{8.34} \\end{equation}\\] or in a slightly easier to digest form (based on (8.33) and the idea that \\(\\epsilon_{t} = y_{t} -\\theta_1 \\epsilon_{t-1}\\), implying that \\(\\epsilon_{t-1} = y_{t-1} -\\theta_1 \\epsilon_{t-2}\\)): \\[\\begin{equation} y_t = \\theta_1 y_{t-1} -\\theta_1^2 \\epsilon_{t-2} + \\epsilon_t = \\theta_1 y_{t-1} -\\theta_1^2 y_{t-2} + \\theta_1^3 \\epsilon_{t-2} + \\epsilon_t = \\sum_{j=1}^\\infty -1^{j-1} \\theta_1^j y_{t-j} + \\epsilon_t. \\tag{8.35} \\end{equation}\\] The recursion in (8.35) shows that the recent actual value \\(y_t\\) depends on the previous infinite number of values of \\(y_{t-j}\\) for \\(j=\\{1,\\dots,\\infty\\}\\). The parameter \\(\\theta_1\\), in this case, is exponentiated and leads to the distribution of weights in this infinite series exponentially (reminds SES from Section 4.1, doesn’t it?). The invertibility condition makes sure that those weights decline over time with the increase of \\(j\\) so that the older observations do not have an increasing impact on the most recent \\(y_t\\). There are different ways how to check both conditions, the conventional one is by calculating the roots of the polynomial equations: \\[\\begin{equation} \\begin{aligned} &amp; \\varphi^p(B) = 0 \\text{ for AR} \\\\ &amp; \\vartheta^q(B) = 0 \\text{ for MA} \\end{aligned} , \\tag{8.36} \\end{equation}\\] or expanding the functions in (8.36) and substituting \\(B\\) with a variable \\(x\\) (for convenience): \\[\\begin{equation} \\begin{aligned} &amp; 1 -\\phi_1 x -\\phi_2 x^2 -\\dots -\\phi_p x^p = 0 \\text{ for AR} \\\\ &amp; 1 + \\theta_1 x + \\theta_2 x^2 + \\dots + \\theta_q x^q = 0 \\text{ for MA} \\end{aligned} . \\tag{8.37} \\end{equation}\\] Solving the first equation for \\(x\\) in (8.37), we get \\(p\\) roots (some of them might be complex numbers). For the model to be stationary, all the roots must be greater than one by absolute value. Similarly, if all the roots of the second equation in (8.37) are greater than one by absolute value, then the model is invertible (aka stable). Calculating roots of polynomials is a difficult task, so there are simpler special cases for both conditions that guarantee that the more complicated ones are satisfied: \\[\\begin{equation} \\begin{aligned} &amp; 0 &lt; \\sum_{j=1}^p \\phi_j &lt; 1 \\\\ &amp; 0 &lt; \\sum_{j=1}^q \\theta_j &lt; 1 \\end{aligned} . \\tag{8.38} \\end{equation}\\] But note that the condition (8.38) is rather restrictive and not generally applicable for all ARIMA models. It can be used to skip the check of the more complicated condition (8.37) if it is satisfied by a set of estimated parameters. Finally, in a special case with AR(p) model with \\(0 &lt; \\sum_{j=1}^p \\phi_j &lt; 1\\) and \\(\\sum_{j=1}^p \\phi_j = 1\\), we end up with the moving weighted average, which is a non-stationary model. This becomes apparent from the connection between Simple Moving Average and AR processes (Svetunkov and Petropoulos, 2018). References "],["seasonal-arima.html", "8.2 Seasonal ARIMA", " 8.2 Seasonal ARIMA 8.2.1 Single seasonal ARIMA When it comes to the actual data, we typically have relations between consecutive observations and between observations happening with some fixed seasonal lags. In the ETS framework, these relations are taken care of via seasonal indices, repeating every \\(m\\) observations. In the ARIMA framework, this is done via introducing lags in the model elements. For example, seasonal AR(P)\\(_m\\) with lag \\(m\\) can be written similar to AR(p), but with some minor modifications: \\[\\begin{equation} {y}_{t} = \\phi_{m,1} y_{t-m} + \\dots + \\phi_{m,P} y_{t-Pm} + \\varepsilon_t , \\tag{8.39} \\end{equation}\\] where \\(\\phi_{m,i}\\) is the parameter for the lagged \\(im\\) actual value in the model, and \\(\\varepsilon_t\\) is the error term of the seasonal AR model. We use the underscore “m” to show that the parameters here refer to the seasonal part of the model. The idea of the model (8.39) on the example of monthly data is that the current observation is influenced by a similar value, the same month a year ago, then the same month two years ago etc. This is hard to justify from the theoretical point of view, but this model allows capturing complex relations in the data. Similarly to seasonal AR(P), we can have seasonal MA(Q)\\(_m\\): \\[\\begin{equation} {y}_{t} = \\theta_{m,1} \\varepsilon_{t-m} + \\dots + \\theta_{m,Q} \\varepsilon_{t-Qm} + \\varepsilon_t , \\tag{8.40} \\end{equation}\\] where \\(\\theta_{m,i}\\) is the parameter for the lagged error term in the model. This model is even more difficult to justify than the MA(q) because it is difficult to explain how the white noise the same month last year can impact the actual value this year. Still, this is a useful instrument for forecasting purposes. Finally, we have the seasonal differences, I(D)\\(_m\\), which are easier to present using the backshift operator: \\[\\begin{equation} y_t (1-B^m)^D = \\varepsilon_t. \\tag{8.41} \\end{equation}\\] The seasonal differences allow dealing with the seasonality that changes its amplitude in the data, i.e. model the multiplicative seasonality via ARIMA by making the seasonality itself stationary. A special case of I(D) model is I(1)\\(_m\\), which is a seasonal Random Walk, underlying Seasonal Naïve method from Section 3.3.5: \\[\\begin{equation} y_t (1-B^m) = \\varepsilon_t, \\tag{8.42} \\end{equation}\\] or equivalently: \\[\\begin{equation} y_t = y_{t-m} + \\varepsilon_t. \\tag{8.43} \\end{equation}\\] Combining (8.39), (8.40) and (8.41) we get pure seasonal ARIMA(P,D,Q)\\(_m\\) model in the compact notation, similar to the one we had for ARIMA(p,d,q): \\[\\begin{equation} y_t (1-B^m)^D (1 -\\phi_{m,1} B^m -\\dots -\\phi_{m,P} B^{Pm}) = \\varepsilon_t (1 + \\theta_{m,1} B^m + \\dots + \\theta_{m,Q} B^{Qm}), \\tag{8.44} \\end{equation}\\] or if we introduce the polynomial functions for seasonal AR and MA and use notation similar to (8.28): \\[\\begin{equation} y_t \\Delta^D(B^m) \\varphi^P(B^m) = \\varepsilon_t \\vartheta^Q(B^m), \\tag{8.45} \\end{equation}\\] where \\[\\begin{equation} \\begin{aligned} &amp; \\Delta^D(B^m) = (1-B^m)^D \\\\ &amp; \\varphi^P(B^m) = 1 -\\phi_{m,1} B^m -\\dots -\\phi_{m,P} B^{Pm} \\\\ &amp; \\vartheta^Q(B^m) = 1 + \\theta_{m,1} B^m + \\dots + \\theta_{m,Q} B^{Qm} . \\end{aligned} \\tag{8.46} \\end{equation}\\] Now that we have taken care of the seasonal part of the model, we should not forget that there is a non-seasonal one. If it exists in the data, then \\(\\varepsilon_t\\) would not be just a white noise, but could be modelled using a non-seasonal ARIMA(p,d,q): \\[\\begin{equation} \\varepsilon_t \\Delta^d(B) \\varphi^p(B) = \\epsilon_t \\vartheta^q(B), \\tag{8.47} \\end{equation}\\] implying that: \\[\\begin{equation} \\varepsilon_t = \\epsilon_t \\frac{\\vartheta^q(B)}{\\Delta^d(B) \\varphi^p(B)}. \\tag{8.48} \\end{equation}\\] Inserting (8.48) into (8.45), we get the final SARIMA(p,d,q)(P,D,Q)\\(_m\\) model in the compact form after regrouping the polynomials: \\[\\begin{equation} y_t \\Delta^D(B^m) \\varphi^P(B^m) \\Delta^d(B) \\varphi^p(B) = \\epsilon_t \\vartheta^Q(B^m) \\vartheta^q(B) . \\tag{8.49} \\end{equation}\\] The equation (8.49) does not tell us much about what happens in the model. It just shows how different elements interact with each other in it. To understand, what SARIMA means, we need to take an example and see what impacts the current actual value. For example, here is what we will have in the case of SARIMA(1,0,1)(1,0,1)\\(_4\\) (i.e. applied to stationary quarterly data): \\[\\begin{equation} y_t \\Delta^0(B^4) \\varphi^1(B^4) \\Delta^0(B) \\varphi^1(B) = \\epsilon_t \\vartheta^1(B^4) \\vartheta^1(B) . \\tag{8.50} \\end{equation}\\] Inserting the values of polynomials (8.46), (8.29) and (8.11) in (8.50), we get: \\[\\begin{equation} y_t (1 -\\phi_{4,1} B^4)(1 -\\phi_{1} B) = \\epsilon_t (1 + \\theta_{4,1} B^4) (1 + \\theta_{1} B), \\tag{8.51} \\end{equation}\\] which is slightly easier to understand but still does not explain how past values impact the present. So, we open the brackets and move all the elements except for \\(y_t\\) to the right-hand side of the equation to get: \\[\\begin{equation} y_t = \\phi_{1} y_{t-1} + \\phi_{4,1} y_{t-4} -\\phi_{1} \\phi_{4,1} y_{t-5} + \\theta_1 \\epsilon_{t-1} + \\theta_{4,1} \\epsilon_{t-4} + \\theta_{1} \\theta_{4,1} \\epsilon_{t-5} + \\epsilon_t . \\tag{8.52} \\end{equation}\\] So, now we see that SARIMA(1,0,1)(1,0,1)\\(_4\\) implies that the present values is impacted by the value in the previous quarter (\\(\\phi_{1} y_{t-1} + \\theta_1 \\epsilon_{t-1}\\)), the value last year (\\(\\phi_{4,1} y_{t-4} + \\theta_{4,1} \\epsilon_{t-4}\\)) on the same quarter and the value from last year on the previous quarter (\\(-\\phi_{1} \\phi_{4,1} y_{t-5} + \\theta_{1} \\theta_{4,1} \\epsilon_{t-5}\\)), which introduces a much more complicated interaction than any ETS model can. However, this complexity is obtained with a minimum number of parameters: we have three lagged actual values and three lagged error terms, but we only have four parameters to estimate, not six. The more complicated SARIMA models would have even more complicated interactions, making it more challenging to interpret, but all of that comes with a benefit of having a parsimonious model with just \\(p+q+P+Q\\) parameters to estimate. When it comes to forecasting from such model as SARIMA(1,0,1)(1,0,1)\\(_4\\), the trajectories would have elements of the classical ARMA model, discussed in Section 8.1.3, converging to zero as long as there is no constant and the model is stationary. The main difference would be in having the seasonal element. Here is an R example of a prediction for such a model for \\(h&gt;m+1\\) (see Figure 8.4; MA part is dropped because the expectation of the error term is assumed to be equal to zero): y &lt;- vector(&quot;numeric&quot;, 100) y[1:5] &lt;- c(97,87,85,94,95) phi &lt;- c(0.6,0.8) for(i in 6:length(y)){ y[i] &lt;- phi[1] * y[i-1] + phi[2] * y[i-4] - phi[1] * phi[2] * y[i-5] } plot(y, type=&quot;l&quot;, xlab=&quot;horizon&quot;, ylab=&quot;Forecast&quot;) Figure 8.4: Forecast trajectory for a SARIMA(1,0,1)(1,0,1)\\(_4\\). As we see from Figure 8.4, the values converge to zero due to \\(0&lt;\\phi_1&lt;1\\) and the seasonality disappears because \\(0&lt;\\phi_{4,1}&lt;1\\) as well. So, this is the forecast implied by the SARIMA without differences. If the differences are introduced, then the model would produce non-stationary and seasonally non-stationary trajectories. 8.2.2 SARIMA with constant In addition, it is possible to add the constant term to the SARIMA model, and it will have a more complex effect on the forecast trajectory, depending on the order of the model. In case of zero differences, the effect will be similar to ARMA (Section 8.1.4), introducing the dampening trajectory. Here is an example (see Figure 8.5): y &lt;- vector(&quot;numeric&quot;, 100) y[1:5] &lt;- c(97,87,85,94,95) phi &lt;- c(0.6,0.8) for(i in 6:length(y)){ y[i] &lt;- phi[1] * y[i-1] + phi[2] * y[i-4] - phi[1] * phi[2] * y[i-5] + 8 } plot(y, type=&quot;l&quot;, xlab=&quot;horizon&quot;, ylab=&quot;Forecast&quot;) Figure 8.5: Forecast trajectory for a SARIMA model with constant. In case of the model with the differences, the constant would have a two-fold effect: working as a drift for the non-seasonal part and increasing the amplitude of seasonality for the seasonal one. Here is an example from SARIMA(1,0,0)(1,1,0)\\(_4\\) with constant: \\[\\begin{equation} y_t (1 -\\phi_{4,1} B^4)(1 -\\phi_{1} B) (1 -B^4) = \\epsilon_t + a_0 , \\tag{8.53} \\end{equation}\\] which can be reformulated as (after opening brackets and moving elements to the right-hand side): \\[\\begin{equation} y_t = \\phi_{1} y_{t-1} + (1+\\phi_{4,1}) y_{t-4} -(1+\\phi_{4,1}) \\phi_{1} y_{t-5} -\\phi_{4,1} y_{t-8} + \\phi_1 \\phi_{4,1} y_{t-9} + a_0 + \\epsilon_t . \\tag{8.54} \\end{equation}\\] This formula can then be used to see, what the trajectory from such model will be: y &lt;- vector(&quot;numeric&quot;, 100) y[1:9] &lt;- c(96,87,85,94,97,88,86,95,98) phi &lt;- c(0.6,0.8) for(i in 10:length(y)){ y[i] &lt;- phi[1] * y[i-1] + (1+phi[2]) * y[i-4] - (1+ phi[2]) *phi[1] * y[i-5] - phi[2] * y[i-8] + phi[1] * phi[2] * y[i-9] + 0.1 } plot(y, type=&quot;l&quot;, xlab=&quot;horizon&quot;, ylab=&quot;Forecast&quot;) Figure 8.6: Forecast trajectory for a SARIMA model with differences and a constant. As we see from Figure 8.6, the trajectory exhibits a drift, coming from the non-seasonal part of the model and a stable seasonality (the amplitude of which does not converge to zero anymore). More complex behaviours for the future trajectories can be obtained with higher orders of seasonal and non-seasonal parts of the SARIMA model. 8.2.3 Multiple seasonal ARIMA Using the same approach as the conventional SARIMA, we can introduce more terms (similar to how James W. Taylor, 2003b did it) with several seasonal frequencies. For example, we can have an hour of the day, a day of the week and a week of year frequencies in the data. Given that we work with the hourly data in this situation, we should introduce three seasonal ARIMA elements with seasonalities \\(m_1=24\\), \\(m_2=24 \\times 7\\) and \\(m_3=24 \\times 7 \\times 365\\). In this example, we would have AR, I and MA polynomials for each seasonal part of the model, introducing a triple seasonal ARIMA, which is not even easy to formulate in the compact form. This type of model with multiple seasonal components can be called “Multiple Seasonal ARIMA,” MSARIMA, which in general can be written as: \\[\\begin{equation} y_t \\Delta^{D_n}(B^{m_n}) \\varphi^{P_n}(B^{m_n}) \\dots \\Delta^{D_0}(B^{m_0}) \\varphi^{P_0}(B^{m_0}) = \\epsilon_t \\vartheta^{Q_n}(B^{m_n}) \\dots \\vartheta^{Q_0}(B^{m_0}) , \\tag{8.55} \\end{equation}\\] where \\(n\\) is the number of seasonal cycles, and \\(D_0=d\\), \\(P_0=p\\), \\(Q_0=q\\) and \\(m_0=1\\) for convenience. The slightly more compact and even less comprehensible form of (8.55) is: \\[\\begin{equation} y_t \\prod_{j=0}^n \\Delta^{D_j} (B^{m_j}) \\varphi^{P_j}(B^{m_j}) = \\epsilon_t \\prod_{j=0}^n \\vartheta^{Q_j}(B^{m_j}) , \\tag{8.56} \\end{equation}\\] Conceptually, the model (8.56) is neat, as it captures all the complex relations in the data, but it is not easy to understand and work with, not to mention the potential estimation and order selection problems. To understand what the forecast from such a model can be, we would need to take a special case, multiply the polynomials and move all the past elements on the right-hand side, leaving only \\(y_t\\) on the left-hand side one, as we did with SARIMA example above. It is worth noting that the msarima() function from the smooth package implements the model (8.56), although not in this form, but in the state space form, discussed in Chapter 9. 8.2.4 Parameters bounds for MSARIMA When it comes to parameters bounds of SARIMA, the logic stays similar to the process discussed for the case of non-seasonal model in Section 8.1.7, with the only difference being that instead of analysing the polynomials of a specific part of a model, we need to consider the product of all polynomials. So, the stationarity condition for the MSARIMA is for all the roots of the following polynomial to be greater than one by absolute value (lie outside the unit circle): \\[\\begin{equation} \\prod_{j=0}^n \\varphi^{P_j}(B^{m_j}) = 0, \\tag{8.57} \\end{equation}\\] while the invertibility condition is for all the roots of the following polynomial to lie outside the unit circle: \\[\\begin{equation} \\prod_{j=0}^n \\vartheta^{Q_j}(B^{m_j}) = 0. \\tag{8.58} \\end{equation}\\] Both of these conditions are difficult to check, especially for high frequencies \\(m_j\\): the polynomial equation of order \\(n\\) has \\(n\\) complex roots, so if you fit a multiple seasonal ARIMA on hourly data, where the maximum frequency is \\(24\\times 7\\times 365 = 61,320\\), then the equation will have at least 61,320 roots (this number will increase if there are lower frequencies or non-seasonal orders of the model). Finding all of them is not a trivial task even for modern computers (for example, the polyroot() function from the base package cannot handle this). So, when considering ARIMA on high-frequency data with high seasonal frequency values, it might make sense to find other ways of checking the stationarity and stability conditions. The msarima() and the adam() functions in the smooth package use the state space form of ARIMA (discussed in Chapter 9.1) and rely on slightly different principles of checking the same conditions. They do that more efficiently than in the case of the conventional approach of finding the roots of polynomials (8.57) and (8.58). References "],["BJApproach.html", "8.3 Box-Jenkins approach", " 8.3 Box-Jenkins approach Now that we are more or less familiar with the idea of ARIMA models, we can move to practicalities. As it might become apparent from the previous sections, one of the issues with the model is the identification of orders p, d, q, P\\(_j\\), D\\(_j\\), Q\\(_j\\) etc. Back in the 20th century, when computers were slow, this was a challenging task, so George Box and Gwilym Jenkins (Box and Jenkins, 1976) developed a methodology for identifying and estimating ARIMA models. While there are more efficient ways of order selection for ARIMA nowadays, some of their principles are still used in time series analysis and in forecasting. We briefly outline the idea in this section, not purporting to give a detailed explanation of the approach. 8.3.1 Identifying stationarity Before doing any time series analysis, we need to make the data stationary, which is done via the differences in the context of ARIMA (Section 8.1.5). But before doing anything, we need to understand whether the data is stationary or not in the first place: over-differencing typically is harmful to the model and would lead to misspecification issues. At the same time, in the case of under-differencing, it might not be possible to identify the model correctly. There are different ways of understanding whether the data is stationary or not. The simplest of them is just looking at the data: in some cases, it becomes evident that the mean of the data changes or that there is a trend in the data, so the conclusion would be relatively easy to make. If it is not stationary, then taking differences and analysing the differenced data again would be the next step to ensure that the second differences are not needed. The more formal approach would be to conduct statistical tests, such as ADF (the adf.test() from the tseries package) or KPSS (the kpss.test() from the tseries package). Note that they test different hypotheses: In the case of ADF, it is: H\\(_0\\): the data is not stationary; H\\(_1\\): the data is stationary; In the case of KPSS: H\\(_0\\): the data is stationary; H\\(_1\\): the data is not stationary; I do not plan to discuss how the tests are conducted and what they imply. It should suffice to say that ADF is based on estimating parameters of the AR model and then testing the hypothesis for those parameters, while KPSS includes the component of Random Walk in a model (with potential trend) and checks whether the variance of that component is zero or not. Both tests have their advantages and disadvantages and sometimes might contradict each other. No matter what test you choose, do not forget what testing a statistical hypothesis means (see Section 5.3 of Svetunkov, 2021): if you fail to reject H\\(_0\\), it does not mean that it is true. Note that even if you select the test-based approach, the procedure should still be iterative: test the hypothesis, take differences if needed, test the hypothesis again etc. This way, we can determine the order of differences I(d). When you work with seasonal data, the situation becomes more complicated. Yes, you can probably spot seasonality by visualising the data, but it is not easy to conclude whether the seasonal differences are needed. In this case, the Canova-Hansen test (the ch.test() in the uroot package) can be used to formally test the hypothesis similar to the one in the KPSS test, but about the seasonal differences. Only after making sure that the data is stationary we can move to the identification of AR and MA orders. 8.3.2 Autocorrelation function (ACF) In the core of the Box-Jenkins approach, lies the idea of autocorrelation and partial autocorrelation functions. Autocorrelation is the correlation (see Section 6.3 of Svetunkov, 2021) of a variable with itself from a different period of time. Here is an example of autocorrelation coefficient for lag 1: \\[\\begin{equation} \\rho(1) = \\frac{\\sigma_{y_t,y_{t-1}}}{\\sigma_{y_t}\\sigma_{y_{t-1}}} = \\frac{\\sigma_{y_t,y_{t-1}}}{\\sigma_{y_t}^2}, \\tag{8.59} \\end{equation}\\] where \\(\\rho(1)\\) is the “true” autocorrelation coefficient, \\(\\sigma_{y_t,y_{t-1}}\\) is the covariance between \\(y_t\\) and \\(y_{t-1}\\), while \\(\\sigma_{y_t}\\) and \\(\\sigma_{y_{t-1}}\\) are the “true” standard deviations of \\(y_t\\) and \\(y_{t-1}\\). Note that \\(\\sigma_{y_t}=\\sigma_{y_{t-1}}\\), because we are talking about one and the same variable, thus the simpler formula on the right-hand side of (8.59). The formula (8.59) corresponds to the classical correlation coefficient, so this interpretation is the same as for the classical one: the value of \\(\\rho(1)\\) shows the closeness of the lagged relation to linear. If it is close to one, then this means that variable has a strong linear relation with itself on the previous observation. It obviously does not tell you anything about the causality, just shows a technical relation between variables, even if it is spurious in real life. Using the formula (8.59), we can calculate the autocorrelation coefficients for other lags as well, just substituting \\(y_{t-1}\\) with \\(y_{t-2}\\), \\(y_{t-3}\\), \\(\\dots\\), \\(y_{t-\\tau}\\) etc. In a way, \\(\\rho(\\tau)\\) can be considered as a function of a lag \\(\\tau\\), which is called the “Autocorrelation function” (ACF). If we know the order of the ARIMA process we deal with, we can plot the values of ACF on the y-axis by changing the \\(\\tau\\) on the x-axis. Box and Jenkins (1976) discuss different theoretical functions for several special cases of ARIMA, which we do not plan to repeat here fully. But, for example, they show that if you deal with AR(1) process, then the \\(\\rho(1)=\\phi_1\\), \\(\\rho(2)=\\phi_1^2\\) etc. This can be seen on the example of \\(\\rho(1)\\) by calculating the covariance for AR(1): \\[\\begin{equation} \\sigma_{y_t,y_{t-1}} = \\mathrm{cov}(y_t,y_{t-1}) = \\mathrm{cov}(\\phi_1 y_{t-1} + \\epsilon_t, y_{t-1}) = \\mathrm{cov}(\\phi_1 y_{t-1}, y_{t-1}) = \\phi_1 \\sigma_{y_t}^2 , \\tag{8.60} \\end{equation}\\] which when inserted in (8.59) leads to \\(\\rho(1)=\\phi_1\\). The ACF for AR(1) with a positive \\(\\phi_1\\) will have the shape shown in Figure 8.7 (on the example of \\(\\phi_1=0.9\\)). Figure 8.7: ACF for AR(1) model. Note that \\(\\rho(0)=1\\) just because the value is correlated with itself, so lag 0 is typically dropped as not being useful. The declining shape of the ACF tells us that if \\(y_t\\) is correlated with \\(y_{t-1}\\), then the correlation between \\(y_{t-1}\\) and \\(y_{t-2}\\) will be exactly the same, also implying that \\(y_{t}\\) is somehow correlated with \\(y_{t-2}\\), even if there is no true correlation between them. It is difficult to say anything for the AR process based on ACF exactly because of this temporal relation of the variable with itself. On the other hand, ACF can be used to judge the order of MA(q) process. For example, if we consider MA(1) (Section 8.1.2), then the \\(\\rho(1)\\) will depend on the following covariance: \\[\\begin{equation} \\sigma_{y_t,y_{t-1}} = \\mathrm{cov}(y_t,y_{t-1}) = \\mathrm{cov}(\\theta_1 \\epsilon_{t-1} + \\epsilon_t, \\theta_1 \\epsilon_{t-2} + \\epsilon_{t-1}) = \\mathrm{cov}(\\theta_1 \\epsilon_{t-1}, \\epsilon_{t-1}) = \\theta_1 \\sigma^2 , \\tag{8.61} \\end{equation}\\] where \\(\\sigma^2\\) is the variance of the error term, which in case of MA(1) is equal to \\(\\sigma^2_{y_t}\\), because E\\((y_t)=0\\). However, the covariance between the higher lags will be equal to zero for the pure MA(1) (given that the usual assumptions from Section 1.4.1 hold). Box and Jenkins (1976) showed that for the moving averages, ACF tells more about the order of the model than for the autoregressive one: ACF will drop rapidly right after the specific lag q for the MA(q) process. When it comes to seasonal models, in the case of seasonal AR(P), ACF will decrease exponentially from season to season (e.g. you would see a decrease on lags 4, 8, 12 etc. for SAR(1) and \\(m=4\\)), while in case of seasonal MA(Q), ACF would drop abruptly, starting from the lag \\((Q+1)m\\) (so, the subsequent seasonal lag from the one that the process has, e.g. on lag 8, if we deal with SMA(1) with \\(m=4\\)). 8.3.3 Partial autocorrelation function (PACF) The other instrument useful for time series analysis with respect to ARIMA is called “partial ACF.” The idea of this follows from ACF directly. As we have spotted, if the process we deal with follows AR(1), then \\(\\rho(2)=\\phi_1^2\\) just because of the temporal relation. In order to get rid of this temporal effect, when calculating the correlation between \\(y_t\\) and \\(y_{t-2}\\) we could remove the correlation \\(\\rho(1)\\) in order to get the clean effect of \\(y_{t-2}\\) on \\(y_t\\). This type of correlation is called “partial,” denoting it as \\(\\varrho(\\tau)\\). There are different ways how to do that. One of the simplest is to estimate the following regression model: \\[\\begin{equation} y_t = a_1 y_{t-1} + a_2 y_{t-2} + \\dots + a_\\tau y_{t-\\tau} + \\epsilon_t, \\tag{8.62} \\end{equation}\\] where \\(a_i\\) is the parameter for the \\(i\\)-th lag of the model. In this regression, all the relations between \\(y_t\\) and \\(y_{t-\\tau}\\) are captured separately, so the last parameter \\(a_\\tau\\) is clean of all the temporal effects discussed above. We then can use the value \\(\\varrho(\\tau) = a_\\tau\\) as the coefficient, showing this relation. In order to obtain the PACF, we would need to construct and estimate regressions (8.62) for each lag \\(\\tau=\\{1, 2, \\dots, p\\}\\) and get the respective parameters \\(a_1\\), \\(a_2\\), …, \\(a_p\\), which would correspond to \\(\\varrho(1)\\), \\(\\varrho(2)\\), …, \\(\\varrho(p)\\). Just to show what this implies, we consider calculating PACF for AR(1) process. In this case, the true model is: \\[\\begin{equation*} y_t = \\phi_1 y_{t-1} + \\epsilon_t. \\end{equation*}\\] For the first lag we estimate exactly the same model, so that \\(\\varrho(1)=\\phi_1\\). For the second lag we estimate the model: \\[\\begin{equation*} y_t = a_1 y_{t-1} + a_2 y_{t-2} + \\epsilon_t. \\end{equation*}\\] But we know that for AR(1), \\(a_2=0\\), so when estimated in population, this would result in \\(\\varrho(2)=0\\) (in case of a sample, this would be a value very close to zero). If we continue with other lags, we will come to the same conclusion: for all lags \\(\\tau&gt;1\\) for the AR(1), we will have \\(\\varrho(\\tau)=0\\). This is one of the properties of PACF: if we deal with AR(p) process, then PACF drops rapidly to zero right after the lag \\(p\\). When it comes to MA(q) process, PACF behaves differently. In order to understand how it would behave, we take an example of MA(1) model, which is formulated as: \\[\\begin{equation*} y_t = \\theta_1 \\epsilon_{t-1} + \\epsilon_t. \\end{equation*}\\] As it was discussed in Section 8.1.7, MA process can be also represented as an infinite AR (see (8.35) for derivation): \\[\\begin{equation*} y_t = \\sum_{j=1}^\\infty -1^{j-1} \\theta_1^j y_{t-j} + \\epsilon_t. \\end{equation*}\\] If we construct and estimate the regression (8.62) for any lag \\(\\tau\\) for such process we will get \\(\\varrho(\\tau)=-1^{\\tau-1} \\theta_1^\\tau\\). This would correspond to the exponentially decreasing curve (if the parameter \\(\\theta_1\\) is positive, this will be an alternating series of values), similar to the one we have seen for the AR(1) and ACF. More generally, PACF will decline exponentially for MA(q) process, starting from the \\(\\varrho(q)=\\theta_q\\). When it comes to seasonal ARIMA models, the behaviour of PACF would resemble one of the non-seasonal ones, but with lags, multiple to the seasonality \\(m\\). e.g., for the SAR(1) process with \\(m=4\\), the \\(\\varrho(4)=\\phi_{4,1}\\), while \\(\\varrho(8)=0\\). 8.3.4 Summary Summarising the discussions in this section, we can conclude that: For AR(p) process, ACF will decrease either exponentially or alternating (depending on the parameters’ values), starting from the lag \\(p\\); For AR(p) process, PACF will drop abruptly right after the lag \\(p\\); For MA(q) process, ACF will drop abruptly right after the lag \\(q\\); For MA(q) process, PACF will decline either exponentially or alternating (based on the specific values of parameters), starting from the lag \\(q\\). These rules are tempting to use when determining the appropriate order of the ARMA model. The Box-Jenkins approach relies on that, considering that the seasonal orders should be identified first. The order selection process is iterative, analysing the ACF / PACF of residuals of tested models. However, these rules are not necessarily bi-directional: e.g. if we deal with MA(q), ACF drops abruptly right after the lag q, but if ACF drops abruptly after the lag q, then this does not necessarily mean that we deal with MA(q). The former follows directly from the assumed “true” model, while the latter refers to the identification of the model on the data, and there can be different reasons for the ACF to behave in a way it does. The logic here is similar to the following: Example 8.1 All birds have wings. Sarah has wings. Thus Sarah is a bird. Here is Sarah: Figure 8.8: Sarah by Yegor Kamelev This slight discrepancy led to issues in ARIMA identification over the years. You should not rely entirely on the Box-Jenkins approach when identifying ARIMA orders. There are more appropriate methods for order selection, which can be used in the context of ARIMA, and we will discuss them in the chapter 16. Still, ACF and PACF could be helpful to see if anything important is missing in the model, but not on their own. They are helpful together with other additional instruments. References "],["ARIMAandETS.html", "8.4 ARIMA and ETS", " 8.4 ARIMA and ETS Box and Jenkins (1976) showed in their textbook that several exponential smoothing methods could be considered special cases of the ARIMA model. Because of that, statisticians have thought for many years that ARIMA is a superior model and paid no attention to exponential smoothing. It took many years, many papers and a lot of effort (Fildes et al., 1998; Makridakis et al., 1982; Makridakis and Hibon, 2000) to show that this is not correct and that if you are interested in forecasting, then exponential smoothing, being a simpler model, typically does a better job than ARIMA. It was only after Ord et al. (1997) that the statisticians have started considering ETS as a separate model with its properties. Furthermore, it seems that some of the conclusions from the previous competitions mainly apply to the Box-Jenkins approach (for example, see Makridakis and Hibon, 1997), pointing out that selecting the correct order of ARIMA models is a much more challenging task than the statisticians have thought before. Still, there is a connection between ARIMA and ETS models, which can benefit both models, so it is worth discussing this in a separate section of the textbook. 8.4.1 ARIMA(0,1,1) and ETS(A,N,N) Muth (1960) was one of the first authors who showed that Simple Exponential Smoothing (Section 4.1) has an underlying ARIMA(0,1,1) model. This becomes apparent, when we study the error correction form of SES: \\[\\begin{equation*} \\hat{y}_{t} = \\hat{y}_{t-1} + \\hat{\\alpha} e_{t-1}. \\end{equation*}\\] Recalling that \\(e_t=y_t-\\hat{y}_t\\), this equation can be rewritten as: \\[\\begin{equation*} y_{t} = y_{t-1} -e_{t-1} + \\hat{\\alpha} e_{t-1} + e_t, \\end{equation*}\\] or after regrouping elements: \\[\\begin{equation*} y_{t} -y_{t-1} = e_t + (\\hat{\\alpha} -1) e_{t-1}. \\end{equation*}\\] Finally, using the backshift operator for ARIMA, substituting the estimated values by their “true” values, we get the ARIMA(0,1,1) model: \\[\\begin{equation*} y_{t}(1 -B) = \\epsilon_t(1 + (\\alpha -1) B) = \\epsilon_t(1 + \\theta_1 B), \\end{equation*}\\] where \\(\\theta_1 = \\alpha-1\\). This relation was one of the first hints that \\(\\alpha\\) in SES should lie in a wider interval: based on the fact that \\(\\theta_1 \\in (-1, 1)\\), the smoothing parameter \\(\\alpha \\in (0, 2)\\). This is the same region we get when we deal with the admissible bounds of the ETS(A,N,N) model (Section 4.2). This connection between the parameters of ARIMA(0,1,1) and ETS(A,N,N) is useful on its own because we can transfer the properties of ETS to ARIMA. For example, we know that the level in ETS(A,N,N) will change slowly when \\(\\alpha\\) is close to zero. Similar behaviour would be observed in ARIMA(0,1,1) with \\(\\theta_1\\) close to -1. In addition, we know that ETS(A,N,N) reverts to Random Walk, when \\(\\alpha=1\\), which corresponds to \\(\\theta_1=0\\). So, the closer \\(\\theta_1\\) to zero, the more abrupt behaviour the ARIMA model exhibits. In cases of \\(\\theta_1&gt;0\\), the model’s behaviour becomes even more uncertain. In a way, this relation gives us the idea of what to expect from more complicated ARIMA(p,d,q) models when the parameters for moving average are negative – the model should typically behave smoother. However, this might differ from one model to another. The main conceptual difference between ARIMA(0,1,1) and ETS(A,N,N) is that the latter still makes sense, when \\(\\alpha=0\\), while in case of ARIMA(0,1,1) the condition \\(\\theta_1=-1\\) is unacceptable. The global level model with \\(\\theta_1=-1\\) corresponds to just a different model, ARIMA(0,0,0) with constant. Finally, the connection between the two models tells us that if we have the ARIMA(0,1,q) model, this model would be suitable for the data called “level” in the ETS framework. The length of \\(q\\) would define the distribution of the weights in the model. The specific impact of each MA parameter on the actual values would differ, depending on the order \\(q\\) and values of parameters. The forecast from the ARIMA(0,1,q) would be a straight line parallel to the x-axis for \\(h\\geq q\\). In order to demonstrate the connection between the two models we consider the following example in R using functions sim.es(), es() and ssarima() from smooth package: # Generate data from ETS(A,N,N) with alpha=0.2 y &lt;- sim.es(&quot;ANN&quot;, obs=120, persistence=0.2) # Estimate ETS(A,N,N) esModel &lt;- es(y$data, &quot;ANN&quot;) # Estimate ARIMA(0,1,1) ssarimaModel &lt;- ssarima(y$data, c(0,1,1), initial=&quot;optimal&quot;) Given the the two models in smooth have the same initialisation mechanism, they should be equivalent. The values of their losses and information criteria should be the same: # Loss values setNames(c(esModel$lossValue, ssarimaModel$lossValue), c(&quot;ETS(A,N,N)&quot;,&quot;ARIMA(0,1,1)&quot;)) ## ETS(A,N,N) ARIMA(0,1,1) ## 576.5409 576.5409 # AIC setNames(c(AIC(esModel), AIC(ssarimaModel)), c(&quot;ETS(A,N,N)&quot;,&quot;ARIMA(0,1,1)&quot;)) ## ETS(A,N,N) ARIMA(0,1,1) ## 1159.082 1159.082 In addition, their parameters should be related based on the formula discussed above. The following two lines should produce the same values: # Smoothing parameter and theta_1 setNames(c(esModel$persistence, ssarimaModel$MA+1), c(&quot;ETS(A,N,N)&quot;,&quot;ARIMA(0,1,1)&quot;)) ## ETS(A,N,N) ARIMA(0,1,1) ## 0.1522508 0.1522508 Finally, the fit and the forecasts from the two models should be exactly the same if the parameters are linearly related (Figure 8.9): par(mfcol=c(2,1), mar=c(2,2,2,1)) plot(esModel,7) plot(ssarimaModel,7) Figure 8.9: ETS(A,N,N) and ARIMA(0,1,1) models producing the same fit and forecast trajectories. We expect the ETS(A,N,N) and ARIMA(0,1,1) models to be equivalent in this example because they are estimated using the respective functions es() and ssarima(), which are implemented in the same way, using the same framework. If the framework, initialisation, construction or estimation would be different, then the relation between the applied models might be not exact but approximate. 8.4.2 ARIMA(0,2,2) and ETS(A,A,N) Nerlove and Wage (1964) showed that there is an underlying ARIMA(0,2,2) for the Holt’s method (Subsection 4.3.1), although they do not say that explicitly in their paper. Skipping the derivations, the relation between Holt’s method and the ARIMA model is expressed in the following two equations about their parameters (in the form of ARIMA discussed in this textbook): \\[\\begin{equation*} \\begin{aligned} &amp;\\theta_1 = \\alpha + \\beta -2 \\\\ &amp;\\theta_2 = 1 -\\alpha \\end{aligned} \\end{equation*}\\] We also know from the section 4.3 that Holt’s method has an underlying ETS(A,A,N) model. Thus there is a connection between this model and ARIMA(0,2,2). This means that ARIMA(0,2,2) will produce linear trajectories for the data and that the MA parameters of the model regulate the speed of updating the values. Because of the second difference, ARIMA(0,2,q) will produce a straight line as a forecasting trajectory for any \\(h\\geq q\\). Similarly to the ARIMA(0,1,1) vs ETS(A,N,N), one of the important differences between the models is that the boundary values for parameters are not possible for ARIMA(0,2,2): \\(\\alpha=0\\) and \\(\\beta=0\\) are possible in ETS, but the respective \\(\\theta_1=2\\) and \\(\\theta_2=-1\\) are not. Furthermore, the model that corresponds to the situation, when \\(\\alpha=1\\) and \\(\\beta=0\\) is formulated as ARIMA(0,1,0) with drift (discussed in Section 8.1.5). The global trend ARIMA could hypothetically appear in the boundary case with \\(\\theta_1=-2\\) and \\(\\theta_2=1\\), implying the following model: \\[\\begin{equation*} y_t (1 -B)^2 = \\epsilon_t -2\\epsilon_{t-1} + \\epsilon_{t-2} = \\epsilon_t (1 -B)^2 , \\end{equation*}\\] which tells us that in ARIMA framework, the global trend model is only available as a global mean on second differences of the data. The more appropriate global level model in ARIMA context would be ARIMA(0,0,0) with constant. Finally, the ETA(A,A,N) and ARIMA(0,2,2) will fit the data similarly and produce the exact forecasts as long as they are constructed, initialised and estimated in the same way. 8.4.3 ARIMA(1,1,2) and ETS(A,Ad,N) Roberts (1982) proposed damped trend exponential smoothing method (Section 4.3.2), showing that it is related to ARIMA(1,1,2) model, with the following connection between the parameters of the two: \\[\\begin{equation*} \\begin{aligned} &amp;\\theta_1 = \\alpha -1 + \\phi (\\beta -1) \\\\ &amp;\\theta_2 = \\phi(1 -\\alpha) \\\\ &amp;\\phi_1 = \\phi \\end{aligned} . \\end{equation*}\\] At the same time, the damped trend method has underlying ETS(A,Ad,N), thus the two models are connected. Recalling that ETS(A,Ad,N) reverts to ETS(A,A,N), when \\(\\phi=1\\), we can see a similar property in ARIMA: when \\(\\phi_1=1\\), the model should be reformulated as ARIMA(0,2,2) instead of ARIMA(1,1,2). Given the direct connection between the dampening parameters and the AR(1) parameter of the two models, we can conclude that AR(1) defines the forecasting trajectory’s dampening effect. We have already noticed this in Section 8.1.4. However, we should acknowledge that the dampening only happens when \\(\\phi_1 \\in (0,1)\\). The case of \\(\\phi_1&gt;1\\) is unacceptable in the ARIMA framework and is not very useful in the case of ETS, producing explosive exponential trajectories. The case of \\(\\phi_1 \\in (-1, 0)\\) is possible but is less useful in practice, as the trajectory will oscillate. The lesson to learn from the connection between the two models is that AR(p) part of ARIMA can act as a dampening element for the forecasting trajectories, although the specific shape would depend on the value of \\(p\\) and the values of parameters. 8.4.4 ARIMA and other ETS models The pure additive seasonal ETS models (Chapter 5.1) also have a connection with ARIMA, but the resulting models are not parsimonious. For example, ETS(A,A,A) is related to SARIMA(0,1,m+1)(0,1,0)\\(_m\\) (Chatfield, 1977; McKenzie, 1976) with some restrictions on parameters. If we were to work with SARIMA and wanted to model the seasonal time series, we would probably apply SARIMA(0,1,1)(0,1,1)\\(_m\\) instead of this larger model. When it comes to pure multiplicative (Chapter 6.1) and mixed (Section 7.2) ETS models, there are no appropriate ARIMA analogues for them. For example, Chatfield (1977) showed that there are no ARIMA models for the exponential smoothing with the multiplicative seasonal component. This makes ETS distinct from ARIMA. The closest one can get to a pure multiplicative model is the ARIMA applied to logarithmically transformed data when the smoothing parameters of ETS are close to zero, coming from the limit (6.5). 8.4.5 ETS + ARIMA Finally, based on the discussion above, it is possible to have a combination of ETS and ARIMA, but not all combinations would be meaningful and helpful. For example, fitting a combination of ETS(A,N,N)+ARIMA(0,1,1) is not a good idea due to the connection of the two models (Subsection 8.4.1). However, doing ETS(A,N,N) and adding ARIMA(1,0,0) component would make sense – the resulting model would exhibit the dampening trend as discussed in Section 8.4.3 but would have fewer parameters to estimate than ETS(A,Ad,N). Gardner (1985) pointed out that using AR(1) with exponential smoothing methods improves forecasting accuracy, so this combination of the two models is potentially beneficial for ETS. In the next chapter, we will discuss how specifically the two models can be united in one framework. References "],["ARIMAExampleInR.html", "8.5 Examples of application", " 8.5 Examples of application 8.5.1 Non-seasonal data Using the time series from the Box-Jenkins textbook (Box and Jenkins, 1976), we fit the ARIMA model to the data but based on our judgment rather than their approach. Just a reminder, here is how the data looks (series BJsales, Figure 8.10): Figure 8.10: Box-Jenkins sales data. It seems to exhibit the trend in the data, so we can consider ARIMA(1,1,2), ARIMA(0,2,2) and ARIMA(1,1,1) models. We do not consider models with drift in this example, because they would imply the same slope over time for the whole series, which does not seem to be the case here: adamModelARIMA &lt;- vector(&quot;list&quot;,3) # ARIMA(1,1,2) adamModelARIMA[[1]] &lt;- adam(BJsales, &quot;NNN&quot;, orders=c(1,1,2), h=10, holdout=TRUE) # ARIMA(0,2,2) adamModelARIMA[[2]] &lt;- adam(BJsales, &quot;NNN&quot;, orders=c(0,2,2), h=10, holdout=TRUE) # ARIMA(1,1,1) adamModelARIMA[[3]] &lt;- adam(BJsales, &quot;NNN&quot;, orders=c(1,1,1), h=10, holdout=TRUE) names(adamModelARIMA) &lt;- c(&quot;ARIMA(1,1,2)&quot;, &quot;ARIMA(0,2,2)&quot;, &quot;ARIMA(1,1,1)&quot;) In the code above, we need to tell adam() to use model=\"NNN\" to switch off the ETS part of the model. Comparing information criteria (we will use AICc) of the three models, we can select the most appropriate one: sapply(adamModelARIMA, AICc) ## ARIMA(1,1,2) ARIMA(0,2,2) ARIMA(1,1,1) ## 503.8630 497.0121 505.7502 Note that this comparison is possible in adam() (and in ssarima() and msarima()) because the implemented ARIMA is formulated in state space form, sidestepping the issue of the conventional ARIMA (where taking differences reduces the sample size). Based on this comparison, it looks like the ARIMA(0,2,2) is the most appropriate model (among the three) for the data. Here how the fit and the forecast from the model looks (Figure 8.11): plot(adamModelARIMA[[2]], 7) Figure 8.11: BJSales series and ARIMA(0,2,2) Comparing this model with the ETS(A,A,N), we will see a slight difference because the two models are initialised and estimated differently: adam(BJsales, &quot;AAN&quot;, h=10, holdout=TRUE) ## Time elapsed: 0.02 seconds ## Model estimated using adam() function: ETS(AAN) ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 243.4073 ## Persistence vector g: ## alpha beta ## 1.0000 0.2392 ## ## Sample size: 140 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 135 ## Information criteria: ## AIC AICc BIC BICc ## 496.8146 497.2624 511.5228 512.6292 ## ## Forecast errors: ## ME: 3.229; MAE: 3.341; RMSE: 3.797 ## sCE: 14.178%; Asymmetry: 91.7%; sMAE: 1.467%; sMSE: 0.028% ## MASE: 2.826; RMSSE: 2.491; rMAE: 0.928; rRMSE: 0.924 If we are interested in a more classical Box-Jenkins approach, we can always analyse the residuals of the constructed model and try improving it further. Here is an example of ACF and PACF of the residuals of the ARIMA(0,2,2): par(mfcol=c(1,2)) plot(adamModelARIMA[[2]], c(10,11), main=&quot;&quot;) Figure 8.12: ACF and PACF of ARIMA(0,2,2) on BJSales data As we see from the plot in Figure 8.12, all autocorrelation coefficients lie inside the confidence interval, implying that there are no significant AR / MA lags to include in the model. 8.5.2 Seasonal data Similarly to the previous cases, we use Box-Jenkins AirPassengers data, which exhibits a multiplicative seasonality and a trend. We will model this using SARIMA(0,2,2)(0,1,1)\\(_{12}\\), SARIMA(0,2,2)(1,1,1)\\(_{12}\\) and SARIMA(0,2,2)(1,1,0)\\(_{12}\\) models, which are selected to see what type of seasonal ARIMA is more appropriate to the data: adamModelSARIMA &lt;- vector(&quot;list&quot;,3) # SARIMA(0,2,2)(0,1,1)[12] adamModelSARIMA[[1]] &lt;- adam(AirPassengers, &quot;NNN&quot;, lags=c(1,12), orders=list(ar=c(0,0), i=c(2,1), ma=c(2,1)), h=12, holdout=TRUE) # SARIMA(0,2,2)(1,1,1)[12] adamModelSARIMA[[2]] &lt;- adam(AirPassengers, &quot;NNN&quot;, lags=c(1,12), orders=list(ar=c(0,1), i=c(2,1), ma=c(2,1)), h=12, holdout=TRUE) # SARIMA(0,2,2)(1,1,0)[12] adamModelSARIMA[[3]] &lt;- adam(AirPassengers, &quot;NNN&quot;, lags=c(1,12), orders=list(ar=c(0,1), i=c(2,1), ma=c(2,0)), h=12, holdout=TRUE) names(adamModelSARIMA) &lt;- c(&quot;SARIMA(0,2,2)(0,1,1)[12]&quot;, &quot;SARIMA(0,2,2)(1,1,1)[12]&quot;, &quot;SARIMA(0,2,2)(1,1,0)[12]&quot;) Note that now that we have a seasonal component, we need to provide the SARIMA lags: 1 and \\(m=12\\) and specify orders differently – as a list with values for AR, I and MA orders separately. This is done because the SARIMA implemented in adam() supports multiple seasonality (e.g. you can have lags=c(1,24,24*7) if you want). The resulting information criteria of models are: sapply(adamModelSARIMA, AICc) ## SARIMA(0,2,2)(0,1,1)[12] SARIMA(0,2,2)(1,1,1)[12] SARIMA(0,2,2)(1,1,0)[12] ## 1028.828 1078.016 1086.773 It looks like the first model is slightly better than the other two, so we will use it in order to produce forecasts (see Figure 8.13): plot(forecast(adamModelSARIMA[[1]], h=12, interval=&quot;prediction&quot;), main=&quot;&quot;) Figure 8.13: Forecast of AirPassengers data from SARIMA(0,2,2)(0,1,1)\\(_{12}\\) model. This model is directly comparable with ETS models, so here is, for example, AICc of ETS(M,A,M) on the same data: AICc(adamModelETS &lt;- adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE)) ## [1] 973.9646 It is lower than for the SARIMA model, which means that ETS(M,A,M) is more appropriate to the data in terms of information criteria than SARIMA(0,2,2)(0,1,1)\\(_{12}\\). We can also investigate if there is a way to improve ETS(M,A,M) by adding some ARMA components (Figure 8.14): par(mfcol=c(1,2)) plot(adamModelETS, c(10,11), main=&quot;&quot;) Figure 8.14: ACF and PACF of ETS(M,A,M) on AirPassengers data Judging by the plots in Figure 8.14, significant correlation coefficients exist for some lags. Still, it is not clear whether they appear due to the type I error or not. Just to check, we will see if adding SARIMA(0,0,0)(0,0,1)\\(_{12}\\) helps (reduces AICc) in this case: AICc(adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE, order=list(ma=c(0,1)), lags=c(1,12))) ## [1] 1010.571 As we see, the increased complexity does not decrease the AICc (probably because now we need to estimate 13 parameters more than in just ETS(M,A,M)), so we should not add the SARIMA component. We could try adding other SARIMA elements to see if they improve the model, but we do not aim to find the best model, so the reader is encouraged to do that as an additional exercise. References "],["ADAMARIMA.html", "Chapter 9 ADAM ARIMA", " Chapter 9 ADAM ARIMA There are different ways to formulate and implement ARIMA. The one discussed in Chapter 8 is the conventional way. The model, in that case, can be estimated directly, assuming that its initialisation happens at some point before the Big Bang: the conventional ARIMA assumes that there is no starting point of the model. We observe a specific piece of data from a population without any beginning or end. Obviously, this assumption is idealistic and does not necessarily agree with reality (imagine the series of infinitely lasting sales of Siemens S45 mobile phones. Do you even remember such a thing?). But besides the conventional formulation, there are also state space forms of ARIMA, the most relevant to our topic being the one implemented in SSOE form (Chapter 11 of Hyndman et al., 2008). Svetunkov and Boylan (2020b) adapted this state space model for supply chain forecasting, developing an order selection mechanism, sidestepping the hypothesis testing and focusing on information criteria. However, the main issue with that approach is that the resulting ARIMA model works very slow on the data with high frequencies (because the model was formulated based on Chapter 11 of Hyndman et al. (2008)). Luckily, an alternative SSOE state space formulation is introduced in Chapter 5.1. This model is already implemented in the msarima() function of the smooth package and was also used as the basis for the ADAM ARIMA. In this chapter, we discuss the state space ADAM ARIMA for both pure additive and pure multiplicative cases, the conditional moments from the model and parameter space, then move to the distributional assumptions of the model (including the conditional distributions) and finish the chapter with the discussion of implications of ETS+ARIMA model. The latter has not been discussed in the literature and might make the model unidentifiable, so an analyst using the combination should be cautious. References "],["StateSpaceARIMA.html", "9.1 State space ARIMA", " 9.1 State space ARIMA 9.1.1 An example of State Space ARIMA In order to understand how the state space ADAM ARIMA can be formulated, we consider an arbitrary example of SARIMA(1,1,2)(0,1,0)\\(_4\\): \\[\\begin{equation*} {y}_{t} (1- \\phi_1 B)(1-B)(1-B^4) = \\epsilon_t (1 + \\theta_1 B + \\theta_2 B^2), \\end{equation*}\\] which can be rewritten in the expanded form: \\[\\begin{equation*} {y}_{t} (1-\\phi_1 B -B + \\phi_1 B^2 -B^4 +\\phi_1 B^5 + B^5 -\\phi_1 B^6) = \\epsilon_t (1 + \\theta_1 B + \\theta_2 B^2), \\end{equation*}\\] or after moving all the lagged values to the right-hand side: \\[\\begin{equation*} {y}_{t} = (1+\\phi_1) {y}_{t-1} -\\phi_1 {y}_{t-2} + {y}_{t-4} -(1+\\phi_1) {y}_{t-5} + \\phi_1 {y}_{t-6} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\epsilon_t . \\end{equation*}\\] Now we can define the states of the model: \\[\\begin{equation} \\begin{aligned} &amp; v_{1,t-1} = (1+\\phi_1) y_{t-1} + \\theta_1 \\epsilon_{t-1} \\\\ &amp; v_{2,t-2} = -\\phi_1 y_{t-2} + \\theta_2 \\epsilon_{t-2} \\\\ &amp; v_{3,t-3} = 0 \\\\ &amp; v_{4,t-4} = y_{t-4} \\\\ &amp; v_{5,t-5} = -(1+\\phi_1) y_{t-5} \\\\ &amp; v_{6,t-6} = \\phi_1 y_{t-6} \\end{aligned} . \\tag{9.1} \\end{equation}\\] In our example all the MA parameters are zero for \\(j&gt;2\\), that is why they disappear from the states above. Furthermore, there are no elements for lag three, so that state can be dropped. The measurement equation of the ARIMA model in this situation can be written as: \\[\\begin{equation*} {y}_{t} = \\sum_{j=1,2,4,5,6} v_{j,t-j} + \\epsilon_t , \\end{equation*}\\] based on which the actual value on some lag \\(i\\) can also be written as: \\[\\begin{equation} {y}_{t-i} = \\sum_{j=1,2,4,5,6} v_{j,t-j-i} + \\epsilon_{t-i}. \\tag{9.2} \\end{equation}\\] Inserting (9.2) in (9.1) and shifting the lags from \\(t-i\\) to \\(t\\) in every equation, we get the state space ARIMA: \\[\\begin{equation*} \\begin{aligned} &amp;{y}_{t} = \\sum_{j=1,2,4,5,6} v_{j,t-j} + \\epsilon_t \\\\ &amp; v_{1,t} = (1+\\phi_1) \\sum_{j=1}^6 v_{j,t-j} + (1+\\phi_1+\\theta_1) \\epsilon_t \\\\ &amp; v_{2,t} = -\\phi_1 \\sum_{j=1}^6 v_{j,t-j} + (-\\phi_1+\\theta_2) \\epsilon_t \\\\ &amp; v_{4,t} = \\sum_{j=1}^6 v_{j,t-j} + \\epsilon_t \\\\ &amp; v_{5,t} = -(1+\\phi_1) \\sum_{j=1}^6 v_{j,t-j} -(1+\\phi_1) \\epsilon_t \\\\ &amp; v_{6,t} = \\phi_1 \\sum_{j=1}^6 v_{j,t-j} + \\phi_1 \\epsilon_t \\end{aligned} . \\end{equation*}\\] This model can then be applied to the data, and forecasts can be produced similarly to how it was done for the pure additive ETS model (see Section 5.1). Furthermore, it can be shown that any ARIMA model can be written in the compact form (5.4), meaning that the same principles as for ETS can be applied to ARIMA and that the two models can be united in one framework. 9.1.2 Additive ARIMA In a more general case, in order to develop state space ARIMA, we will use the multiple seasonal ARIMA, discussed in Section 8.2.3: \\[\\begin{equation*} y_t \\prod_{j=0}^n \\Delta^{D_j} (B^{m_j}) \\varphi^{P_j}(B^{m_j}) = \\epsilon_t \\prod_{j=0}^n \\vartheta^{Q_j}(B^{m_j}) , \\end{equation*}\\] This model can be represented in an easier to digest form by expanding the polynomials on the left hand side of the equation and moving all the previous values to the right hand side and then expanding the MA polynomials: \\[\\begin{equation} y_t = \\sum_{j=1}^K \\eta_j y_{t-j} + \\sum_{j=1}^K \\theta_j \\epsilon_{t-j} + \\epsilon_t . \\tag{9.3} \\end{equation}\\] Each element before the lagged \\(y_{t-j}\\) can be called the parameter of polynomial. In our example with SARIMA(1,1,2)(0,1,0)\\(_4\\) in the previous subsection they were: \\[\\begin{equation*} \\begin{aligned} &amp; \\eta_1 = 1+\\phi_1 \\\\ &amp; \\eta_2 = -\\phi_1 \\\\ &amp; \\eta_3 = 0 \\\\ &amp; \\eta_4 = 1 \\\\ &amp; \\eta_5 = -(1+\\phi_1) \\\\ &amp; \\eta_6 = \\phi_1 \\end{aligned} . \\end{equation*}\\] In the equation (9.3), \\(K\\) is the order of the highest polynomial, calculated as \\(K=\\max\\left(\\sum_{j=0}^n (P_j + D_j)m_j, \\sum_{j=0}^n Q_j m_j\\right)\\). If, for example, the MA order is higher than the sum of ARI orders, then polynomials \\(\\eta_i=0\\) for \\(i&gt;\\sum_{j=0}^n (P_j + D_j)m_j\\). The same holds for the opposite situation of the sum of ARI orders being higher than the MA orders, where \\(\\theta_i=0\\) for all \\(i&gt;\\sum_{j=0}^n Q_j m_j\\). Based on this we could define states for each of the previous elements: \\[\\begin{equation} v_{i,t-i} = \\eta_i y_{t-i} + \\theta_i \\epsilon_{t-i}, \\tag{9.4} \\end{equation}\\] leading to the following model based on (9.4) and (9.3): \\[\\begin{equation} y_t = \\sum_{j=1}^K v_{j,t-j} + \\epsilon_t . \\tag{9.5} \\end{equation}\\] This can be considered as a measurement equation of the state space ARIMA. Now if we consider the previous values of \\(y_t\\) based on (9.5), for \\(y_{t-i}\\), it will be equal to: \\[\\begin{equation} y_{t-i} = \\sum_{j=1}^K v_{j,t-j-i} + \\epsilon_{t-i} . \\tag{9.6} \\end{equation}\\] The value (9.6) can be inserted into (9.4), in order to get the transition equation: \\[\\begin{equation} v_{i,t-i} = \\eta_i \\sum_{j=1}^K v_{j,t-j-i} + (\\eta_i + \\theta_i) \\epsilon_{t-i}. \\tag{9.7} \\end{equation}\\] This leads to the SSOE state space model based on (9.6) and (9.7): \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = \\sum_{j=1}^K v_{j,t-j} + \\epsilon_t \\\\ &amp;v_{i,t} = \\eta_i \\sum_{j=1}^K v_{j,t-j} + (\\eta_i + \\theta_i) \\epsilon_{t} \\text{ for each } i=\\{1, 2, \\dots, K \\} \\end{aligned}, \\tag{9.8} \\end{equation}\\] which can be formulated in the conventional form as a pure additive ADAM model (Section 5.1): \\[\\begin{equation*} \\begin{aligned} &amp;{y}_{t} = \\mathbf{w}^\\prime \\mathbf{v}_{t-\\mathbf{l}} + \\epsilon_t \\\\ &amp;\\mathbf{v}_{t} = \\mathbf{F} \\mathbf{v}_{t-\\mathbf{l}} + \\mathbf{g} \\epsilon_t \\end{aligned}, \\end{equation*}\\] with the following values for matrices: \\[\\begin{equation} \\begin{aligned} \\mathbf{F} = \\begin{pmatrix} \\eta_1 &amp; \\eta_1 &amp; \\dots &amp; \\eta_1 \\\\ \\eta_2 &amp; \\eta_2 &amp; \\dots &amp; \\eta_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\eta_K &amp; \\eta_K &amp; \\dots &amp; \\eta_K \\end{pmatrix}, &amp; \\mathbf{w} = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}, \\\\ \\mathbf{g} = \\begin{pmatrix} \\eta_1 + \\theta_1 \\\\ \\eta_2 + \\theta_2 \\\\ \\vdots \\\\ \\eta_K + \\theta_K \\end{pmatrix}, &amp; \\mathbf{v}_{t} = \\begin{pmatrix} v_{1,t} \\\\ v_{2,t} \\\\ \\vdots \\\\ v_{K,t} \\end{pmatrix}, &amp; \\mathbf{l} = \\begin{pmatrix} 1 \\\\ 2 \\\\ \\vdots \\\\ K \\end{pmatrix} \\end{aligned}. \\tag{9.9} \\end{equation}\\] States in this model do not have any specific meaning, they just represent a combination of actual values and error terms, some pieces of ARIMA model. Furthermore, there are zero states in this model, corresponding to zero polynomials of ARI and MA. These can be dropped to make the model even more compact. In general, state space ARIMA looks more complicated than the original one in the conventional form, but it brings the model to the same ground as ETS in ADAM (Section 5), making them directly comparable via information criteria and allowing to easily combine the two models, not to mention comparing ARIMA of any order with another ARIMA (e.g. with different orders of differencing) or introduce multiple seasonality and explanatory variables. 9.1.3 State space ARIMA with constant If we want to add the constant (similar to how it was done in Section 8.1.4) to the model, we need to modify the equation (9.3): \\[\\begin{equation} y_t = \\sum_{j=1}^K \\eta_j y_{t-j} + \\sum_{j=1}^K \\theta_j \\epsilon_{t-j} + a_0 + \\epsilon_t . \\tag{9.10} \\end{equation}\\] This then leads to the appearance of the new state: \\[\\begin{equation} v_{K+1,t} = a_0 , \\tag{9.11} \\end{equation}\\] and modified measurement equation: \\[\\begin{equation} y_t = \\sum_{j=1}^{K+1} v_{j,t-j} + \\epsilon_t , \\tag{9.12} \\end{equation}\\] with the following transition equation: \\[\\begin{equation} \\begin{aligned} &amp; v_{i,t} = \\eta_i \\sum_{j=1}^{K+1} v_{j,t-j} + (\\eta_i + \\theta_i) \\epsilon_{t} , \\text{ for } i=\\{1, 2, \\dots, K\\} \\\\ &amp; v_{K+1, t} = v_{K+1, t-1} . \\end{aligned} \\tag{9.13} \\end{equation}\\] The state space equations (9.12) and (9.13) lead to the following matrices: \\[\\begin{equation} \\begin{aligned} \\mathbf{F} = \\begin{pmatrix} \\eta_1 &amp; \\dots &amp; \\eta_1 &amp; \\eta_1 \\\\ \\eta_2 &amp; \\dots &amp; \\eta_2 &amp; \\eta_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\eta_K &amp; \\dots &amp; \\eta_K &amp; \\eta_K \\\\ 0 &amp; \\dots &amp; 0 &amp; 1 \\end{pmatrix}, &amp; \\mathbf{w} = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\\\ 1 \\end{pmatrix}, \\\\ \\mathbf{g} = \\begin{pmatrix} \\eta_1 + \\theta_1 \\\\ \\eta_2 + \\theta_2 \\\\ \\vdots \\\\ \\eta_K + \\theta_K \\\\ 0 \\end{pmatrix}, &amp; \\mathbf{v}_{t} = \\begin{pmatrix} v_{1,t} \\\\ v_{2,t} \\\\ \\vdots \\\\ v_{K,t} \\\\ v_{K+1,t} \\end{pmatrix}, &amp; \\mathbf{l} = \\begin{pmatrix} 1 \\\\ 2 \\\\ \\vdots \\\\ K \\\\ 1 \\end{pmatrix} \\end{aligned}. \\tag{9.14} \\end{equation}\\] Note that the constant term introduced in this model has a different meaning, depending on the order of differences of the model. For example, if \\(D_j=0\\) for all \\(j\\), it acts as an intercept, while for the \\(d=1\\), it will act as a drift. 9.1.4 Multiplicative ARIMA In order to connect ARIMA with ETS, we also need to define cases for multiplicative models. This implies that the error term \\((1+\\epsilon_t)\\) is multiplied by components of the model. The state space ARIMA in this case is formulated using logarithms in the following way: \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = \\exp \\left( \\sum_{j=1}^K \\log v_{j,t-j} + \\log(1+\\epsilon_t) \\right) \\\\ &amp;\\log v_{i,t} = \\eta_i \\sum_{j=1}^K \\log v_{j,t-j} + (\\eta_i + \\theta_i) \\log(1+\\epsilon_t) \\text{ for each } i=\\{1, 2, \\dots, K \\} \\end{aligned}. \\tag{9.15} \\end{equation}\\] The model (9.15) can be written in the following more general form: \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = \\exp \\left( \\mathbf{w}^\\prime \\log \\mathbf{v}_{t-\\mathbf{l}} + \\log(1+\\epsilon_t) \\right) \\\\ &amp;\\log \\mathbf{v}_{t} = \\mathbf{F} \\log \\mathbf{v}_{t-\\mathbf{l}} + \\mathbf{g} \\log(1+\\epsilon_t) \\end{aligned}, \\tag{9.16} \\end{equation}\\] where \\(\\mathbf{w}\\), \\(\\mathbf{F}\\), \\(\\mathbf{v}_t\\), \\(\\mathbf{g}\\) and \\(\\mathbf{l}\\) are defined as before for the pure additive ARIMA (Section 9.1), e.g. in equation (9.14). This model is equivalent to applying ARIMA to log-transformed data but at the same time shares some similarities with pure multiplicative ETS from Section 6.1. The main advantage of this formulation is that this model has analytical solutions for the conditional moments and has well-defined h steps ahead conditional distribution, which simplifies the work with the model in contrast with the pure multiplicative ETS. To distinguish the additive ARIMA from the multiplicative one, we will use the notation “logARIMA” for the latter in this book, pointing out what such model is equivalent to (applying ARIMA to the log-transformed data). Finally, it is worth mentioning that due to the logarithmic transform, the logARIMA model would be suitable for the cases of time-varying heteroscedasticity, similar to the multiplicative error ETS models. "],["ADAMARIMARecursive.html", "9.2 Recursive relation", " 9.2 Recursive relation Both additive and multiplicative ARIMA models can be written in the recursion form, similar to pure additive ETS (see Section 5.2). For the pure additive ARIMA: \\[\\begin{equation} y_{t+h} = \\sum_{i=1}^K \\mathbf{w}_{i}^\\prime \\mathbf{F}_{i}^{\\lceil\\frac{h}{i}\\rceil-1} \\mathbf{v}_{t} + \\mathbf{w}_{i}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{i}\\rceil-1} \\mathbf{F}_{i}^{j-1} \\mathbf{g}_{i} \\epsilon_{t+i\\lceil\\frac{h}{i}\\rceil-j} + \\epsilon_{t+h} , \\tag{9.17} \\end{equation}\\] and for the pure multiplicative one: \\[\\begin{equation} \\log y_{t+h} = \\sum_{i=1}^K \\mathbf{w}_{i}^\\prime \\mathbf{F}_{i}^{\\lceil\\frac{h}{i}\\rceil-1} \\log \\mathbf{v}_{t} + \\mathbf{w}_{i}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{i}\\rceil-1} \\mathbf{F}_{i}^{j-1} \\mathbf{g}_{i} \\log (1+\\epsilon_{t+i\\lceil\\frac{h}{i}\\rceil-j}) + \\log(1+ \\epsilon_{t+h}) , \\tag{9.18} \\end{equation}\\] where \\(i\\) corresponds to each lag of the model from 1 to \\(K\\), \\(\\mathbf{w}_{i}\\) is the measurement vector, \\(\\mathbf{g}_{i}\\) is the persistence vector, both including only \\(i\\)-th elements, \\(\\mathbf{F}_{i}\\) is the transition matrix, including only \\(i\\)-th column. Based on these recursions, point forecasts can be produced from the additive and multiplicative ARIMA models, which will be respectively: \\[\\begin{equation} \\hat{y}_{t+h} = \\sum_{i=1}^K \\mathbf{w}_{i}^\\prime \\mathbf{F}_{i}^{\\lceil\\frac{h}{i}\\rceil-1} \\mathbf{v}_{t} \\tag{9.19} \\end{equation}\\] and: \\[\\begin{equation} \\hat{y}_{t+h} = \\exp\\left(\\sum_{i=1}^K \\mathbf{w}_{i}^\\prime \\mathbf{F}_{i}^{\\lceil\\frac{h}{i}\\rceil-1} \\log \\mathbf{v}_{t} \\right) . \\tag{9.20} \\end{equation}\\] Note however that similarly to the multiplicative ETS, the point forecasts of logARIMA will not necessarily coincide with the conditional expectations. Still, based on the recursions (9.17) and (9.18), we can calculate conditional moments of ADAM ARIMA. 9.2.1 Conditional moments of ADAM ARIMA In case of the pure additive ARIMA, the moments correspond to the ones for ETS, discussed in Section 5.1 and follow directly from (9.17): \\[\\begin{equation*} \\begin{aligned} \\mu_{y,t+h} = \\mathrm{E}(y_{t+h}|t) = &amp; \\sum_{i=1}^K \\left(\\mathbf{w}_{i}&#39; \\mathbf{F}_{i}^{\\lceil\\frac{h}{i}\\rceil-1} \\right) \\mathbf{v}_{t} \\\\ \\sigma^2_{h} = \\mathrm{V}(y_{t+h}|t) = &amp; \\left( \\sum_{i=1}^K \\left(\\mathbf{w}_{i}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{i}\\rceil-1} \\mathbf{F}_{i}^{j-1} \\mathbf{g}_{i} \\mathbf{g}&#39;_{i} (\\mathbf{F}_{i}&#39;)^{j-1} \\mathbf{w}_{i} \\right) + 1 \\right) \\sigma^2 \\end{aligned} . \\end{equation*}\\] When it comes to the multiplicative ARIMA model, the conditional moments would depend on the assumed distribution and might become quite complicated. Here is an example of the conditional logarithmic mean for Log Normal distribution, assuming that \\((1+\\epsilon_t) \\sim \\mathrm{log}\\mathcal{N}\\left(\\frac{\\sigma^2}{2},\\sigma^2 \\right)\\) based on (9.18): \\[\\begin{equation} \\mu_{\\log y,t+h} = \\mathrm{E}(\\log y_{t+h}|t) = \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i}&#39; \\mathbf{F}_{m_i}^{\\lceil\\frac{h}{m_i}\\rceil-1} \\right) \\log \\mathbf{v}_{t} -\\left(\\mathbf{w}_{i}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{i}\\rceil-1} \\mathbf{F}_{i}^{j-1} \\mathbf{g}_{i} + 1\\right) \\frac{\\sigma^2}{2} . \\tag{9.21} \\end{equation}\\] Note that the variance conditional logarithmic variance of the model will be the same for all logARIMA models, independent of the distributional assumptions: \\[\\begin{equation} \\sigma^2_{\\log y,h} = \\mathrm{V}(\\log y_{t+h}|t) = \\left( \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{m_i}\\rceil-1} \\mathbf{F}_{m_i}^{j-1} \\mathbf{g}_{m_i} \\mathbf{g}&#39;_{m_i} (\\mathbf{F}_{m_i}&#39;)^{j-1} \\mathbf{w}_{m_i} \\right) + 1 \\right) \\sigma^2 . \\tag{9.22} \\end{equation}\\] The obtained logarithmic moments can then be used to get the ones in the original scale, after using the connection between the moments in Log Normal distribution. The conditional expectation and variance in this case can be calculated as: \\[\\begin{equation} \\begin{aligned} &amp; \\mu_{y,t+h} = \\mathrm{E}(y_{t+h}|t) = \\exp \\left(\\mu_{\\log y,t+h} + \\frac{\\sigma^2_{\\log y,h}}{2} \\right) \\\\ &amp; \\sigma^2_{h} = \\mathrm{V}(y_{t+h}|t) = \\left(\\exp\\left( \\sigma^2_{\\log y,h} \\right) -1 \\right)\\exp\\left(2 \\times \\mu_{\\log y,t+h} + \\sigma^2_{\\log y,h} \\right) . \\end{aligned} \\tag{9.23} \\end{equation}\\] Inserting the values (9.21) and (9.22) in (9.23), we will get the analytical solutions for the two moments. If some other distributions are assumed in the model, then the conditional logarithmic mean would change because the variable \\(\\log(1+\\epsilon_t)\\) would follow a different distribution with a different mean. For example: Gamma: if \\(\\left(1+\\epsilon_t \\right) \\sim \\mathcal{\\Gamma}(\\sigma^{-2}, \\sigma^2)\\), then \\(\\log\\left(1+\\epsilon_t \\right) \\sim \\mathrm{exp}\\mathcal{\\Gamma}(\\sigma^{-2}, \\sigma^2)\\), which is exponential Gamma distribution, which has the following logarithmic mean: \\(\\mathrm{E}(\\log(1+\\epsilon_t)) = \\psi\\left(\\sigma^{-2}\\right)+2\\log(\\sigma)\\); Inverse Gaussian: if \\(\\left(1+\\epsilon_t \\right) \\sim \\mathcal{IG}(1, \\sigma^2)\\), then \\(\\log\\left(1+\\epsilon_t \\right) \\sim \\mathrm{exp}\\mathcal{IG}(1, \\sigma^2)\\), exponential Inverse Gaussian distribution, which does not have a simple formula for the logarithmic mean (but it can be calculated based on its connection with Generalised \\(\\mathcal{IG}\\) and formulae provided in Sichel et al., 1997). After that, similarly to how it was done for Log-Normal distribution above, the connection between the logarithmic and normal moments should be used to get the conditional expectation and variance. If these relations are not available or are too complicated, then simulations can be used to obtain the numeric approximations (see discussion in Section 18.1.1). Finally, we should remark that the formulae for the conditional moments in logARIMA are complicated mainly because of the distributional assumptions inherited from ETS. However, this allows the construction of more complicated models, some of which are discussed in Section 9.4. 9.2.2 Parameters bounds Finally, modifying the recursions (9.17) and (9.18), we can get the stability condition for the parameters, similar to the one for pure additive ETS from Section 5.4. The advantage of the pure multiplicative ARIMA formulated in the form (9.16) is that the adequate stability condition can be obtained in contrast with the pure multiplicative ETS models. It will be the same as the pure additive ARIMA and/or ETS. The ARIMA model will be stable, when the absolute values of all non-zero eigenvalues of the discount matrices \\(\\mathbf{D}_{i}\\) are lower than one, given that: \\[\\begin{equation} \\mathbf{D}_{i} = \\mathbf{F}_{i} -\\mathbf{g}_{i} \\mathbf{w}_{i}&#39; . \\tag{9.24} \\end{equation}\\] Hyndman et al. (2008) show that the stability condition for SSOE models corresponds to the invertibility condition of ARIMA (Section 8.2.4), so the model can either be checked via the discount matrix (9.24) or via the MA polynomials (8.58). When it comes to stationarity, state space ARIMA is always non-stationary if the differences \\(\\mathbf{D}_j \\neq 0\\) for any \\(j\\). So, there needs to be a different mechanism for the stationarity check. The simplest thing to do would be to expand the AR(p) polynomials, ignoring I(d), fill in the transition matrix \\(\\mathbf{F}\\) and then calculate its eigenvalues. If they are lower than one by absolute value, the model is stationary. The same condition can be checked via the roots of the polynomial of AR(p) (8.57). However, the eigenvalues approach is more computationally efficient, and I recommend using it instead of the conventional polynomials calculation. If both stability and stationarity conditions for ARIMA are satisfied, we will call the bounds that the AR / MA parameters form “admissible,” similar to how they are called in ETS. Note that ARIMA has no “usual” or “traditional” bounds. References "],["ADAMARIMADistributions.html", "9.3 Distributional assumptions of ADAM ARIMA", " 9.3 Distributional assumptions of ADAM ARIMA Following the same idea as in pure additive (Section 5.5) and pure multiplicative (Section 6.5) ETS models, we can have state space ARIMA with different distributional assumptions, but with the distributions aligning more appropriately with the types of models. For additive ARIMA: Normal: \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\); Laplace: \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\); S: \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\); Generalised Normal: \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\); Asymmetric Laplace: \\(\\epsilon_t \\sim \\mathcal{ALaplace}(0, s, \\alpha)\\). For multiplicative ARIMA: Inverse Gaussian: \\(\\left(1+\\epsilon_t \\right) \\sim \\mathcal{IG}(1, \\sigma^2)\\); Log Normal: \\(\\left(1+\\epsilon_t \\right) \\sim \\text{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\); Gamma: \\(\\left(1+\\epsilon_t \\right) \\sim \\mathcal{\\Gamma}(\\sigma^{-2}, \\sigma^2)\\). The restrictions imposed on the model parameters correspond to the ones for ETS. In the case of pure additive models, they ensure that the conditional h steps ahead mean is not impacted by the location of distribution (thus \\(\\mu_\\epsilon=0\\)); in case of pure multiplicative models, they ensure that the conditional h steps ahead mean just equals the point forecast (thus imposing \\(\\mathrm{E}(1+\\epsilon_t)=1\\)). 9.3.1 Conditional distributions When it comes to conditional distribution of variables, additive ADAM ARIMA with the assumptions discussed in subsection 9.3 has closed forms for all of them. For example, if we work with additive ARIMA, then according to recursive relation (9.17) from subsection 9.2, the h steps ahead value follows the same distribution but with different conditional mean and variance. For example, if \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\), then \\(y_{t+h} \\sim \\mathcal{GN}(\\mu_{y,t+h}, s_{h}, \\beta)\\), where \\(s_{h}\\) is the conditional h steps ahead scale, found from the connection between variance and scale in Generalised Normal distribution via: \\[\\begin{equation*} s_h = \\sqrt{\\frac{\\sigma^2_h \\Gamma(1/\\beta)}{\\Gamma(3/\\beta)}}. \\end{equation*}\\] Using similar principles, we can calculate scale parameters for the other distributions. When it comes to the multiplicative models, the conditional distribution has the closed form in case of log Normal (it is log Normal as well), but does not have it in case of Inverse Gaussian and Gamma. In the former case, the logarithmic moments can be directly used to define the parameters of distribution, i.e. if \\(\\left(1+\\epsilon_t \\right) \\sim \\text{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\), then \\(y_{t+h} \\sim \\text{log}\\mathcal{N}\\left(\\mu_{\\log y,t+h}, \\sigma^2_{\\log y,h} \\right)\\). In the other case, simulations need to be used in order to get the quantile, cumulative and density functions. "],["ETSAndARIMA.html", "9.4 ETS + ARIMA", " 9.4 ETS + ARIMA Coming back to the topic of ETS and ARIMA, we can now look at it from the point of view of the SSOE state space model. 9.4.1 Pure additive models A pure additive ETS + ARIMA model can be formulated in the general form, which we have already discussed several times in this textbook: \\[\\begin{equation*} \\begin{aligned} &amp;{y}_{t} = \\mathbf{w}^\\prime \\mathbf{v}_{t-\\mathbf{l}} + \\epsilon_t \\\\ &amp;\\mathbf{v}_{t} = \\mathbf{F} \\mathbf{v}_{t-\\mathbf{l}} + \\mathbf{g} \\epsilon_t \\end{aligned}, \\end{equation*}\\] but now the matrices and vectors of the model contain ETS and ARIMA components, stacked one after another. For example, if we want to construct ETS(A,N,A)+ARIMA(2,0,0), we can formulate this model as: \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = l_{t-1} + s_{t-m} + v_{1,t-1} + v_{2,t-2} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\alpha \\epsilon_t \\\\ &amp;s_t = s_{t-m} + \\gamma \\epsilon_t \\\\ &amp;v_{1,t} = \\phi_1 v_{1,t-1} + \\phi_1 v_{2,t-2} + \\phi_1 \\epsilon_t \\\\ &amp;v_{2,t} = \\phi_1 v_{1,t-1} + \\phi_2 v_{2,t-2} + \\phi_2 \\epsilon_t \\end{aligned}, \\tag{9.25} \\end{equation}\\] where \\(\\phi_1\\) is the parameter of the AR(1) part of the model. This model represented in the conventional additive SSOE state space model leads to the following matrices and vectors: \\[\\begin{equation} \\begin{aligned} \\mathbf{w} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, &amp; \\mathbf{F} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\phi_1 &amp; \\phi_1 \\\\ 0 &amp; 0 &amp; \\phi_2 &amp; \\phi_2 \\end{pmatrix}, &amp; \\mathbf{g} = \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\phi_1 \\\\ \\phi_2 \\end{pmatrix}, \\\\ \\mathbf{v}_{t} = \\begin{pmatrix} l_t \\\\ s_t \\\\ v_{1,t} \\\\ v_{2,t} \\end{pmatrix}, &amp; \\mathbf{v}_{t-\\mathbf{l}} = \\begin{pmatrix} l_{t-1} \\\\ s_{t-m} \\\\ v_{1,t-1} \\\\ v_{2,t-2} \\end{pmatrix} &amp; \\mathbf{l} = \\begin{pmatrix} 1 \\\\ m \\\\ 1 \\\\ 2\\end{pmatrix} \\end{aligned}. \\tag{9.26} \\end{equation}\\] So, in this formulation the states of ETS and ARIMA are independent and form a combination of models only in the measurement equation. In a way, this model becomes similar to fitting first ETS to the data and then ARIMA to the residuals, but estimating both elements at the same time. ADAM introduces the flexibility necessary for fitting any ETS+ARIMA combination, but not all combinations make sense. For example, here how ETS(A,N,N)+ARIMA(0,1,1) would look like: \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = l_{t-1} + v_{1,t-1} + \\epsilon_t \\\\ &amp;l_t = l_{t-1} + \\alpha \\epsilon_t \\\\ &amp;v_{1,t} = v_{1,t-1} + (1+\\theta_1) \\epsilon_t \\end{aligned}. \\tag{9.27} \\end{equation}\\] In the transition part of the model (9.27), the two equations duplicate each other because they have exactly the same mechanism of update of states. In fact, as we know from Section 8.4.1, ETS(A,N,N) and ARIMA(0,1,1) are equivalent, when \\(\\alpha=1+\\theta_1\\). If we estimate this model, then we are duplicating the state, splitting it into two parts with some arbitrary weights. This becomes apparent if we insert the transition equations in the measurement one, obtaining: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp; l_{t-2} + \\alpha \\epsilon_{t-1} + v_{1,t-2} + (1+\\theta_1) \\epsilon_{t-1} + \\epsilon_t =\\\\ &amp; l_{t-2} + v_{1,t-2} + (1+\\theta_1+\\alpha) \\epsilon_{t-1} + \\epsilon_t \\end{aligned}, \\tag{9.28} \\end{equation}\\] which leads to an infinite combination of values of parameters \\(\\theta\\) and \\(\\alpha\\) that would produce exactly the same model fit. So, the model (9.27) does not have unique parameters and thus is unidentifiable. This means that we cannot reach the “true model” based on ETS(A,N,N)+ARIMA(0,1,1) and thus the model selection via information criteria becomes inappropriate. Furthermore, the estimates of parameters of such a model might become biased, inefficient and inconsistent due to the “infinite combination” issue mentioned above. In some other cases, some parts of the model might be duplicated, making the whole model unidentifiable, so it makes sense to switch to either ETS or ARIMA, depending on the circumstances. For example, if we have ETS(A,A,N)+ARIMA(0,2,3), then some parts of the models will be duplicated (because ETS(A,A,N) is equivalent to ARIMA(0,2,2)), so it would be more reasonable to switch to pure ARIMA(0,2,3) instead. On the other hand, if we deal with ETS(A,Ad,N)+ARIMA(0,1,2), then dropping the ARIMA part would be more appropriate. These examples show that, when using ETS+ARIMA, model building needs to be done with care, not to get an unreasonable model that cannot be identified. As a general recommendation, keep the ETS and ARIMA connection (see Section 8.4) in mind, when deciding, what to construct. And here is a short list of guidelines of what to do in some special cases: For ETS(A,N,N)+ARIMA(0,1,q): use ARIMA(0,1,q) in case of \\(q &gt;1\\), use ETS(A,N,N) in case of \\(q\\leq 1\\); For ETS(A,A,N)+ARIMA(0,2,q): use ARIMA(0,2,q) in case of \\(q &gt;2\\), use ETS(A,A,N) in case of \\(q \\leq 2\\); For ETS(A,Ad,N)+ARIMA(p,1,q): use ARIMA(p,1,q), when either \\(p&gt;1\\) or \\(q&gt;2\\), use ETS(A,Ad,N), when \\(p \\leq 1\\) and \\(q \\leq 2\\). Regarding seasonal models, the relation between ETS and ARIMA is more complex. It is highly improbable to get to equivalent ARIMA models, so it makes sense to make sure that the three rules above hold for the non-seasonal part of the model. 9.4.2 Pure multiplicative models When it comes to the multiplicative error and mixed ETS models, the ETS+ARIMA might not have the same issues as the pure additive one. This is because the multiplicative ETS (Section 6.1) and multiplicative ARIMA (Section 9.1.4) are formulated differently. An example is an ETS(M,N,N)+logARIMA(0,1,1), which is formulated as: \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = l_{t-1} v_{1,t-1} (1 + \\epsilon_t) \\\\ &amp;l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\ &amp;\\log v_{1,t} = \\log v_{1,t-1} + (1+\\theta_1) \\log (1 + \\epsilon_t) \\end{aligned}. \\tag{9.29} \\end{equation}\\] The last equation in (9.29) can be rewritten as \\(v_{1,t} = v_{1,t-1} (1 + \\epsilon_t)^{(1+\\theta_1)}\\), demonstrating the difference between the transition equation of ETS(M,N,N) and multiplicative ARIMA(0,1,1). Still, the two models will be similar in cases, when \\(\\alpha\\) is close to zero and (respectively) \\(\\theta\\) is close to -1. So this combination of models should be treated with care, along with other potentially similar combinations. The following combinations of the two models can be considered as potentially unidentifiable under some conditions: ETS(M,N,N)+logARIMA(0,1,1); ETS(M,M,N)+logARIMA(0,2,2); ETS(M,Md,N)+logARIMA(1,1,1). In addition, the recommendations discussed for the pure additive ETS+ARIMA can be applied here for the pure multiplicative ETS+ARIMA to guarantee that the resulting model is identifiable no matter what. Finally, mixing additive ETS with multiplicative ARIMA or multiplicative ETS with additive ARIMA does not make sense from the modelling point of view. It only complicates the model building process, so we do not consider these exotic cases in this book, although they are theoretically possible. In the most general case the pure multiplicative ETS+ARIMA model can be written as (based on (6.1) and (9.16)): \\[\\begin{equation} \\begin{aligned} &amp;{y}_{t} = \\exp \\left( \\mathbf{w}_{E}^\\prime \\log \\mathbf{v}_{E,t-\\mathbf{l}_E} + \\mathbf{w}_{A}^\\prime \\log \\mathbf{v}_{A,t-\\mathbf{l}_A} + \\log(1+\\epsilon_t) \\right) \\\\ &amp;\\log \\mathbf{v}_{E,t} = \\mathbf{F}_{E} \\log \\mathbf{v}_{E,t-\\mathbf{l}_E} + \\log(\\mathbf{1}_k + \\mathbf{g}_E \\epsilon_t) \\\\ &amp;\\log \\mathbf{v}_{A,t} = \\mathbf{F}_{A} \\log \\mathbf{v}_{A,t-\\mathbf{l}_A} + \\mathbf{g}_A \\log(1+\\epsilon_t) \\end{aligned} , \\tag{9.30} \\end{equation}\\] where the subscript reflects, which part corresponds to which model: “E” - ETS, “A” - ARIMA. "],["ADAMARIMAExamples.html", "9.5 Examples of application", " 9.5 Examples of application Building upon the example with AirPassengers data from Section 8.5.2, we will construct several multiplicative ARIMA models and see which one is the most appropriate for the data. As a reminder, the best additive ARIMA model was SARIMA(0,2,2)(0,2,2)\\(_{12}\\), which had AICc of 1028.828. We will do something similar here, using Log-Normal distribution, thus working with logARIMA. To understand what model can be used in this case, we can take the logarithm of data and see what happens with the components of time series: plot(log(AirPassengers)) We still have the trend in the data, and the seasonality now corresponds to the additive rather than the multiplicative (as expected). While we might still need the second differences for the non-seasonal part of the model, taking the first differences for the seasonal should suffice because the logarithmic transform will take care of the expanding seasonal pattern in the data. So we can test several models with different options for ARIMA orders: adamModelLogSARIMA &lt;- vector(&quot;list&quot;,3) # logSARIMA(0,1,1)(0,1,1)[12] adamModelLogSARIMA[[1]] &lt;- adam(AirPassengers, &quot;NNN&quot;, lags=c(1,12), orders=list(ar=c(0,0), i=c(1,1), ma=c(1,1)), h=12, holdout=TRUE, distribution=&quot;dlnorm&quot;) # logSARIMA(0,2,2)(0,1,1)[12] adamModelLogSARIMA[[2]] &lt;- adam(AirPassengers, &quot;NNN&quot;, lags=c(1,12), orders=list(ar=c(0,0), i=c(2,1), ma=c(2,2)), h=12, holdout=TRUE, distribution=&quot;dlnorm&quot;) # logSARIMA(1,1,2)(0,1,1)[12] adamModelLogSARIMA[[3]] &lt;- adam(AirPassengers, &quot;NNN&quot;, lags=c(1,12), orders=list(ar=c(1,0), i=c(1,1), ma=c(2,1)), h=12, holdout=TRUE, distribution=&quot;dlnorm&quot;) names(adamModelLogSARIMA) &lt;- c(&quot;logSARIMA(0,1,1)(0,1,1)[12]&quot;, &quot;logSARIMA(0,2,2)(0,1,1)[12]&quot;, &quot;logSARIMA(1,1,2)(0,1,1)[12]&quot;) The thing that is different between the models is the non-seasonal part. Using the connection with ETS (discussed in Section 8.4), the first model should work on local level data, the second should be optimal for the local trend series, and the third one is placed somewhere in between the two. We can compare the models using AICc: sapply(adamModelLogSARIMA, AICc) ## logSARIMA(0,1,1)(0,1,1)[12] logSARIMA(0,2,2)(0,1,1)[12] ## 982.5764 1096.8616 ## logSARIMA(1,1,2)(0,1,1)[12] ## 993.0431 It looks like the logSARIMA(0,1,1)(0,1,1)\\(_{12}\\) is more appropriate for the data. In order to make sure that we did not miss anything, we analyse the residuals of this model (Figure 9.1): par(mfcol=c(2,1), mar=c(2,2,2,1)) plot(adamModelLogSARIMA[[1]],10:11) Figure 9.1: ACF and PACF of logSARIMA(0,1,1)(0,1,1)\\(_{12}\\). We can see that there are no significant coefficient on either ACF or PACF, so there is nothing else to improve in this model. We can then produce forecast from the model and see how it performed on the holdout sample (Figure 9.2): plot(forecast(adamModelLogSARIMA[[1]],h=12,interval=&quot;prediction&quot;), main=paste0(adamModelLogSARIMA[[1]]$model,&quot; with Log-Normal distribution&quot;)) Figure 9.2: Forecast from logSARIMA(0,1,1)(0,1,1)\\(_{12}\\). The ETS model closest to the logSARIMA(0,1,1)(0,1,1)\\(_{12}\\) would probably be ETS(M,M,M), because the former has both seasonal and non-seasonal differences (see discussion in Subsection 8.4.4): adamModelETS &lt;- adam(AirPassengers, &quot;MMM&quot;, h=12, holdout=TRUE) adamModelETS ## Time elapsed: 0.1 seconds ## Model estimated using adam() function: ETS(MMM) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 468.5176 ## Persistence vector g: ## alpha beta gamma ## 0.7684 0.0206 0.0000 ## ## Sample size: 132 ## Number of estimated parameters: 17 ## Number of degrees of freedom: 115 ## Information criteria: ## AIC AICc BIC BICc ## 971.0351 976.4036 1020.0428 1033.1492 ## ## Forecast errors: ## ME: -5.617; MAE: 15.496; RMSE: 21.938 ## sCE: -25.677%; Asymmetry: -23.1%; sMAE: 5.903%; sMSE: 0.698% ## MASE: 0.643; RMSSE: 0.7; rMAE: 0.204; rRMSE: 0.213 Comparing information criteria, ETS(M,M,M) should be preferred to logARIMA, but in terms of accuracy on the holdout, logARIMA is more accurate than ETS on this data: adamModelLogSARIMA[[1]] ## Time elapsed: 0.22 seconds ## Model estimated using adam() function: SARIMA(0,1,1)[1](0,1,1)[12] ## Distribution assumed in the model: Log-Normal ## Loss function type: likelihood; Loss function value: 472.923 ## ARMA parameters of the model: ## MA: ## theta1[1] theta1[12] ## -0.2785 -0.5530 ## ## Sample size: 132 ## Number of estimated parameters: 16 ## Number of degrees of freedom: 116 ## Information criteria: ## AIC AICc BIC BICc ## 977.8460 982.5764 1023.9708 1035.5197 ## ## Forecast errors: ## ME: -12.968; MAE: 13.971; RMSE: 19.143 ## sCE: -59.285%; Asymmetry: -91.7%; sMAE: 5.322%; sMSE: 0.532% ## MASE: 0.58; RMSSE: 0.611; rMAE: 0.184; rRMSE: 0.186 "],["ADAMX.html", "Chapter 10 Explanatory variables in ADAM", " Chapter 10 Explanatory variables in ADAM In real life, the need for explanatory variables arises when some external factors influence the response variable, which cannot be ignored and impact the final forecasts. Examples of such variables in the demand forecasting context include price changes, promotional activities, temperature etc. In some cases, the changes in these factors would not substantially impact the demand, but they would be essential for improving accuracy in others. If we omit this information from the model, this will be damaging for both point forecasts and prediction intervals (see discussion in Chapter 12 of Svetunkov, 2021). While the inclusion of explanatory variables in the context of ARIMA models is a relatively well-studied topic (for example, this was discussed by Box and Jenkins, 1976), in the case of ETS, there is only a Chapter 9 in Hyndman et al. (2008) and a handful of papers. Koehler et al. (2012) discuss the mechanism detection and approximation of outliers via an ETSX model (ETS with explanatory variables). The authors show that if an outlier appears at the end of the series, it will seriously impact the final forecast. However, if it appears either in the middle or at the beginning of the series, the impact on the final forecast is typically negligible. This is relevant to our discussion because there is a direct link between dealing with outliers in Koehler et al. (2012) and including explanatory variables in ETSX in terms of how the model is formulated in these two situations. Kourentzes and Petropoulos (2016) used ETSX successfully for promotional modelling, demonstrating that it outperforms the conventional ETS in terms of point forecasts accuracy in cases when promotions happen. The state-space model (7.1) can be easily extended by including additional components and explanatory variables. This chapter discusses the main aspects of ADAM with explanatory variables, how it is formulated, and how the more advanced models can be built upon it. Furthermore, the parameters for these additional components can either be fixed (static) or change over time (dynamic). We discuss both in the following sections. We also show that the stability and forecastability conditions, discussed in Section 5.4 for the pure additive ETS model, will be different in the case of the ETSX model and that the classical definitions should be updated to cater for the introduction of the explanatory variables. We also briefly discuss the inclusion of categorical variables in the ETSX model and show that the seasonal ETS models can be considered special cases of ADAM ETSX in some situations. Furthermore, we will use the terms “deterministic” and “stochastic” explanatory variables to denote the situations when the values of these variables are known in advance or can be controlled by us. An example of the former would be the price of a product or a promotion that we decide to have. An example of the latter would be the temperature. As a final note, we will carry out the discussion of the topic on the example of ADAM ETSX, keeping in mind that the same principles will hold for ADAM ARIMAX because the two are formulated in the same way. The more general dynamic model (encompassing ETS and/or ARIMA) with explanatory variables is called “ADAMX” in this and further chapters. References "],["ADAMXFormulation.html", "10.1 ADAMX: Model formulation", " 10.1 ADAMX: Model formulation As discussed previously, there are two types of errors in ADAM: Additive discussed in Hyndman et al. (2008) in Chapter 5 in case of ETS and Chapter 9 for ARIMA, Multiplicative covered in Chapter 6 for ETS and in Subsection 9.1.4. The inclusion of explanatory variables in ADAMX is determined by the type of the error, so that in case of (1) the measurement equation of the model is: \\[\\begin{equation} {y}_{t} = a_{0,t} + a_{1,t} x_{1,t} + a_{2,t} x_{2,t} + \\dots + a_{n,t} x_{n,t} + \\epsilon_t , \\tag{10.1} \\end{equation}\\] where \\(a_{0,t}\\) is the point value based on all ETS components (for example, \\(a_{0,t}=l_{t-1}\\) in case of ETS(A,N,N)), \\(x_{i,t}\\) is the \\(i\\)-th explanatory variable, \\(a_{i,t}\\) is its parameter and \\(n\\) is the number of explanatory variables. We will call the estimated parameters of such model \\(\\hat{a}_{i,t}\\). In the simple case, the transition equation for such model would imply that the parameters \\(a_{i,t}\\) do not change over time: \\[\\begin{equation} \\begin{aligned} &amp;a_{1,t} = a_{1,t-1} \\\\ &amp;a_{2,t} = a_{2,t-1} \\\\ &amp;\\vdots \\\\ &amp;a_{n,t} = a_{n,t-1} \\end{aligned} . \\tag{10.2} \\end{equation}\\] Various complex mechanisms for the states update can be proposed instead of (10.2), but we do not discuss them at this point. Typically, the initial values of parameters would be estimated at the optimisation stage, either based on likelihood or some other loss function, so the index \\(t\\) can be dropped, substituting \\(a_{i,t}=a_{i}\\) for all \\(i=1,\\dots,n\\). When it comes to the multiplicative error model, it should be formulated differently. The most straight forward would be to formulate the model in logarithms in order to linearise it: \\[\\begin{equation} \\log {y}_{t} = \\log a_{0,t} + a_{1,t} x_{1,t} + a_{2,t} x_{2,t} + \\dots + a_{n,t} x_{n,t} + \\log(1+ \\epsilon_t). \\tag{10.3} \\end{equation}\\] Note that if log-log model is required, all that needs to be done, is to substitute \\(x_{i,t}\\) with \\(\\log x_{i,t}\\). The compact form of the ADAMX model implies that the explanatory variables \\(x_{i,t}\\) are included in the measurement vector \\(\\mathbf{w}_{t}\\), making it change over time. The parameters are then moved to the state vector, and a diagonal matrix is added to the existing transition matrix. Finally, the persistence vector for the parameters of explanatory variables should contain zeroes. The state space model, in that case, can be represented as: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = \\mathbf{w}&#39;_t \\mathbf{v}_{t-\\mathbf{l}} + \\epsilon_t \\\\ &amp; \\mathbf{v}_t = \\mathbf{F} \\mathbf{v}_{t-\\mathbf{l}} + \\mathbf{g} \\epsilon_t \\end{aligned} \\tag{10.4} \\end{equation}\\] for the pure additive and \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp; \\exp\\left(\\mathbf{w}&#39;_t \\log \\mathbf{v}_{t-\\mathbf{l}} + \\log(1 + \\epsilon_t)\\right) \\\\ \\log \\mathbf{v}_t = &amp; \\mathbf{F} \\log \\mathbf{v}_{t-\\mathbf{l}} + \\log(\\mathbf{1}_k + \\mathbf{g} \\epsilon_t) \\end{aligned}. \\tag{10.5} \\end{equation}\\] for the pure multiplicative models. So, the only thing that changes in these models is the time varying measurement vector \\(\\mathbf{w}&#39;_t\\) instead of the fixed one. For example, in case of ETSX(A,Ad,A) we will have: \\[\\begin{equation} \\begin{aligned} \\mathbf{F} = \\begin{pmatrix} 1 &amp; \\phi &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\phi &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 1 \\end{pmatrix}, &amp; \\mathbf{w}_t = \\begin{pmatrix} 1 \\\\ \\phi \\\\ 1 \\\\ x_{1,t} \\\\ \\vdots \\\\x_{n,t} \\end{pmatrix}, &amp; \\mathbf{g} = \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}, \\\\ &amp; \\mathbf{v}_{t} = \\begin{pmatrix} l_t \\\\ b_t \\\\ s_t \\\\ a_{1,t} \\\\ \\vdots \\\\ a_{n,t} \\end{pmatrix}, &amp; \\mathbf{l} = \\begin{pmatrix} 1 \\\\ 1 \\\\ m \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\end{aligned}, \\tag{10.6} \\end{equation}\\] which is equivalent to the combination of equations (10.1) and (10.2): \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} + a_{1,t} x_{1,t} + \\dots + a_{n,t} x_{n,t} + \\epsilon_t \\\\ &amp; l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\ &amp; b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\\\ &amp; s_t = s_{t-m} + \\gamma \\epsilon_t \\\\ &amp; a_{1,t} = a_{1,t-1} \\\\ &amp; \\vdots \\\\ &amp; a_{n,t} = a_{n,t-1} \\end{aligned}. \\tag{10.7} \\end{equation}\\] Alternatively, the state, measurement and persistence vectors and transition matrix can be split each into two parts, separating the ETS and X parts in the state space equations: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = \\mathbf{w}&#39; \\mathbf{v}_{t-\\mathbf{l}} + \\mathbf{x}&#39;_{t} \\mathbf{a}_{t-1} + \\epsilon_t \\\\ &amp; \\mathbf{v}_{1,t} = \\mathbf{F} \\mathbf{v}_{t-\\mathbf{l}} + \\mathbf{g} \\epsilon_t \\\\ &amp; \\mathbf{a}_{t} = \\mathbf{a}_{t-1} \\end{aligned} , \\tag{10.8} \\end{equation}\\] where \\(\\mathbf{w}\\), \\(\\mathbf{F}\\), \\(\\mathbf{g}\\) and \\(\\mathbf{v}_{t}\\) contain the elements of the conventional components of ADAM and \\(\\mathbf{a}_{t}\\) is the vector of parameters for the explanatory variables. When all the smoothing parameters of the ETS part of the model are equal to zero, the ETSX reverts to a deterministic model, directly related to the multiple linear regression. For example, in case of ETSX(A,N,N) with \\(\\alpha=0\\) we get: \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = l_{t-1} + a_{1,t} x_{1,t} + \\dots + a_{n,t} x_{n,t} + \\epsilon_t \\\\ &amp; l_t = l_{t-1} \\\\ &amp; a_{1,t} = a_{1,t-1} \\\\ &amp; \\vdots \\\\ &amp; a_{n,t} = a_{n,t-1} \\end{aligned}, \\tag{10.9} \\end{equation}\\] where \\(l_t=a_0\\) is the intercept of the model. (10.9) can be rewritten in the conventional way, dropping the transition part of the state space model: \\[\\begin{equation} y_{t} = a_0 + a_{1} x_{1,t} + \\dots + a_{n} x_{n,t} + \\epsilon_t . \\tag{10.10} \\end{equation}\\] In the case of models with trend and/or seasonal, the model becomes equivalent to the regression with deterministic trend and/or seasonality. This means that, in general, ADAMX implies that we are dealing with a regression with time-varying intercept, where the principles of this variability are defined by the ADAM components (e.g. intercept can vary seasonally). Similar properties are obtained with the multiplicative error model. The main difference is that the impact of explanatory variables on the response variable will vary with the intercept changes. The model, in this case, combines the strengths of the multiplicative regression and the dynamic model, where the variability of the response variable changes with the change of the baseline model (ADAM ETS and/or ADAM ARIMA in this case). References "],["ADAMXConventionalConditionalMoments.html", "10.2 Conditional expectation and variance of ADAMX", " 10.2 Conditional expectation and variance of ADAMX 10.2.1 ADAMX with deterministic explanatory variables ETS models have a severe limitation, which will be discussed in chapter 17: they assume that the model’s parameters are known, i.e. there is no variability in them and that the in-sample estimates are fixed no matter how the sample size changes. This limitation also impacts the ETSX part. While in the case of point forecasts, this is not important, this affects the conditional variance and prediction intervals. As a result, the conditional mean and variance of the conventional ADAMX assume that the parameters \\(a_0, \\dots a_n\\) are also known, leading to the following formulae in the case of the pure additive model, based on what was discussed in the section 5.3: \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = \\text{E}(y_{t+h}|t) = &amp; \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i,t}&#39; \\mathbf{F}_{m_i}^{\\lceil\\frac{h}{m_i}\\rceil-1} \\right) \\mathbf{v}_{t} \\\\ \\text{V}(y_{t+h}|t) = &amp; \\left( \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i,t}&#39; \\sum_{j=1}^{\\lceil\\frac{h}{m_i}\\rceil-1} \\mathbf{F}_{m_i}^{j-1} \\mathbf{g}_{m_i} \\mathbf{g}&#39;_{m_i} (\\mathbf{F}_{m_i}&#39;)^{j-1} \\mathbf{w}_{m_i,t} \\right) + 1 \\right) \\sigma^2 \\end{aligned}, \\tag{10.11} \\end{equation}\\] the main difference from the moments of the conventional model (from Section 5.3) being is the index \\(t\\) in the measurement vector \\(\\mathbf{w}_t\\). As an example, here how the two statistics will look in case of ETSX(A,N,N): \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = \\text{E}(y_{t+h}|t) = &amp; l_{t} + \\sum_{i=1}^n a_i x_{i,t+h} \\\\ \\text{V}(y_{t+h}|t) = &amp; \\left((h-1) \\alpha^2 + 1 \\right) \\sigma^2 \\end{aligned}, \\tag{10.12} \\end{equation}\\] where the variance ignores the potential variability rising from the explanatory variables because of the ETS limitations. This assumes that the future values of explanatory variables \\(x_{i,t}\\) are known (the variable is deterministic). As a result, the prediction and confidence intervals for the ADAMX model would typically be narrower than expected and would only be adequate in cases of large samples, where the law of large numbers would start working (Section 4.2 of Svetunkov, 2021), reducing the variance of parameters (this is assuming that the typical assumptions of the model from Section 1.4.1 hold). 10.2.2 ADAMX with stochastic explanatory variables Note that the ADAMX works well in cases when the future values of \\(x_{i,t+h}\\) are known. It is a realistic assumption when we control the explanatory variables (e.g. prices and promotions for our product). But when the variables are out of our control, they need to be forecasted somehow. In this case we are assuming that each \\(x_{i,t}\\) is a stochastic variable with some dynamic conditional one step ahead expectation \\(\\mu_{x_{i,t}}\\) and a one step ahead variance \\(\\sigma^2_{x_{i,1}}\\). Note that in this case, we treat the available explanatory variables as models on their own, not just as values given to us from above. This assumption of randomness will change the conditional moments of the model. Here is what we will have in the case of ETSX(A,N,N) (given that the typical assumptions from Section 1.4.1 hold): \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = \\text{E}(y_{t+h}|t) = &amp; l_{t} + \\sum_{i=1}^n a_i \\mu_{x_{i,t+h}} \\\\ \\text{V}(y_{t+h}|t) = &amp; \\left((h-1) \\alpha^2 + 1 \\right) \\sigma^2 + \\sum_{i=1}^n a^2_i \\sigma^2_{x_{i,h}} + 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j \\sigma_{x_{i,h},x_{j,h}} \\end{aligned}, \\tag{10.13} \\end{equation}\\] where \\(\\sigma^2_{x_{i,h}}\\) is the conditional variance of \\(x_{i}\\) h steps ahead, \\(\\sigma_{x_{i,h},x_{j,h}}\\) is the h steps ahead covariance between the explanatory variables \\(x_{i,h}\\) and \\(x_{j,h}\\), both conditional on the information available at the observation \\(t\\). similarly, if we are interested in one step ahead point forecast from the model, it should take the randomness of explanatory variables into account and become: \\[\\begin{equation} \\begin{aligned} \\mu_{y,t |t-1} = &amp; \\left. \\mathrm{E}\\left(l_{t-1} + \\sum_{i=1}^n a_i x_{i,t} + \\epsilon_{t} \\right| t-1 \\right) = \\\\ = &amp; l_{t-1} + \\sum_{i=1}^n a_i \\mu_{x_{i,t}} \\end{aligned}. \\tag{10.14} \\end{equation}\\] So, in the case of ADAMX with random explanatory variables, the model should be constructed based on the expectations of those variables, not the random values themselves. This explains, for example, why Athanasopoulos et al. (2011) found that some models with predicted explanatory variables work better than the model with the variables themselves. This means that, when estimating the model, such as ETS(A,N,N), the following should be constructed: \\[\\begin{equation} \\begin{aligned} &amp; \\hat{y}_{t} = \\hat{l}_{t-1} + \\sum_{i=1}^n \\hat{a}_{i,t} \\hat{x}_{i,t} \\\\ &amp; e_t = y_t -\\hat{y}_{t} \\\\ &amp; \\hat{l}_{t} = \\hat{l}_{t-1} + \\hat{\\alpha} e_t \\\\ &amp; \\hat{a}_{i,t} = \\hat{a}_{i,t-1} \\text{ for each } i \\in \\{1, \\dots, n\\} \\end{aligned}, \\tag{10.15} \\end{equation}\\] where \\(\\hat{x}_{i,t}\\) is the in-sample conditional one step ahead mean for the explanatory variable \\(x_i\\). Finally, as discussed previously, the conditional moments for the pure multiplicative and mixed models do not generally have closed forms, implying that the simulations need to be carried out. The situation becomes more challenging in the case of random explanatory variables because that randomness needs to be introduced in the model itself and propagated throughout the time series. This is not a trivial task, which we will discuss in Section 17.4. References "],["ADAMXDynamic.html", "10.3 Dynamic X in ADAMX", " 10.3 Dynamic X in ADAMX Note: the model discussed in this section assumes particular dynamics of parameters, aligning with what the conventional ETS assumes: parameters are correlated with the states of the model. It does not treat parameters as independent as, for example, MSOE state space models do, which makes this model restrictive in its application. But this type of model works well with categorical variables, as I show later in this section. As discussed in Section 10.1, the parameters of the explanatory variables in ADAMX can be assumed to be constant over time or can be assumed to vary according to some mechanism. The most reasonable one in the SSOE framework relies on the same error for different components of the model because this mechanism aligns with the model itself. Osman and King (2015) proposed one of such mechanisms, relying on the differences of the data. The primary motivation of their approach was to make the dynamic ADAMX model stable, which is a challenging task. However, this mechanism relies on the assumption of non-stationarity of the explanatory variables, which does not always make sense (for example, it is unreasonable in the case of promotional data). An alternative approach discussed in this section is the one initially proposed by Svetunkov (1985) based on the stochastic approximation mechanism and further developed in Svetunkov and Svetunkov (2014). We start with the following linear regression model: \\[\\begin{equation} y_{t} = a_{0,t-1} + a_{1,t-1} x_{1,t} + \\dots + a_{n,t-1} x_{n,t} + \\epsilon_t , \\tag{10.16} \\end{equation}\\] where all parameters vary over time and \\(a_{0,t}\\) represents the value from the conventional additive error ETS model. The updating mechanism for the parameters is straight forward and relies on the ratio of the error term and the respective explanatory variables: \\[\\begin{equation} a_{i,t} = a_{i,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_i \\frac{\\epsilon_t}{x_{i,t}} \\text{ for each } i \\in \\{1, \\dots, n\\}, \\text{ if } x_{i,t}\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. , \\tag{10.17} \\end{equation}\\] where \\(\\delta_i\\) is the smoothing parameter of the \\(i\\)-th explanatory variable. The same model can be represented in the state space form, based on the equations, similar to (10.4): \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = \\mathbf{w}&#39;_t \\mathbf{v}_{t-\\mathbf{l}} + \\epsilon_t \\\\ &amp; \\mathbf{v}_t = \\mathbf{F} \\mathbf{v}_{t-\\mathbf{l}} + \\mathbf{z}_t \\mathbf{g} \\epsilon_t \\end{aligned} \\tag{10.18} \\end{equation}\\] where \\(\\mathbf{z}_t = \\mathrm{diag}\\left(\\mathbf{w}_t\\right)^{-1}=\\mathbf{I}_{k+n} \\odot (\\mathbf{w}_t \\mathbf{1}_{k+n})\\) is the diagonal matrix consisting of inverses of explanatory variables, \\(\\mathbf{I}_{k+n}\\) is the identity matrix for \\(k\\) ADAM components and \\(n\\) explanatory variables and \\(\\odot\\) is Hadamard product for element-wise multiplication. This is the inverse of the diagonal matrix based on the measurement vector, for which those values that cannot be inverted (due to division by zero) are substitute by zeroes in order to reflect the condition in (10.17). In addition to what (10.4) contained, we add smoothing parameters \\(\\delta_i\\) in the persistence vector \\(\\mathbf{g}\\) for each of the explanatory variables. If the error term is multiplicative, then the model changes to: \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = \\exp \\left(a_{0,t-1} + a_{1,t-1} x_{1,t} + \\dots + a_{n,t-1} x_{n,t} + \\log(1+ \\epsilon_t) \\right) \\\\ &amp; a_{i,t} = a_{i,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_i \\frac{\\log(1+\\epsilon_t)}{x_{i,t}} \\text{ for each } i \\in \\{1, \\dots, n\\}, \\text{ if } x_{i,t}\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\end{aligned} . \\tag{10.19} \\end{equation}\\] The formulation (10.19) differs from the conventional pure multiplicative ETS model because the smoothing parameter \\(\\delta_i\\) is not included inside the error term \\(1+\\epsilon_t\\), which simplifies some derivations and makes the model easier to work with. Mixed ETS models can also have explanatory variables, but I suggest aligning the type of explanatory variable model with the error term. Note that if it is suspected that the explanatory variables exhibit non-stationarity and are not cointegrated with the response variable, then their differences can be used instead of \\(x_{i,t}\\) in (10.18) and (10.19). In this case, the model would coincide with the one proposed by Osman and King (2015). The decision of taking the differences for the different parts of the model should be made based on each specific situation. Here is an example of the ETSX(A,N,N) model with differenced explanatory variables: \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = a_{0,t-1} + a_{1,t-1} \\Delta x_{1,t} + \\dots + a_{n,t-1} \\Delta x_{n,t} + \\epsilon_t , \\\\ &amp; a_{i,t} = a_{i,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_i \\frac{\\epsilon_t}{\\Delta x_{i,t}} \\text{ for each } i \\in \\{1, \\dots, n\\}, \\text{ if } \\Delta x_{i,t}\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. , \\end{aligned} \\tag{10.20} \\end{equation}\\] where \\(\\Delta x_{i,t} = x_{i,t} -x_{i,t-1}\\) is the differences of the \\(i\\)-th exogenous variable. Finally, to distinguish the ADAMX with static parameters from the ADAMX with dynamic ones, we will use the letters “S” and “D” in the names of models. So, the model (10.9) can be called ETSX(A,N,N){S}, while the model (10.19), assuming that \\(a_{0,t-1}=l_{t-1}\\), would be called ETSX(M,N,N){D}. We use curly brackets to split the ETS states from the type of X. Furthermore, given that the model with static regressors is assumed in many contexts to be the default one, the ETSX(*,*,*){S} model can also be denoted as just ETSX(*,*,*). 10.3.1 Recursion for dynamic ADAMX Similar to how it was discussed in Section 10.2.2, we can have two cases in the dynamic model: (1) deterministic explanatory variables, (2) stochastic explanatory variables. For illustrative purposes, we will use a non-seasonal model for which the lag vector \\(\\mathbf{l}\\) contains ones only, keeping in mind that other pure additive models can be easily used instead. The cases of non-additive ETS models are not discussed in this part in detail – the moments for these models need to be calculated based on simulations. So, as discussed previously, the model can be written in the following general way, assuming that all elements of \\(\\mathbf{l}\\) are equal to one: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = \\mathbf{w}&#39;_t \\mathbf{v}_{t-1} + \\epsilon_t \\\\ &amp; \\mathbf{v}_t = \\mathbf{F} \\mathbf{v}_{t-1} + \\mathbf{z}_t \\mathbf{g} \\epsilon_t \\end{aligned} . \\tag{10.21} \\end{equation}\\] Based on this model, we can get the recursive relation for \\(h\\) steps ahead, similar to how it was done in Section 5.2: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t+h} = \\mathbf{w}&#39;_{t+h} \\mathbf{v}_{t+h-1} + \\epsilon_{t+h} \\\\ &amp; \\mathbf{v}_{t+h-1} = \\mathbf{F} \\mathbf{v}_{t+h-2} + \\mathbf{z}_{t+h-1} \\mathbf{g} \\epsilon_{t+h-1} \\end{aligned} , \\tag{10.22} \\end{equation}\\] where the second equation can be represented based on the values available on observation \\(t\\): \\[\\begin{equation} \\mathbf{v}_{t+h-1} = \\mathbf{F}^{h-1} \\mathbf{v}_{t} + \\sum_{j=1}^{h-1} \\mathbf{F}^{h-1-j} \\mathbf{z}_{t+j} \\mathbf{g} \\epsilon_{t+j} . \\tag{10.23} \\end{equation}\\] Substituting the equation (10.23) in the measurement equation of (10.22) leads to the final recursion: \\[\\begin{equation} {y}_{t+h} = \\mathbf{w}&#39;_{t+h} \\mathbf{F}^{h-1} \\mathbf{v}_{t} + \\mathbf{w}&#39;_{t+h} \\sum_{j=1}^{h-1} \\mathbf{F}^{h-1-j} \\mathbf{z}_{t+j} \\mathbf{g} \\epsilon_{t+j} + \\epsilon_{t+h} . \\tag{10.24} \\end{equation}\\] 10.3.2 Conditional moments for deterministic explanatory variables in ADAMX{D} Based on this recursion, we can calculate the conditional mean and variance for the model. First, we assume that the explanatory variables are controlled by an analyst, and are known for \\(j=1, \\dots, h\\): \\[\\begin{equation} \\begin{aligned} \\mu_{y,t+h} = &amp; \\text{E}(y_{t+h}|t) = \\mathbf{w}&#39;_{t+h} \\mathbf{F}^{h-1} \\mathbf{v}_{t} \\\\ &amp; \\text{V}(y_{t+h}|t) = \\left(\\mathbf{w}&#39;_{t+h} \\sum_{j=1}^{h-1} \\mathbf{F}^{h-1-j} \\mathbf{z}_{t+j} \\mathbf{g} \\right)^2 \\sigma^2 + \\sigma^2 \\end{aligned} . \\tag{10.25} \\end{equation}\\] The formulae for conditional moments in this case look similar to the ones from the pure additive ETS model in Section 5.3 with only difference being the interaction with time varying measurument vector. 10.3.3 Conditional mean for stochastic explanatory variables in ADAMX{D} In the case of stochastic explanatory variables, the conditional expectation is straightforward and is similar to the one in the static ADAMX model: \\[\\begin{equation} \\mu_{y,t+h} = \\text{E}(y_{t+h}|t) = \\boldsymbol{\\mu}&#39;_{w,t+h} \\mathbf{F}^{h-1} \\mathbf{v}_{t} , \\tag{10.26} \\end{equation}\\] where \\(\\boldsymbol{\\mu}&#39;_{w,t+h}\\) is the vector of conditional h steps ahead expectations for each element in the \\(\\mathbf{w}_{t+h}\\). In the case of ETS components, the vector would contain ones. However, when it comes to conditional variance, it is more complicated because it introduces complex interactions between variances of different variables and the error term. As a result, it would be easier to get the correct variance based on simulations, assuming that the explanatory variables and the error term change according to some assumed models. References "],["stability-and-forecastability-conditions-of-adamx.html", "10.4 Stability and forecastability conditions of ADAMX", " 10.4 Stability and forecastability conditions of ADAMX It can be shown that any static ADAMX is not stable (as it was defined for pure additive ETS models in Section 5.4), meaning that the weights of such model do not decline to zero over time. To see this, we can draw an analogy with a deterministic model, discussed in the context of pure additive ETS (from Section 5.4). For example, we have already discussed that when \\(\\alpha=0\\), the ETS(A,N,N) model becomes equivalent to the global level, loses the stability condition, but still can be forecastable. It becomes a simple but still useful and efficient model: \\[\\begin{equation} y_{t} = a_0 + \\epsilon_t . \\tag{10.27} \\end{equation}\\] Similarly, the X part of ADAMX will always be unstable, but can be useful. For example, with \\(\\alpha=0\\), ETSX(A,N,N) reverts back to the linear regression: \\[\\begin{equation} y_{t} = a_0 + a_{1} x_{1,t} + \\dots + a_n x_{n,t} + \\epsilon_t . \\tag{10.28} \\end{equation}\\] This does not make the model inappropriate and only means that the weights do not decline over time. According to the conventional approach to ETS, if the dynamic part of the model is stable, but the overall model does not pass stability check just because of the X part, then the whole model will be considered unstable and potentially dangerous to use. But this is absurd. Following the same logic, we would need to avoid regression models in forecasting because they are not stable. Furthermore, there are no issues constructing ARIMAX models, but Hyndman et al. (2008) claim that there are some with ETSX, which does not make sense if we recall the connection between ETS and ARIMA (discussed in Section 8.4). This only means that the stability/forecastability conditions should be checked for the dynamic part of the model (ETS or ARIMA) separately, ignoring the X part. Technically, this implies creating a separate transition matrix, persistence and measurement vectors and calculating the discount matrix for the ETS / ARIMA part to check already discussed stability and forecastability conditions (Section 5.4). When it comes to the dynamic ADAMX, the situation changes because now the smoothing parameters for the model coefficients determine how weights decline over time. It can be shown based on (5.9) that the values of the state vector on the observation \\(t\\) can be calculated via the recursion (here we provide a formula for the non-seasonal case, keeping in mind that in case of the seasonal one, the derivation and the main message will be similar): \\[\\begin{equation} \\mathbf{v}_{t} = \\prod_{j=1}^{t-1}\\mathbf{D}_{t-j} \\mathbf{v}_{0} + \\sum_{j=0}^{t-1} \\prod_{i=0}^{j} \\mathbf{D}_{t-i} y_{t-j}, \\tag{10.29} \\end{equation}\\] where \\(\\mathbf{D}_t=\\mathbf{F} -\\mathbf{z}_t \\mathbf{g} \\mathbf{w}_{t}&#39;\\) is the time varying discount matrix. The main issue in the case of dynamic ADAMX is that the stability condition varies over time together with the values of explanatory variables in \\(\\mathbf{z}_t\\). So, it is not possible to derive the stability condition for the general case. In order to make sure that the model is stable, we need for all eigenvalues of each \\(\\mathbf{D}_{j}\\) for all \\(j=\\{1,\\dots,t\\}\\) to lie in the unit circle. Alternatively, we can introduce a new condition. We say that the model is stable on average if the eigenvalues of \\(\\mathbf{\\bar{D}}=\\frac{1}{t}\\sum_{j=1}^t\\mathbf{D}_j\\) all lie in the unit circle. This way, some of the observations might have a higher impact on the final value, but they will be cancelled out by those with much lower weights in the product in (10.29). This condition can be checked during the model estimation, similar to how the conventional stability condition is checked. As for the forecastability condition, for the ADAMX{D} it should be (based on the same logic as in Section 5.4): \\[\\begin{equation} \\lim\\limits_{t\\rightarrow\\infty}\\left(\\mathbf{w}&#39;_{t}\\prod_{j=1}^{t-1}\\mathbf{D}_{t-j} \\mathbf{v}_{0}\\right) = \\text{const} . \\tag{10.30} \\end{equation}\\] However, this condition will always be violated for the ADAMX models, just because the explanatory variables in \\(\\mathbf{w}_{t}\\) have their own variability and typically do not converge to a stable value with the increase of the sample size. So, if a forecastability condition needs to be checked for either ADAMX{D} or ADAMX{S}, we recommend checking it separately for the dynamic part of the model. References "],["ETSXDynamicCategories.html", "10.5 Dealing with categorical variables in ADAMX", " 10.5 Dealing with categorical variables in ADAMX When dealing with categorical variables in a regression context, they are typically expanded to a set of dummy variables (see Chapter 10 of Svetunkov, 2021). So, for example, a variable “promotions” that can be “light,” “medium” and “heavy” for different observations \\(t\\) would be expanded to three dummy variables, promoLight, promoMedium and promoHeavy, each one of which is equal to 1, when the respective promotion type happens and equal to zero otherwise. When including these variables in the model, we would typically drop one of them (which is sometimes called pivot variable) and have a model with two dummy variables of a type: \\[\\begin{equation} y_t = a_0 + a_1 x_{1,t} + \\dots + a_n x_{n,t} + d_1 promoLight_t + d_2 promoMedium_t + \\epsilon_t, \\tag{10.31} \\end{equation}\\] where \\(d_i\\) is the parameter for the \\(i\\)-th dummy variable. The same procedure can be done in the context of ADAMX, and the principles will be exactly the same for ADAMX{S}. However, when it comes to the dynamic model, the parameters have time indeces, and there can be different ways of formulating the model. Here is the first one: \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = a_{0,t-1} + a_{1,t-1} x_{1,t} + \\dots + a_{n,t-1} x_{n,t} + d_1 promoLight_t + d_2 promoMedium_t + \\epsilon_t \\\\ &amp; a_{i,t} = a_{i,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_i \\frac{\\log(1+\\epsilon_t)}{x_{i,t}} \\text{ for each } i \\in \\{1, \\dots, n\\}, \\text{ if } x_{i,t}\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\\\ &amp; d_{1,t} = d_{1,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_{n+1} \\epsilon_t, \\text{ if } promoLight_t\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\\\ &amp; d_{2,t} = d_{2,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_{n+2} \\epsilon_t, \\text{ if } promoMedium_t\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\end{aligned} . \\tag{10.32} \\end{equation}\\] Here we assume that each specific category of the variable promotion changes over time on its own with its own smoothing parameters \\(\\delta_{n+1}\\) and \\(\\delta_{n+2}\\). Alternatively, we can assume that they have the same smoothing parameters, implying that the changes of the parameters are similar throughout different categories of the variable: \\[\\begin{equation} \\begin{aligned} &amp; d_{1,t} = d_{1,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_{n+1} \\epsilon_t, \\text{ if } promoLight_t\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\\\ &amp; d_{2,t} = d_{2,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_{n+1} \\epsilon_t, \\text{ if } promoMedium_t\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\end{aligned} . \\tag{10.33} \\end{equation}\\] The rationale for such restriction is that we might expect the adaptation mechanism to apply to the promo variable as a whole, not to its specific values. This case also becomes useful in connecting the ETSX and the conventional seasonal ETS model. Let’s assume that we deal with quarterly data with no trend, and we have a categorical variable quarterOfYear, which can be First, Second, Third and Fourth, depending on the specific observation. For convenience, I will call the parameters for the dummy variables, created from this categorical variable \\(s_{1,t}, s_{2,t}, s_{3,t} \\text{ and } s_{4,t}\\). Based on (10.33), the model can then be formulated as: \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = l_{t-1} + s_{1,t} quarterOfYear_{1,t} + s_{2,t} quarterOfYear_{2,t} \\\\ &amp; + s_{3,t} quarterOfYear_{3,t} + s_{4,t} quarterOfYear_{4,t} + \\epsilon_t \\\\ &amp; l_t = l_{t-1} + \\alpha \\epsilon_t \\\\ &amp; s_{i,t} = s_{i,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta \\epsilon_t \\text{ for each } i \\in \\{1, \\dots, 4\\}, \\text{ if } quarterOfYear_{i,t}\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\end{aligned} . \\tag{10.34} \\end{equation}\\] We intentionally added all four dummy variables in (10.34) to separate the seasonal effect from the level component. While in regression and ETSX{S} contexts, this does not make much sense, in the ETSX{D} we avoid the trap of dummy variables due to the dynamic update of parameters. Having done that, we have just formulated the conventional ETS(A,N,A) model using a set of dummy variables and one smoothing parameter, the difference being that the latter relies on the lag of component: \\[\\begin{equation} \\begin{aligned} &amp; y_{t} = l_{t-1} + s_{t-4} + \\epsilon_t \\\\ &amp; l_t = l_{t-1} + \\alpha \\epsilon_t \\\\ &amp; s_t = s_{t-4} + \\gamma \\epsilon_t \\\\ \\end{aligned} . \\tag{10.35} \\end{equation}\\] So, this comparison shows on one hand that the mechanism of ADAMX{D} is natural for the ADAM model, and on the other hand that using the same smoothing parameters for categorical variables can be a reasonable idea, especially in cases when we do not have grounds to assume that each category of the variable should evolve independently. References "],["ETSXRExample.html", "10.6 Examples of application", " 10.6 Examples of application For this example, we will use the data of Road Casualties in Great Britain 1969–84, Seatbelts dataset in datasets package for R, which contains several variables (the description is provided in the documentation for the data and can be accessed via ?Seatbelts command). The variable of interest, in this case, is drivers, and the dataset contains more variables than needed, so we will restrict the data with drivers, kms (distance driven), PetrolPrice and law – the latter three seem to influence the number of injured/killed drivers in principle: SeatbeltsData &lt;- Seatbelts[,c(&quot;drivers&quot;,&quot;kms&quot;,&quot;PetrolPrice&quot;,&quot;law&quot;)] The dynamics of these variables over time is shown in Figure 10.1. Figure 10.1: The time series dynamics of variables from Seatbelts dataset. Apparently, the drivers variable exhibits seasonality but does not seem to have a trend. The type of seasonality is challenging to determine, but we will assume that it is multiplicative for now. A simple ETS(M,N,M) model applied to the data will produce the following (we will withhold the last 12 observations for the forecast evaluation, Figure 10.2): adamModelETSMNM &lt;- adam(SeatbeltsData[,&quot;drivers&quot;], &quot;MNM&quot;, h=12, holdout=TRUE) plot(forecast(adamModelETSMNM, h=12, interval=&quot;prediction&quot;)) Figure 10.2: The actual values for drivers and a forecast from ETS(M,N,M) model. This simple model already does a fine job fitting the data and producing forecasts. However, the forecast is biased and is lower than needed because of the sudden drop in the level of series, which can only be explained by the introduction of the new law in the UK in 1983, making the seatbelts compulsory for drivers. Due to the sudden drop, the smoothing parameter for the level of series is higher than needed, leading to wider intervals and less accurate forecasts. Here is the output of the model: adamModelETSMNM ## Time elapsed: 0.08 seconds ## Model estimated using adam() function: ETS(MNM) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 1125.509 ## Persistence vector g: ## alpha gamma ## 0.4133 0.0000 ## ## Sample size: 180 ## Number of estimated parameters: 15 ## Number of degrees of freedom: 165 ## Information criteria: ## AIC AICc BIC BICc ## 2281.019 2283.945 2328.913 2336.512 ## ## Forecast errors: ## ME: 117.9; MAE: 117.9; RMSE: 137.596 ## sCE: 83.695%; Asymmetry: 100%; sMAE: 6.975%; sMSE: 0.663% ## MASE: 0.684; RMSSE: 0.611; rMAE: 0.504; rRMSE: 0.542 In order to further explore the data we will produce the scatterplots and boxplots between the variables using spread() function from greybox package (Figure 10.3): spread(SeatbeltsData) Figure 10.3: The relation between variables from Seatbelts dataset Figure 10.3 shows a negative relation between kms and drivers: the higher the distance driven, the lower the total of car drivers killed or seriously injured. A similar relation is observed between the PetrolPrice and drivers (when the prices are high, people tend to drive less, thus causing fewer incidents). Interestingly, the increase of both variables causes the variance of the response variable to decrease (heteroscedasticity effect). Using a multiplicative error model and including the variables in logarithms, in this case, might address this potential issue. Note that we do not need to take the logarithm of drivers, as we already use the model with multiplicative error. Finally, the legislation of a new law seems to have caused a decrease in the number of causalities. To have a better model in terms of explanatory and predictive power, we should include all three variables. This is how we can do that using adam(): adamModelETSXMNM &lt;- adam(SeatbeltsData, &quot;MNM&quot;, h=12, holdout=TRUE, formula=drivers~log(kms)+log(PetrolPrice)+law) The parameter formula in general is not compulsory. It can either be substituted by formula=drivers~. or dropped completely – the function would fit the model of the first variable in the matrix from everything else. We need it in our case because we introduce log-transformations of some explanatory variables. plot(forecast(adamModelETSXMNM, h=12, interval=&quot;prediction&quot;)) Figure 10.4: The actual values for drivers and a forecast from ETSX(M,N,M) model. Figure 10.4 shows the forecast from the second model, which is slightly more accurate. More importantly, the prediction interval is narrower than in the simple ETS(M,N,M) because now the model takes the external information into account. Here is the summary of the second model: adamModelETSXMNM ## Time elapsed: 0.33 seconds ## Model estimated using adam() function: ETSX(MNM) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 1114.042 ## Persistence vector g (excluding xreg): ## alpha gamma ## 0.2042 0.0000 ## ## Sample size: 180 ## Number of estimated parameters: 18 ## Number of degrees of freedom: 162 ## Information criteria: ## AIC AICc BIC BICc ## 2264.085 2268.333 2321.558 2332.589 ## ## Forecast errors: ## ME: 96.28; MAE: 97.238; RMSE: 123.401 ## sCE: 68.347%; Asymmetry: 97.1%; sMAE: 5.752%; sMSE: 0.533% ## MASE: 0.564; RMSSE: 0.548; rMAE: 0.416; rRMSE: 0.486 Note that the smoothing parameters \\(\\alpha\\) has reduced from 0.41 to 0.2. This led to the reduction in error measures. For example, based on MASE, we can conclude that the model with explanatory variables is more precise than the simple univariate ETS(M,N,M). Still, we could try introducing the update of the parameters for the explanatory variables to see how it works (it might be unnecessary for this data): adamModelETSXMNMD &lt;- adam(SeatbeltsData, &quot;MNM&quot;, h=12, holdout=TRUE, formula=drivers~log(kms)+log(PetrolPrice)+law, regressors=&quot;adapt&quot;) In this specific case, the difference between the ETSX and ETSX{D} models is infinitesimal in terms of the accuracy of final forecasts and prediction intervals. Here is the output of the model: adamModelETSXMNMD ## Time elapsed: 0.37 seconds ## Model estimated using adam() function: ETSX(MNM){D} ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 1114.215 ## Persistence vector g (excluding xreg): ## alpha gamma ## 0.1758 0.0000 ## ## Sample size: 180 ## Number of estimated parameters: 21 ## Number of degrees of freedom: 159 ## Information criteria: ## AIC AICc BIC BICc ## 2270.430 2276.278 2337.482 2352.667 ## ## Forecast errors: ## ME: 98.539; MAE: 99.332; RMSE: 125.691 ## sCE: 69.951%; Asymmetry: 97.4%; sMAE: 5.876%; sMSE: 0.553% ## MASE: 0.576; RMSSE: 0.558; rMAE: 0.425; rRMSE: 0.495 We can spot that the error measures of the dynamic model are a bit higher than the ones from the static one (e.g., compare MASE and RMSSE of models). In addition, the information criteria are slightly lower for the static model. So based on all of this, we should probably use the static one for forecasting and analytical purposes. To see the effect of the explanatory variables on the number of incidents with drivers, we can look at the parameters for those variables: adamModelETSXMNM$initial$xreg ## log.kms. log.PetrolPrice. law ## -0.09554412 -0.30249416 -0.23809816 Based on that, we can conclude that the introduction of the law reduced on average the number of incidents by approximately 24%, while the increase of the petrol price by 1% leads on average to decrease in the number of incidents by 0.3%. Finally, the distance negatively impacts the incidents as well, reducing it on average by 0.1% for each 1% increase in the distance. This is the standard interpretation of parameters, which we can use based on the estimated model (see, for example, discussion in Section 8.3 of Svetunkov, 2021). We will discuss how to do the analysis using ADAM in Chapter 17, introducing the standard errors and confidence intervals for the parameters. Finally, adam() has some shortcuts when a matrix of variables is provided with no formula, assuming that the necessary expansion has already been done. This leads to the decrease in computational time of the function and becomes especially useful when working on large samples of data. Here is an example with ETSX(M,N,N): # Create matrix for the model SeatbeltsDataExpanded &lt;- ts(model.frame(drivers~log(kms)+log(PetrolPrice)+law, SeatbeltsData), start=start(SeatbeltsData), frequency=frequency(SeatbeltsData)) # Fix the names of variables colnames(SeatbeltsDataExpanded) &lt;- make.names(colnames(SeatbeltsDataExpanded)) # Apply the model adamModelETSXMNMExpanded &lt;- adam(SeatbeltsDataExpanded, &quot;MNM&quot;, lags=12, h=12, holdout=TRUE) References "],["ADAMETSEstimation.html", "Chapter 11 Estimation of ADAM models", " Chapter 11 Estimation of ADAM models Now that we have discussed the properties of ETS, ARIMA, ETSX and ARIMAX models, we need to understand how to estimate them. As mentioned earlier, when we apply a model to the data, we assume that it is suitable and see how it fits the data and produces forecasts. In this case, all the model parameters are substituted by the estimated ones (observed in the sample), and the error term becomes an estimate of the true one. In general, this means that the state space model (7.1) is substituted by: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp;w(\\hat{\\mathbf{v}}_{t-\\mathbf{l}}) + r(\\hat{\\mathbf{v}}_{t-\\mathbf{l}}) e_t \\\\ \\hat{\\mathbf{v}}_{t} = &amp;f(\\hat{\\mathbf{v}}_{t-\\mathbf{l}}) + \\hat{g}(\\hat{\\mathbf{v}}_{t-\\mathbf{l}}) e_t \\end{aligned}, \\tag{11.1} \\end{equation}\\] implying that the initial values of components and the smoothing parameters of the model are estimated. An example is the ETS(A,A,A) model applied to the data: \\[\\begin{equation} \\begin{aligned} y_{t} = &amp; \\hat{l}_{t-1} + \\hat{b}_{t-1} + \\hat{s}_{t-m} + e_t \\\\ \\hat{l}_t = &amp; \\hat{l}_{t-1} + \\hat{b}_{t-1} + \\hat{\\alpha} e_t \\\\ \\hat{b}_t = &amp; \\hat{b}_{t-1} + \\hat{\\beta} e_t \\\\ \\hat{s}_t = &amp; \\hat{s}_{t-m} + \\hat{\\gamma} e_t \\end{aligned}, \\tag{11.2} \\end{equation}\\] where the initial values \\(\\hat{l}_0, \\hat{b}_0\\) and \\(\\hat{s}_{-m+2}, ... \\hat{s}_0\\) are estimated and then influence all the future values of components via the recursion (11.2) and \\(e_t = y_t -\\hat{y}_t\\) is the one step ahead in sample forecast error, also known in statistics as the residual of the model. The estimation itself does not happen on its own, a complicated process of minimisation/maximisation of the pre-selected loss function by changing the values of parameters are involved. Typically, there is no analytical solution for estimates of ADAM parameters because of the model’s recursive nature. As a result, numerical optimisation is used to obtain the estimates of parameters. The results of the estimation will differ depending on: the assumed distribution, the used loss function, the initial values of parameters that are fed to the optimiser, the parameters of the optimiser (such as sensitivity, number of iterations etc.), the sample size, the number of parameters to estimate and, the restriction imposed on parameters. The aspects above are covered in this chapter. "],["ADAMETSEstimationLikelihood.html", "11.1 Maximum Likelihood Estimation", " 11.1 Maximum Likelihood Estimation This section relies on the approach explained in detail in Chapter 13 of Svetunkov (2021), so an interested reader is directed to that chapter. The maximum likelihood estimation (MLE) of the ADAM model relies on the distributional assumptions of each specific model. It might differ from one model to another (see Sections 5.5, 6.5 and 9.3). There are several options for the distribution supported by the adam() function in the smooth package, here we briefly discuss how the estimation is done for each one. We start with the additive error models, for which the assumptions, log-likelihoods and MLE of scales are provided in Table 11.1. The likelihoods are derived based on the probability density functions, discussed in Chapter 3 of Svetunkov (2021), by taking logarithms of the product of probability density functions for all in sample observations. For example, this is what we get for the normal distribution: \\[\\begin{equation} \\begin{aligned} \\mathcal{L}(\\boldsymbol{\\theta}, {\\sigma}^2 | \\mathbf{y}) = &amp; \\prod_{t=1}^T \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{\\left(y_t -\\mu_t \\right)^2}{2 \\sigma^2} \\right)\\right) \\\\ \\mathcal{L}(\\boldsymbol{\\theta}, {\\sigma}^2 | \\mathbf{y}) = &amp; \\frac{1}{\\left(\\sqrt{2 \\pi \\sigma^2}\\right)^T} \\exp \\left( \\sum_{t=1}^T -\\frac{\\epsilon_t^2}{2 \\sigma^2} \\right) \\\\ \\ell(\\boldsymbol{\\theta}, {\\sigma}^2 | \\mathbf{y}) = &amp; \\log \\mathcal{L}(\\boldsymbol{\\theta}, {\\sigma}^2 | \\mathbf{y}) \\\\ \\ell(\\boldsymbol{\\theta}, {\\sigma}^2 | \\mathbf{y}) = &amp; -\\frac{T}{2} \\log(2 \\pi \\sigma^2) -\\frac{1}{2} \\sum_{t=1}^T \\frac{\\epsilon_t^2}{\\sigma^2} \\end{aligned}, \\tag{11.3} \\end{equation}\\] where \\(\\mathbf{y}\\) is the vector of all in-sample actual values. As for the scale, it is obtained by solving the equation after taking derivative of the log-likelihood (11.3) with respect to \\(\\sigma^2\\) and setting it equal to zero. We do not discuss the concentrated log-likelihoods (obtained after inserting the estimated scale in the respective log-likelihood function), because they are not useful in understanding of how the model is estimated, but knowing how to calculate scale helps, because it simplifies the model estimation process. Table 11.1: Likelihood approach for additive error models Assumption log-likelihood MLE of scale \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\) \\(-\\frac{T}{2} \\log(2 \\pi \\sigma^2) -\\frac{1}{2} \\sum_{t=1}^T \\frac{\\epsilon_t^2}{\\sigma^2}\\) \\(\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T e_t^2\\) \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\) \\(-T \\log(2 s) -\\sum_{t=1}^T \\frac{|\\epsilon_t|}{s}\\) \\(\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T |e_t|\\) \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\) \\(-T \\log(4 s^2) -\\sum_{t=1}^T \\frac{\\sqrt{|\\epsilon_t|}}{s}\\) \\(\\hat{s} = \\frac{1}{2T} \\sum_{t=1}^T \\sqrt{|e_t|}\\) \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\) \\(\\begin{aligned} &amp;T\\log\\beta -T \\log(2 s \\Gamma\\left(\\beta^{-1}\\right)) -\\\\ &amp;\\sum_{t=1}^T \\frac{\\left|\\epsilon_t\\right|^\\beta}{s}\\end{aligned}\\) \\(\\hat{s} = \\sqrt[^{\\beta}]{\\frac{\\beta}{T} \\sum_{t=1}^T\\left| e_t \\right|^{\\beta}}\\) \\(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\sim \\mathcal{IG}(1, \\sigma^2)\\) \\(\\begin{aligned} &amp;-\\frac{T}{2} \\log \\left(2 \\pi \\sigma^2 \\right) -\\frac{1}{2} \\sum_{t=1}^T \\left( \\left(\\frac{y_t}{\\mu_{y,t}}\\right)^3 \\right) -\\\\ &amp;\\frac{3}{2}\\sum_{t=1}^T \\log y_t -\\frac{1}{2\\sigma^2} \\sum_{t=1}^{T} \\frac{\\epsilon_t^2}{\\mu_{y,t}y_t}\\end{aligned}\\) \\(\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{e_t^2}{\\hat{\\mu}_{y,t} y_t}\\) \\(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\sim \\mathcal{\\Gamma}(\\sigma^{-2}, \\sigma^2)\\) \\(\\begin{aligned} &amp;-T \\log \\Gamma \\left(\\sigma^{-2}\\right) -\\frac{T}{\\sigma^2} \\log \\sigma^2 + \\\\ &amp;\\frac{1}{\\sigma^2} \\sum_{t=1}^T \\log \\left(\\frac{y_t/\\mu_{y,t}}{ \\exp(y_t/\\mu_{y,t})}\\right) -\\sum_{t=1}^T \\log y_t\\end{aligned}\\) \\(\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T \\left(\\frac{e_t}{\\mu_{y,t}}\\right)^2\\) * \\(1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\) \\(\\begin{aligned} &amp;-\\frac{T}{2} \\log \\left(2 \\pi \\sigma^2\\right) -\\\\ &amp;\\frac{1}{2\\sigma^2} \\sum_{t=1}^{T} \\left(\\log \\left(\\frac{y_t}{\\mu_{y,t}}\\right)+\\frac{\\sigma^2}{2}\\right)^2 -\\\\ &amp;\\sum_{t=1}^T \\log y_t \\end{aligned}\\) \\(\\hat{\\sigma}^2 = 2\\left(1-\\sqrt{ 1-\\frac{1}{T} \\sum_{t=1}^{T} \\log^2\\left(\\frac{y_t}{\\hat{\\mu}_{y,t}}\\right)}\\right)\\) Note that the MLE of scale parameter for Gamma distribution (formulated in ADAM) does not have a closed-form. While there are no proofs for it, it seems that the maximum of the likelihood of Gamma distribution is achieved, when \\(\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T \\left(\\frac{e_t}{\\mu_{y,t}}\\right)^2\\), which corresponds to the estimate based on method of moments. Other distributions can be used in ADAM ETS (for example, Logistic distribution or Student’s t), but we do not discuss them here. Note that for the additive error models \\(y_t = \\mu_{y,t}+\\epsilon_t\\), thus some formulae in Table 11.1 are simplified. In all cases in Table 11.1, the assumptions imply that the actual value follows the same distribution but with a different location and/or scale. For example, for the Normal distribution, we have: \\[\\begin{equation} \\begin{aligned} &amp; \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2) \\\\ &amp; \\mathrm{or} \\\\ &amp; y_t = \\mu_{y,t}+\\epsilon_t \\sim \\mathcal{N}(\\mu_{y,t}, \\sigma^2) \\end{aligned}. \\tag{11.4} \\end{equation}\\] When it comes to the multiplicative error models, the likelihoods become slightly different. For example, when we assume that \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\) in the multiplicative error model, this implies that: \\[\\begin{equation} y_t = \\mu_{y,t}(1+\\epsilon_t) \\sim \\mathcal{N}(\\mu_{y,t}, \\mu_{y,t}^2 \\sigma^2) . \\tag{11.5} \\end{equation}\\] As a result, the log-likelihoods would have the \\(\\mu_{y,t}\\) part in the formulae. Similar logic is applicable to \\(\\mathcal{Laplace}\\), \\(\\mathcal{S}\\), \\(\\mathcal{GN}\\) and \\(\\mathcal{ALaplace}\\) distributions. From the practical point of view, these assumptions imply that the scale (and variance) of the distribution of \\(y_t\\) changes together with the level of the series. When it comes to the \\(\\mathcal{IG}\\), \\(\\Gamma\\) and log\\(\\mathcal{N}\\), the assumptions are imposed on the \\(1+\\epsilon_t\\) part of the model, the respective likelihoods do not involve the expectation \\(\\mu_{y,t}\\), but the formulation still implies that the variance of the data increases together with the increase of the level of data. All the likelihoods for the multiplicative error models are summarised in Table 11.2. Table 11.2: Likelihood approach for multiplicative error models Assumption log-likelihood MLE of scale \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\) \\(\\begin{aligned} &amp;-\\frac{T}{2} \\log(2 \\pi \\sigma^2) -\\frac{1}{2} \\sum_{t=1}^T \\frac{\\epsilon_t^2}{\\sigma^2} -\\\\ &amp;\\sum_{t=1}^T \\log |\\mu_{y,t}|\\end{aligned}\\) \\(\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T e_t^2\\) \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\) \\(\\begin{aligned} &amp;-T \\log(2 s) -\\sum_{t=1}^T \\frac{|\\epsilon_t|}{s} -\\\\ &amp;\\sum_{t=1}^T \\log \\mu_{y,t}\\end{aligned}\\) \\(\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T |e_t|\\) \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\) \\(\\begin{aligned} &amp;-T \\log(4 s^2) -\\sum_{t=1}^T \\frac{\\sqrt{|\\epsilon_t|}}{s} -\\\\ &amp;\\sum_{t=1}^T \\log \\mu_{y,t}\\end{aligned}\\) \\(\\hat{s} = \\frac{1}{2T} \\sum_{t=1}^T \\sqrt{|e_t|}\\) \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\) \\(\\begin{aligned} &amp;T\\log\\beta -T \\log(2 s \\Gamma\\left(\\beta^{-1}\\right)) -\\\\ &amp;\\sum_{t=1}^T \\frac{\\left|\\epsilon_t\\right|^\\beta}{s} -\\sum_{t=1}^T \\log \\mu_{y,t}\\end{aligned}\\) \\(\\hat{s} = \\sqrt[^{\\beta}]{\\frac{\\beta}{T} \\sum_{t=1}^T\\left| e_t \\right|^{\\beta}}\\) \\(1+\\epsilon_t \\sim \\mathcal{IG}(1, \\sigma^2)\\) \\(\\begin{aligned} &amp;-\\frac{T}{2} \\log \\left(2 \\pi \\sigma^2 \\right) -\\frac{1}{2}\\sum_{t=1}^T \\left(1+\\epsilon_{t}\\right)^3 -\\\\ &amp;\\frac{3}{2}\\sum_{t=1}^T \\log y_t -\\frac{1}{2\\sigma^2} \\sum_{t=1}^{T} \\frac{\\epsilon_t^2}{1+\\epsilon_t}\\end{aligned}\\) \\(\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{e_t^2}{1+e_t}\\) \\(1+\\epsilon_t \\sim \\mathcal{\\Gamma}(\\sigma^{-2}, \\sigma)\\) \\(\\begin{aligned} &amp;-T \\log \\Gamma \\left(\\sigma^{-2}\\right) -\\frac{T}{\\sigma^2} \\log \\sigma^2 + \\\\ &amp;\\frac{1}{\\sigma^2} \\sum_{t=1}^T \\log \\left(\\frac{1+\\epsilon_t}{\\exp(1+\\epsilon_t)}\\right) -\\sum_{t=1}^T \\log y_t\\end{aligned}\\) \\(\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T e_t^2\\) * \\(1+\\epsilon_t \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\) \\(\\begin{aligned} &amp;-\\frac{T}{2} \\log \\left(2 \\pi \\sigma^2\\right) -\\\\ &amp;\\frac{1}{2\\sigma^2} \\sum_{t=1}^{T} \\left(\\log \\left(1+\\epsilon_t\\right)+\\frac{\\sigma^2}{2}\\right)^2 -\\\\ &amp;\\sum_{t=1}^T \\log y_t \\end{aligned}\\) \\(\\hat{\\sigma}^2 = 2\\left(1-\\sqrt{ 1-\\frac{1}{T} \\sum_{t=1}^{T} \\log^2\\left(1+e_t\\right)}\\right)\\) When it comes to practicalities, the optimiser in adam() function calculates the scale from Tables 11.1 and 11.2 on each iteration and then uses it in the log-likelihood based on the respective distribution function. For additive error models: \\(\\mathcal{N}\\) – dnorm(x=actuals, mean=fitted, sd=scale, log=TRUE) from stats package; \\(\\mathcal{Laplace}\\) – dlaplace(q=actuals, mu=fitted, scale=scale, log=TRUE) from greybox package; \\(\\mathcal{S}\\) – ds(q=actuals, mu=fitted, scale=scale, log=TRUE) from greybox package; \\(\\mathcal{GN}\\) – dgnorm(x=actuals, mu=fitted, alpha=scale, beta=beta, log=TRUE) implemented in greybox package based on gnorm package (the version on CRAN is outdated); \\(\\mathcal{IG}\\) – dinvgauss(x=actuals, mean=fitted, dispersion=scale/fitted, log=TRUE) from statmod package; log\\(\\mathcal{N}\\) – dlnorm(x=actuals, meanlog=fitted-scale^2/2, sdlog=scale, log=TRUE) from stats package; \\(\\mathcal{Gamma}\\) – dgamma(x=actuals, shape=1/scale, scale=scale*fitted, log=TRUE) from stats package. and for multiplicative error models: \\(\\mathcal{N}\\) – dnorm(x=actuals, mean=fitted, sd=scale*fitted, log=TRUE); \\(\\mathcal{Laplace}\\) – dlaplace(q=actuals, mu=fitted, scale=scale*fitted, log=TRUE); \\(\\mathcal{S}\\) – ds(q=actuals, mu=fitted, scale=scale*sqrt(fitted), log=TRUE); \\(\\mathcal{GN}\\) – dgnorm(x=actuals, mu=fitted, alpha=scale*fitted^beta, beta=beta, log=TRUE); \\(\\mathcal{IG}\\) – dinvgauss(x=actuals, mean=fitted, dispersion=scale/fitted, log=TRUE); log\\(\\mathcal{N}\\) – dlnorm(x=actuals, meanlog=fitted-scale^2/2, sdlog=scale, log=TRUE); \\(\\mathcal{Gamma}\\) – dgamma(x=actuals, shape=1/scale, scale=scale*fitted, log=TRUE). Note that in cases of \\(\\mathcal{GN}\\), additional parameter (namely \\(\\beta\\)) is needed. If the user does not provide it, it will be estimated together with the other parameters via the maximisation of respective likelihoods. The MLE of parameters of ADAM ETS models makes them comparable with each other irrespective of the types of components and distributional assumptions. As a result, model selection based on information criteria can be made using the auto.adam() function from the smooth, which will select the most appropriate distribution for ADAM. 11.1.1 An example in R adam() function in smooth package has the parameter distribution, which allows selecting between several options discussed in this chapter, based on the respective density functions (see the list above). Here is a brief example in R with ADAM ETS(M,A,M) applied to the AirPassengers data with several distributions: adamModel &lt;- vector(&quot;list&quot;,5) adamModel[[1]] &lt;- adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE, distribution=&quot;dnorm&quot;) adamModel[[2]] &lt;- adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE, distribution=&quot;dlaplace&quot;) adamModel[[3]] &lt;- adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE, distribution=&quot;dgnorm&quot;) adamModel[[4]] &lt;- adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE, distribution=&quot;dinvgauss&quot;) adamModel[[5]] &lt;- adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE, distribution=&quot;dgamma&quot;) In this case, the function will select the most appropriate ETS model for each distribution. We can see what was selected in each case and compare the models using information criteria: setNames(sapply(adamModel, AICc), c(&quot;dnorm&quot;,&quot;dlaplace&quot;,&quot;dgnorm&quot;,&quot;dinvgauss&quot;,&quot;dgamma&quot;)) ## dnorm dlaplace dgnorm dinvgauss dgamma ## 971.5172 977.4446 980.0950 973.5000 973.9646 We could compare the performance of models in detail, but for demonstration, it should suffice to say that among the four models considered above, the model with the Normal distribution should be preferred based on the AICc value. This process of fit and selection can be automated using auto.adam() model, which accepts the vector of distributions to test and by default would consider distribution=c(\"default\", \"dnorm\", \"dlaplace\", \"ds\", \"dgnorm\", \"dlnorm\", \"dinvgauss\", \"dgamma\"): adamModel &lt;- auto.adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE) This command should return the ADAM ETS(M,A,M) model with the most appropriate distribution, selected based on the AICc. References "],["non-mle-based-loss-functions.html", "11.2 Non MLE-based loss functions", " 11.2 Non MLE-based loss functions An alternative approach for estimating ADAM ETS is using the conventional loss functions, such as MSE, MAE etc. In this case, the model selection using information criteria would not work, but this might not matter when you have already decided what model to use and want to improve it. In some special cases (as discussed in Chapter 3 of Svetunkov, 2021), the minimisation of these losses would give the same results as the maximisation of some likelihood functions: Minimum of MSE corresponds to the maximum likelihood of Normal distribution (see discussion in Kolassa, 2016); Minimum of MAE corresponds to the maximum likelihood of Laplace distribution (Schwertman et al., 1990); Minimum of pinball function (Koenker and Bassett, 1978, quantile regression) corresponds to the maximum of the likelihood of Asymmetric Laplace distribution (Yu and Zhang, 2005); The main difference between using these losses and maximising respective likelihoods is in the number of estimated parameters (see discussion in Section 13.3 of Svetunkov, 2021): the latter implies that the scale is estimated together with the other parameters, while the former does not consider it and in a way provides it for free. The assumed distribution does not necessarily depend on the used loss and vice versa. For example, we can assume that the actuals follow the Inverse Gaussian distribution but estimate the MAE model. The estimates of parameters might not be as efficient as in the case of MLE, but it is still possible to do. The error term, which is minimised in respective losses, depends on the error type: Additive error: \\(e_t = y_t -\\hat{\\mu}_{y,t}\\); Multiplicative error: \\(e_t = \\frac{y_t -\\hat{\\mu}_{y,t}}{\\hat{\\mu}_{y,t}}\\). This follows directly from the respective ETS models. 11.2.1 MSE and MAE MSE and MAE have been discussed in Section 2.1, but in the context of forecasts evaluation. If they are used for the estimation of a model, then the formulae (2.1) and (2.2) would need to be amended to: \\[\\begin{equation} \\mathrm{MSE} = \\sqrt{\\frac{1}{T} \\sum_{j=1}^T \\left( e_t \\right)^2 } \\tag{11.6} \\end{equation}\\] and \\[\\begin{equation} \\mathrm{MAE} = \\frac{1}{T} \\sum_{j=1}^T \\left| e_t \\right| , \\tag{11.7} \\end{equation}\\] where the specific formula for \\(e_t\\) would depend on the error type. The main difference between the two estimators is what they are minimised with: MSE is minimised by mean, while MAE is minimised by the median. This means that models estimated using MAE will typically be more conservative (i.e. in the case of ETS, have lower smoothing parameters). Gardner (2006) recommended using MAE in cases of outliers in the data, as the model becomes not as reactive as MSE in this situation. Note that in case of intermittent demand, MAE should be in general avoided due to the issues discussed in Section 2.1. 11.2.2 HAM Along with the discussed MSE and MAE, there is also HAM – “Half Absolute Moment”: \\[\\begin{equation} \\mathrm{HAM} = \\frac{1}{T} \\sum_{j=1}^T \\sqrt{\\left|e_t\\right|}, \\tag{11.8} \\end{equation}\\] the minimum of which corresponds to the maximum likelihood of S distribution (see Chapter 3 of Svetunkov, 2021). The idea of this estimator is to minimise the errors that happen very often, close to the location of the distribution. It will typically ignore the outliers and focus on the most frequently appearing values. As a result, if used for the integer values, the minimum of HAM would correspond to the mode of that distribution if it is unimodal. I do not have proof of this property, but it becomes apparent, given that the square root in (11.8) would reduce the influence of all values lying above one and increase the values of everything that lies between (0, 1) (e.g. \\(\\sqrt{0.16}=0.4\\), but \\(\\sqrt{16}=4\\)). Similar to HAM, one can calculate other fractional losses, which would be even less sensitive to outliers and more focused on the frequently appearing values, e.g. by using the \\(\\sqrt[^\\alpha]{\\left|e_t\\right|}\\) with \\(\\alpha&gt;1\\). This would then correspond to the maximum of Generalised Normal distribution with shape parameter \\(\\beta=\\frac{1}{\\alpha}\\). 11.2.3 LASSO and RIDGE While this is not a well studied approach yet, it is possible to use LASSO (Tibshirani, 1996) and RIDGE for the estimation of ETS models (James et al., 2017 give a good overview of these losses with examples in R), which is formulated in ADAM ETS as: \\[\\begin{equation} \\begin{aligned} \\mathrm{LASSO} = &amp;(1-\\lambda) \\sqrt{\\frac{1}{T} \\sum_{j=1}^T e_t^2} + \\lambda \\sum |\\hat{\\theta}| \\\\ \\mathrm{RIDGE} = &amp;(1-\\lambda) \\sqrt{\\frac{1}{T} \\sum_{j=1}^T e_t^2} + \\lambda \\sum \\hat{\\theta}^2, \\end{aligned} \\tag{11.9} \\end{equation}\\] where \\(\\theta\\) is the vector of all parameters in the model and \\(\\lambda\\) is the regularisation parameter. The idea of these losses is in the shrinkage of parameters. If \\(\\lambda=0\\), then the losses become equivalent to the MSE, when \\(\\lambda=1\\), the optimiser would minimise the values of parameters, ignoring the MSE part. In ETS, it makes sense to shrink everything but the initial level. For different components to shrink with a similar speed, they need to be normalised, so here are some tricks used in ADAM ETS: The smoothing parameters are left intact because they typically lie between 0 and 1, where 0 means that the respective states are not updated over time; Damping parameter is modified to shrink towards 1, enforcing no dampening of the trend via \\(\\hat{\\phi}^\\prime=1-\\hat{\\phi}\\); The initial level is normalised using the formula: \\(\\hat{l}_0&#39; = \\frac{\\hat{l}_0 -\\bar{y}_m}{\\hat{\\sigma}_{y,m}}\\), where \\(\\bar{y}_m\\) is the mean and \\(\\hat{\\sigma}_{y,m}\\) is the standard deviation of the first \\(m\\) actual observations, where \\(m\\) is the highest frequency of the data (if \\(m=1\\), then both values are taken based on the first two observations). Shrinking it to zero implies that it becomes equivalent to the mean of the first \\(m\\) observations; If the trend is additive, then the initial trend component is scaled using: \\(\\hat{b}_0&#39; = \\frac{\\hat{b}_0}{\\hat{\\sigma}_{y,m}}\\), making sure that it shrinks towards zero (no trend). In case of the multiplicative trend, this becomes: \\(\\hat{b}_0&#39; = \\log{\\hat{b}_0}\\) , making it shrink towards 1 in the original scale (again, no trend case). The multiplicative trend can be anything between 0 and \\(\\infty\\), corresponding to no trend. Taking the logarithm of it allows balancing out the cases, when \\(\\hat{b}_0&lt;1\\) with \\(\\hat{b}_0&gt;1\\); If the seasonal component is additive, then the normalisation similar to the one in trend is done for every seasonal index: \\(\\hat{s}_i&#39; = \\frac{\\hat{s}_i}{\\hat{\\sigma}_{y,m}}\\), making sure that they shrink towards zero. If the seasonal component is multiplicative, then the logarithm of components is taken: \\(\\hat{s}_i&#39; = \\log{\\hat{s}_i}\\), making sure that they shrink towards 1 in the original scale. If there are explanatory variables and the error term of the model is additive, then the respective parameters are divided by the standard deviations of the respective variables. In the case of the multiplicative error term, nothing is done because the parameters would typically be close to zero anyway (see a chapter on the ADAM ETSX). Finally, in order to make \\(\\lambda\\) slightly more meaningful, in case of additive error model, we also divide the MSE part of the loss by \\(\\mathrm{V}\\left({\\Delta}y_t\\right)\\), where \\({\\Delta}y_t=y_t-y_{t-1}\\). This sort of scaling helps in both cases: when there is a trend in the data and when the data does not exhibit one. We do not do anything for the multiplicative error models because, typically, the error is already quite small. These tricks ensure that different components shrink towards zero simultaneously, not breaking the model. If we would not do these steps, then, for example, the initial trend could dominate in \\(\\hat{\\theta}\\) and would shrink faster than the smoothing parameters. As a result, one can potentially use the model with trend and seasonality and use regularisation to shrink the unnecessary parameters towards zero. The adam() function does not select the most appropriate \\(\\lambda\\) and will set it equal to zero if the user does not provide it. Note that both LASSO and RIDGE are experimental options. If you have better ideas of how to implement them, send me a message, I will improve the mechanism in adam(). 11.2.4 Custom losses It is also possible to use other non-standard loss functions. adam() function allows doing that via the parameter loss. For example, we could estimate an ETS(A,A,N) model on the BJsales data using an absolute cubic loss (note that the parameters actual, fitted and B are compulsory for the function): lossFunction &lt;- function(actual, fitted, B){ return(mean(abs(actual-fitted)^3)); } adam(BJsales, &quot;AAN&quot;, loss=lossFunction, h=10, holdout=TRUE) where actual is the vector of actual values \\(y_t\\), fitted is the estimate of the one-step-ahead point forecast \\(\\hat{y}_t\\), and \\(B\\) is the vector of all estimated parameters, \\(\\hat{\\theta}\\). This way, one can use more advanced estimators, such as, for example, M-estimators (Barrow et al., 2020). 11.2.5 Examples in R adam() has two parameters, one regulating the assumed distribution, and another one, regulating how the model will be estimated, what loss will be used for these purposes. Here are examples with combinations of different losses and an assumed Inverse Gaussian distribution for ETS(M,A,M) on AirPassengers data. We start with MSE, MAE and HAM: adamModel &lt;- vector(&quot;list&quot;,6) names(adamModel) &lt;- c(&quot;likelihood&quot;, &quot;MSE&quot;, &quot;MAE&quot;, &quot;HAM&quot;, &quot;LASSO&quot;, &quot;Huber&quot;) adamModel[[1]] &lt;- adam(AirPassengers, &quot;MAM&quot;,h=12, holdout=TRUE, distribution=&quot;dinvgauss&quot;, loss=&quot;likelihood&quot;) adamModel[[2]] &lt;- adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE, distribution=&quot;dinvgauss&quot;, loss=&quot;MSE&quot;) adamModel[[3]] &lt;- adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE, distribution=&quot;dinvgauss&quot;, loss=&quot;MAE&quot;) adamModel[[4]] &lt;- adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE, distribution=&quot;dinvgauss&quot;, loss=&quot;HAM&quot;) In these three cases, the models assuming the same distribution for the error term are estimated using MSE, MAE and HAM. Their smoothing parameters should differ, with MSE producing fitted values closer to the mean, MAE – closer to the median and HAM – closer to the mode (but not exactly the mode) of the distribution. In addition, we introduce ADAM ETS(M,A,M) estimated using LASSO with arbitrarily selected \\(\\lambda=0.1\\): adamModel[[5]] &lt;- adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE, distribution=&quot;dinvgauss&quot;, loss=&quot;LASSO&quot;, lambda=0.1) And, finally, we estimate the same model using a custom loss, which in this case is Huber loss (Huber, 1992) with the threshold of 1.345: # Huber loss with a threshold of 1.345 lossFunction &lt;- function(actual, fitted, B){ errors &lt;- actual-fitted; return(sum(errors[errors&lt;=1.345]^2) + sum(abs(errors)[errors&gt;1.345])); } adamModel[[6]] &lt;- adam(AirPassengers, &quot;MAM&quot;, h=12, holdout=TRUE, distribution=&quot;dinvgauss&quot;, loss=lossFunction) Now we can compare the performance of the six models. First, we can compare the smoothing parameters: round(sapply(adamModel,&quot;[[&quot;,&quot;persistence&quot;),3) ## likelihood MSE MAE HAM LASSO Huber ## alpha 0.776 0.775 0.888 0.909 0.030 0.664 ## beta 0.000 0.000 0.019 0.017 0.028 0.001 ## gamma 0.000 0.000 0.000 0.008 0.092 0.001 What we observe in this case is that LASSO has the lowest smoothing parameters because they are shrunk directly in the model. Likelihood and MSE should probably give similar values. They both rely on squared errors, but not the same because the likelihood of Inverse Gaussian also has additional elements. Unfortunately, we do not have information criteria for models 2 – 6 in this case because the likelihood function is not maximised with these losses, so it’s not possible to compare them via the in-sample statistics. But we can compare their holdout sample performance: round(sapply(adamModel,&quot;[[&quot;,&quot;accuracy&quot;),3)[c(&quot;ME&quot;,&quot;MAE&quot;,&quot;MSE&quot;),] ## likelihood MSE MAE HAM LASSO Huber ## ME 12.021 9.347 5.048 5.356 -20.043 47.426 ## MAE 22.792 20.698 17.059 17.389 24.741 51.732 ## MSE 752.472 680.667 524.190 551.972 951.980 3552.266 And we can also produce forecasts and plot them for the visual inspection (Figure 11.1): adamModelForecasts &lt;- lapply(adamModel,forecast, h=12, interval=&quot;empirical&quot;) layout(matrix(c(1:6),3,2,byrow=TRUE)) for(i in 1:6){ plot(adamModelForecasts[[i]], main=paste0(&quot;ETS(MAM) estimated using &quot;,names(adamModel)[i])) } Figure 11.1: Forecasts from ETS(M,A,M) model estimated using different loss functions. What we observe in Figure 11.1 is that different losses led to different forecasts and prediction intervals (we used the empirical ones, discussed in Section 18.2.5). What can be done to make this practical is the rolling origin evaluation (Section 2.4) for different losses and then comparing forecast errors between them to select the most accurate one. References "],["multistepLosses.html", "11.3 Multistep losses", " 11.3 Multistep losses Another class of losses that can be used to estimate ADAM models is the multistep losses. The idea behind them is to produce the point forecast for \\(h\\) steps ahead from each observation in-sample and then calculate a measure based on that, which the optimiser will minimise to find the most suitable values of parameters. There is a lot of literature on this topic. Svetunkov et al. (2021) studied them in detail, showing that their usage implies shrinkage of smoothing parameters in ETS models. In this section, we will discuss the most popular multistep losses, see what they imply and make a connection between them and predictive likelihoods from the ADAM models. 11.3.1 \\(\\mathrm{MSE}_h\\) – MSE for h steps ahead One of the simplest estimators is the \\(\\mathrm{MSE}_h\\) – mean squared \\(h\\)-steps ahead error: \\[\\begin{equation} \\mathrm{MSE}_h = \\frac{1}{T-h} \\sum_{t=1}^{T-h} e_{t+h|t}^2 , \\tag{11.10} \\end{equation}\\] where \\(e_{t+h|t}\\) is the conditional \\(h\\) steps ahead forecast error on the observation \\(t+h\\) produced from the point at time \\(t\\). In case of additive error model, it will be calculated as \\(e_{t+h|t}=y_{t+h}-\\hat{y}_{t+h}\\), while in the case of the multiplicative one it is \\(e_{t+h|t}=\\frac{y_{t+h}-\\hat{y}_{t+h}}{\\hat{y}_{t+h}}\\). This estimator is sometimes used to fit a model several times, for each horizon from 1 to \\(h\\) steps ahead, resulting in \\(h\\) different values of parameters for each \\(j=1, \\ldots, h\\). The estimation process, in this case, becomes at least \\(h\\) times more complicated than estimating one model but is reported to result in increased accuracy (see for example Kourentzes et al., 2019b). Svetunkov et al. (2021) show that MSE\\(_h\\) is proportional to the h steps ahead forecast error variance \\(V(y_{t+h}|t)=\\sigma^2_{h}\\), which implies that the minimisation of (11.10) leads to the minimisation of the variance and in turn to the minimisation of both one step ahead MSE and a combination of smoothing parameters of a model. This becomes more obvious in the case of pure additive ETS (Section 5.1), where the analytical formulae for variance(from Section 5.3) are available. In the case of ETS, the parameters are shrunk towards zero, making the model deterministic. The effect is softened on large samples when the ratio \\(\\frac{T-h}{T-1}\\) becomes close to one. In the case of ARIMA, the shrinkage mechanism is similar, making models closer to the deterministic ones. However, shrinkage direction is more complicated and differs from one model to another. The shrinkage strength is proportional to the forecast horizon \\(h\\) and is weakened with the increase of the sample size. Svetunkov et al. (2021) demonstrate that the minimum of MSE\\(_h\\) corresponds to the maximum of the predictive likelihood based on the normal distribution, assuming that \\(\\epsilon_t \\sim N(0,\\sigma^2)\\). The log-likelihood in this case is: \\[\\begin{equation} \\ell_{\\mathrm{MSE}_h}(\\boldsymbol{\\theta}, {\\sigma^2_h} | \\mathbf{y}) = -\\frac{T-h}{2} \\left( \\log(2 \\pi) + \\log \\sigma^2_h \\right) -\\frac{1}{2} \\sum_{t=1}^{T-h} \\left( \\frac{\\eta_{t+h|}^2}{\\sigma^2_h} \\right) , \\tag{11.11} \\end{equation}\\] where \\(\\eta_{t+h|} \\sim N(0, \\sigma_h^2)\\) is the h steps ahead forecast error, conditional on the information available at time \\(t\\), \\(\\boldsymbol{\\theta}\\) is the vector of all estimated parameters of the model and \\(\\mathbf{y}\\) is the vector of \\(y_{t+h}\\) for all \\(t=1,..,T-h\\). The MLE of the scale parameter in (11.11) coincides with the MSE\\(_h\\): \\[\\begin{equation} \\hat{\\sigma}_h = \\frac{1}{T-h} \\sum_{t=1}^{T-h} e_{t+h|t}^2 , \\tag{11.12} \\end{equation}\\] where \\(e_{t+h|t}\\) is the in sample estimate of the \\(\\eta_{t+h}\\). The formula (11.11) can be used for the calculation of information criteria and in turn for the model selection in cases, when MSE\\(_h\\) is used for the model estimation. Svetunkov et al. (2021) demonstrate that (11.10) is more efficient (see Section 4.3 of Svetunkov, 2021) than the conventional MSE\\(_1\\) one when the true smoothing parameters are close to zero and is less efficient otherwise. On smaller samples, MSE\\(_h\\) produces biased estimates of parameters due to shrinkage. This can still be considered an advantage if you are interested in forecasting and do not want the smoothing parameters to vary. 11.3.2 TMSE – Trace MSE An alternative to MSE\\(_h\\) is to produce 1 to \\(h\\) steps ahead forecasts and calculate the respective forecast errors. Then, based on that, we can calculate the overall measure, which we will call “Trace MSE”: \\[\\begin{equation} \\mathrm{TMSE} = \\sum_{j=1}^h \\frac{1}{T-h} \\sum_{t=1}^{T-h} e_{t+j|t}^2 = \\sum_{j=1}^h \\mathrm{MSE}_j. \\tag{11.13} \\end{equation}\\] The benefit of this estimator is in minimising the error for the whole 1 to \\(h\\) steps ahead in one model – there is no need to construct \\(h\\) models, minimising MSE\\(_j\\) for \\(j=1,...,h\\). However, this comes with a cost: typically, short-term forecast errors have lower MSE than the longer-term ones, so if we sum their squares up, we are mixing different values, and the minimisation will be done mainly for the ones on the longer horizons. TMSE does not have a related predictive likelihood, so it is difficult to study its properties. Still, the simulations show that it tends to produce less biased and more efficient estimates of parameters than MSE\\(_h\\). Kourentzes et al. (2019c) showed that TMSE performs well compared to the conventional MSE\\(_1\\) and MSE\\(_h\\) in terms of forecasting accuracy and does not take as much time as the estimation of \\(h\\) models using MSE\\(_h\\). 11.3.3 GTMSE – Geometric Trace MSE An estimator that addresses the issues of TMSE, is the GTMSE, which is derived from a so called General Predictive Likelihood (GPL by Clements and Hendry, 1998; Svetunkov et al., 2021). The word “Geometric” sums up how the value is calculated: \\[\\begin{equation} \\mathrm{GTMSE} = \\sum_{j=1}^h \\log \\left(\\frac{1}{T-h} \\sum_{t=1}^{T-h} e_{t+j|t}^2 \\right) = \\sum_{j=1}^h \\log \\mathrm{MSE}_j. \\tag{11.14} \\end{equation}\\] Logarithms in the formula (11.14) bring the MSEs on different horizons to the same level so that both short term and long term errors are minimised with similar power. As a result, the shrinkage effect in this estimator is milder than in MSE\\(_h\\) and TMSE, and the estimates of parameters are less biased and more efficient on smaller samples. It still has the benefits of other multistep estimators, shrinking the parameters towards zero. Although it is possible to derive a predictive likelihood that would be maximised, when GTMSE is minimised, it relies on unrealistic assumptions of independence of multistep forecast errors (they are always correlated as long as smoothing parameters are not zero Svetunkov et al., 2021). 11.3.4 MSCE – Mean Squared Cumulative Error This estimator aligns the loss function with a specific inventory decision: ordering based on the lead time \\(h\\): \\[\\begin{equation} \\mathrm{MSCE} = \\frac{1}{T-h} \\sum_{t=1}^{T-h} \\left( \\sum_{j=1}^h e_{t+j|t} \\right)^2 . \\tag{11.15} \\end{equation}\\] Kourentzes et al. (2019b) demonstrated that it produces more accurate forecasts in cases of intermittent demand and leads to fewer revenue losses. Svetunkov et al. (2021) show that the shrinkage effect is much stronger in this estimator than in the other estimators discussed in this section. In addition, it is possible to derive a predictive log-likelihood related to this estimator: \\[\\begin{equation} \\ell_{\\mathrm{MSCE}}(\\theta, {\\varsigma^2_h} | \\mathbf{Y}_c) = -\\frac{T-h}{2} \\left( \\log(2 \\pi) + \\log {\\varsigma^2_h} \\right) -\\frac{1}{2} \\sum_{t=1}^{T-h} \\left( \\frac{\\left(\\sum_{j=1}^h \\eta_{t+j|t}\\right)^2}{2 {\\varsigma^2_h}} \\right) , \\tag{11.16} \\end{equation}\\] where \\(\\mathbf{y}_c\\) is the cumulative sum of actual values, the vector of \\(y_{c,t}=\\sum_{j=1}^h y_{t+j}\\) for all \\(t=1, \\ldots, T-h\\) and \\({\\varsigma^2_h}\\) is the variance of the cumulative error term, the MLE of which is equal to (11.15). Having the likelihood (11.16), permits the model selection and combination using information criteria (Section 13.4 Svetunkov, 2021) and also means that the parameters estimated using MSCE will be asymptotically consistent and efficient. 11.3.5 GPL – General Predictive Likelihood Finally, Svetunkov et al. (2021) studied the General Predictive Likelihood for normally distributed variable from Clements and Hendry (1998), p.77, logarithm version of which can be written as: \\[\\begin{equation} \\ell_{\\mathrm{GPL}_h}(\\theta, \\boldsymbol{\\Sigma} | \\mathbf{Y}) = -\\frac{T-h}{2} \\left( h \\log(2 \\pi) + \\log | \\mathbf{\\Sigma}| \\right) -\\frac{1}{2} \\sum_{t=1}^T \\left( \\mathbf{E_t^\\prime} \\mathbf{\\Sigma}^{-1} \\mathbf{E_t} \\right) , \\tag{11.17} \\end{equation}\\] where \\(\\boldsymbol{\\Sigma}\\) is the conditional covariance matrix for the vector of variables \\(\\mathbf{y}_t=\\begin{pmatrix} y_{t+1|t} &amp; y_{t+2|t} &amp; \\ldots &amp; y_{t+h|t} \\end{pmatrix}\\), \\(\\mathbf{Y}\\) is the matrix consisting of \\(\\mathbf{y}_t\\) for all \\(t=1, \\ldots, T-h\\) and \\(\\mathbf{E_t}^{\\prime} = \\begin{pmatrix} \\eta_{t+1|t} &amp; \\eta_{t+2|t} &amp; \\ldots &amp; \\eta_{t+h|t} \\end{pmatrix}\\) is the vector of 1 to \\(h\\) steps ahead forecast errors. Svetunkov et al. (2021) showed that the maximisation of the likelihood (11.17) is equivalent to minimising the generalised variance of the error term, \\(|\\hat{\\boldsymbol{\\Sigma}}|\\), where: \\[\\begin{equation} \\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{T-h} \\sum_{t=1}^{T-h} \\mathbf{E_t} \\mathbf{E_t^\\prime} = \\begin{pmatrix} \\hat{\\sigma}_1^2 &amp; \\hat{\\sigma}_{1,2} &amp; \\dots &amp; \\hat{\\sigma}_{1,h} \\\\ \\hat{\\sigma}_{1,2} &amp; \\hat{\\sigma}_2^2 &amp; \\dots &amp; \\hat{\\sigma}_{2,h} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\hat{\\sigma}_{1,h} &amp; \\hat{\\sigma}_{2,h} &amp; \\dots &amp; \\hat{\\sigma}_h^2 \\end{pmatrix} , \\tag{11.18} \\end{equation}\\] where \\(\\hat{\\sigma}_{i,j}\\) is the covariance between \\(i\\)-th and \\(j\\)-th steps ahead forecast errors. Svetunkov et al. (2021) show that this estimator encompassess all the other estimators discussed in this section: minimising MSE\\(_h\\) is equivalent to minimising the \\(\\hat{\\sigma}^2_{h}\\); minimising TMSE is equivalent to minimising the trace of the matrix \\(\\hat{\\boldsymbol{\\Sigma}}\\); minimising GTMSE is the same as minimising the determinant of \\(\\hat{\\boldsymbol{\\Sigma}}\\) but with the restriction that all off-diagonal elements are equal to zero; minimising MSCE produces the same results as minimising the sum of all elements in \\(\\hat{\\boldsymbol{\\Sigma}}\\). However, maximum of GPL is equivalent to the maximum of the conventional one step ahead likelihood for a normal model in case, when all the basic assumptions (discussed in Subsection 1.4.1) hold. In other cases, they would be different, but it is still not clear, whether the difference would be favouring the conventional likelihood of the GPL. Nonetheless, GPL, being the likelihood, guarantees that the estimates of parameters will be efficient and consistent and permits model selection and combination via information criteria. When it comes to models with multiplicative error term, the formula of GPL (11.17) will need to be amended by analogy with the log-likelihood of normal distribution in the same situation (Table 11.2): \\[\\begin{equation} \\begin{aligned} \\ell_{\\mathrm{GPL}_h}(\\theta, \\boldsymbol{\\Sigma} | \\mathbf{Y}) = &amp; -\\frac{T-h}{2} \\left( h \\log(2 \\pi) + \\log | \\boldsymbol{\\Sigma}| \\right) -\\frac{1}{2} \\sum_{t=1}^T \\left( \\mathbf{E_t^\\prime} \\boldsymbol{\\Sigma}^{-1} \\mathbf{E_t} \\right) \\\\ &amp; -\\sum_{t=1}^{T-h} \\sum_{j=1}^h \\log |\\mu_{y,t+j|t}|, \\end{aligned} \\tag{11.19} \\end{equation}\\] where the term \\(\\sum_{t=1}^{T-h} \\sum_{j=1}^h \\log |\\mu_{y,t+j|t}|\\) appears because we assume that the actual value \\(h\\) steps ahead follows multivariate normal distribution with the conditional expectation \\(\\mu_{y,t+j|t}\\). Note that this is only a very crude approximation, as the conditional distribution for \\(h\\) steps ahead is not defined for multiplicative error models. So, when dealing with GPL, it is recommended to use pure additive models only. 11.3.6 Other multistep estimators It is also possible to derive multistep estimators based on MAE, HAM and other error measures. adam() unofficially supports the following multistep losses by analogy with MSE\\(_h\\), TMSE and MSCE discussed in this section: MAE\\(_h\\); TMAE; MACE; HAM\\(_h\\); THAM; CHAM. When calculating likelihoods based on these losses, adam() will assume Laplace distribution for (1) – (3) and S distribution for (4) – (6) if the user does not specify the distribution parameter. 11.3.7 An example in R In order to see how different estimators perform, we will us Box-Jenkins Sales data, ETS(A,A,N) model and \\(h=10\\): adamModel &lt;- vector(&quot;list&quot;,6) names(adamModel) &lt;- c(&quot;MSE&quot;,&quot;MSEh&quot;,&quot;TMSE&quot;,&quot;GTMSE&quot;,&quot;MSCE&quot;,&quot;GPL&quot;) for(i in 1:length(adamModel)){ adamModel[[i]] &lt;- adam(BJsales, &quot;AAN&quot;, h=10, holdout=TRUE, loss=names(adamModel)[i]) } We can compare the smoothing parameters of these models to see how the shrinkage effect worked in different estimators: round(sapply(adamModel,&quot;[[&quot;,&quot;persistence&quot;),5) ## MSE MSEh TMSE GTMSE MSCE GPL ## alpha 1.00000 1 1 1.00000 1 1 ## beta 0.23915 0 0 0.14617 0 0 The table above shows that \\(\\beta\\) is close to zero for the estimators that impose harder shrinkage on parameters: MSE\\(_h\\), TMSE and MSCE. MSE does not shrink the parameters, while GTMSE has a mild shrinkage effect. While the models estimated using these losses are in general not comparable in-sample (although MSE, MSE\\(_h\\), MSCE and GPL could be compared via information criteria if they are scaled appropriately), they are comparable on the holdout: round(sapply(adamModel,&quot;[[&quot;,&quot;accuracy&quot;),5)[c(&quot;ME&quot;,&quot;MAE&quot;,&quot;MSE&quot;),] ## MSE MSEh TMSE GTMSE MSCE GPL ## ME 3.22900 1.06479 1.05233 3.44962 1.04604 0.95515 ## MAE 3.34075 1.41264 1.40153 3.53730 1.39593 1.31495 ## MSE 14.41862 2.89067 2.85880 16.26344 2.84288 2.62394 In this case, ETS(A,A,N) estimated using GPL produced a more accurate forecast than the other estimators. Repeating the experiment on many samples and selecting the approach that produces more accurate forecasts would allow choosing the most appropriate approach for the specific model on specific data. References "],["ADAMInitialisation.html", "11.4 Initialisation of ADAM", " 11.4 Initialisation of ADAM To construct a model, we need to initialise it, defining the values of \\(\\mathbf{v}_{-m+1}, \\dots, \\mathbf{v}_0\\) – initial states of the model. There are different ways of doing that, but here we only discuss the following three: Optimisation of initials, Backcasting, Provided values. The first option implies that the values of initial states are found in the same procedure as the other parameters of the model. (2) means that the initials are refined iteratively when the model is fit to the data from observation \\(t=1\\) to \\(t=T\\) and backwards. Finally, (3) is when a user knows initials and provides them to the model. As a side note, we assume in ADAM that the model is initialised at the moment just before \\(t=1\\). We do not believe that it was initialised at some point before the Big Bang (as ARIMA typically does), and we do not initialise it at the start of the sample. This way, we make all models in ADAM comparable, making them work on the same sample, no matter how many differences are taken or how many seasonal components they contain. 11.4.1 Optimisation vs backcasting In the case of optimisation, all the model parameters are estimated together. This includes (depending on the type of model): Smoothing parameters of ETS, Smoothing parameters for the regression part of the model (from Section 10.3), Dampening parameter of ETS, Parameters of ARIMA: both AR(p) and MA(q), Initials of ETS, Initials of ARIMA, Initial values for parameters of explanatory variables, Constant / drift for ARIMA, Other additional parameters are needed by assumed distributions. The more complex the selected model is, the more parameters we will need to estimate, and all of this will happen in one and the same iterative process in the optimiser: Choose parameters, Fit the model, Calculate loss function, Compare the loss with the previous one, Update the parameters based on (4), Go to (2) and repeat until a specific criterion is met. The user can specify the stopping criteria. There are several options accepted by the optimiser of adam(): Maximum number of iterations (maxeval), which by default is equal to \\(40\\times k\\) in case of ETS / ARIMA and \\(100 \\times k\\) for the model with explanatory variables, where \\(k\\) is the number of all estimated parameters; The relative precision of the optimiser (xtol_rel) with the default value of \\(10^{-6}\\), which regulates the relative change of parameters; The absolute precision of the optimiser (xtol_abs) with the default value of \\(10^{-8}\\), which regulates the absolute change of parameters; The stopping criterion in case of the relative change in the loss function (ftol_rel) with the default value of \\(10^{-8}\\); All these parameters are explained in more detail in the documentation of the nloptr() function from the nloptr package for R (Steven G., 2021), which handles the estimation of ADAM. adam() accepts several other stopping criteria, which can be found in the documentation of the function. The mechanism explained above can become quite complicated if a complex model is constructed and might take a lot of time and manual tuning of parameters to get to the optimum. In some cases, reducing the number of estimated parameters is worth considering, and one way of doing so is backcasting. In case of backcasting we do not need to estimate initials of ETS, ARIMA and regression. What model does in this case is goes through the series from \\(t=1\\) to \\(t=T\\), fitting to the data and then reverses and goes back from \\(t=T\\) to \\(t=1\\) based on the following state space model: \\[\\begin{equation} \\begin{aligned} {y}_{t} = &amp;w(\\mathbf{v}_{t+\\boldsymbol{l}}) + r(\\mathbf{v}_{t+\\mathbf{l}}) \\epsilon_t \\\\ \\mathbf{v}_{t} = &amp;f(\\mathbf{v}_{t+\\mathbf{l}}) + g(\\mathbf{v}_{t+\\mathbf{l}}) \\epsilon_t \\end{aligned}. \\tag{11.20} \\end{equation}\\] The new values of \\(\\mathbf{v}_t\\) for \\(t&lt;1\\) are then used to fit the model to the data again. The procedure can be repeated several times for the initial states to converge to more reasonable values. The backcasting procedure implies the extended fitting process for the model, removing the need to estimate all the initials. It works exceptionally well on large samples of data (thousands of observations) and with models with several seasonal components. The bigger your model is, the more time the optimisation will take, and the more likely backcasting would do better. On the other hand, you might also prefer backcasting to optimisation in small samples when you do not have more than two seasons of data – estimation of initial seasonal components might become challenging and can lead to overfitting. When discussing specific models, ADAM ARIMA works better (faster and more accurate) with backcasting than with optimisation because it does not need to estimate as many parameters as in the latter case. On the other hand, ADAM ETS typically works quite well in case of optimisation, when there is enough data to train the model on. Last but not least, if you introduce explanatory variables, then optimising the initial states might be a better option than backcasting unless you use a dynamic ETSX / ARIMAX (Section 10.3) because otherwise, the initial parameters for the explanatory variables will not be updated. It is also important to note that the information criteria of models with backcasting are typically lower than in the case of the optimised initials. This is because the difference in the number of estimated parameters is substantial in these two cases, and the models are initialised differently. So, it is advised not to mix the model selection between the two initialisation techniques. Nonetheless, no matter what initialisation method is used, we need to start the fitting process from \\(t=1\\). This cannot be done unless we provide some pre-initialised values of parameters to the optimiser. The better we guess the initial values, the faster the optimiser will converge to the optimum. adam() uses several heuristics in this stage, explained in more detail in the following subsections. 11.4.2 Pre-initialisation of ADAM parameters In this subsection, we discuss how the values of smoothing parameters, damping parameters and coefficients of ARIMA are preset before the initialisation. All the things discussed here are heuristics, developed based on my experience and many experiments with ADAM. Depending on the model type, the vector of estimated parameters will have different lengths. We start with smoothing parameters of ETS: For the unsafe mixed models ETS(A,A,M), ETS(A,M,A), ETS(M,A,A) and ETS(M,A,M): \\(\\hat{\\alpha}=0.01\\), \\(\\hat{\\beta}=0\\) and \\(\\hat{\\gamma}=0\\). This is needed because the models listed above are susceptible to the changes in smoothing parameters and might fail for time series with actual values close to zero; For the one of the most complicated and sensitive models ETS(M,M,A) \\(\\hat{\\alpha}=\\hat{\\beta}=\\hat{\\gamma}=0\\). The combination of additive seasonality and the multiplicative trend is the most difficult one. The multiplicative error makes estimation even more challenging in cases of low-level data. So starting from the deterministic model, that will work for sure is a safe option; ETS(M,A,N) is slightly easier to estimate than ETS(M,A,M) and ETS(M,A,A), so \\(\\hat{\\alpha}=0.2\\), \\(\\hat{\\beta}=0.01\\). The low value for the trend is needed to avoid the difficult situations with low level data, when the fitted values become negative; ETS(M,M,N) and ETS(M,M,M) have \\(\\hat{\\alpha}=0.1\\), \\(\\hat{\\beta}=0.05\\) and \\(\\hat{\\gamma}=0.01\\), making the trend and seasonal components a bit more conservative. The high values are typically not needed in this model as they might lead to explosive behaviour; Other models with multiplicative components (ETS(M,N,N), ETS(M,N,A), ETS(M,N,M), ETS(A,N,M), ETS(A,M,N) and ETS(A,M,M)) are slightly easier to estimate and harder to break, so their parameters are set to \\(\\hat{\\alpha}=0.1\\), \\(\\hat{\\beta}=0.05\\) and \\(\\hat{\\gamma}=0.05\\); Finally, pure additive models are initialised with \\(\\hat{\\alpha}=0.1\\), \\(\\hat{\\beta}=0.05\\) and \\(\\hat{\\gamma}=0.11\\). Their parameter space is the widest, and the models do not break any data. The smoothing parameter for the explanatory variables (Section 10.3) is set to \\(\\hat{\\delta}=0.01\\) in case of additive error and \\(\\hat{\\delta}=0\\) in case of the multiplicative one. The latter is done because the model might break if some ETS components are additive. If a dampening parameter is needed in the model, then its pre-initialised value is \\(\\hat{\\phi}=0.95\\). In the case of ARIMA, the parameters are pre-initialised based on ACF and PACF (Sections 8.3.2 and 8.3.3). First, the in-sample actual values are differenced, according to the selected order \\(D_j\\) for all \\(j=0,\\dots,n\\), after which the ACF and PACF are calculated. Then the initials for AR parameters are taken from the PACF, while the initials for MA parameters are taken from ACF, making sure that the sum of parameters is not greater than one in both cases. If it is, then the parameters are renormalised to satisfy the condition. This mechanism aims to get a potentially correct direction towards the optimal parameters of the model and make sure that the initial values meet the basic stationarity and invertibility conditions. In cases when it is not possible to calculate ACF and PACF for the specified lags and orders, AR parameters are set to -0.1, while the MA parameters are set to 0.1, making sure that the conditions mentioned above hold. In the case of Generalised Normal distribution, the shape parameter is set to 2 (if it is estimated), making the optimiser start from the conventional Normal distribution. If the skewness parameter of Asymmetric Laplace distribution is not provided, then its pre-initial value is set to 0.5, corresponding to the median of the data. The pre-initialisations described above guarantee that the model is estimable for a wide variety of time series and that the optimiser will reach the optimum in a limited time. If it does not work for a specific case, a user can provide their vector of pre-initialised parameters via the parameter B in the ellipsis of the model. Furthermore, the typical bounds for the parameters can be tuned as well. For example, the bounds for smoothing parameters in ADAM ETS are (-5, 5), and they are needed only to simplify the optimisation procedure. The function will check the violation of either usual or admissible bounds inside the optimiser, but having some ideas of where to search for optimal parameters, helps. A user can provide their vector for the lower bound via lb and the upper one via ub. 11.4.3 Pre-initialisation of ADAM states, ETS After defining the pre-initial parameters, we need to provide similar values for the initial state vector \\(\\mathbf{v}_t\\). The steps explained below are based on my experience and typically lead to a robust model. The pre-initialisation of states of ADAM ETS differs depending on whether the model is seasonal or not. If it is seasonal, then the multiple seasonal decomposition is done using the msdecompose() function from the smooth with the seasonality set to “multiplicative” if either error or seasonal component of ETS is multiplicative. After that: Initial level is equal to the first initial value from the function (which is the back forecasted de-seasonalised series); The value is corrected if regressors are included to remove their impact on the value (either by subtracting the fitted of the regression part or by dividing by them – depending on the type of error); If the trend is additive and seasonality is multiplicative, then the trend component is obtained by multiplying the initial level and trend from the decomposition (remember, the assumed model is multiplicative in this case) and then subtracting the previous level; If the trend is multiplicative and seasonality is additive, then the initials are added and then divided by the previous level to get the initial multiplicative trend component; If there is no seasonality and the trend is multiplicative, then the initial trend is set to 1. This is done to avoid the potentially explosive behaviour of the model; If the trend is multiplicative and the level is negative, then the level is substituted by the first actual value. This might happen in some weird cases of time series with low values; When it comes to seasonal components, if we have a pure additive or a pure multiplicative ETS model or ETS(A,Z,M), we use the seasonal indices obtained from the msdecompose() function, making sure that they are normalised. The type of seasonality in msdecompose() corresponds to the seasonal component of ETS in this case, and nothing additional needs to be done; The situation is more challenging with ETS(M,Z,A), for which the decomposition would return the multiplicative seasonal components. To convert them to the additive, we take their logarithm and multiply them by the minimum value of the actual time series. This way, we guarantee that the seasonal components are closer to the optimal ones. In the case of the non-seasonal model, the algorithm is more straightforward: The initial level is equal to either arithmetic or geometric mean (depending on the type of trend component) of the first \\(\\max(m_1,\\dots,m_n)\\) observations, where \\(m_j\\) is the model lag (e.g. in case of ARIMA(1,1,2), the components will have lags of 1 and 2). If the length of this mean is smaller than 20% of the sample, then the arithmetic mean of the first 20% of actual values is used; If regressors are included, then the value is modified, similar to how it is done in the seasonal case discussed above; If the model has an additive trend, then its initial value is equal to the mean difference between first \\(\\max(m_1,\\dots,m_n)\\) observations; In the case of multiplicative trend, the initial value is equal to the geometric mean of ratios between first \\(\\max(m_1,\\dots,m_n)\\) observations; In cases of the small samples (less than two seasonal periods), the procedure is similar to the one above. However, the seasonal indices are obtained by taking the actual values and either subtracting arithmetic mean or dividing them by the geometric one of the first \\(m_j\\) observations, depending on the seasonality type, normalising them afterwards. Finally, to ensure that the safe initials were provided, for the ETS(M,Z,Z) models, if the initial level contains a negative value, it is substituted by the global mean of the series. The pre-initialisation described here is not simple, but it guarantees that any ETS model can be constructed and estimated almost to any data. Yes, there might still be some issues with mixed ETS models, but the mechanism used in ADAM is quite robust. 11.4.4 Pre-initialisation of ADAM states, ARIMA ADAM ARIMA models have as many states as the number of polynomials \\(K\\) (see Section 9.1.2). Each state \\(v_{i,t}\\) needs to be initialised with \\(i\\) values (e.g. 1 for the first state, 2 for the second etc). This leads in general to more initial values for states than the SSARIMA from Svetunkov and Boylan (2020b): \\(\\frac{K(K+1)}{2}\\) instead of \\(K\\). However, we can reduce the number of initial seeds to estimate either by using a different initialisation procedure (e.g. backcasting) or using the following trick. In order to reduce the number of estimated parameters to \\(K\\), we can take the conditional expectations for all ARIMA states, in which case we will have: \\[\\begin{equation} \\mathrm{E}(v_{i,t} | t) = \\eta_i y_{t} \\text{ for } t=\\{-K+1, -K+2, \\dots, 0\\}, \\tag{11.21} \\end{equation}\\] and then use these expectations for the initialisation of ARIMA. This still implies calculating a lot of initials, but we can further reduce their number. We can express the actual value in terms of the state and error from (9.4) for the last state \\(K\\): \\[\\begin{equation} y_{t} = \\frac{v_{K,t} -\\theta_K \\epsilon_{t}}{\\eta_K}. \\tag{11.22} \\end{equation}\\] We select the last state \\(K\\) because it has the highest number of initials to estimate among all states. We can then insert the value (11.22) in each formula (11.21) for each state for \\(i=\\{1, 2, \\dots, K-1\\}\\) and take their expectations: \\[\\begin{equation} \\mathrm{E}(v_{i,t}|t) = \\frac{\\eta_i}{\\eta_K} \\mathrm{E}(v_{K,t}|t) \\text{ for } t=\\{-i+1, -i+2, \\dots, 0\\}. \\tag{11.23} \\end{equation}\\] This formula shows how the expectation of each state \\(i\\) depends on the expectation of the state \\(K\\). We can use it to propagate the values of the last state to the previous ones. However, this strategy will only work for the states corresponding to the ARI elements of the model. In the case of MA, using the same principle of initialisation via the conditional expectation, we can set the initial MA states to zero and estimate only ARI states. This is a crude but relatively simple way to pre-initialise ADAM ARIMA. Having said all that, we need to point out that it is advised to use backcasting in the ADAM ARIMA model – this is a more reliable and faster procedure for initialisation of ARIMA than the optimisation. 11.4.5 Pre-initialisation of ADAM states, Regressors and constant When it comes to the initials for the regressors, they are obtained from the parameters of the alm() model based on the rules below: The model with the logarithm of the response variable is constructed, if the error term is multiplicative and one of the following distributions has been selected: Normal, Laplace, S, Generalised Normal or Asymmetric Laplace; Otherwise, the model is constructed based on the provided formula and selected distribution; In any case, the global trend is added to the formula to make sure that its effect on the values of parameters is reduced; If the data contains categorical variables (aka “factors” in R), then the factors are expanded to dummy variables, adding the baseline value. While the classical multiple regression would not be estimable in this situation, dynamic models like ETSX and ARIMAX can work with the complete set of levels of categorical variables. To get the missing level, the intercept is added to the parameters of dummy variables, after which the obtained vector is normalised. This way, we can get, for example, all seasonal components if we want to model seasonality via X part of the model, not merging one of the components with level (see discussion in Section 10.5). Finally, the initialisation of constant (if needed in the model) depends on the selected model. In the case of ARIMA with all \\(D_j=0\\), the mean of the data is used. In all other cases, the arithmetic mean of difference or the geometric mean of ratios of all actual values is used depending on the error type. This is because the constant acts as a drift in the model in the case of non-zero differences. In the case of ETS, the impact of the constant is removed from the level in ETS and the states of ARIMA by either subtraction or division, again depending on the type of the error term. References "],["multiple-frequencies-in-adam.html", "Chapter 12 Multiple frequencies in ADAM", " Chapter 12 Multiple frequencies in ADAM When we work with weekly, monthly or quarterly data, we do not have more than one seasonal cycle. In this case, one and the same pattern will repeat itself only once a year. For example, we might see an increase in ski equipment sales over Winter, so the seasonal component for December will be typically higher than the same component in August. However, we might see several seasonal patterns when moving to the data with higher granularity. For example, daily sales of the product will have a time of year seasonal pattern and the day of week one. If we move to hourly data, then the number of seasonal elements might increase to three: the hour of the day, the day of the week and the time of year. Note that from the modelling point of view, these seasonal patterns should be called either “periodicities” or “frequencies” as the hour of the day cannot be considered as a proper “season.” But it is customary to refer to them as “seasonality” in forecasting literature. To correctly capture such a complicated structure in the data, we need to have a model that includes these multiple frequencies. In this chapter, we discuss how this can be done in the ADAM framework for both ETS and ARIMA. In addition, when we move to modelling high granularity data, there appear several fundamental issues related to how the calendar works and how human beings make their lives more complicated by introducing daylight saving related time changes over the year. Finally, we will discuss a simpler approach, relying on the explanatory variables (mentioned in Chapter 10). Among the papers related to the topic, we should start with James W. Taylor (2003a), who proposed an exponential smoothing model with double seasonality and applied it to energy data. Since then, the topic was developed by Gould et al. (2008), Taylor (2008), Taylor (2010), De Livera (2010) and De Livera et al. (2011). In this chapter, we will discuss some of the proposed models, how they relate to the ADAM framework and can be implemented. References "],["model-formulation.html", "12.1 Model formulation", " 12.1 Model formulation Multiple seasonal ARIMA has already been discussed in Subsections 8.2.3 and 9.1. Therefore, here we focus the discussion on ETS. Roughly, the idea of a model with multiple seasonalities is in introducing additional seasonal components. For the general framework this means that the state vector (for example, in a model with trend and seasonality) becomes: \\[\\begin{equation} \\mathbf{v}_t^\\prime = \\begin{pmatrix} l_t &amp; b_t &amp; s_{1,t} &amp; s_{2,t} &amp; \\dots &amp; s_{n,t} \\end{pmatrix}, \\tag{12.1} \\end{equation}\\] where \\(n\\) is the number of seasonal components (e.g. hour of day, hour of week and hour of year components). The lag matrix in this case is: \\[\\begin{equation} \\mathbf{l}^\\prime=\\begin{pmatrix}1 &amp; 1 &amp; m_1 &amp; m_2 &amp; \\dots &amp; m_n \\end{pmatrix}, \\tag{12.2} \\end{equation}\\] where \\(m_i\\) is the \\(i\\)-th seasonal periodicity. While, in theory, there can be combinations between additive and multiplicative seasonal components, I argue that such a mixture does not make sense, and the components should align with each other. This means that in the case of ETS(M,N,M), all seasonal components should be multiplicative, while in ETS(A,A,A), they should be additive. This results fundamentally in two types of models: Additive seasonality: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = \\check{y}_t + s_{1,t-m_1} + \\dots + s_{n,t-m_n} \\epsilon_t \\\\ &amp; \\vdots \\\\ &amp; s_{1,t} = s_{1,t-m_1} + \\gamma_1 \\epsilon_t \\\\ &amp; \\vdots \\\\ &amp; s_{n,t} = s_{n,t-m_n} + \\gamma_n \\epsilon_t \\end{aligned}, \\tag{12.3} \\end{equation}\\] where \\(\\check{y}_t\\) is the point value based on all non-seasonal components (e.g. \\(\\check{y}_t=l_{t-1}\\) in case of no trend model) and \\(\\gamma_i\\) is the \\(i\\)-th seasonal smoothing parameter. Multiplicative seasonality: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = \\check{y}_t \\times s_{1,t-m_1} \\times \\dots \\times s_{n,t-m_n} \\times(1+\\epsilon_t) \\\\ &amp; \\vdots \\\\ &amp; s_{1,t} = s_{1,t-m_1} (1 + \\gamma_1 \\epsilon_t) \\\\ &amp; \\vdots \\\\ &amp; s_{n,t} = s_{n,t-m_n} (1+ \\gamma_n \\epsilon_t) \\end{aligned}. \\tag{12.4} \\end{equation}\\] Depending on a specific model, the number of seasonal components can be 1, 2, 3 or more (although more than three might not make much sense from the modelling point of view). De Livera (2010) introduced components based on Fourier terms, updated over time via smoothing parameters. This feature is not yet fully supported in adam(), but it is possible to substitute some of the seasonal components (especially those that have fractional periodicity) with Fourier terms via explanatory variables and update them over time. The explanatory variables idea was discussed in Chapter 10 and will also be addressed in Section 12.3. References "],["ADAMMultiplIssues.html", "12.2 Estimation of multiple seasonal model", " 12.2 Estimation of multiple seasonal model 12.2.1 ADAM ETS issues Estimating a multiple seasonal ETS model is challenging because it implies a large optimisation task. The number of parameters related to seasonal components is equal in general to \\(\\sum_{j=1}^n m_j + n\\): \\(\\sum_{j=1}^n m_j\\) initial values and \\(n\\) smoothing parameters. For example, in case of hourly data, a triple seasonal model for hours of day, hours of week and hours of year will have: \\(m_1 = 24\\), \\(m_2 = 24 \\times 7 = 168\\) and \\(m_3= 7 \\times 24 \\times 365 = 61320\\), resulting overall in \\(24 + 168 + 61320 + 3 = 61498\\) parameters related to seasonal components to estimate. This is not a trivial task and would take hours to converge to optimum unless the pre-initials (Section 11.4) are already close to optimum. So, if you want to construct multiple seasonal ADAM ETS model, it makes sense to use a different initialisation (see discussion in Section 11.4), reducing the number of estimated parameters. A possible solution in this case is backcasting (Section 11.4.1). The number of parameters in our example would reduce from 61498 to 3, substantially speeding up the model estimation process. Another consideration is a fitting model to the data. In the conventional ETS, the size of the transition matrix is equal to the number of initial parameters, which makes it too slow to be practical on high-frequency data (multiplying a \\(61498 \\times 61498\\) matrix by a vector is a challenging task even for modern computers). But due to the lagged structure of the ADAM model (discussed in Section 5), construction of multiple seasonal models does not take as much time for ADAM ETS because we end up multiplying a matrix of \\(3 \\times 3\\) by a vector with three rows (skipping level and trend, which would add two more elements). So, in ADAM, the main computational burden comes from recursive relation in the state space model’s transition equation because this operation needs to be repeated at least \\(T\\) times, whatever the sample size \\(T\\) is. As a result, you would want to get to the optimum with as few iterations as possible, not needing to refit the model with different parameters to the same data many times. This gives another motivation for reducing the number of parameters to estimate (and thus for using backcasting). Another potential simplification would be to use deterministic seasonality for some seasonal frequencies. The possible solution, in this case, is to use explanatory variables (Section 10) for the higher frequency states (see discussion in Section 12.3) or use multiple seasonal ETS, setting some of smoothing parameters equal to zero. Finally, given that we deal with large samples, some states of ETS might become more reactive than needed, having higher than required smoothing parameters. One of the possible ways to overcome this limitation is by using multistep loss functions(Section 11.3). For example, Kourentzes and Trapero (2018) showed that using such loss functions as TMSE (from Subsection 11.3.2) in the estimation of ETS models on high-frequency data leads to improvements in accuracy due to the shrinkage of parameters towards zero, mitigating the potential overfitting issue. The only problem with this approach is that it is more computationally expensive and thus would take more time (at least \\(h\\) times more, where \\(h\\) is the length of the forecast horizon) than the conventional likelihood estimation. 12.2.2 ADAM ARIMA issues It is also possible to fit multiple seasonal ARIMA (discussed partially in Subsection 8.2.3) to the high-frequency data, and, for example, Taylor (2010) used triple seasonal ARIMA on the example of two time series and demonstrated that it produced more accurate forecasts than other ARIMAs under consideration, even slightly outperforming ETS. The main issue with ARIMA arises in the order selection stage. While in case of ETS, one can decide what model to use based on judgment (e.g. there is no apparent trend, and the amplitude increases with the increase of level so that we will fit ETS(M,N,M) model), ARIMA requires more careful consideration of possible orders of the model. Selecting appropriate orders of ARIMA is not a trivial task on its own, but choosing the orders on high-frequency data (where correlations might appear significant just because of the sample size) becomes an even more challenging task than usual. Furthermore, while on monthly data, we typically set maximum AR and MA orders to 3 or 5, this does not have any merit in the case of high-frequency data. If the first seasonal component has a lag of 24, then, in theory, anything up until 24 might be helpful for the model. Long story short, be prepared for the lengthy investigation of appropriate ARIMA orders. While ADAM ARIMA implements an efficient order selection mechanism (see Section 16.2), it does not guarantee that the most appropriate model will be applied to the data. Inevitably, you would need to analyse the residuals, add higher orders and see if there is an improvement in the model’s performance. The related issue to this in the context of ADAM ARIMA (Section 9.1) is the dimensionality problem. The more orders you introduce in the model, the bigger the transition matrix becomes. This leads to the same issues as in the ADAM ETS, discussed in the previous subsection. There is no unique recipe in this challenging situation, but backcasting (Section 11.4.1) addresses some of these issues. You might also want to fine-tune the optimiser to get a balance between speed and accuracy in the estimation of parameters (see discussion in Section 11.4.1). References "],["ETSXMultipleSeasonality.html", "12.3 Using explanatory variables for multiple seasonalities", " 12.3 Using explanatory variables for multiple seasonalities The conventional way of introducing several seasonal components discussed in Section 12.2 has several issues: It only works with the data with fixed periodicity (the problem sometimes referred to as “fractional frequency”): if \\(m_i\\) is not fixed and changes from period to period, the model becomes misaligned. An example of such a problem is fitting ETS on daily data with \\(m=365\\), while there are leap years that contain 366 days; If the model fits high-frequency data, the parameter estimation problem becomes non-trivial. Indeed, on daily data with \\(m=365\\), we need to estimate 364 initial seasonal indices together with the other parameters; Different seasonal indices would “compete” with each other for each observation, thus making the model overfit the data. An example is daily data with \\(m_1=7\\) and \\(m_2=365\\), where both seasonal components are updated on each observation based on the same error but with different smoothing parameters. In this situation, the model implies that the day of year seasonality should be updated together with the day of week one. The situation becomes even more complicated when the model has more than two seasonal components. But there are at least two ways of resolving these issues in the ADAM framework. The first is based on the idea of De Livera (2010) and the dynamic ETSX (discussed in Section 10.3). In this case, we need to generate fourier series and use them as explanatory variables in the model, turning on the mechanism of adaptation. For example, for the pure additive model, in this case, we will have: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = \\check{y}_t + \\sum_{i=1}^p a_{i,t-1} x_{i,t} + \\epsilon_t \\\\ &amp; \\vdots \\\\ &amp; a_{i,t} = a_{i,t-1} + \\delta_i \\frac{\\epsilon_t}{x_{i,t}} \\text{ for each } i \\in \\{1, \\dots, p\\} \\end{aligned}, \\tag{12.5} \\end{equation}\\] where \\(p\\) is the number of Fourier harmonics. In this case, we can introduce the conventional seasonal part of the model for the fixed periodicity (e.g. days of the week) in \\(\\check{y}_t\\) and use the updated harmonics for the non-fixed one. This approach is not the same as De Livera (2010) but might lead to similar results. The only issue here is selecting the number of harmonics, which can be done via the variables selection mechanism (which will be discussed in Sectio 16.3) but would inevitably increase computational time. The second option is based on the idea of a dynamic model with categorical variables (from Section 10.5). Instead of trying to fix the problem with days of the year, we first introduce the categorical variables for days of the week and then for the weeks of the year (or months of the year if we can assume that the effects of months are more appropriate). After that, we can introduce both categorical variables in the model, using a similar adaptation mechanism to (12.5). If some variables have fixed periodicity, we can substitute them with the conventional seasonal components. So, for example, ETSX(M,N,M)[7]{D} could be written as: \\[\\begin{equation} \\begin{aligned} &amp; {y}_{t} = l_{t-1} s_{t-7} \\times \\prod_{i=1}^q \\exp(a_{i,t-1} x_{i,t}) (1 + \\epsilon_t) \\\\ &amp; l_t = l_{t-1} (1 + \\alpha\\epsilon_t) \\\\ &amp; s_t = s_{t-7} (1 + \\gamma\\epsilon_t) \\\\ &amp; a_{i,t} = a_{i,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta \\log(1+\\epsilon_t) \\text{ for each } i \\in \\{1, \\dots, q\\}, &amp;\\text{ if } x_{i,t} = 1 \\\\ &amp;0 &amp;\\text{ otherwise } \\end{aligned} \\right. \\end{aligned}, \\tag{12.6} \\end{equation}\\] where \\(q\\) is the number of levels in the categorical variable (for weeks of year, this should be 53). The number of parameters to estimate in this situation might be greater than the number of harmonics in the first case, but this type of model resolves all three issues as well and does not have the dilemma of the number of harmonics selection. References "],["MultipleFrequenciesDSTandLeap.html", "12.4 Dealing with daylight saving and leap years", " 12.4 Dealing with daylight saving and leap years Another problem that arises in the case of data with high frequency is the change of local time due to daylight saving (DST). This happens in some countries two times a year: in Spring, the time is moved one hour forward (typically at 1 am to 2 am), while in the Autumn, it is moved back one hour. The implications of this are terrifying from a forecasting point of view because one day of the year has 23 hours, while the other has 25 hours. This leads to modelling difficulties because all the business processes are typically aligned with the local time. This means that if the conventional seasonal ETS model with \\(m=24\\) fits the data, it will only work correctly in half of the year. It will adapt to the new patterns after some time, but this implies that the smoothing parameter \\(\\gamma\\) will be higher than needed. There are two solutions to this problem: Shift the periodicity for one day, when the time changes from 24 to either 23, or 25, depending on the time of year; Introduce categorical variables for factors, which will mark specific hours of the day; The former is more challenging to formalise mathematically and implement in software. Still, the latter relies on the already discussed mechanism of ETSX{D} with categorical variables (Section 10.5) and should be more straightforward. Given the connection between seasonality in the conventional ETS model and the ETSX{D} with categorical variables for seasonality, both approaches should be equivalent in terms of parameters estimation and final forecasts. Similarly, the problem with leap years can be solved either using the shift from \\(m=365\\) to \\(m=366\\) on 29th February in the spirit of option (1) or using the categorical variables approach (2). There is a difference, however: the latter assumes the separate estimation of the parameter (so it has one more parameter to estimate), while the former would be suitable for the data with only one leap year, where the estimation of the seasonal index for 29th February might be difficult. However, given the discussion in Section 12.3, maybe we should not bother with \\(m=365\\) in the first place and rethink the problem, if possible. Having 52 / 53 weeks in a year has similar difficulties but at least does not involve the estimation of so many initial seasonal states. "],["ADAMMultipleFrequenciesExamples.html", "12.5 Examples of application", " 12.5 Examples of application 12.5.1 ADAM ETS We will use the taylor series from the forecast package to see how ADAM can be applied to high-frequency data. This is half-hourly electricity demand in England and Wales from Monday 5th June 2000 to Sunday 27th August 2000, used in James W. Taylor (2003b). library(zoo) y &lt;- zoo(forecast::taylor, order.by=as.POSIXct(&quot;2000/06/05&quot;)+ (c(1:length(forecast::taylor))-1)*60*30) ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo plot(y) Figure 12.1: Half-hourly electricity demand in England and Wales The series in Figure 12.1 does not exhibit an apparent trend but has two seasonal cycles: a half-hour of day and day of the week. Seasonality seems to be multiplicative because, with the reduction of the level of series, the amplitude of seasonality also reduces. We will try several different models and see how they compare. In all the cases below, we will use backcasting to initialise the model. We will use the last 336 observations (\\(48 \\times 7\\)) as the holdout to see whether models perform adequately or not. Note that when we have data with DST or Leap years (as discussed in Section 12.4), adam() will automatically correct the seasonal lags as long as your data contains specific dates (as zoo objects have, for example). First, it is ADAM ETS(M,N,M) with lags=c(48,7*48): adamModelETSMNM &lt;- adam(y, &quot;MNM&quot;, lags=c(1,48,336), h=336, holdout=TRUE, initial=&quot;back&quot;) adamModelETSMNM ## Time elapsed: 0.47 seconds ## Model estimated using adam() function: ETS(MNM)[48, 336] ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 25682.88 ## Persistence vector g: ## alpha gamma1 gamma2 ## 0.1357 0.2813 0.2335 ## ## Sample size: 3696 ## Number of estimated parameters: 4 ## Number of degrees of freedom: 3692 ## Information criteria: ## AIC AICc BIC BICc ## 51373.76 51373.77 51398.62 51398.66 ## ## Forecast errors: ## ME: 625.221; MAE: 716.941; RMSE: 817.796 ## sCE: 709.966%; Asymmetry: 90.4%; sMAE: 2.423%; sMSE: 0.076% ## MASE: 1.103; RMSSE: 0.867; rMAE: 0.107; rRMSE: 0.1 Notice that the seasonal smoothing parameters are relatively high in this model. For example, the second \\(\\gamma\\) is equal to 0.2335, which means that the model adapts the seasonal profile to the data substantially (takes 23.35% of the error from the previous observation in it). Furthermore, the smoothing parameter \\(\\alpha\\) is equal to 0.1357, which is also potentially too high, given that we have well-behaved data and that we deal with a multiplicative model. This might indicate that the model overfits the data. To see if this is the case, we can produce the plot of components over time (Figure 12.2). plot(adamModelETSMNM,12) Figure 12.2: Half-hourly electricity demand data decomposition according to ETS(M,N,M)[48,336] As the plot in Figure 12.2 shows, the level of series repeats the seasonal pattern in the original data, although in a diminished way. In addition, the second seasonal component repeats the intra-day seasonality in it, although it is also reduced. Next, we can plot the fitted values and forecasts to see how the model performs overall (Figure 12.3). plot(adamModelETSMNM,7) Figure 12.3: The fit and the forecast of the ETS(M,N,M)[48,336] model on half-hourly electricity demand data. As we might notice from Figure 12.3 the model was constructed in 0.47 seconds, and while it might not be the most accurate model for the data, it fits it well and produces reasonable forecasts. So, it is a good starting point. If we want to improve upon it, we can try one of the multistep estimators, for example, GTMSE (Subsection 11.3.3): adamModelETSMNMGTMSE &lt;- adam(y, &quot;MNM&quot;, lags=c(1,48,336), h=336, holdout=TRUE, initial=&quot;back&quot;, loss=&quot;GTMSE&quot;) The function will take more time due to complexity in the loss function calculation, but hopefully, it will produce more accurate forecasts due to shrinkage of smoothing parameters: adamModelETSMNMGTMSE ## Time elapsed: 23.06 seconds ## Model estimated using adam() function: ETS(MNM)[48, 336] ## Distribution assumed in the model: Normal ## Loss function type: GTMSE; Loss function value: -2648.698 ## Persistence vector g: ## alpha gamma1 gamma2 ## 0.0314 0.2604 0.1414 ## ## Sample size: 3696 ## Number of estimated parameters: 3 ## Number of degrees of freedom: 3693 ## Information criteria are unavailable for the chosen loss &amp; distribution. ## ## Forecast errors: ## ME: 216.71; MAE: 376.291; RMSE: 505.375 ## sCE: 246.084%; Asymmetry: 63%; sMAE: 1.272%; sMSE: 0.029% ## MASE: 0.579; RMSSE: 0.535; rMAE: 0.056; rRMSE: 0.062 The smoothing parameters of the second model are closer to zero than in the first one, which might mean that it does not overfit the data as much. We can analyse the components of the second model by plotting them over time, similarly to how we did it for the previous model (Figure 12.4): plot(adamModelETSMNMGTMSE,12) Figure 12.4: Half-hourly electricity demand data decomposition according to ETS(M,N,M)[48,336] estimated with GTMSE. The components on the plot in Figure 12.4 are still not ideal, but at least the level does not seem to contain the seasonality in it anymore. The seasonal components could still be improved if, for example, the initial seasonal indices were smoother (this applies especially to the seasonal component 2). Comparing the accuracy of the two models, for example, using RMSSE, we can conclude that the one with GTMSE was more accurate than the one estimated using the conventional likelihood. Another potential way of improvement for the model is the inclusion of AR(1) term, as for example done by Taylor (2010). This might take more time than the first model, but could also lead to some improvements in the accuracy: adamModelETSMNMAR &lt;- adam(y, &quot;MNM&quot;, lags=c(1,48,336), initial=&quot;back&quot;, orders=c(1,0,0), h=336, holdout=TRUE, maxeval=1000) Note that estimating the ETS+ARIMA model is a complicated task because of the increase of dimensionality of the matrices in the transition equation. Still, by default, the number of iterations would be restricted by 160, which might not be enough to get to the minimum of the loss. This is why I increased the number of iterations in the example above to 1000. If you want to get more feedback on how the optimisation has been carried out, you can ask the function to print details via print_level=41. adamModelETSMNMAR ## Time elapsed: 2.07 seconds ## Model estimated using adam() function: ETS(MNM)[48, 336]+ARIMA(1,0,0) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 24108.2 ## Persistence vector g: ## alpha gamma1 gamma2 ## 0.1129 0.2342 0.3180 ## ## ARMA parameters of the model: ## AR: ## phi1[1] ## 0.6923 ## ## Sample size: 3696 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 3691 ## Information criteria: ## AIC AICc BIC BICc ## 48226.39 48226.41 48257.47 48257.54 ## ## Forecast errors: ## ME: 257.38; MAE: 435.476; RMSE: 561.237 ## sCE: 292.266%; Asymmetry: 67.2%; sMAE: 1.472%; sMSE: 0.036% ## MASE: 0.67; RMSSE: 0.595; rMAE: 0.065; rRMSE: 0.069 In this specific example, we see that the ADAM ETS(M,N,M)+AR(1) leads to a slight improvement in accuracy in comparison with the ADAM ETS(M,N,M) estimated using the conventional loss function. 12.5.2 ADAM ETSX Another option of dealing with multiple seasonalities, as discussed in Section 12.3, is the introduction of explanatory variables. We start with a static model that captures half-hours of the day via its seasonal component and days of week frequency via an explanatory variable. We will use the temporaldummy() function from the greybox package to create respective categorical variables. This function works much better when the data contains proper time stamps and, for example, is of class zoo or xts: x1 &lt;- temporaldummy(y,type=&quot;day&quot;,of=&quot;week&quot;,factors=TRUE) x2 &lt;- temporaldummy(y,type=&quot;hour&quot;,of=&quot;day&quot;,factors=TRUE) taylorData &lt;- data.frame(y=y,x1=x1,x2=x2) This function becomes especially useful when dealing with DST and Leap years (see Section 12.4) because it will encode the dummy variables based on dates, allowing to sidestep the issue with changing frequency in the data. We can now fit the ADAM ETSX model with dummy variables for days of the week: adamModelETSXMNN &lt;- adam(taylorData, &quot;MNN&quot;, h=336, holdout=TRUE, initial=&quot;back&quot;) In the code above, we use the initialisation via backcasting (see discussion in Section 11.4.1), because otherwise the calculation will take much more time and might require additional manual tuning. Here is what we get as a result: adamModelETSXMNN ## Time elapsed: 0.61 seconds ## Model estimated using adam() function: ETSX(MNN) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 30155.54 ## Persistence vector g (excluding xreg): ## alpha ## 0.6182 ## ## Sample size: 3696 ## Number of estimated parameters: 2 ## Number of degrees of freedom: 3694 ## Information criteria: ## AIC AICc BIC BICc ## 60315.08 60315.09 60327.51 60327.53 ## ## Forecast errors: ## ME: -1664.294; MAE: 1781.472; RMSE: 2070.321 ## sCE: -1889.878%; Asymmetry: -92.3%; sMAE: 6.021%; sMSE: 0.49% ## MASE: 2.74; RMSSE: 2.194; rMAE: 0.266; rRMSE: 0.253 The resulting model produces biased forecasts (they are consistently higher than needed). This is mainly because the smoothing parameter \\(\\alpha\\) is too high, and the model frequently changes the level. We can see that in the plot of the state (Figure @ref(fig:adamModelETSXMNN12_1)): plot(adamModelETSXMNN$states[,1], ylab=&quot;Level&quot;) (#fig:adamModelETSXMNN12_1)Plot of the level of the ETSX model. As we see from Figure @ref(fig:adamModelETSXMNN12_1), the level component absorbs seasonality, which causes forecasting accuracy issues. However, the obtained value did not happen due to randomness – this is what the model does when seasonality is fixed and is not allowed to evolve. To reduce the model’s sensitivity, we can shrink the smoothing parameter using a multistep estimator (discussed in Section 11.3). But as discussed earlier, these estimators are typically slower than the conventional ones, so that they might take more computational time: adamModelETSXMNNGTMSE &lt;- adam(taylorData, &quot;MNN&quot;, h=336, holdout=TRUE, initial=&quot;back&quot;, loss=&quot;GTMSE&quot;) adamModelETSXMNNGTMSE ## Time elapsed: 39.26 seconds ## Model estimated using adam() function: ETSX(MNN) ## Distribution assumed in the model: Normal ## Loss function type: GTMSE; Loss function value: -2044.705 ## Persistence vector g (excluding xreg): ## alpha ## 0.0153 ## ## Sample size: 3696 ## Number of estimated parameters: 1 ## Number of degrees of freedom: 3695 ## Information criteria are unavailable for the chosen loss &amp; distribution. ## ## Forecast errors: ## ME: 105.462; MAE: 921.897; RMSE: 1204.967 ## sCE: 119.757%; Asymmetry: 18%; sMAE: 3.116%; sMSE: 0.166% ## MASE: 1.418; RMSSE: 1.277; rMAE: 0.138; rRMSE: 0.147 While the model’s performance with GTMSE has improved due to the shrinkage of \\(\\alpha\\) to zero, the seasonal states are still deterministic and do not adapt to the changes in data. We could adapt them via regressors=\"adapt\", but then we would be constructing the ETS(M,N,M)[48,336] model but in a less efficient way. Alternatively, we could assume that one of the seasonal states is deterministic and, for example, construct the ETSX(M,N,M) model: adamModelETSXMNMGTMSE &lt;- adam(taylorData, &quot;MNM&quot;, lags=48, h=336, holdout=TRUE, initial=&quot;back&quot;, loss=&quot;GTMSE&quot;, formula=y~x1) adamModelETSXMNMGTMSE ## Time elapsed: 33.56 seconds ## Model estimated using adam() function: ETSX(MNM) ## Distribution assumed in the model: Normal ## Loss function type: GTMSE; Loss function value: -2082.17 ## Persistence vector g (excluding xreg): ## alpha gamma ## 0.0135 0.0769 ## ## Sample size: 3696 ## Number of estimated parameters: 2 ## Number of degrees of freedom: 3694 ## Information criteria are unavailable for the chosen loss &amp; distribution. ## ## Forecast errors: ## ME: 146.436; MAE: 830.332; RMSE: 1055.372 ## sCE: 166.284%; Asymmetry: 27.1%; sMAE: 2.806%; sMSE: 0.127% ## MASE: 1.277; RMSSE: 1.118; rMAE: 0.124; rRMSE: 0.129 We can see an improvement compared to the previous model, so the seasonal states do change over time, which means that the deterministic seasonality is not appropriate in our example. However, it might be more suitable in some other cases, producing more accurate forecasts than the models assuming stochastic seasonality. 12.5.3 ADAM ARIMA Another model we can try on this data is ARIMA. We have not yet discussed the order selection mechanism for ARIMA, so I will construct a model based on my judgment. Keeping in mind that ETS(A,N,N) is equivalent to ARIMA(0,1,1) and that the changing seasonality in ARIMA context can be modelled with seasonal differences, I will construct SARIMA(0,1,1)(0,1,1)\\(_{336}\\), skipping the frequencies for a half-hour of the day. Hopefully, this will be enough to model: (a) changing level of data; (b) changing seasonal amplitude. Here is how we can construct this model using adam(): adamModelARIMA &lt;- adam(y, &quot;NNN&quot;, lags=c(1,336), initial=&quot;back&quot;, orders=list(i=c(1,1),ma=c(1,1)), h=336, holdout=TRUE) adamModelARIMA ## Time elapsed: 0.3 seconds ## Model estimated using adam() function: SARIMA(0,1,1)[1](0,1,1)[336] ## Distribution assumed in the model: Normal ## Loss function type: likelihood; Loss function value: 26098.76 ## ARMA parameters of the model: ## MA: ## theta1[1] theta1[336] ## 0.5086 -0.1977 ## ## Sample size: 3696 ## Number of estimated parameters: 3 ## Number of degrees of freedom: 3693 ## Information criteria: ## AIC AICc BIC BICc ## 52203.53 52203.53 52222.17 52222.20 ## ## Forecast errors: ## ME: 49.339; MAE: 373.387; RMSE: 499.661 ## sCE: 56.027%; Asymmetry: 18.6%; sMAE: 1.262%; sMSE: 0.029% ## MASE: 0.574; RMSSE: 0.529; rMAE: 0.056; rRMSE: 0.061 Figure 12.5 shows the fit and forecast from this model. plot(adamModelARIMA,7) Figure 12.5: The fit and the forecast of the ARIMA(0,1,1)(0,1,1)\\(_336\\) model on half-hourly electricity demand data. This model is directly comparable with ADAM ETS via information criteria, and as we can see, it is worse than ADAM ETS(M,N,M)+AR(1) and multiple seasonal ETS(M,N,M) in terms of AICc. But it is better in terms of the holdout RMSSE, producing more accurate forecasts. We could analyse the residuals of this model and iteratively test whether the addition of AR terms and a half-hour of day seasonality improves the model’s accuracy. We could also try ARIMA models with different distributions, compare them and select the most appropriate one. The reader is encouraged to do this on their own. References "],["ADAMIntermittent.html", "Chapter 13 Intermittent State Space Model", " Chapter 13 Intermittent State Space Model So far, we have discussed data that has regular occurrence. This is a characteristic of a variable with non-zero values on every observation. For example, daily sales of bread in a supermarket would have this regularity. However, there are time series where non-zero values do not happen on every observation. In the context of demand forecasting, this is called “intermittent demand.” The conventional example of such demand is monthly sales of jet engines: they will contain a lot of zeroes, when nobody buys the product and then all of a sudden several units when somebody decides to buy it, again followed by zeroes. However, this problem does not only apply to such expensive exotic products – the retailers face this all the time for various products, mainly when sales are recorded daily or on an even higher frequency. One of the most straightforward definitions of intermittent demand is that it is the demand that happens at irregular frequency. This definition relies on the specific frequency of data. If we work with hourly sales of products in the supermarket, it will inevitably be intermittent because we cannot predict when a customer will come and buy a product. However, if we aggregate this data to a weekly level, the intermittence will probably disappear, making the demand look regular. So, the zeroes problem in the data might disappear if you select the time aggregation level correctly. You might also meet the term “count data” (or “integer-valued data”) in a similar context, but there is a difference between count and intermittent data. The former implies that demand can take integer values only and can be typically modelled via Poisson, Binomial or Negative Binomial distributions. It does not necessarily contain zeroes and does not explicitly allow demand to happen at random. If there are zeroes, then it is assumed that they are just one of the possible values of a distribution. In the case of intermittent demand, we explicitly acknowledge that demand might not happen, but if it happens then, the demand size will be greater than zero. Furthermore, intermittent demand does not necessarily need to be integer-valued. For example, daily energy consumption for charging electric vehicles would typically be intermittent (because the vehicle owners do not charge them every day), but the non-zero consumption will not be integer. Still, count distributions can be used in some cases of intermittent demand, but they do not necessarily always provide a good approximation of complex reality. Before we move towards the proper discussion of the topic in the context of ADAM, we should acknowledge that at the heart of what follows, there lies the following model (Croston, 1972): \\[\\begin{equation} y_t = o_t z_t , \\tag{13.1} \\end{equation}\\] where \\(o_t\\) is the demand occurrence variable, which can be either zero or one and has some probability of occurrence \\(p_t\\), \\(z_t\\) is the demand sizes captured by a model (for example, ADAM ETS) and \\(y_t\\) is the final observed demand. In the context of intermittent demand, this model was originally proposed by Croston (1972), but similar models (e.g. Hurdle and Zero Inflated Poisson) exist in other, non-forecasting related literature. In this chapter, we will discuss the intermittent state space model (13.1), both parts of which can be modelled via ADAM models, and we will see how they can be used, what they imply and how they connect to the conventional regular demand. If ETS model is used for \\(z_t\\) then (13.1) will be called iETS. So, iETS(M,N,N) model refers to the intermittent state space model, where demand sizes are modelled via ETS(M,N,N). ETS can also be used for the occurrence part of the model, so if the discussion is focused on the demand occurrence part of the model, \\(o_t\\) (as in Section 13.1), we will use ``oETS’’ instead. Similarly, we can use the terms iARIMA and oARIMA, referring either to the whole model or just its occurrence part. Note, however, that while ARIMA can be used in theory, it is not yet implemented for the occurrence part of the model. So we will focus the discussion in this chapter on ADAM ETS. Furthermore, depending on how the occurrence part is modelled, the notations above can be expanded to include references to specific parts of the occurrence part of the model. This is discussed in detail in Section 13.1. This chapter is based on Svetunkov and Boylan (2019). If you want to know more about intermittent demand forecasting, Boylan and Syntetos (2021) is an excellent textbook on the topic, covering all the main aspects in appropriate detail. References "],["ADAMOccurrence.html", "13.1 Occurrence part of the model", " 13.1 Occurrence part of the model The general model (13.1) assumes that demand occurs randomly and that the variable \\(o_t\\) can be either zero (no demand) or one (there is some demand). While this process can be modelled using different distributions, Svetunkov and Boylan (2019) proposed using Bernoulli with a time varying probability (in the most general case): \\[\\begin{equation} o_t \\sim \\text{Bernoulli} \\left(p_t \\right) , \\tag{13.2} \\end{equation}\\] where \\(p_t\\) is the probability of occurrence. The higher it is, the more frequently the demand will happen. If \\(p_t=1\\), then the demand becomes continuous, while with \\(p_t=0\\), nobody buys the product at all. This section will discuss different types of models for this probability. For each of them, there are different mechanisms of the model construction, estimation, error calculation, update of the states and the generation of forecasts. To estimate any of these models using likelihood, we need to know the Probability Mass Function (PMF), which for Bernoulli distribution is: \\[\\begin{equation} f_o(o_t, p_t) = p_t^{o_t}(1-p_t)^{1-o_t}. \\tag{13.3} \\end{equation}\\] The parameters of the occurrence part of the model can be then estimated via the maximisation of the log-likelihood function, which comes directly from (13.3), and in the most general case is: \\[\\begin{equation} \\ell \\left(\\boldsymbol{\\theta}_o | o_t \\right) = \\sum_{o_t=1} \\log(\\hat{p}_t) + \\sum_{o_t=0} \\log(1-\\hat{p}_t) , \\tag{13.4} \\end{equation}\\] where \\(\\hat{p}_t\\) is the in-sample conditional one step ahead expectation of the probability on observation \\(t\\), given the information on observation \\(t-1\\), which depends on the vector of estimated parameters for the occurrence part of the model \\(\\boldsymbol{\\theta}_o\\). In order to demonstrate the difference between specific types of oETS models, we will use the following artificial data: y &lt;- ts(c(rpois(20,0.25), rpois(20,0.5), rpois(20,1), rpois(20,2), rpois(20,3), rpois(20,5))) Figure 13.1 shows how the data looks like: plot(y) Figure 13.1: Example of intermittent demand data. The probability of occurrence in this example increases together with the demand sizes. This example corresponds to the situation of intermittent demand for a product becoming regular. 13.1.1 Fixed probability model, oETS\\(_F\\) We start with the simplest case of fixed probability of occurrence, oETS\\(_F\\) model: \\[\\begin{equation} o_t \\sim \\text{Bernoulli}(p) , \\tag{13.5} \\end{equation}\\] This model assumes that demand happens with the same probability no matter what. This might sound exotic because, in practice, there might be many factors influencing customers’ desire to purchase, and the impact of these factors might change over time. But this is a basic model, which can be used as a benchmark on intermittent demand data. Furthermore, it might be suitable for modelling demand on expensive high-tech products, such as jet engines, which is “very slow” in its nature and typically does not evolve much over time. When estimated via maximisation of likelihood function (13.4), the probability of occurrence is equal to: \\[\\begin{equation} \\hat{p} = \\frac{T_1}{T}, \\tag{13.6} \\end{equation}\\] where \\(T_1\\) is the number of non-zero observations and \\(T\\) is the number of all the available observations in sample. The occurrence part of the model, oETS\\(_F\\) can be constructed using oes() function from smooth package: oETSFModel &lt;- oes(y, h=10, holdout=TRUE, occurrence=&quot;fixed&quot;) oETSFModel ## Occurrence state space model estimated: Fixed probability ## Underlying ETS model: oETS[F](MNN) ## Vector of initials: ## level ## 0.6455 ## ## Error standard deviation: 1.0909 ## Sample size: 110 ## Number of estimated parameters: 1 ## Number of degrees of freedom: 109 ## Information criteria: ## AIC AICc BIC BICc ## 145.0473 145.0844 147.7478 147.8349 The oETS\\(_F\\) model produces the straight line for the probability of 0.65, ignoring that in our example, the probability of occurrence has increased over time. plot(oETSFModel) Figure 13.2: Demand occurrence and probability of occurrence in the oETS\\(_F\\) model. The plot in Figure 13.2 above demonstrates the dynamics of the occurrence variable \\(o_t\\) and the fitted and predicted probabilities. The solid line shows when zeroes and ones happen, depicting the variable \\(o_t\\). The dashed line corresponds to the fixed probability of occurrence \\(\\hat{p}\\) in the sample. 13.1.2 Odds ratio model, oETS\\(_O\\) In this model, it is assumed that the update of the probability is driven by the occurrence of variable. It is more complicated than the previous as the probability now changes over time and can be modelled, for example, with ETS(M,N,N) model: \\[\\begin{equation} \\begin{aligned} &amp; o_t \\sim \\text{Bernoulli} \\left(p_t \\right) \\\\ &amp; p_t = \\frac{\\mu_{a,t}}{\\mu_{a,t}+1} \\\\ &amp; a_t = l_{a,t-1} \\left(1 + \\epsilon_{a,t} \\right) \\\\ &amp; l_{a,t} = l_{a,t-1}( 1 + \\alpha_{a} \\epsilon_{a,t}) \\\\ &amp; \\mu_{a,t} = l_{a,t-1} \\end{aligned}, \\tag{13.7} \\end{equation}\\] where \\(l_{a,t}\\) is the unobserved level component, \\(\\alpha_{a}\\) is the smoothing parameter, \\(1+\\epsilon_{a,t}\\) is the error term , which is positive, has mean of one and follows an unknown distribution, and \\(\\mu_{a,t}\\)is the conditional expectation for the unobservable shape variable \\(a_t\\). The measurement and transition equations in (13.7) can be substituted by any other ADAM ETS or ARIMA model if it is reasonable to assume that the probability dynamics has some additional components, such as trend, seasonality or exogenous variables. This model is called the “odds ratio” because the probability of occurrence in (13.7) is calculated using the classical logistic transform. This also means that \\(\\mu_{a,t}\\) is equal to: \\[\\begin{equation} \\label{eq:oETS_O_oddsRatio} \\mu_{a,t} = \\frac{p_t}{1 -p_t} . \\end{equation}\\] When \\(\\mu_{a,t}\\) increases in the oETS model, the odds ratio increases as well, meaning that the probability of occurrence goes up. Svetunkov and Boylan (2019) explain that this model is, in theory, appropriate for the demand for products becoming obsolescent. Still, given the updating mechanism, it should also work fine on other types of intermittent data. When it comes to the application of the model to the data, its construction is done via the following set of equations (example with oETS\\(_O\\)(M,N,N)): \\[\\begin{equation} \\begin{aligned} &amp; \\hat{p}_t = \\frac{\\hat{\\mu}_{a,t}}{\\hat{\\mu}_{a,t}+1} \\\\ &amp; \\hat{\\mu}_{a,t} = \\hat{l}_{a,t-1} \\\\ &amp; \\hat{l}_{a,t} = \\hat{l}_{a,t-1}( 1 + \\hat{\\alpha}_{a} e_{a,t}) \\\\ &amp; 1+e_{a,t} = \\frac{u_t}{1-u_t} \\\\ &amp; u_{t} = \\frac{1 + o_t -\\hat{p}_t}{2} \\end{aligned}, \\tag{13.8} \\end{equation}\\] where \\(e_{a,t}\\) is the proxy for the unobservable error term \\(\\epsilon_{a,t}\\) and \\(\\hat{\\mu}_t\\) is the estimate of \\(\\mu_{a,t}\\). If a multiple steps ahead forecast for the probability is needed from this model, then the formulae discussed in Section 3.5 can be used to get \\(\\hat{\\mu}_{a,t}\\), which then can be inserted in the first equation of (13.8) to get the final conditional multiple steps ahead probability of occurrence. Finally, to estimate the parameters of the model (13.8), the likelihood (13.4) can be used. The occurrence part of the model oETS\\(_O\\) is constructed using the very same oes() function, but also allows specifying the ETS model to use. For example, here is the oETS\\(_O\\)(M,M,N) model: oETSOModel &lt;- oes(y, model=&quot;MMN&quot;, h=10, holdout=TRUE, occurrence=&quot;odds-ratio&quot;) oETSOModel ## Occurrence state space model estimated: Odds ratio ## Underlying ETS model: oETS[O](MMN) ## Smoothing parameters: ## level trend ## 0.0209 0.0000 ## Vector of initials: ## level trend ## 0.0986 1.0448 ## ## Error standard deviation: 2.0989 ## Sample size: 110 ## Number of estimated parameters: 4 ## Number of degrees of freedom: 106 ## Information criteria: ## AIC AICc BIC BICc ## 103.5230 103.9040 114.3250 115.2203 In this example, we introduce the multiplicative trend in the model, which is supposed to reflect the idea of demand building up over time. Figure 13.3: Demand occurrence and probability of occurrence in the oETS(M,M,N)\\(_O\\) model. Figure 13.3 shows that the model captures the changing probability of occurrence well, reflecting that it increases over time. 13.1.3 Inverse odds ratio model, oETS\\(_I\\) Using similar approach to the oETS\\(_O\\) model, we can formulate the “inverse odds ration” model oETS\\(_I\\)(M,N,N): \\[\\begin{equation} \\begin{aligned} &amp; o_t \\sim \\text{Bernoulli} \\left(p_t \\right) \\\\ &amp; p_t = \\frac{1}{1+\\mu_{b,t}} \\\\ &amp; b_t = l_{b,t-1} \\left(1 + \\epsilon_{b,t} \\right) \\\\ &amp; l_{b,t} = l_{b,t-1}( 1 + \\alpha_{b} \\epsilon_{b,t}) \\\\ &amp; \\mu_{b,t} = l_{b,t-1} \\end{aligned}, \\tag{13.9} \\end{equation}\\] where similarly to (13.8), \\(l_{b,t}\\) is the unobserved level component, \\(\\alpha_{b}\\) is the smoothing parameter, \\(1+\\epsilon_{b,t}\\) is the positive error term with mean of one, and \\(\\mu_{b,t}\\) is the one step ahead conditional expectation for the unobservable shape parameters \\(b_t\\). The main difference of this model with the previous one is in the different mechanism of probability calculation, which focuses on the probability of “inoccurrence,” i.e. on zeroes of data rather than on ones. This type of model should be more appropriate for cases of demand building up (Svetunkov and Boylan, 2019). The probability calculation mechanism in (13.9) implies that \\(\\mu_{b,t}\\) can be expressed as: \\[\\begin{equation} \\mu_{b,t} = \\frac{1-p_t}{p_t} . \\end{equation}\\] The construction of the model (13.9) is similar to (13.8): \\[\\begin{equation} \\begin{aligned} &amp; \\hat{p}_t = \\frac{1}{1+\\hat{\\mu}_{b,t}} \\\\ &amp; \\hat{\\mu}_{b,t} = \\hat{l}_{b,t-1} \\\\ &amp; \\hat{l}_{b,t} = l_{b,t-1}( 1 + \\hat{\\alpha}_{b} e_{b,t}) \\\\ &amp; 1+e_{b,t} = \\frac{1-u_t}{u_t} \\\\ &amp; u_{t} = \\frac{1 + o_t -\\hat{p}_t}{2} \\end{aligned}, \\tag{13.10} \\end{equation}\\] where \\(e_{b,t}\\) is the proxy for the unobservable error term \\(\\epsilon_{b,t}\\) and \\(\\hat{\\mu}_{b,t}\\) is the estimate of \\(\\mu_{b,t}\\). Once again, we refer an interested reader to Subsection 3.5 for the discussion of the multiple steps ahead conditional expectations from the ETS(M,N,N) model. Svetunkov and Boylan (2019) show that the oETS\\(_I\\)(M,N,N) model, in addition to (13.10), can be estimated using Croston’s method, as long as we can assume that the probability does not change over time substantially. In this case the demand intervals (the number of zeroes between demand sizes) can be used instead of \\(\\hat{\\mu}_{b,t}\\) in (13.10). So the iETS(M,N,N)\\(_I\\)(M,N,N) can be considered as a model underlying Croston’s method. The function oes() implements the oETS\\(_I\\) model as well. For example, here is the oETS\\(_I\\)(M,M,N) model: oETSIModel &lt;- oes(y, model=&quot;MMN&quot;, h=10, holdout=TRUE, occurrence=&quot;inverse-odds-ratio&quot;) oETSIModel ## Occurrence state space model estimated: Inverse odds ratio ## Underlying ETS model: oETS[I](MMN) ## Smoothing parameters: ## level trend ## 0.0419 0.0000 ## Vector of initials: ## level trend ## 25.2308 0.8982 ## ## Error standard deviation: 4.1508 ## Sample size: 110 ## Number of estimated parameters: 4 ## Number of degrees of freedom: 106 ## Information criteria: ## AIC AICc BIC BICc ## 105.3722 105.7531 116.1741 117.0694 Figure 13.4: Demand occurrence and probability of occurrence in the oETS(M,M,N)\\(_I\\) model. Figure 13.4 shows that similarly to the oETS\\(_O\\), the model captures the trend in the probability of occurrence but has a higher smoothing parameter \\(\\alpha_b\\). 13.1.4 General oETS model, oETS\\(_G\\) Uniting the models oETS\\(_O\\) with oETS\\(_I\\), we can obtain the “general” model, which in the most general case can be summarised in the following set of state space equations: \\[\\begin{equation} \\begin{aligned} &amp; p_t = f_p(\\mu_{a,t}, \\mu_{b,t}) \\\\ &amp; a_t = w_a(\\mathbf{v}_{a,t-\\mathbf{l}}) + r_a(\\mathbf{v}_{a,t-\\mathbf{l}}) \\epsilon_{a,t} \\\\ &amp; \\mathbf{v}_{a,t} = f_a(\\mathbf{v}_{a,t-\\mathbf{l}}) + g_a(\\mathbf{v}_{a,t-\\mathbf{l}}) \\epsilon_{a,t} \\\\ &amp; b_t = w_b(\\mathbf{v}_{b,t-\\mathbf{l}}) + r_b(\\mathbf{v}_{b,t-\\mathbf{l}}) \\epsilon_{b,t} \\\\ &amp; \\mathbf{v}_{b,t} = f_b(\\mathbf{v}_{b,t-\\mathbf{l}}) + g_b(\\mathbf{v}_{b,t-\\mathbf{l}}) \\epsilon_{b,t} \\end{aligned} , \\tag{13.11} \\end{equation}\\] where \\(\\epsilon_{a,t}\\), \\(\\epsilon_{b,t}\\), \\(\\mu_{a,t}\\) and \\(\\mu_{b,t}\\) have been defined in previous subsectionsa and the other elements correspond to the ADAM model discussed in Section 5. Note that in this case two ETS models similar to the one discussed in Section 5 are used for modelling of \\(a_t\\) and \\(b_t\\). The general formula for the probability in case of the multiplicative error model is: \\[\\begin{equation} p_t = \\frac{\\mu_{a,t}}{\\mu_{a,t}+\\mu_{b,t}} , \\tag{13.12} \\end{equation}\\] while for the additive one, it is: \\[\\begin{equation} p_t = \\frac{\\exp(\\mu_{a,t})}{\\exp(\\mu_{a,t})+\\exp(\\mu_{b,t})} . \\tag{13.13} \\end{equation}\\] This is because both \\(\\mu_{a,t}\\) and \\(\\mu_{b,t}\\) need to be strictly positive, while the additive error models support the real plane. The canonical oETS model assumes that the pure multiplicative model is used for \\(a_t\\) and \\(b_t\\). This model type is positively defined for any values of error, trend and seasonality, which is essential for the values of \\(a_t\\) and \\(b_t\\) and their expectations. If a combination of additive and multiplicative error models is used, then the additive part should be exponentiated before using the formulae to calculate the probability. So, \\(f_p(\\cdot)\\) function from (13.11) maps the expectations from models A and B to the probability of occurrence, depending on the error type of the respective models: \\[\\begin{equation} p_t = f_p(\\mu_{a,t}, \\mu_{b,t}) = \\left \\lbrace \\begin{aligned} &amp; \\frac{\\mu_{a,t}}{\\mu_{a,t} + \\mu_{b,t}} &amp; \\text{ when both have multiplicative errors} \\\\ &amp; \\frac{\\mu_{a,t}}{\\mu_{a,t} + \\exp(\\mu_{b,t})} &amp; \\text{ when model B has additive error} \\\\ &amp; \\frac{\\exp(\\mu_{a,t})}{\\exp(\\mu_{a,t}) + \\mu_{b,t}} &amp; \\text{ when model A has additive error} \\\\ &amp; \\frac{\\exp(\\mu_{a,t})}{\\exp(\\mu_{a,t}) + \\exp(\\mu_{b,t})} &amp; \\text{ when both have additive errors} \\end{aligned} . \\right. \\tag{13.14} \\end{equation}\\] An example of the oETS model is the one based on two local level models (see discussion in Subsection 4.1), oETS\\(_G\\)(M,N,N)(M,N,N): \\[\\begin{equation} \\begin{aligned} &amp; o_t \\sim \\text{Bernoulli} \\left(p_t \\right) \\\\ &amp; p_t = \\frac{\\mu_{a,t}}{\\mu_{a,t}+\\mu_{b,t}} \\\\ \\\\ &amp; a_t = l_{a,t-1} \\left(1 + \\epsilon_{a,t} \\right) \\\\ &amp; l_{a,t} = l_{a,t-1}( 1 + \\alpha_{a} \\epsilon_{a,t}) \\\\ &amp; \\mu_{a,t} = l_{a,t-1} \\\\ \\\\ &amp; b_t = l_{b,t-1} \\left(1 + \\epsilon_{b,t} \\right) \\\\ &amp; l_{b,t} = l_{b,t-1}( 1 + \\alpha_{b} \\epsilon_{b,t}) \\\\ &amp; \\mu_{b,t} = l_{b,t-1} \\\\ \\end{aligned}, \\tag{13.15} \\end{equation}\\] where all the parameters have already been defined in subsections 13.1.2 and 13.1.3. More advanced models can be constructing for \\(a_t\\) and \\(b_t\\) by specifying the ETS models for each part and / or adding explanatory variables. The construction of the model (13.15) is done via the following set of equations: \\[\\begin{equation} \\begin{aligned} &amp; e_{a,t} = \\frac{u_t}{1-u_t} -1 \\\\ &amp; \\hat{a}_t = \\hat{l}_{a,t-1} \\\\ &amp; \\hat{l}_{a,t} = \\hat{l}_{a,t-1}( 1 + \\alpha_{a} e_{a,t}) \\\\ &amp; e_{b,t} = \\frac{1-u_t}{u_t} -1 \\\\ &amp; \\hat{b}_t = \\hat{l}_{b,t-1} \\\\ &amp; \\hat{l}_{b,t} = \\hat{l}_{b,t-1}( 1 + \\alpha_{b} e_{b,t}) \\end{aligned} . \\tag{13.16} \\end{equation}\\] In R, there is a separate function for the oETS\\(_G\\) model, called oesg(). It has twice more parameters than oes(), because it allows fine tuning of the models for the both variables \\(a_t\\) and \\(b_t\\). This gives an additional flexibility. For example, here is how we can use ETS(M,N,N) for the \\(a_t\\) and ETS(A,A,N) for the \\(b_t\\), resulting in oETS\\(_G\\)(M,N,N)(A,A,N): oETSGModel &lt;- oesg(y, modelA=&quot;MNN&quot;, modelB=&quot;AAN&quot;, h=10, holdout=TRUE) oETSGModel ## Occurrence state space model estimated: General ## Underlying ETS model: oETS[G](MNN)(AAN) ## ## Sample size: 110 ## Number of estimated parameters: 6 ## Number of degrees of freedom: 104 ## Information criteria: ## AIC AICc BIC BICc ## 107.1447 107.9603 123.3476 125.2643 Figure 13.5: Demand occurrence and probability of occurrence in the oETS(M,N,N)(A,A,N)\\(_G\\) model. We can also analyse models separately for \\(a_t\\) and \\(b_t\\) from the saved variable. Here is, for example, model A: oETSGModel$modelA ## Occurrence state space model estimated: Odds ratio ## Underlying ETS model: oETS[G](MNN)_A ## Smoothing parameters: ## level ## 0.0118 ## Vector of initials: ## level ## 1.2764 ## ## Error standard deviation: 1.8449 ## Sample size: 110 ## Number of estimated parameters: 2 ## Number of degrees of freedom: 108 ## Information criteria: ## AIC AICc BIC BICc ## 99.1447 99.2569 104.5457 104.8093 The experiments that I have done so far show that oETS\\(_G\\) very seldom brings improvements in comparison with oETS\\(_O\\) or oETS\\(_I\\) in terms of forecasting accuracy. Besides, selecting models for each of the parts is a challenging task. So, this model is theoretically attractive, being more general than the other oETS models, but is not very practical. Still it is useful because we can introduce different oETS models by restricting \\(a_t\\) and \\(b_t\\). For example, we can get: oETS\\(_F\\), when \\(\\mu_{a,t} = \\text{const}\\), \\(\\mu_{b,t} = \\text{const}\\) for all \\(t\\); oETS\\(_O\\), when \\(\\mu_{b,t} = 1\\) for all \\(t\\); oETS\\(_I\\), when \\(\\mu_{a,t} = 1\\) for all \\(t\\); oETS\\(_D\\), when \\(\\mu_{a,t} + \\mu_{b,t} = 1\\), \\(\\mu_{a,t} \\in [0,1]\\) for all \\(t\\) (discussed in the Subsection 13.1.5); oETS\\(_G\\), when there are no restrictions. 13.1.5 Direct probability model, oETS\\(_D\\) The last model in the family of oETS is the “Direct probability.” It appears, when the following restriction is imposed on the oETS\\(_G\\): \\[\\begin{equation} \\mu_{a,t} + \\mu_{b,t} = 1, \\mu_{a,t} \\in [0, 1] . \\tag{13.17} \\end{equation}\\] This restriction is inspired by the mechanism for the probability update proposed by Teunter et al. (2011) (TSB method). Their paper uses SES (discussed in Section 4.1) to model the time-varying probability of occurrence. Based on this idea and the restriction (13.17), we can formulate oETS\\(_D\\)(M,N,N) model, which will underly the occurrence part of the TSB method: \\[\\begin{equation} \\begin{aligned} &amp; o_t \\sim \\text{Bernoulli} \\left(\\mu_{a,t} \\right) \\\\ &amp; a_t = l_{a,t-1} \\left(1 + \\epsilon_{a,t} \\right) \\\\ &amp; l_{a,t} = l_{a,t-1}( 1 + \\alpha_{a} \\epsilon_{a,t}) \\\\ &amp; \\mu_{a,t} = \\min(l_{a,t-1}, 1) \\end{aligned}. \\tag{13.18} \\end{equation}\\] There is also an option with the additive error for the occurrence part (also underlying TSB), which has a different, more complicated form: \\[\\begin{equation} \\begin{aligned} &amp; o_t \\sim \\text{Bernoulli} \\left(\\mu_{a,t} \\right) \\\\ &amp; a_t = l_{a,t-1} + \\epsilon_{a,t} \\\\ &amp; l_{a,t} = l_{a,t-1} + \\alpha_{a} \\epsilon_{a,t} \\\\ &amp; \\mu_{a,t} = \\max \\left( \\min(l_{a,t-1}, 1), 0 \\right) \\end{aligned}. \\tag{13.19} \\end{equation}\\] The estimation of the oETS\\(_D\\)(M,N,M) model can be done using the following set of equations: \\[\\begin{equation} \\begin{aligned} &amp; \\hat{\\mu}_{a,t} = \\hat{l}_{a,t-1} \\\\ &amp; \\hat{l}_{a,t} = \\hat{l}_{a,t-1}( 1 + \\hat{\\alpha}_{a} e_{a,t}) \\end{aligned}, \\tag{13.20} \\end{equation}\\] where \\[\\begin{equation} e_{a,t} = \\frac{o_t (1 -2 \\kappa) + \\kappa -\\hat{\\mu}_{a,t}}{\\hat{\\mu}_{a,t}}, \\tag{13.21} \\end{equation}\\] and \\(\\kappa\\) is a very small number (for example, \\(\\kappa = 10^{-10}\\)), needed only in order to make the model estimable. The estimate of the error term in case of the additive model is much simpler and does not need any specific tricks to work: \\[\\begin{equation} e_{a,t} = o_t -\\hat{\\mu}_{a,t} , \\tag{13.22} \\end{equation}\\] which is directly related to TSB method. Note that equation (13.20) does not contain \\(\\min\\) function, because the estimated error (13.21) will always guarantee that the level will lie between 0 and 1 as long as the smoothing parameter lies in the [0, 1] region (which is the conventional assumption for both ETS(A,N,N) and ETS(M,N,N) models). This also applies for the oETS\\(_D\\)(A,N,N) model, where the \\(\\max\\) and \\(\\min\\) functions can be dropped as long as the smoothing parameter lies in [0,1]. An important feature of this model is that it allows probability to become either 0 or 1, thus implying either that there is no demand on the product or that the demand for the product has become regular. No other oETS model permits that – they assume that probability might become very close to bounds but can never reach them. Here’s an example of the application of the oETS\\(_D\\)(M,M,N) to the same artificial data: oETSDModel &lt;- oes(y, model=&quot;MMN&quot;, h=10, holdout=TRUE, occurrence=&quot;direct&quot;) oETSDModel ## Occurrence state space model estimated: Direct probability ## Underlying ETS model: oETS[D](MMN) ## Smoothing parameters: ## level trend ## 0.0439 0.0000 ## Vector of initials: ## level trend ## 0.2351 1.0111 ## ## Error standard deviation: 1.2292 ## Sample size: 110 ## Number of estimated parameters: 4 ## Number of degrees of freedom: 106 ## Information criteria: ## AIC AICc BIC BICc ## 109.9171 110.2981 120.7191 121.6144 Figure 13.6: Demand occurrence and probability of occurrence in the oETS(M,M,N)\\(_D\\) model. From Figure 13.6, we can see that the probability of occurrence increases rapidly and reaches the bound of one after 90 observations. The empirical analysis I have done on different datasets shows that the oETS\\(_D\\) model works effectively in many cases and produces accurate forecasts. So, if you are unsure which oETS model to choose for your intermittent data, I recommend starting with oETS\\(_D\\). 13.1.6 Model selection in oETS There are two dimensions for the model selection in the oETS model: Selection of the type of occurrence; Selection of the model type for the occurrence part. The solution in this situation is simple. Given the formula (13.4), we can try each of the models, calculate log-likelihoods, the number of all estimated parameters and then select the one that has the lowest information criterion. The demand occurrence models discussed in this section will have: oETS\\(_F\\): 1 parameter for the probability of occurrence; oETS\\(_O\\), oETS\\(_I\\) and oETS\\(_D\\): initial values, smoothing and dampening parameters; oETS\\(_G\\): initial values, smoothing and dampening parameters for models A and B; For example, if the oETS(M,N,N) model is constructed, the overall number of parameters for the models will be: oETS(M,N,N)\\(_F\\) – 1 parameter: the probability of occurrence \\(\\hat{p}\\); oETS(M,N,N)\\(_O\\), oETS(M,N,N)\\(_I\\) and oETS(M,N,N)\\(_D\\) – 2: the initial value of level and the smoothing parameter; oETS(M,N,N)\\(_G\\) – 4: the initial values of \\(\\hat{l}_{a,0}\\) and \\(\\hat{l}_{b,0}\\) and the smoothing parameters \\(\\hat{\\alpha}_a\\) and \\(\\hat{\\alpha}_b\\). This implies that the selection between models in (2) will come to the best fit to the demand occurrence data, while oETS(M,N,N)\\(_G\\) will only be selected if it provides a much better fit to the data. Given that intermittent demand typically does not have many observations, the selection of oETS(M,N,N)\\(_G\\) becomes highly improbable. When it comes to selecting the most appropriate demand occurrence model (e.g. between the local level and local trend models), the approach would be similar: estimate the pool of models via likelihood, calculate their number of parameters, select the model with the lowest IC. Given that the likelihood part of the demand occurrence comes to probabilities, the selection in the occurrence part can be made based on the likelihood (13.4) independently of the demand sizes part of the model. References "],["ADAMDemandSizes.html", "13.2 Demand sizes part of the model", " 13.2 Demand sizes part of the model So far, we have discussed the occurrence part of the model \\(o_t\\) and how to capture the probability of demand occurrence \\(p_t\\). But this is only half of the intermittent state space model. The second one is the model for the demand sizes \\(z_t\\), which focuses on how many units of product will be sold if our customers decide to buy in a specific period of time. This can be modelled with any ADAM model but has its own implications. We start discussion with analysis of iETS(M,N,N)\\(_F\\) model, which can be formulate as: \\[\\begin{equation} \\begin{aligned} &amp; y_t = o_t z_t \\\\ &amp; z_t = l_{z,t-1}(1 + \\epsilon_{z,t}) \\\\ &amp; l_{z,t} = l_{z,t-1}(1 + \\alpha_{z} \\epsilon_{z,t}) \\\\ &amp; o_t \\sim \\text{Bernoulli}(p) \\\\ \\end{aligned}, \\tag{13.23} \\end{equation}\\] where the subscript \\(z\\) refers to the components and parameters of demand sizes. This model assumes that there is always a potential demand on the product which evolves over time (even when \\(o_t=0\\)), we do not always observe it. This model’s main properties have already been discussed in Section 6.1. The main challenge appears when this model needs to be constructed and estimated because \\(z_t\\) is not observable when \\(o_t=0\\). In these instances, the error term cannot be estimated, but according to the model, it still exists, thus impacting the level of demand \\(l_{z,t}\\). To construct the model in the cases of no demand, we propose taking the conditional expectation for these periods, given the last available non-zero observation. This means that the model can be constructed using the following set of equations: \\[\\begin{equation} \\begin{aligned} &amp; e_{z,t} = \\frac{z_t -\\hat{\\mu}_{z,t}}{\\hat{\\mu}_{z,t}}, \\text{ when } o_t=1 \\\\ &amp; \\hat{\\mu}_{z,t} = \\hat{l}_{z,t-1} \\\\ &amp; \\hat{l}_{z,t} = \\left \\lbrace \\begin{aligned} &amp; \\hat{l}_{z,t-1} (1 + \\hat{\\alpha}_z e_t ), &amp; \\text{ when } o_t=1 \\\\ &amp; \\hat{l}_{z,t-1} , &amp; \\text{ when } o_t=0 \\end{aligned} \\right. \\end{aligned}. \\tag{13.24} \\end{equation}\\] This is only possible if \\(\\mathrm{E}(1+\\epsilon_{z,t})=1\\), which is an important assumption for multiplicative error models, discussed in Section 6.5. If this is violated, then the formula for the calculation of the level in (13.24) will become more complicated, involving the expectation of products of random variables. In a similar way, we can construct more complicated models for the demand sizes. In a more general case (Section 5) this can be written as: \\[\\begin{equation} \\begin{aligned} &amp; e_{z,t} = \\frac{z_t -\\hat{\\mu}_{z,t}}{\\hat{\\mu}_{z,t}}, \\text{ when } o_t=1 \\\\ &amp; \\hat{\\mathbf{v}}_{t} = \\left \\lbrace \\begin{aligned} &amp; f(\\hat{\\mathbf{v}}_{t-\\mathbf{l}}) + g(\\hat{\\mathbf{v}}_{t-\\mathbf{l}}) e_t, &amp; \\text{ when } o_t=1 \\\\ &amp; f(\\hat{\\mathbf{v}}_{t-\\mathbf{l}}) , &amp; \\text{ when } o_t=0 \\end{aligned} \\right. \\end{aligned}, \\tag{13.25} \\end{equation}\\] where all the functions and vectors have been defined for the original ADAM ETS model (7.1) in Section 5. 13.2.1 Additive vs multiplicative ETS for demand sizes iETS supports any type of ETS model, including pure additive (Section 5.1), pure multiplicative (Section 6.1) and mixed (Section 7.2) ones. But the selection of the appropriate model should be made based on the understanding of the problem. Typically, we expect demands to be non-negative: people want to buy our product, and usually, the business does not want to buy from customers. In this case, we should use pure multiplicative models, as they will always produce meaningful results, as long as the assumption of positivity of \\((1+\\epsilon_{z,t})\\) holds. This is important because the data would typically have low volume, and the model might generate unreasonable (negative) point and interval forecasts if a non-positive distribution is used (e.g. Normal). Thus, it is important to use Inverse Gaussian, or Gamma, or Log-Normal distribution (see discussion in Section 6.5) for the error term of the demand sizes part of the model when the volume of data is low, and you expect the non-zero values to be strictly positive. The main difficulty with pure multiplicative models arises from the construction point of view. As discussed in Section 6.3, the point forecasts of such model, in general, do not correspond to the conditional expectations (the only exclusion is the ETS(M,N,N) model). At the same time, the construction of the model for demand sizes assumes that the conditional expectations are equal to point forecasts when demand is not observed. If this is violated, then (13.25) is no longer the correct way to construct the model. This problem becomes especially important for the models with the multiplicative trend, where the conditional expectation might differ from point forecasts substantially. Still, point forecasts can be considered proxies for the conditional expectation, especially when smoothing parameters are close to zero. The conditional expectation coincides with the point forecast in the boundary case with \\(\\alpha=0\\) and \\(\\beta=0\\) in ETS(M,M,N). The higher the smoothing parameters are, the more significant discrepancy will be, implying that the model for the demand sizes is constructed incorrectly. The pure additive models do not have the issue with the conditional expectation and thus can be constructed easily in case of intermittent demand. But as discussed earlier, they might violate the non-negativity assumption of the model. So, in practice, they should be used with care. 13.2.2 Using ARIMA for demand sizes Finally, ADAM ARIMA can also be used for demand sizes, resulting in the iARIMA model. All the discussions in the previous subsection would apply to ARIMA as well, keeping in mind that ADAM ARIMA can be either pure additive (Section 9.1.2) or pure multiplicative (Section 9.1.4). Given that the multiplicative ARIMA is formulated via logarithms and still has the error term with the expectation of one, any ARIMA model can be used for the variable \\(z_t\\) and can be constructed via (13.25). This can also be used for the cases when a pure multiplicative model with the trend is needed, and there are difficulties with the construction of ETS(M,M,N) (i.e. smoothing parameters are not close to zero). The relation between ARIMA and ETS (discussed in Section 8.4) might be useful in this case. For example, instead of constructing ETS(M,M,N) we can construct logARIMA(0,2,2) (see Section 9.1.4), sidestepping the aforementioned problem. "],["ADAMIntermittentFull.html", "13.3 The full ADAM model", " 13.3 The full ADAM model Uniting demand occurrence (from Section 13.1) with the demand sizes (Section 13.2) parts of the model, we can now discuss the full iETS model, which in the most general form can be represented as: \\[\\begin{equation} \\begin{aligned} &amp; y_t = o_t z_t , \\\\ &amp; {z}_{t} = w_z(\\mathbf{v}_{z,t-\\mathbf{l}}) + r_z(\\mathbf{v}_{z,t-\\mathbf{l}}) \\epsilon_{z,t} \\\\ &amp; \\mathbf{v}_{z,t} = f_z(\\mathbf{v}_{z,t-\\mathbf{l}}) + g_z(\\mathbf{v}_{z,t-\\mathbf{l}}) \\epsilon_{z,t} \\\\ &amp; \\\\ &amp; o_t \\sim \\text{Bernoulli} \\left(p_t \\right) , \\\\ &amp; p_t = f_p(\\mu_{a,t}, \\mu_{b,t}) \\\\ &amp; a_t = w_a(\\mathbf{v}_{a,t-\\mathbf{l}}) + r_a(\\mathbf{v}_{a,t-\\mathbf{l}}) \\epsilon_{a,t} \\\\ &amp; \\mathbf{v}_{a,t} = f_a(\\mathbf{v}_{a,t-\\mathbf{l}}) + g_a(\\mathbf{v}_{a,t-\\mathbf{l}}) \\epsilon_{a,t} \\\\ &amp; b_t = w_b(\\mathbf{v}_{b,t-\\mathbf{l}}) + r_b(\\mathbf{v}_{b,t-\\mathbf{l}}) \\epsilon_{b,t} \\\\ &amp; \\mathbf{v}_{b,t} = f_b(\\mathbf{v}_{b,t-\\mathbf{l}}) + g_b(\\mathbf{v}_{b,t-\\mathbf{l}}) \\epsilon_{b,t} \\end{aligned} , \\tag{13.26} \\end{equation}\\] where the elements of the demand size and demand occurrence parts have been defined in Sections 5 and 13.1.4 respectively. The model (13.26) can also be considered as a more general one to the conventional ETS and ARIMA models: if the probability of occurrence \\(p_t\\) is equal to one for all observations, then the model reverts to them. Another important thing to note about this model is that it relies on the following assumptions: The demand sizes variable \\(z_t\\) is continuous. This is a reasonable assumption for many contexts, including, for example, energy forecasting. But even when we deal with integer values, Svetunkov and Boylan (2019) showed that such model does not perform worse than count data models; Potential demand size may change over time even when \\(o_t=0\\). This means that the states evolve even when demand is not observed; Demand sizes and demand occurrence are independent. This simplifies many derivations and makes the model estimable. If the assumption is violated, a different model with different properties would need to be constructed. My gut feeling tells me that the model (13.26) will work well even if this is violated. Depending on the specific model for each part and restrictions on \\(\\mu_{a,t}\\) and \\(\\mu_{b,t}\\), we might have different types of iETS models. To distinguish one model from another, we introduce the notation of iETS models of the form “iETS(demand sizes model)\\(_\\text{type of occurrence}\\)(model A type)(model B type).” For example, in the iETS(M,N,N)\\(_G\\)(A,N,N)(M,M,N), the first brackets say that ETS(M,N,N) was applied to the demand sizes, the underscored letter points out that this is the “general probability” model (Subsection 13.1.4), which has ETS(A,N,N) for the model A and ETS(M,M,N) for the model B. If only one variable is needed (either \\(a_t\\) or \\(b_t\\)), then the redundant brackets are dropped, and the notation is simplified, for example, to: iETS(M,N,N)\\(_O\\)(M,N,N). If the same type of model is used for both demand sizes and demand occurrence, then the second brackets can be dropped as well, simplifying this further to iETS(M,N,N)\\(_O\\) (odds ratio model with ETS(M,N,N) for all parts, Section 13.1.2). All these models are implemented in the adam() function for the smooth package in R. Similar notations and principles can be used for models based on ARIMA. Note that oARIMA is not yet implemented, but in theory, a model like iARIMA(0,1,1)\\(_O\\)(0,1,1) could be constructed in the ADAM framework. Last but not least, in some cases, we might have explanatory variables, such as promotions, prices, weather etc. They would impact both demand occurrence and demand sizes. In ADAM, we can include them in respective oes() and adam() functions. Remember that when you include explanatory variables in the occurrence part, you are modelling the probability of occurrence, not the occurrence itself. So, for example, a promotional effect in this situation would mean a higher chance of having sales. In some other situations, we might not need dynamic models, such as ETS and ARIMA, and can focus on static regression. While adam() supports this, the alm() function from the greybox might be more suitable in this situation. It supports similar parameters, but its occurrence parameter accepts either the type of transform (plogis for logit model and pnorm for the probit one) or a previously estimated occurrence model (either from alm() or from oes()). 13.3.1 Maximum Likelihood Estimation While there are different ways of estimating the parameters of the ADAM model (13.26), it is worth focusing on likelihood estimation (Section 11.1) for consistency with other ADAM models. The likelihood of the model will consist of several parts: The PDF of demand sizes part of the model when demand occurs; The probability of occurrence; The probability of inoccurrence. When demand occurs the likelihood is: \\[\\begin{equation} \\mathcal{L}(\\boldsymbol{\\theta} | y_t, o_t=1) = p_t f_z(z_t | \\mathbf{v}_{z,t-\\mathbf{l}}) , \\tag{13.27} \\end{equation}\\] while in the opposite case it is: \\[\\begin{equation} \\mathcal{L}(\\boldsymbol{\\theta} | y_t, o_t=0) = (1-p_t) f_z(z_t | \\mathbf{v}_{z,t-\\mathbf{l}}), \\tag{13.28} \\end{equation}\\] where \\(\\boldsymbol{\\theta}\\) includes all the estimated parameters of the model and parameters of assumed distribution (i.e. scale). Note that because the model assumes that the demand evolves over time even when it is not observed (\\(o_t=0\\)), we have a probability density function of demand sizes, \\(f_z(z_t | \\mathbf{v}_{z,t-\\mathbf{l}})\\) in (13.28). Based on the equations (13.27) and (13.28), we can summarise the likelihood for the whole sample of \\(T\\) observations: \\[\\begin{equation} \\mathcal{L}(\\boldsymbol{\\theta} | \\textbf{y}) = \\prod_{o_t=1} p_t \\prod_{o_t=0} (1-p_t) \\prod_{t=1}^T f_z(z_t | \\mathbf{v}_{z,t-\\mathbf{l}}) , \\tag{13.29} \\end{equation}\\] or in logarithms: \\[\\begin{equation} \\ell(\\boldsymbol{\\theta} | \\textbf{y}) = \\sum_{o_t=1} \\log(p_t) + \\sum_{o_t=0} \\log(1-p_t) + \\sum_{t=1}^T f_z(z_t | \\mathbf{v}_{z,t-\\mathbf{l}}), \\tag{13.30} \\end{equation}\\] where \\(\\textbf{y}\\) is the vector of all actual values and \\(f_z(z_t | \\mathbf{v}_{z,t-\\mathbf{l}})\\) can be substituted by a likelihood of the assumed distribution from the list of candidates in Section 11.1. The main issue in calculating the likelihood (13.30) is that the demand sizes are not observable when \\(o_t=0\\). This means that we cannot calculate the likelihood using the conventional approach. We need to use something else. Svetunkov and Boylan (2019) proposed using the Expectation Maximisation (EM) algorithm for this purpose, which is typically done in the following stages: Take Expectation of the likelihood; Maximise it with the obtained parameters; Go to (1) with the new set of parameters if the likelihood has not converged to the maximum. A classic example with EM is when several samples have different parameters, and we need to split them. Still, we do not know where specific observations belong and what the probability that each observation belongs to one of the groups is. In our context, it is a slightly different idea: we know probabilities, but we do not observe some of the demand sizes. If we take the expectation of (13.30) with respect to the unobserved demand sizes, we will get: \\[\\begin{equation} \\begin{aligned} \\mathrm{E}\\left(\\ell(\\boldsymbol{\\theta} | \\textbf{y})\\right) &amp; = \\sum_{o_t=1} \\log f_z \\left(z_{t} | \\mathbf{v}_{z,t-\\mathbf{l}} \\right) + \\sum_{o_t=0} \\text{E} \\left( \\log f_z \\left(z_{t} | \\mathbf{v}_{z,t-\\mathbf{l}} \\right) \\right) \\\\ &amp; + \\sum_{o_t=1} \\log(p_t) + \\sum_{o_t=0} \\log(1- p_t) \\end{aligned}. \\tag{13.31} \\end{equation}\\] The expectation in (13.31) is known in statistics as “Differential Entropy” (it is the negative differential entropy in the formula above). It will differ from one case to another, depending on the assumed demand sizes distribution. Table 13.1 summarises differential entropies for the distributions used in ADAM. Table 13.1: Differential entropies for different distributions. \\(\\Gamma(\\cdot)\\) is the Gamma function, while \\(\\psi(\\cdot)\\) is the digamma function. Assumption Differential Entropy \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\) \\(\\frac{1}{2}\\left(\\log(2\\pi\\sigma^2)+1\\right)\\) \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\) \\(1+\\log(2s)\\) \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\) \\(2+2\\log(2s)\\) \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\) \\(\\beta^{-1}-\\log\\left(\\frac{\\beta}{2s\\Gamma\\left(\\beta^{-1}\\right)}\\right)\\) \\(\\epsilon_t \\sim \\mathcal{ALaplace}(0, s, \\alpha)\\) \\(1+\\log(2s)\\) \\(1+\\epsilon_t \\sim \\mathcal{IG}(1, \\sigma^2)\\) \\(\\frac{1}{2}\\left(\\log \\pi e \\sigma^2 -\\log(2) \\right)\\) \\(1+\\epsilon_t \\sim \\mathcal{\\Gamma}(\\sigma^{-2}, \\sigma^2)\\) \\(\\sigma^{-2} + \\log \\Gamma\\left(\\sigma^{-2} \\right) + \\left(1-\\sigma^{-2}\\right)\\psi\\left(\\sigma^{-2}\\right)\\) \\(1+\\epsilon_t \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\) \\(\\frac{1}{2}\\left(\\log(2\\pi\\sigma^2)+1\\right)-\\frac{\\sigma^2}{2}\\) The majority of formulae for differential entropy in Table 13.1 are taken from Wikipedia (2021e) with the exclusion of the one for \\(\\mathcal{IG}\\), which was derived by Mudholkar and Tian (2002). These values can be inserted instead of the \\(\\text{E} \\left( \\log f_z \\left(z_{t} | \\mathbf{v}_{z,t-\\mathbf{l}} \\right) \\right)\\) in the formula (13.31), leading to the expected likelihood for respective distributions. For example, for Inverse Gaussian distribution (using the pdf from the Table 11.2), we get: \\[\\begin{equation} \\begin{aligned} \\mathrm{E}\\left(\\ell(\\boldsymbol{\\theta} | \\textbf{y})\\right) &amp; = -\\frac{T_1}{2} \\log \\left(2 \\pi \\sigma^2 \\right) -\\frac{1}{2}\\sum_{o_t=1} \\left(1+\\epsilon_{t}\\right)^3 -\\frac{3}{2} \\sum_{o_t=1} \\log y_t -\\frac{1}{2 \\sigma^2} \\sum_{o_t=1} \\frac{\\epsilon_t^2}{1+\\epsilon_t} \\\\ &amp; -\\frac{T_0}{2}\\left(\\log \\pi e \\sigma^2 -\\log(2) \\right) \\\\ &amp; + \\sum_{o_t=1} \\log(p_t) + \\sum_{o_t=0} \\log(1- p_t) \\end{aligned} , \\tag{13.32} \\end{equation}\\] where \\(T_0\\) is the number of zeroes in the data and \\(T_1\\) is the number of non-zeroe values. Luckily, the EM process in our specific situation does not need to be iterative – the obtained likelihood can then be maximised directly by changing the values of parameters \\(\\boldsymbol{\\theta}\\). It is also possible to derive analytical formulae for parameters of some of distributions based on (13.31) and the values from Table 13.1. For example, in case of \\(\\mathcal{IG}\\) the estimate of scale parameter is: \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{o_t=1}\\frac{e_t^2}{1+e_t}. \\tag{13.33} \\end{equation}\\] It can be shown that the likelihood estimates of scales for different distributions correspond to the conventional formulae from Section 11.1, but with the summation over \\(o_t=1\\) instead of all the observations. Note, however, that the division in (13.33) is done by the whole sample \\(T\\). This implies that the scale estimate will be biased, similarly to the classical bias of the sample variance (see Section 4.3 of Svetunkov (2021)). Svetunkov and Boylan (2019) show that in the full ADAM model, the estimate of scale is biased not only in-sample but also asymptotically, implying that with the increase of the sample size, it will be consistently lower than needed. This is because the summation is done over the non-zero values, while the division is done over the whole sample. This proportion of non-zeroes impacts the scale in (13.33), deflating its value. The only situation when the bias will be reduced is when the probability of occurrence reaches 1 (product demand becomes regular). Still, the value (13.33) will maximise the expected likelihood (13.31) and is useful for inference. However, if one needs to construct prediction intervals, this bias needs to be addressed, which can be done using the conventional correction: \\[\\begin{equation} \\hat{\\sigma}^2{^{\\prime}} = \\frac{T}{T_1-k} \\hat{\\sigma}^2, \\tag{13.34} \\end{equation}\\] where \\(k\\) is the number of all estimated parameters. 13.3.2 Conditional expectation and variance Now that we have discussed how the model is formulated and how it can be estimated, we can move to the discussion of conditional expectation and variance from it. The former is needed to produce point forecasts, while the latter might be required for different inventory decisions. The conditional \\(h\\) steps ahead expectation of the model can be obtained easily based on the assumption of independence of demand occurrence and demand sizes discussed earlier in Section 13.3: \\[\\begin{equation} \\mu_{y,t+h|t} = \\mu_{o,t+h|t} \\mu_{z,t+h|t}, \\tag{13.35} \\end{equation}\\] where \\(\\mu_{o,t+h|t}\\) is the conditional expectation of the occurrence variable (the conditional \\(h\\) steps ahead probability of occurrence) and \\(\\mu_{z,t+h|t}\\) is the conditional expectation of the demand sizes variable \\(z_t\\). So, the forecast from the model (13.26) relies on the probability of occurrence of the variable and will reflect an average demand per period of time. As a result, it might be less than one in some cases, implying that the product is not sold every day. Consequentially, Kourentzes (2014) argues that the term “demand rate” should be used in this context instead of the conditional expectation. However, any forecasting model will produce “demand per period” forecasts. They just typically assume that the probability of occurrence is equal to one (\\(p_t=1\\)) for all observations. So, there is no conceptual difference between the forecasts produced by regular and intermittent demand models, and I do not see the point in using the “demand rate” term. As for the conditional variance, it is slightly trickier than the conditional expectation, because the variance of a product involves not only variances, but expectations as well (assuming that two variables are independent): \\[\\begin{equation} \\mathrm{V}(y_{t+h}) = \\mathrm{V}(o_{t+h|t}) \\mathrm{V}(z_{t+h|t}) + \\mathrm{E}(o_{t+h|t})^2 \\mathrm{V}(z_{t+h|t}) + \\mathrm{V}(o_{t+h|t}) \\mathrm{E}(z_{t+h|t})^2 . \\tag{13.36} \\end{equation}\\] Given that we use Bernoulli distribution for the variable \\(o_t\\), its variance is equal to \\(\\mu_{o,t+h|t} (1-\\mu_{o,t+h|t})\\). In our context this implies that the conditional \\(h\\) steps ahead variance for the full model (from Section 13.3) is: \\[\\begin{equation} \\sigma^2_h = \\mu_{o,t+h|t} (1-\\mu_{o,t+h|t}) \\sigma^2_{z,h} + \\mu_{o,t+h|t}^2 \\sigma^2_{z,h} + \\mu_{o,t+h|t} (1-\\mu_{o,t+h|t}) \\mu_{z,t+h|t}^2 , \\tag{13.37} \\end{equation}\\] or after some manipulations: \\[\\begin{equation} \\sigma^2_h = \\mu_{o,t+h|t} \\left(\\sigma^2_{z,h} + (1 -\\mu_{o,t+h|t}) \\mu_{z,t+h|t}^2 \\right). \\tag{13.38} \\end{equation}\\] All the elements of the formula (13.38) are available and have been discussed in Sections 5.3, 6.3 and 13.1. References "],["IntermittentExample.html", "13.4 Examples of application", " 13.4 Examples of application We consider the same example from Section 13.1. We remember that in that example, both demand occurrence and demand sizes increase over time (Figure 13.1), meaning that we can try the model with the trend for both parts. This can be done using the adam() function from the smooth package, defining the type of occurrence to use. We will try several options and select the one that has the lowest AICc: adamModelsiETS &lt;- vector(&quot;list&quot;,4) adamModelsiETS[[1]] &lt;- adam(y, &quot;MMdN&quot;, h=10, holdout=TRUE, occurrence=&quot;odds-ratio&quot;) adamModelsiETS[[2]] &lt;- adam(y, &quot;MMdN&quot;, h=10, holdout=TRUE, occurrence=&quot;inverse-odds-ratio&quot;) adamModelsiETS[[3]] &lt;- adam(y, &quot;MMdN&quot;, h=10, holdout=TRUE, occurrence=&quot;direct&quot;) adamModelsiETS[[4]] &lt;- adam(y, &quot;MMdN&quot;, h=10, holdout=TRUE, occurrence=&quot;general&quot;) adamModelsiETSAICcs &lt;- setNames(sapply(adamModelsiETS,AICc), c(&quot;odds-ratio&quot;, &quot;inverse-odds-ratio&quot;, &quot;direct&quot;, &quot;general&quot;)) adamModelsiETSAICcs ## odds-ratio inverse-odds-ratio direct general ## 359.5494 360.5085 354.4810 371.6503 Based on this, we can see that the model with direct probability has the lowest AICc. We can show how the model approximates the data and produces forecasts for the holdout: Figure 13.7: The fit of the best model to the intermittent data. Figure 13.7 shows that the model captured the trend well for both demand occurrence and demand sizes parts. It forecasts that the mean demand will increase for the holdout period. We can also explore the demand occurrence part of this model by typing: adamModelsiETS[[i]]$occurrence ## Occurrence state space model estimated: Direct probability ## Underlying ETS model: oETS[D](MMdN) ## Smoothing parameters: ## level trend ## 0 0 ## Vector of initials: ## level trend ## 0.0463 1.1343 ## ## Error standard deviation: 1.4805 ## Sample size: 110 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 105 ## Information criteria: ## AIC AICc BIC BICc ## 103.5314 104.1083 117.0338 118.3897 In our example, the smoothing parameters are equal to zero for the demand occurrence part, which makes sense because the selected model is the damped multiplicative trend one, which should capture the increasing probability of occurrence well. Depending on the generated data, there might be issues in the ETS(M,Md,N) model for demand sizes, if the smoothing parameters are large. So, we can try out the logARIMA(1,1,2) to see how it compares with this model. Given that ARIMA is not yet implemented for the occurrence part of the model, we need to construct it separately and then use in adam(): oETSModel &lt;- oes(y, &quot;MMdN&quot;, h=10, holdout=TRUE, occurrence=names(adamModelsiETSAICcs)[i]) adamModeliARIMA &lt;- adam(y, &quot;NNN&quot;, h=10, holdout=TRUE, occurrence=oETSModel, orders=c(1,1,2), distribution=&quot;dlnorm&quot;) adamModeliARIMA ## Time elapsed: 0.09 seconds ## Model estimated using adam() function: iARIMA(1,1,2)[D] ## Occurrence model type: Direct ## Distribution assumed in the model: Mixture of Bernoulli and Log-Normal ## Loss function type: likelihood; Loss function value: 128.0496 ## ARMA parameters of the model: ## AR: ## phi1[1] ## -0.209 ## MA: ## theta1[1] theta2[1] ## -0.4984 0.0283 ## ## Sample size: 110 ## Number of estimated parameters: 6 ## Number of degrees of freedom: 104 ## Information criteria: ## AIC AICc BIC BICc ## 361.6306 362.4461 377.8335 379.7502 ## ## Forecast errors: ## Asymmetry: -70.393%; sMSE: 46.286%; rRMSE: 1.071; sPIS: 3196.961%; sCE: -386.317% Comparing the iARIMA model with the previous iETS based on AIC would not be fair because as soon as the occurrence model is provided to the adam(), he does not count the parameters estimated in that part towards the overall number of estimated parameters. To make the comparison fair, we need to estimate ADAM iETS similarly: adamModelsiETS[[i]] &lt;- adam(y, &quot;MMdN&quot;, h=10, holdout=TRUE, occurrence=oETSModel) adamModelsiETS[[i]] ## Time elapsed: 0.03 seconds ## Model estimated using adam() function: iETS(MMdN)[D] ## Occurrence model type: Direct ## Distribution assumed in the model: Mixture of Bernoulli and Gamma ## Loss function type: likelihood; Loss function value: 119.067 ## Persistence vector g: ## alpha beta ## 0 0 ## Damping parameter: 0.9986 ## Sample size: 110 ## Number of estimated parameters: 6 ## Number of degrees of freedom: 104 ## Information criteria: ## AIC AICc BIC BICc ## 343.6655 344.4810 359.8683 361.7850 ## ## Forecast errors: ## Asymmetry: -88.615%; sMSE: 53.438%; rRMSE: 1.151; sPIS: 3724.208%; sCE: -546.114% Comparing information criteria, the iETS model is more appropriate for this data. But this might be due to different distributional assumptions and difficulties estimating the ARIMA model. If you want to experiment more with iARIMA, you might try fine-tuning its parameters (see Section 11.4.1) for the data either by increasing the maxeval or changing the initialisation, for example: adamModeliARIMA &lt;- adam(y, &quot;NNN&quot;, h=10, holdout=TRUE, occurrence=oETSModel, orders=c(1,1,2), distribution=&quot;dgamma&quot;, initial=&quot;back&quot;) Finally, we can produce point and interval forecasts from either of the model via the forecast() method. Here is an example: Figure 13.8: Point forecasts and prediction interval from the iETS(M,Md,N)\\(_D\\) model. In Figure 13.8, the interval is expanding, reflecting the captured tendency of level for growth. The prediction intervals produced from multiplicative ETS models will typically be simulated if interval=\"prediction\", so to make them smoother, you might need to increase the nsim parameter, for example, to nsim=100000. "],["intermittent-demand-challenges.html", "13.5 Intermittent demand challenges", " 13.5 Intermittent demand challenges Intermittent demand is complicated and is difficult to work with. As a result, several challenges are related to the ADAM model specifically and to the intermittent demand in general that are worth discussing. First, given the presence of zeroes, the decomposition (Section 3.2) of intermittent time series does not make sense. The classical time series model assumes that the demand happens on every observation, while the intermittent demand happens irregularly. This makes all the conventional models inapplicable to the problem, although some of them might still work well in some cases (for example, SES from Section 4.1 in case of mildly intermittent data). The second follows directly from the previous point. While, in theory, it is possible to use any ETS / ARIMA model for both demand occurrence and demand sizes of the ADAM model, some of the specific model types are either impossible or very difficult to estimate. For example, seasonality on intermittent data is not very well pronounced, so estimating the initial values of components of seasonal models (such as ETS(M,N,M)) is not a trivial task. In some cases, if we have several products in a group that exhibit the same seasonal patterns, we can aggregate them to the group level to get a better estimate of seasonal indices, and then use them on the lower level. adam() function allows doing that via initial=list(seasonal=seasonalIndices). But in all the other cases, the estimation of seasonal models might fail. Third, in some cases, you might know when specifically demand will happen (for example, kiwis stop growing in New Zealand from May till September, so the crop will go down around that time). In this case, you do not need a proper intermittent demand model, you just need to deal with the demand sizes via ADAM ETS / ARIMA and provide zeroes and ones in the demand occurrence part for the variable \\(o_t\\). This can be done in adam() via occurrence=ot, where ot would contain zeroes and ones for the sample. This can also be done for the holdout sample in the forecast() function in a similar manner, something like this: forecast(ourModel, occurrence=ot, h=h) where ot should contain the values of the occurrence variable in the future. Fourth, more specialised models, such as iETS, will produce positively biased estimates of the smoothing parameters, whatever the estimator is used (see explanation in Svetunkov and Boylan, 2019). This is caused by the assumption that the potential demand might change between the observed sales. In this situation, the components would evolve slowly, while we would only see their values before the set of zeroes and afterwards, which will make the applied model catch up to the data, increasing the values of smoothing parameters. This also implies that such forecasting methods as Croston (Croston, 1972) and TSB (Teunter et al., 2011) would also result in positively biased estimates of parameters if we assume that demand might change between the non-zero observations. Practically speaking, this means that the smoothing parameters will be higher than needed, implying more rapid changes in components and higher uncertainty in final forecasts. There is currently no solution to this problem. Finally, summarising this chapter, intermittent demand forecasting is a complex problem. Differences between various forecasting models and methods on such data might be insignificant, and it would be challenging to select the appropriate one. Furthermore, point forecasts on intermittent demand are difficult to grasp and make actionable (unless you are interested in lead time forecasts, to get an idea about the expected demand over a period of time). All of this means that intermittent demand should be avoided if possible. Yes, you can have fancy models for it, but do you need to? For example, do you need to look at daily demand on products if decisions are made on a weekly basis (e.g. how many units of pasta should a supermarket order for the next week)? In many cases thinking about the problem carefully would allow avoiding intermittent demand, making the life of analysts easier. But if it is not possible, then ADAM iETS and iARIMA models can be considered potential solutions in some situations. References "],["ADAMscaleModel.html", "Chapter 14 Scale model for ADAM", " Chapter 14 Scale model for ADAM So far, we have focused our discussion on the location of a model (e.g. conditional mean, point forecasts), neglecting the fact that in some situations the variance of a model might exhibit some time-varying patterns. In statistics, this is called “heteroscedasticity” and implies that the variance of the residuals of a model is not constant. In some cases we might get away with multiplicative model, which takes care of heteroscedasticity caused by changing level of data if the variance is proportional to its value. But there might be situations, where variance changes due to some external factors, not necessarily available to the analyst. In this situation, it should be captured separately using a different model. Hereafter the original model producing conditional mean will be called location model, while the model for the variance will be called scale model. In this chapter, we discuss the scale model for ADAM ETS / ARIMA / Regression, the model that allows capturing time varying variance and using it for forecasting. We discuss how this model is formulated, how it can be estimated and then move to the discussion of its relation to such models as ARCH and GARCH. This chapter is inspired by GAMLSS model, which models the scale of distribution using functions of explanatory variables. We build upon that introducing the dynamic element in the model. "],["model-formulation-1.html", "14.1 Model formulation", " 14.1 Model formulation We start our discussion with an example of ETS model and then move to the more general one. 14.1.1 An example with ETS(A,N,N) with Normal distribution Consider ETS(A,N,N) model (which was discussed in Section 4.2), which has the following measurement equation: \\[\\begin{equation} y_t = l_{t-1} + \\epsilon_t, \\tag{14.1} \\end{equation}\\] where the most commonly used assumption for the error term is: \\[\\begin{equation*} \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2) . \\end{equation*}\\] The same error term can be represented as a multiplication of the standard normal variable by the standard deviation: \\[\\begin{equation} \\epsilon_t = \\sigma \\eta_t, \\tag{14.2} \\end{equation}\\] where \\(\\eta_t \\sim \\mathcal{N}(0, 1)\\). Now consider the situation when instead of the constant variance \\(\\sigma^2\\) we have one that changes over time either because of its own dynamics or because of the influence of explanatory variables. In that case we will add the subscript \\(t\\) to the variance in (14.2) to have: \\[\\begin{equation} \\epsilon_t = \\sigma_t \\eta_t. \\tag{14.3} \\end{equation}\\] The variance in the model (14.3) can be modelled explicitly using a model. The thing to keep in mind is that such model needs to be pure multiplicative to guarantee that the variance will not become zero or even negative. For simplicity, consider that we use ETS(M,N,N) for the variance in (14.3). The only thing to keep in mind is that in the case of Normal distribution, the scale is equal to variance rather than standard deviation, so such model can be written in the conventional form as: \\[\\begin{equation} \\begin{aligned} &amp;\\epsilon_t^2 = l_{\\sigma,t-1} \\eta_t^2 \\\\ &amp;l_{\\sigma,t} = l_{\\sigma,t-1} \\left(1 + \\alpha_\\sigma (\\eta_t^2-1)\\right) \\end{aligned}, \\tag{14.4} \\end{equation}\\] Note that although the part \\(\\left(1 + \\alpha_\\sigma (\\eta_t^2-1)\\right)\\) looks slightly different than the respective part \\(\\left(1 + \\alpha \\epsilon_t \\right)\\) in the conventional ETS(M,N,N), they are equivalent: if we substitute \\(\\xi_t = \\eta_t^2-1\\) in (14.4), we will arrive to the conventional ETS(M,N,N) model. Another thing to notice in this formulation is that because \\(\\eta_t\\) follows standard Normal distribution, its square will follow Chi-squared distribution: \\(\\eta_t \\sim \\chi^2(1)\\). This is just a curious observation. Finally, we can use all the properties of pure multiplicative models discussed in Chapter 6 to get the fitted values and forecasts from the model (14.4): \\[\\begin{equation} \\begin{aligned} &amp;\\sigma_{t|t-1}^2 = l_{\\sigma,t-1} \\\\ &amp;\\sigma_{t+h|t}^2 = l_{\\sigma,t} \\end{aligned}. \\tag{14.5} \\end{equation}\\] In order to construct this model, we need to collect the residuals \\(e_t\\) of the location model (14.1), square them and use in the following system of equations: \\[\\begin{equation} \\begin{aligned} &amp;\\hat{\\sigma}^2_{t} = \\hat{l}_{\\sigma,t-1} \\\\ &amp;\\hat{\\eta}_t^2 = \\frac{e_t^2}{\\hat{\\sigma}^2_{t}} \\\\ &amp;\\hat{l}_{\\sigma,t} = \\hat{l}_{\\sigma,t-1} (1 + \\hat{\\alpha}_\\sigma (\\hat{\\eta}_t^2-1)) \\end{aligned}. \\tag{14.6} \\end{equation}\\] In order for this to work, we need to estimate \\(\\hat{l}_{\\sigma,0}\\) and \\(\\hat{\\alpha}_\\sigma\\), which can be done in the conventional way by maximising the log-likelihood function of the normal distribution (see Section 11.1) - the only thing that will change in comparison with the conventional estimation is the fitted values of \\(\\hat{\\sigma}^2_{t}\\) for the variance generated from (14.6): \\[\\begin{equation} \\ell(\\boldsymbol{\\theta}, {\\sigma}_t^2 | \\mathbf{y}) = -\\frac{T}{2} \\log(2 \\pi \\sigma_t^2) -\\frac{1}{2} \\sum_{t=1}^T \\frac{\\epsilon_t^2}{\\sigma_t^2} . \\tag{14.7} \\end{equation}\\] The thing to keep in mind is that the final ETS(A,N,N) model (14.1) with the scale model (14.4) will have four parameters instead of 3 as in the case of a simpler model: 2 for the location part of the model (initial level \\(l_{0}\\) and smoothing parameter \\(\\alpha\\)) and 2 for the scale part of the model (initial level \\(l_{\\sigma,0}\\) and smoothing parameter \\(\\alpha_\\sigma\\)). As can be seen from this example, constructing and estimating the scale model for ADAM is not a difficult task. Following similar principles, we can apply any other pure multiplicative model for scale, including ETS(Y,Y,Y) (Chapter 6), log ARIMA (Section 9.1.4) or a multiplicative regression. Furthermore, the location model ETS(A,N,N) can be substituted by any other ETS / ARIMA / Regression model - the same principles as discussed in this Subsection can be applied to them. The only restriction in all of this is that both parts of the model should be estimated via maximisation of likelihood - other methods typically do not estimate the scale of distribution explicitly. 14.1.2 General case More generally speaking, scale model can be created for any ETS/ARIMA/Regression location model. Following from the discussion in Section 7.1, the general location model can be written as: \\[\\begin{equation*} \\begin{aligned} {y}_{t} = &amp; w(\\mathbf{v}_{t-\\mathbf{l}}) + r(\\mathbf{v}_{t-\\mathbf{l}}) \\epsilon_t \\\\ \\mathbf{v}_{t} = &amp; f(\\mathbf{v}_{t-\\mathbf{l}}) + g(\\mathbf{v}_{t-\\mathbf{l}}) \\epsilon_t \\end{aligned} \\end{equation*}\\] If we assume that \\(\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2_t)\\), then the general scale ETS model can be formulated as (Section 6.1: \\[\\begin{equation} \\begin{aligned} \\epsilon_t^2 = &amp; \\exp \\left(\\mathbf{w}^\\prime \\log(\\mathbf{v}_{\\sigma,t-\\mathbf{l}})\\right)\\eta_{t}^2\\\\ \\log \\mathbf{v}_{\\sigma,t} = &amp; \\mathbf{F}_\\sigma \\log \\mathbf{v}_{\\sigma,t-\\mathbf{l}} + \\log(\\mathbf{1}_k + \\mathbf{g}_\\sigma (\\eta_t^2-1)) \\end{aligned}, \\tag{14.8} \\end{equation}\\] while the scale model with ARIMA elements will have a different transition equation (Subsection 9.1.4): \\[\\begin{equation} \\log \\mathbf{v}_{\\sigma,t} = \\mathbf{F}_\\sigma \\log \\mathbf{v}_{\\sigma,t-\\mathbf{l}} + \\mathbf{g}_\\sigma \\log(\\eta_t^2) \\tag{14.9} \\end{equation}\\] Finally, the transition equation for the scale model with regression (dynamic parameters, discussed in Section 10.3) should be: \\[\\begin{equation} a_{i,t} = a_{i,t-1} + \\left \\lbrace \\begin{aligned} &amp;\\delta_i \\frac{\\log(\\eta_t^2)}{x_{i,t}} \\text{ for each } i \\in \\{1, \\dots, n\\}, \\text{ if } x_{i,t}\\neq 0 \\\\ &amp;0 \\text{ otherwise } \\end{aligned} \\right. \\tag{14.10} \\end{equation}\\] The main thing that unites the parts (14.8), (14.9) and (14.10) is that all the three parts of the model should be pure multiplicative in order to avoid potential issues with negative numbers. The construction and estimation of the scale model in this case becomes similar to the one discussed for the ETS(A,N,N) example above. When it comes to forecasting of the conditional h-steps-ahead scale, given the limitations of the pure multiplicative model discussed in Section 6.3, it needs to be obtained via simulations - this way the forecast from the ADAM model will coincide with the expectation, which in case of (14.8), (14.9) and (14.10) will give the conditional h-steps-ahead scale. All the principles discussed in Sections 6.1, 9.1.4 and 10.3 can be used directly for the scale model without any limitations. "],["diagnostics.html", "Chapter 15 Model diagnostics", " Chapter 15 Model diagnostics In this chapter, we investigate how ADAM models can be diagnosed and improved. Most topics will build upon the typical model assumptions discussed in Subsection 1.4.1 and Section 12 of Svetunkov (2021) textbook. Some of the assumptions cannot be diagnosed properly, but there are well-established instruments for the others. We will consider the following assumptions and discuss how to check whether they are violated or not: Model is correctly specified: No omitted variables; No redundant variables; The necessary transformations of the variables are applied; No outliers in the model. Residuals are i.i.d.: They are not autocorrelated; They are homoscedastic; The expectation of residuals is zero, no matter what; The residuals follow the specified distribution; The distribution of residuals does not change over time. The explanatory variables are not correlated with anything but the response variable: No multicollinearity; No endogeneity (not discussed in the context of ADAM). All the model diagnostics is aimed at spotting patterns in residuals. If there are patterns, then something is probably missing in the model. In this chapter, we will discuss which instruments can be used to diagnose different types of assumptions. Note that the proposed analysis is based mainly on visual inspection of various plots. While there are statistical tests for some assumptions, we do not discuss them here. This is because human judgment is typically more reliable than just p-values, and people tend to misuse the latter. To make this more actionable, we will consider a conventional regression model on Seatbelts data, discussed in Section 10.6. We start with pure regression, which can be estimated equally well with the adam() function from the smooth package or the alm() from the greybox in R. In general, I recommend using alm() when no dynamic elements are present in the model. Otherwise, use adam() in the following way: adamModelSeat01 &lt;- adam(Seatbelts, &quot;NNN&quot;, formula=drivers~PetrolPrice+kms) plot(adamModelSeat01, 7, main=&quot;&quot;) Figure 15.1: Basic regression model for the data of Road Casualties in Great Britain 1969–84. This model has several issues, and in this chapter, we will discuss how to diagnose and fix them. References "],["diagnosticsOmitted.html", "15.1 Model specification: Omitted variables", " 15.1 Model specification: Omitted variables We start with one of the most critical assumptions for models: the model has not omitted important variables. This is difficult to diagnose because it is typically challenging to identify what is missing if we do not have it in front of us. The best thing one can do is a mental experiment, trying to comprise a list of all theoretically possible variables that would impact the variable of interest. If you manage to come up with such a list and realise that some of the variables are missing, the next step would be to collect the variables themselves or use their proxies. The proxies are variables that are expected to be correlated with the missing variables and can partially substitute them. We would need to add the missing information in the model one way or another. In some cases, we might be able to diagnose this. For example, with our regression model estimated in the previous section, we have a set of variables not included in the model. A simple thing to do is to see if the residuals of our model are correlated with any of the omitted variables. We can either produce scatterplots or calculate measures of association (see Section 2.2 and Chapter 6 of Svetunkov, 2021) to see if there are relations in the residuals. I will use assoc() and spread() functions from greybox for this: # Create a new matrix, removing the variables that are already # in the model SeatbeltsWithResiduals &lt;- cbind(as.data.frame(residuals(adamModelSeat01)), Seatbelts[,-c(2,5,6)]) colnames(SeatbeltsWithResiduals)[1] &lt;- &quot;residuals&quot; # Spread plot greybox::spread(SeatbeltsWithResiduals) Figure 15.2: Spread plot for the residuals of model vs not included variables. spread() function automatically detects the type of variable and produces based on that scatterplot / boxplot() / tableplot() between them, making the final plot more readable. The plot above tells us that residuals are correlated with DriversKilled, front, rear and law, so some of these variables can be added to the model to improve it. VanKilled might have a weak relation with drivers, but judging by description does not make sense in the model (this is a part of the drivers variable). Also, I would not add DriversKilled, as it seems not to drive the number of deaths and injuries (based on our understanding of the problem), but is just correlated with it for obvious reasons (DriversKilled is included in drivers). The variables front and rear should not be included in the model, because they do not explain injuries and deaths of drivers, they are impacted by similar factors and can be considered as output variables. So, only law can be safely added to the model, because it makes sense. We can also calculate measures of association between variables: greybox::assoc(SeatbeltsWithResiduals) ## Associations: ## values: ## residuals DriversKilled front rear VanKilled law ## residuals 1.0000 0.7826 0.6121 0.4811 0.2751 0.1892 ## DriversKilled 0.7826 1.0000 0.7068 0.3534 0.4070 0.3285 ## front 0.6121 0.7068 1.0000 0.6202 0.4724 0.5624 ## rear 0.4811 0.3534 0.6202 1.0000 0.1218 0.0291 ## VanKilled 0.2751 0.4070 0.4724 0.1218 1.0000 0.3949 ## law 0.1892 0.3285 0.5624 0.0291 0.3949 1.0000 ## ## p-values: ## residuals DriversKilled front rear VanKilled law ## residuals 0.0000 0 0 0.0000 0.0001 0.0086 ## DriversKilled 0.0000 0 0 0.0000 0.0000 0.0000 ## front 0.0000 0 0 0.0000 0.0000 0.0000 ## rear 0.0000 0 0 0.0000 0.0925 0.6890 ## VanKilled 0.0001 0 0 0.0925 0.0000 0.0000 ## law 0.0086 0 0 0.6890 0.0000 0.0000 ## ## types: ## residuals DriversKilled front rear VanKilled law ## residuals &quot;none&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;mcor&quot; ## DriversKilled &quot;pearson&quot; &quot;none&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;mcor&quot; ## front &quot;pearson&quot; &quot;pearson&quot; &quot;none&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;mcor&quot; ## rear &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;none&quot; &quot;pearson&quot; &quot;mcor&quot; ## VanKilled &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;none&quot; &quot;mcor&quot; ## law &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;none&quot; Technically speaking, the output of this function tells us that all variables are correlated with residuals and can be considered in the model. This is because p-values are lower than my favourite significance level of 1%, so we can reject the null hypothesis for each of the tests (which is that the respective parameters are equal to zero in the population). I would still prefer not to add DriversKilled, VanKilled, front and rear variables in the model for the reasons explained earlier. We can construct a new model in the following way: adamModelSeat02 &lt;- adam(Seatbelts, &quot;NNN&quot;, formula=drivers~PetrolPrice+kms+law) The model now fits the data differently (Figure 15.3): plot(adamModelSeat02, 7, main=&quot;&quot;) Figure 15.3: The data and the fitted values for the second model. How can we know that we have not omitted any important variables in our new model? Unfortunately, there is no good way of knowing that. In general, we should use judgment to decide whether anything else is needed or not. But given that we deal with time series, we can analyse residuals over time and see if there is any structure left (Figure 15.4): plot(adamModelSeat02, 8, main=&quot;&quot;) Figure 15.4: Standardised residuals vs time plot. Plot in Figure 15.4 shows that the model has not captured seasonality and that there is still some structure left in the residuals. In order to address this, we will add ETS(A,N,A) element to the model, estimating ETSX instead of just regression: adamModelSeat03 &lt;- adam(Seatbelts, &quot;ANA&quot;, formula=drivers~PetrolPrice+kms+law) We can produce similar plots to do model diagnostics (Figue 15.5): par(mfcol=c(1,2)) plot(adamModelSeat03,7:8) Figure 15.5: Diagnostic plots for Model 3. In Figure 15.5, we do not see any apparent missing structure in the data and any obvious omitted variables. We can now move to the next steps of diagnostics. References "],["diagnosticsRedundant.html", "15.2 Model specification: Redundant variables", " 15.2 Model specification: Redundant variables While there are some ways of testing for omitted variables, the redundant ones are challenging to diagnose. Yes, we could look at the significance of variables (Section 5.3 of Svetunkov, 2021) or compare models with and without some variables based on information criteria (Section 13.4 of Svetunkov, 2021). Still, even if our approaches say that a variable is not significant, this does not mean that it is not needed in the model. There can be many reasons why a test would fail to reject H\\(_0\\), and AIC would prefer a model without the variable under consideration. So, it comes to using judgment, trying to figure out whether a variable is needed in the model or not. In the example with Seatbelt data, DriversKilled would be a redundant variable for the reasons explained in Section 15.1. Let us see what happens with the model if we include it: adamModelSeat04 &lt;- adam(Seatbelts, &quot;NNN&quot;, formula=drivers~PetrolPrice+kms+ law+DriversKilled) par(mfcol=c(1,2)) plot(adamModelSeat04,7:8) Figure 15.6: Diagnostic plots for Model 4. The residuals from this model look adequate, with the only issue being the first 45 observations lying below the zero line. The summary of this model is: summary(adamModelSeat04) ## ## Model estimated using alm() function: Regression ## Response variable: drivers ## Distribution used in the estimation: Normal ## Loss function type: likelihood; Loss function value: 1189.274 ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## (Intercept) 905.6559 115.0935 678.6073 1132.6294 * ## PetrolPrice -1603.7772 827.8145 -3236.8326 28.7384 ## kms -0.0112 0.0035 -0.0182 -0.0043 * ## law -91.2672 31.9765 -154.3483 -28.2070 * ## DriversKilled 9.0423 0.3831 8.2866 9.7978 * ## ## Error standard deviation: 120.1081 ## Sample size: 192 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 187 ## Information criteria: ## AIC AICc BIC BICc ## 2388.549 2388.871 2404.836 2405.684 The uncertainty around the parameter DriversKilled is narrow, showing that the variable positively impacts the drivers. However, the issue here is not statistical but rather fundamental: we have included the variable that is a part of our response variable. It does not explain why drivers get injured and killed, and it just reflects a specific element of this relation. So it approximates part of the variance, which should have been explained by other variables (e.g. kms and law), making them statistically not significant. So, based on the technical analysis, we would be inclined to keep the variable, but based on our understanding of the problem, we should not. If we have redundant variables in the model, then the model might overfit the data, leading to narrower prediction intervals and biased forecasts. The parameters of such a model are typically unbiased but inefficient (Section 4.3 of Svetunkov, 2021). References "],["diagnosticsTransformations.html", "15.3 Model specification: Transformations", " 15.3 Model specification: Transformations The question of appropriate transformations for variables in the model is challenging, because it is difficult to decide, what sort of transformation is needed, if needed at all. In many cases, this comes to selecting between additive linear model and a multiplicative one. This implies that we compare the model: \\[\\begin{equation} y_t = a_0 + a_1 x_{1,t} + \\dots + a_n x_{n,t} + \\epsilon_t, \\tag{15.1} \\end{equation}\\] and \\[\\begin{equation} y_t = \\exp\\left(a_0 + a_1 x_{1,t} + \\dots + a_n x_{n,t} + \\epsilon_t\\right) . \\tag{15.2} \\end{equation}\\] (15.2) is equivalent to the so called “log-linear” model, but can also include logarithms of explanatory variables instead of the variables themselves if needed. There are different ways to diagnose the problem with wrong transformations. The first one is the actuals vs fitted plot (Figure 15.7): plot(adamModelSeat03, 1, main=&quot;&quot;) Figure 15.7: Actuals vs Fitted for Model 3. The grey dashed line on the plot in Figure 15.7 corresponds to the situation when actuals and fitted coincide (100% fit). The red line on the plot is the LOWESS line (Cleveland, 1979), produced by the lowess() function in R, smoothing the scatterplot to reflect the potential tendencies in the data. This red line should coincide with the grey line in the ideal situation. In addition, the variability around the line should not change with the increase of fitted values. In our case, there is a slight U-shape in the red line and a slight rise in variability around the middle of the data. This could either be due to pure randomness and thus should be ignored or indicate a slight non-linearity in the data. After all, we have constructed a pure additive model on the data that exhibits seasonality with multiplicative characteristics, which becomes especially apparent at the end of the series, where the drop in level is accompanied by the decrease of the variability of the data (Figure 15.8): plot(adamModelSeat03, 7, main=&quot;&quot;) Figure 15.8: Actuals and Fitted values for Model 3. To diagnose this properly, we might use other instruments. One of these is the analysis of standardised residuals. The formula for the standardised residuals \\(u_t\\) will differ depending on the assumed distribution and for some of them comes to the value inside the “\\(\\exp\\)” part of the probability density function: Normal, \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\): \\(u_t = \\frac{e_t -\\bar{e}}{\\hat{\\sigma}}\\); Laplace, \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\): \\(u_t = \\frac{e_t -\\bar{e}}{\\hat{s}}\\); S, \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\): \\(u_t = \\frac{e_t -\\bar{e}}{\\hat{s}^2}\\); Generalised Normal, \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\): \\(u_t = \\frac{e_t -\\bar{e}}{\\hat{s}^{\\frac{1}{\\beta}}}\\); Inverse Gaussian, \\(1+\\epsilon_t \\sim \\mathcal{IG}(1, \\sigma^2)\\): \\(u_t = \\frac{1+e_t}{\\bar{e}}\\); Gamma, \\(1+\\epsilon_t \\sim \\mathcal{\\Gamma}(\\sigma^{-2}, \\sigma^2)\\): \\(u_t = \\frac{1+e_t}{\\bar{e}}\\); Log Normal, \\(1+\\epsilon_t \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\\): \\(u_t = \\frac{e_t -\\bar{e} +\\frac{\\hat{\\sigma}^2}{2}}{\\hat{\\sigma}}\\). Here \\(\\bar{e}\\) is the mean of residuals, which is typically assumed to be zero, and \\(u_t\\) is the value of standardised residuals. Note that the scales in the formulae above should be calculated via the formula with the bias correction, i.e. with the division by degrees of freedom, not the number of observations. Also, note that in the case of \\(\\mathcal{IG}\\), \\(\\Gamma\\) and \\(\\mathrm{log}\\mathcal{N}\\) and additive error models, the formulae for the standardised residuals will be the same, only the assumptions will change (see Section 5.5). Here is an example of a plot of fitted vs standardised residuals in R (Figure 15.9): plot(adamModelSeat03, 2, main=&quot;&quot;) Figure 15.9: Standardised residuals vs Fitted for pure additive ETSX model. Given that the scale of the original variable is now removed in the standardised residuals, it might be easier to spot the non-linearity. In our case, in Figure 15.9, it is still not apparent, but there is a slight curvature in the LOWESS line and a slight change in the variance. Another plot that we have already used before is standardised residuals over time (Figure 15.10): plot(adamModelSeat03, 8, main=&quot;&quot;) Figure 15.10: Standardised residuals vs Time for pure additive ETSX model. The plot in Figure 15.10 does not show any apparent non-linearity in the residuals, so it is not clear whether any transformations are needed or not. However, based on my judgment and understanding of the problem, I would expect the number of injuries and deaths to change proportionally to the change of the level of the data. If, after some external interventions, the overall level of injuries and deaths would decrease, we would expect a percentage decline, not a unit decline with a change of already existing variables in the model. This is why I will try a multiplicative model next (transforming explanatory variables as well): adamModelSeat05 &lt;- adam(Seatbelts, &quot;MNM&quot;, formula=drivers~log(PetrolPrice)+log(kms)+law) plot(adamModelSeat05, 2, main=&quot;&quot;) Figure 15.11: Standardised residuals vs Fitted for pure multiplicative ETSX model. The plot in Figure 15.11 shows that the variability is now slightly more uniform across all fitted values, but the difference between Figures 15.9 and 15.11 is not very prominent. One of the potential solutions in this situation is to compare the models in terms of information criteria: setNames(c(AICc(adamModelSeat03), AICc(adamModelSeat05)), c(&quot;Additive model&quot;, &quot;Multiplicative model&quot;)) ## Additive model Multiplicative model ## 2424.123 2406.366 Based on this, we would be inclined to select the multiplicative model. My judgment in this specific case agrees with the information criterion. We could also investigate the need for transformations of explanatory variables, but the interested reader is encouraged to do this analysis on their own. Finally, the non-linear transformations are not limited with logarithm only, there are more of them, some of which are discussed in Chapter 11 of Svetunkov (2021). References "],["diagnosticsOutliers.html", "15.4 Model specification: Outliers", " 15.4 Model specification: Outliers One of the important assumptions in forecasting and analytics is the correct specification of the model, which implies that there are no outliers in the model. They might appear for several reasons: We missed some important information (e.g. promotion) and did not include a respective variable in the model; There was an error in recordings of the data, i.e. a value of 2000 was recorded as 200; We did not miss anything predictable, we just dealt with a distribution with fat tails. In any of these cases, outliers might impact estimates of parameters of our models. With ETS, this will lead to higher than needed smoothing parameters, which leads to wider prediction intervals and potentially biased forecasts. In the case of ARIMA, the mechanism is more complicated, leading to widened intervals and biased forecasts. Finally, in regression, they might lead to biased estimates of parameters. So, it is important to identify outliers and deal with them. 15.4.1 Outliers detection One of the simplest ways of identifying outliers is based on distributional assumptions. For example, if we assume that our data follows the normal distribution, we would expect 95% of observations to lie inside the bounds with approximately \\(\\pm 1.96\\sigma\\) and 99.8% of them to lie inside the \\(\\pm 3.09 \\sigma\\). Sometimes these values are substituted by heuristics “values lying inside 2 / 3 sigmas,” which is not precise and works only for Normal distribution. Still, based on this, we could flag the values outside these bounds and investigate them to see if any of them are indeed outliers. Given that the ADAM framework supports different distributions, the heuristics mentioned above is inappropriate. We need to get proper quantiles for each of the assumed distributions. Luckily, this is not difficult because the quantile functions for all the distributions supported by ADAM either have analytical forms or can be obtained numerically. Here is an example in R with the same multiplicative ETSX model and the standardised residuals vs fitted values with the 95% bounds (Figure 15.12): plot(adamModelSeat05, 2, level=0.95, main=&quot;&quot;) Figure 15.12: Standardised residuals vs Fitted for the pure multiplicative ETSX model. Note that in the case of \\(\\mathcal{IG}\\), \\(\\Gamma\\) and \\(\\mathrm{log}\\mathcal{N}\\), the function will plot \\(\\log u_t\\) to make the plot more readable. The plot in Figure 15.12 demonstrates that there are outliers, some of which are further away from the bounds. Although the amount of outliers is not large, it would make sense to investigate why they happened. Given that we deal with time series, plotting residuals vs time is also sometimes helpful (Figure 15.13): plot(adamModelSeat05, 8, main=&quot;&quot;) Figure 15.13: Standardised residuals vs Time for the pure multiplicative ETSX model. We see in Figure 15.13 that there is no specific pattern in the outliers, they happen randomly, so they appear not because of the omitted variables or wrong transformations. We have nine observations lying outside the bounds, which given that the sample size of 192 observations, means that the 95% interval contains \\(\\frac{192-9}{192} \\times 100 \\% \\approx 95.3 \\%\\) of observations, which is close to the nominal value. In some cases, the outliers might impact the scale of distribution and will lead to wrong standardised residuals, distorting the picture. This is where studentised residuals come into play. They are calculated similarly to the standardised ones, but the scale of distribution is recalculated for each observation by considering errors on all but the current observation. So, in a general case, this is an iterative procedure that involves looking through \\(t=\\{1,\\dots,T\\}\\) and that should, in theory, guarantee that the real outliers do not impact the scale of distribution. This procedure is simplified for the normal distribution and has an analytical solution. We do not discuss it in the context of ADAM. Here is how they can be analysed in R: par(mfcol=c(2,1)) plot(adamModelSeat05, c(3,9)) Figure 15.14: Studentised residuals analysis for the pure multiplicative ETSX model. In many cases (ours included), the standardised and studentised residuals will look very alike, having a similar amount of outliers. But in some cases of extreme outliers, they might differ, and the latter might show outliers better than the former. Given the situation with outliers in our case, we could investigate when they happen in the original data to understand better whether they need to be taken care of. But instead of manually recording which of the observations lie beyond the bounds, we can get their ids via the outlierdummy method from the package greybox, which extracts either standardised or studentised residuals and flags those observations that lie outside the constructed interval, automatically creating dummy variables for these observations. Here is how it works: adamModelSeat05Outliers &lt;- outlierdummy(adamModelSeat05, level=0.95, type=&quot;rstandard&quot;) The method returns several objects (see the documentation for details), including the ids of outliers: adamModelSeat05Outliers$id ## [1] 14 33 61 66 86 92 109 156 177 These ids can be used to produce additional plots. For example: # Plot actuals plot(actuals(adamModelSeat05)) lines(fitted(adamModelSeat05),col=&quot;grey&quot;) # Add points for the outliers points(time(Seatbelts[,&quot;drivers&quot;])[adamModelSeat05Outliers$id], Seatbelts[adamModelSeat05Outliers$id,&quot;drivers&quot;], col=&quot;red&quot;, pch=16) # Add the text with ids of outliers text(time(Seatbelts[,&quot;drivers&quot;])[adamModelSeat05Outliers$id], Seatbelts[adamModelSeat05Outliers$id,&quot;drivers&quot;], adamModelSeat05Outliers$id, col=&quot;red&quot;, pos=2) Figure 15.15: Actuals over time with points corresponding to outliers of the pure multiplicative ETSX model. We cannot see any peculiarities in the appearance of outliers in Figure 15.15. They seem to happen at random. There might be some external factors, leading to those unexpected events (for example, the number of injuries being much lower than expected on observation 156, in November 1981), but investigation of these events is outside of the scope of this demonstration. Remark. As a side note, in R, there are several methods for extracting residuals: resid() or residuals() will extract either \\(e_t\\) or \\(1+e_t\\), depending on the distributional assumptions of the model; rstandard() will extract the standardised residuals \\(u_t\\); rstudent() will do the same for the studentised ones. The smooth package also introduces the rmultistep function, which extracts multiple steps ahead in-sample forecast errors. We do not discuss this method here, but we will return to it in Section 15.7.1. 15.4.2 Dealing with outliers Based on the output of the outlierdummy() method from the previous example, we can construct a model with explanatory variables to interpolate the outliers and neglect their impact on the model: # Add outliers to the data SeatbeltsWithOutliers &lt;- cbind(as.data.frame(Seatbelts[,-c(1,3,4,7)]), adamModelSeat05Outliers$outliers) # Transform the drivers variable into time series object SeatbeltsWithOutliers$drivers &lt;- ts(SeatbeltsWithOutliers$drivers, start=start(Seatbelts), frequency=frequency(Seatbelts)) # Run the model with all the explanatory variables in the data adamModelSeat06 &lt;- adam(SeatbeltsWithOutliers, &quot;MNM&quot;, lags=12, formula=drivers~.) In order to decide, whether the dummy variables help or not, we can use information criteria, comparing the two models: setNames(c(AICc(adamModelSeat05), AICc(adamModelSeat06)), c(&quot;ETSX&quot;, &quot;ETSXOutliers&quot;)) ## ETSX ETSXOutliers ## 2406.366 2377.058 Comparing the two values above, we would conclude that adding dummies improves the model. However, this could be a mistake, given that we do not know the reasons behind most of them. In general, we should not include dummy variables for the outliers unless we know why they happened. If we do, we might overfit the data. Still, if we have good reasons for this, we could add explanatory variables for outliers in the function to remove their impact on the response variable: adamModelSeat07 &lt;- adam(SeatbeltsWithOutliers, &quot;MNM&quot;, lags=12, formula=drivers~PetrolPrice+kms+ law+outlier8) AICc(adamModelSeat07) ## [1] 2403.929 While this model is worse than the one with all the outliers, it would have a better theoretical rationale than the model adamModelSeat06. 15.4.3 An automatic mechanism A similar automated mechanism is implemented in the adam() function, which has the outliers parameter, defining what to do with them if there are any with the following three options: “ignore” – do nothing; “use” – create the model with explanatory variables including all of them, as shown in the previous subsection, and see if it is better than the simpler model in terms of an information criterion; “select” – create lags and leads of dummies from outlierdummy() and then select the dummies based on the explanatory variables selection mechanism (discussed in Section 16.3). Lags and leads are needed for cases when the effect of outlier is carried over to neighbouring observations. Here how this works in our case: adamModelSeat08 &lt;- adam(Seatbelts, &quot;MNM&quot;, lags=12, formula=drivers~PetrolPrice+kms+law, outliers=&quot;select&quot;, level=0.95) AICc(adamModelSeat08) ## [1] 2401.043 This automated procedure will form a matrix that will include original variables and the outliers, their lags, and leads and then select those that minimise AICc in a stepwise process (discussed in Section 16.3). Given that this is an automated approach, it is prone to potential mistakes. It needs to be treated with care as it might select unnecessary dummy variables and lead to overfitting. I would recommend exploring the outliers manually when possible and not relying too much on the automated procedures. 15.4.4 Final remarks Koehler et al. (2012) explored the impact of outliers on ETS performance in terms of forecasting accuracy. They found that if outliers happen at the end of the time series, it is important to take them into account in a model. If they happen much earlier, their impact on the final forecast will be negligible. Unfortunately, the authors did not explore the impact of outliers on the prediction intervals. Based on my experience, I can tell that the outliers typically impact the width of the interval rather than the point forecasts. References "],["diagnosticsResidualsIIDAuto.html", "15.5 Residuals are i.i.d.: autocorrelation", " 15.5 Residuals are i.i.d.: autocorrelation One of the typical characteristics of time series models is the dynamic relation between variables. Even if fundamentally, the sales of ice cream on Monday do not impact sales of the same ice cream on Tuesday, they might impact advertising expenses or sales of a competing product on Tuesday, Wednesday or next week. Missing this structure might lead to the autocorrelation of residuals, influencing the estimates of parameters and final forecasts. Autocorrelations might also arise due to wrong transformations of variables, where the model would systematically underforecast the actuals, producing autocorrelated residuals. In this section, we will see one of the potential ways for the regression diagnostics and try to improve the model stepwise, trying out different orders of the ARIMA model (Section 9). As an example, we continue with the same seatbelts data, dropping the dynamic part to see what would happen in this case: adamModelSeat09 &lt;- adam(Seatbelts, &quot;NNN&quot;, lags=12, formula=drivers~log(PetrolPrice)+log(kms)+law) AICc(adamModelSeat09) ## [1] 2651.191 There are different ways to diagnose this model. We start with a basic plot of residuals over time (Figure 15.16): plot(adamModelSeat09, 8, main=&quot;&quot;) Figure 15.16: Standardised residuals vs time for the model 9. We see in Figure 15.16 that on one hand the residuals still contain seasonality and on the other, they do not look stationary. We could conduct ADF and / or KPSS tests (which will be discussed in one of the later Chapters of Svetunkov, 2021) to get a formal answer to the stationarity question: tseries::kpss.test(resid(adamModelSeat09)) ## Warning in tseries::kpss.test(resid(adamModelSeat09)): p-value greater than ## printed p-value ## ## KPSS Test for Level Stationarity ## ## data: resid(adamModelSeat09) ## KPSS Level = 0.12844, Truncation lag parameter = 4, p-value = 0.1 tseries::adf.test(resid(adamModelSeat09)) ## Warning in tseries::adf.test(resid(adamModelSeat09)): p-value smaller than ## printed p-value ## ## Augmented Dickey-Fuller Test ## ## data: resid(adamModelSeat09) ## Dickey-Fuller = -6.8851, Lag order = 5, p-value = 0.01 ## alternative hypothesis: stationary The tests have opposite null hypotheses, and in our case, we would fail to reject H\\(_0\\) on 1% for the KPSS test and reject H\\(_0\\) on 1% for the ADF, which means that the residuals look stationary. The main problem in the residuals is the seasonality, which formally makes the residuals non-stationary (their mean changes from month to month), but which cannot be detected by these tests. Yes, there is a Canova-Hansen test, which is implemented in ch.test function in uroot package in R, which tests the seasonal unit root, but I instead of trying it out and coming to conclusions, I will try the model in seasonal differences and see if it is better than the one without it: # SARIMAX(0,0,0)(0,1,0)_12 adamModelSeat10 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~log(PetrolPrice)+log(kms)+law, orders=list(i=c(0,1))) AICc(adamModelSeat10) ## [1] 2547.274 Remark. While in general models in differences are not comparable with the models applied to the original data, adam() allows such comparison, because ARIMA model implemented in it is initialised before the start of the sample and does not loose any observations. This leads to an improvement in AICc in comparison with the previous model. The residuals of the model are now also better behaved (Figure 15.17): plot(adamModelSeat10, 8, main=&quot;&quot;) Figure 15.17: Standardised residuals vs time for the model 10. In order to see whether there are any other dynamic elements left, we will plot ACF and PACF (discussed in Subsections 8.3.2 and 8.3.3) of residuals (Figure 15.18): par(mfcol=c(2,1)) plot(adamModelSeat10, 10:11, level=0.99) Figure 15.18: ACF and PACF of residuals of the model 10. In the case of the adam() objects, these plots, by default, will always have the range for the y-axis from -1 to 1 and will start from lag one on the x-axis. The red horizontal lines represent the “non-rejection” region. If a point lies inside the region, it is not statistically different from zero on the selected confidence level (the uncertainty around it is so high that it covers zero). The points with numbers are those that are statistically significantly different from zero. So, the ACF / PACF analysis might show the statistically significant lags on the selected level (the default one is 0.95). Given that this is a statistical instrument, we expect that approximately (1-level)% (e.g. 1%) of lags lie outside these bounds just due to randomness, even if the null hypothesis is correct. So it is okay if we do not see all points lying inside them. However, we should not see any patterns there, and we might need to investigate the suspicious lags (low orders of up to 3 – 5 and the seasonal lags if they appear). In our example in Figure 15.18, we see that there are spikes in lag 12 for both ACF and PACF, which means that we have missed the seasonal element in the data. There is also a suspicious lag 1 on PACF and lags 1 and 2 on ACF, which could potentially indicate that MA(1) and/or AR(1), AR(2) elements are needed in the model. While it is not clear what specifically is needed here, we can try out several models and see which one is better to determine the appropriate order of ARIMA. We should start with the seasonal part of the model, as it might obscure the non-seasonal one. # SARIMAX(0,0,0)(1,1,0)_12 adamModelSeat11 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~log(PetrolPrice)+log(kms)+law, orders=list(ar=c(0,1),i=c(0,1))) AICc(adamModelSeat11) ## [1] 2534.39 # SARIMAX(0,0,0)(0,1,1)_12 adamModelSeat12 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~log(PetrolPrice)+log(kms)+law, orders=list(i=c(0,1),ma=c(0,1))) AICc(adamModelSeat12) ## [1] 2426.688 # SARIMAX(0,0,0)(1,1,1)_12 adamModelSeat13 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~log(PetrolPrice)+log(kms)+law, orders=list(ar=c(0,1),i=c(0,1),ma=c(0,1))) AICc(adamModelSeat13) ## [1] 2542.092 Based on this analysis, we would be inclined to include in the model seasonal MA(1) only. Next step in our iterative process – another ACF / PACF plot of the residuals: Figure 15.19: ACF and PACF of residuals of the model 12. In this case, there is a spike on PACF for lag 1 and a textbook exponential decrease in ACF starting from lag 1, which might mean that we need to include AR(1) component in the model: # ARIMAX(1,0,0)(0,1,1)_12 adamModelSeat14 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~log(PetrolPrice)+log(kms)+law, orders=list(ar=c(1,0),i=c(0,1),ma=c(0,1))) AICc(adamModelSeat14) ## [1] 2386.444 Choosing between the new model and the old one, we should give preference to the model 14, which has a lower AICc than model 12. So, adding MA(1) to the model leads to further improvement in AICc compared to the previous models. Using this iterative procedure, we could continue our investigation to find the most suitable SARIMAX model for the data. Nevertheless, this example should suffice in providing a general idea of how it can be done. What we could do else to simplify the process is to use the automated ARIMA selection algorithm (see discussion in Section 16.2) in adam(), which is built on the principles discussed in this section: adamModelSeat15 &lt;- adam(Seatbelts,&quot;NNN&quot;,lags=12, formula=drivers~log(PetrolPrice)+log(kms)+law, orders=list(ar=c(3,2),i=c(2,1),ma=c(3,2),select=TRUE)) adamModelSeat15$model ## [1] &quot;SARIMAX(1,0,0)[1](0,1,1)[12]&quot; AICc(adamModelSeat15) ## [1] 2386.444 This new constructed model is SARIMAX(1,0,0)(0,1,1)\\(_{12}\\), which is the model we achieved manually. Note that it is even better than the ETSX(MNM) (model 5) from the previous section in terms of AICc. Its residuals are much better behaved than the ones of model 5 (although we might need to analyse the residuals for the potential outliers, as discussed in Subsection 15.4): par(mfcol=c(3,1)) plot(adamModelSeat15, c(8,10:11), level=0.99) Figure 15.20: Residuals of the model 16. So for the purposes of analytics and forecasting, we would be inclined to use SARIMAX(2,0,0)(0,1,1)\\(_{12}\\) rather than ETSX(MNM). As a final word for this section, we have focused our discussion on the visual analysis of time series, ignoring the statistical tests (we only used ADF and KPSS). Yes, there is Durbin-Watson (Wikipedia, 2021f) test for AR(1) in residuals, and yes, there are Ljung-Box (Wikipedia, 2021g), Box-Pierce and Breusch–Godfrey (Wikipedia, 2021h) tests for multiple AR elements. But visual inspection of time series is not less powerful than hypothesis testing. It makes you think and analyse the model and its assumptions, while the tests are the lazy way out that might lead to wrong conclusions because they have the standard limitations of any hypothesis tests (as discussed in Section 5.3 of Svetunkov, 2021). After all, if you fail to reject H\\(_0\\), it does not mean that the effect does not exist. Having said that, the statistical tests become extremely useful when you need to process many time series simultaneously and cannot inspect them manually. So, if you are in that situation, I would recommend reading more about them, but I do not aim to retell the content of Wikipedia in this textbook. References "],["diagnosticsResidualsIIDHetero.html", "15.6 Residuals are i.i.d.: heteroscedasticity", " 15.6 Residuals are i.i.d.: heteroscedasticity Another vital assumption for conventional models is that the residuals are homoscedastic, meaning that the variance of the residuals stays the same (no matter what). This section will see how the issue can be resolved in some cases. 15.6.1 Detecting heteroscedasticity Building upon our previous example, we will use the ETSX(A,N,A) model, which has some issues, as we remember from Section 15.3. One of those is the wrong type of model – additive instead of multiplicative. This is also related to the variance of residuals. To see if they are homoscedastic, we can plot them against the fitted values (Figure 15.21): par(mfcol=c(1,2)) plot(adamModelSeat03,4:5) Figure 15.21: Absolute and Squared residuals vs Fitted of model 3. The two plots in Figure 15.21 allow detecting a specific type of heteroscedasticity when the residuals variability changes with the increase of fitted values. The plot of absolute residuals vs fitted is more appropriate for models, where the scale parameter is calculated based on absolute values of residuals (e.g. the model with Laplace distribution) and relates to MAE (Subsection 11.2.1), while the squared residuals vs fitted shows whether the variance of residuals is stable or not (thus making it more suitable for models with Normal and related distributions). Furthermore, the squared residuals plot might be challenging to read due to outliers, so the first one might help detect the heteroscedasticity even when the scale is supposed to rely on squared errors. What we want to see on these plots is for all the points to lie in the same corridor for lower and for the higher fitted values and for the red line to be parallel to the x-axis. In our case, there is a slight increase in the line. Furthermore, the variability of residuals around 1000 is lower than the one around 2000, indicating that we have heteroscedasticity in residuals. In our case, this is caused by the wrong transformations in the model (see Section 15.3), so to fix the issue, we should switch to a multiplicative model. Another diagnostics tool that might become useful in some situations is the plot of absolute and squared standardised residuals versus fitted values. They have a similar idea to the previous plots, but they might change slightly because of the standardisation (mean is equal to 0 and scale is equal to 1). These plots become especially useful if the changing variance is modelled explicitly (e.g. via a regression model or a GARCH-type of model. This feature is not yet supported in ADAM): par(mfcol=c(2,1)) plot(adamModelSeat03,13:14) Figure 15.22: Absolute and Squared standardised residuals vs Fitted of model 3. In our case, the plots in Figure 15.22 do not give an additional message. We already know that there is slight heteroscedasticity and that we need to transform the response variable. If we suspect that there are some specific variables that might cause heteroscedasticity, we can plot absolute or squared residuals vs these variables to see if they are indeed cause it. For example, here how we can produce a basic plot of residuals vs all explanatory variables included in the model: spread(cbind( as.data.frame(abs(resid(adamModelSeat03))), adamModelSeat03$data[,all.vars(formula(adamModelSeat03))[-1]]), lowess=TRUE) Figure 15.23: Spread plot of absolute residuals vs variables included in the model 3. The plot in Figure 15.23 can be read similarly to the plots discussed above: if we notice a change in variability of residuals or a change (increase or decrease) in the lowess lines with the change of a variable, then this might indicate that the respective variable causes heteroscedasticity. In our example, it looks like the variable law causes the most significant issue – all the other variables do not cause as substantial change in the variance as this one. We already know that we need to use a multiplicative model instead of the additive one in our example, so we will see how the residuals look for the correctly specified model in Figure 15.24. Figure 15.24: Absolute and Squared residuals vs Fitted of model 5. The plots in Figure 15.24 do not demonstrate any substantial issues: the residuals look homoscedastic, and given the scale of residuals, the change of lowess line does not reflect significant changes in the residuals. An additional plot of absolute residuals vs explanatory variables does not show any severe issues (Figure 15.25). So, we can conclude that the multiplicative model resolves the issue with heteroscedasticity. spread(cbind( as.data.frame(abs(log(resid(adamModelSeat05)))), adamModelSeat05$data[,all.vars(formula(adamModelSeat05))[-1]]), lowess=TRUE) Figure 15.25: Spread plot of absolute residuals vs variables included in the model 5. If a variable would still cause an issue with it, it would make sense to construct a model for the variance to address the problem and improve the model’s performance (e.g. a scale model, such as GARCH, see discussion in Chapter 14). There are formal statistical tests for heteroscedasticity, such as White (Wikipedia, 2021i), Breusch-Pagan (Wikipedia, 2021j) and Bartlett’s (Wikipedia, 2021k) tests and others. We do not discuss them here for a reason outlined in Section 15.5. References "],["diagnosticsResidualsIIDExpectation.html", "15.7 Residuals are i.i.d.: zero expectation", " 15.7 Residuals are i.i.d.: zero expectation This assumption only works for the additive error models (Section 5.1). In the case of the multiplicative error models, it is changed to “expectation of the error term is equal to one” (Section 6.5). It does not make sense to check this assumption unconditionally because it does not mean anything in-sample. Yes, it will hold automatically for the residuals of a model in the case of OLS estimation. The observed mean of the residuals might not be equal to zero in other cases, but this does not give any helpful information. In fact, when we work with exponential smoothing models, the in-sample residuals being equal to zero might imply for some of them that the final values of components are identical to the initial ones. For example, in the case of ETS(A,N,N) (from Section 4.2), we can use the measurement equation from (4.7) to express the final value of level via the previous values up until \\(t=0\\): \\[\\begin{equation} \\begin{aligned} \\hat{l}_t &amp;= \\hat{l}_{t-1} + \\hat{\\alpha} e_t = \\hat{l}_{t-2} + \\hat{\\alpha} e_{t-1} + \\hat{\\alpha} e_t = \\\\ &amp; \\hat{l}_0 + \\hat{\\alpha} \\sum_{j=1}^t e_{t-j} . \\end{aligned} \\tag{15.3} \\end{equation}\\] If the mean of the residuals in-sample is indeed equal to zero, then the equation (15.3) reduces to \\(\\hat{l}_t=\\hat{l}_0\\). So, this assumption cannot be checked in-sample, meaning that it is all about the true model and the asymptotic behaviour rather than the model applied to the data. On the other hand, if for some reason the mean of residuals is not equal to zero in the population, then the model will change. For example, if we have ETS(A,N,N) model with the non-zero mean of residuals \\(\\mu_\\epsilon\\), then the residuals can be represented in the form \\(\\epsilon_t = \\mu_\\epsilon + \\xi_t\\), where \\(\\mathrm{E}(\\xi_t)=0\\) which leads to a different model than ETS(A,N,N): \\[\\begin{equation} \\begin{aligned} &amp; y_t = l_{t-1} + \\mu_\\epsilon + \\xi_t \\\\ &amp; l_t = l_{t-1} + \\alpha \\mu_\\epsilon + \\alpha \\xi_t \\end{aligned}, \\tag{15.4} \\end{equation}\\] which is a different model than ETS(A,N,N). If we apply ETS(A,N,N) model to the data instead of (15.4), we will omit an important element and thus the estimated smoothing parameter will be higher than needed. The same logic applies to the multiplicative error models: the mean of residuals \\(1+\\epsilon_t\\) should be equal to one for them, otherwise the model would change. This phenomenon arises because of the “pull-to-centre” effect of dynamic models (ETS and ARIMA with non-zero MA terms), where due to the presence of residuals in the transition equations, the model updates the states so that they become closer to the conditional mean of data. Summarising this discussion, the expectation of residuals of the applied ADAM models should be equal to zero asymptotically, which cannot be tested in-sample. The more valuable part of this assumption that can be checked is whether the expectation of the residuals conditional on some variables is equal to zero (or one). In a way, this comes to ensuring that there are no patterns in the residuals and thus no consecutive parts of the data, where residuals have non-zero expectations systematically. There are different ways to diagnose this. The first is the already discussed plot of standardised (or studentised) residuals vs fitted values from Section 15.3. The other one is the plot of residuals over time, which we have already discussed in Section 15.5. In addition, you can also plot residuals vs some of the variables to see if they cause the change in mean. But in a way, all these methods might also mean that the residuals are autocorrelated and/or some transformations of variables are needed. There is also an effect related to this, called “endogeneity” (discussed briefly in Section 12.3 of Svetunkov, 2021). According to econometrics literature, it implies that the residuals are correlated with some variables. This becomes equivalent to the situation when the expectation of residuals changes with the change of a variable. The most prominent cause of this is the omitted variables (discussed in Section 15.1), which can be sometimes diagnosed by looking at correlations between the residuals and omitted variables if the latter are available. While econometricians propose using other estimation methods (such as Instrumental Variables) to diminish the effect of endogeneity, the forecasters cannot do that because we need to fix the problem to get more reasonable forecasts rather than better estimates of parameters. Unfortunately, there is no universal recipe for the solution of this problem, but in some cases transforming variables, adding the omitted ones or substituting them by proxies (if the omitted variables are unavailable) might resolve the issue to some extent. 15.7.1 Multistep forecast errors have zero mean This follows from the previous assumption if the model is correctly specified and its residuals are i.i.d. In that situation, we would expect the multiple steps ahead forecast errors to have zero mean. In practice, this might be violated if some structural changes or level shifts are not taken into account by the model. The only thing to note is that this approach requires defining the forecast horizon \\(h\\). This should typically come from the task itself and the decisions made. The diagnostics of this assumption can be done using the rmultistep() method for adam(). This method would apply the estimated model and produce multiple steps ahead forecasts from each in-sample observation to the horizon \\(h\\), stacking the forecast errors by rows. Whether we use an additive or multiplicative error model, the method will report the residual \\(e_t\\). Here is an example of the code for extraction and plotting of multistep forecast errros for the multiplicative model 5 from the previous sections: # Extract multistep errors adamModelSeat05ResidMulti &lt;- rmultistep(adamModelSeat05, h=12) # Give adequate names to the columns colnames(adamModelSeat05ResidMulti) &lt;- c(1:12) # Produce boxplots boxplot(adamModelSeat05ResidMulti, xlab=&quot;horizon&quot;) # Add the zero line abline(h=0, col=&quot;red&quot;) # Add mean values points(apply(adamModelSeat05ResidMulti,2,mean), col=&quot;red&quot;, pch=16) Figure 15.26: Boxplot of multistep forecast errors extracted from model 5. As the plot in Figure 15.26 demonstrates, the mean of the residuals does not increase substantially with the increase of the forecast horizon, which indicates that the model has captured the tendencies in the data correctly. References "],["diagnosticsResidualsIIDDistribution.html", "15.8 Residuals are i.i.d.: distributional assumptions", " 15.8 Residuals are i.i.d.: distributional assumptions Finally, we come to the distributional assumptions of ADAM. As discussed earlier (for example, in Section 11.1), the ADAM framework supports several distributions. The specific parts of assumptions will change depending on the type of error term in the model. Given that, it is relatively straightforward to see if the residuals of the model follow the assumed distribution or not. There exist several tools for that. The simplest one is called Quantile-Quantile (QQ) plot. It produces a figure with theoretical vs actual quantiles and shows whether they are close to each other or not. Here is, for example, how the QQ plot will look for one of the previous models, assuming Normal distribution (Figure 15.27): plot(adamModelSeat03, 6) Figure 15.27: QQ plot of residuals extracted from model 3. If the residuals do not contradict the assumed distribution, all the points should lie either very close to or on the line. In our case, in Figure 15.27, most points are close to the line, but the tails (especially the right one) are slightly off. This might mean that we should either use a different error type or a different distribution. Just for the sake of argument, we can try ETSX(M,N,M) model, with the same set of explanatory variables as in the model adamModelSeat03, and with the same Normal distribution: adamModelSeat16 &lt;- adam(Seatbelts, &quot;MNM&quot;, formula=drivers~log(PetrolPrice)+log(kms)+law, distribution=&quot;dnorm&quot;) plot(adamModelSeat16, 6) Figure 15.28: QQ plot of residuals extracted from multiplicative model with normal distribution. According to the QQ plot in Figure 15.28, the residuals of the new model are much closer to the theoretical ones. Only the right tail has a slight deviation from normality – the actual values are a bit further away than expected. This can be addressed by using a skewed distribution, for example, Gamma: adamModelSeat17 &lt;- adam(Seatbelts, &quot;MNM&quot;, formula=drivers~log(PetrolPrice)+log(kms)+law, distribution=&quot;dinvgauss&quot;) plot(adamModelSeat17, 6) Figure 15.29: QQ plot of residuals extracted from the multiplicative model with Gamma distribution. The QQ plot in Figure 15.29 does not demonstrate any significant improvement in comparison with the previous model. We could use AICc to select between the two models if we are not sure, which of them to prefer: AICc(adamModelSeat16) ## [1] 2405.49 AICc(adamModelSeat17) ## [1] 2406.554 Based on these results, we can conclude that the Normal distribution is more suitable for this situation than the Inverse Gaussian one. Another way to analyse the distribution of residuals is to plot histogram together with the theoretical density function. Here is an example: # Plot histogram of residuals hist(residuals(adamModelSeat03), probability=TRUE, xlab=&quot;Residuals&quot;, main=&quot;&quot;, ylim=c(0,0.0035)) # Add density line of the theoretical distribution lines(seq(-400,400,1), dnorm(seq(-400,400,1), 0, sd(residuals(adamModelSeat03))), col=&quot;red&quot;) Figure 15.30: Histogram and density line for the residuals from model 3 (assumed to follow Normal distribution). However, the plot in Figure 15.30 is much more challenging to analyse than the QQ plot – it is not clear whether the distribution is close to the theoretical one or not. So, in general, I would recommend using QQ plots instead. There are also formal tests for the distribution of residuals, such as Shapiro-Wilk (Wikipedia, 2021l) and Kolmogorov-Smirnov (Wikipedia, 2021m). The former tests the hypothesis that residuals follow Normal distribution. The latter is much more flexible and allows comparing the empirical distribution with any other one (theoretical or empirical). However, I prefer to use visual inspection when possible instead of these tests because, as discussed in Section 5.3 of Svetunkov (2021), the null hypothesis is always wrong. It will inevitably be rejected with the increase of the sample size. Besides, if you fail to reject H\\(_0\\), it does not mean that your variable follows the assumed distribution. It only means that you have not found enough evidence to reject the null hypothesis. References "],["diagnosticsMulticollinearity.html", "15.9 Multicollinearity", " 15.9 Multicollinearity For the discussion of multicollinearity and how to diagnose it in the regression model, we refer the reader to Section 12.3 of the Svetunkov (2021) textbook. When it comes to dynamic models, the situation might differ, so we will focus on several aspects that might not be relevant to regression. First, in the conventional ARIMA model (Chapter 8), multicollinearity is inevitable by construct because of the autocorrelations between actual values. This is why sometimes Heteroskedasticity- and autocorrelation-consistent (HAC) estimators of the covariance matrix (see Section 15.4 of Hanck et al., 2020) of parameters are used instead of the standard ones. They are designed to fix the issue and produce standard errors of parameters that are close to those without the problem. Second, in the case of state space models, and specifically in ETS, multicollinearity might not cause as severe issues as in the case of regression. For example, it is possible to use all the values of a categorical variable (Section 10.5), avoiding the trap of dummy variables. The values of a categorical variable, in this case, are considered as changes relative to the baseline. The classic example of this is the seasonal model, for example, ETS(A,A,A), where the seasonal components can be considered as a set of parameters for dummy variables, expanded from the seasonal categorical variable (e.g. months of year variable). If we set \\(\\gamma=0\\), thus making the seasonality deterministic, the ETS can still be estimated even though all variable values are used. This becomes apparent with the conventional ETS model, for example, from the forecast package for R: etsModel &lt;- forecast::ets(AirPassengers, &quot;AAA&quot;) # Calculate determination coefficients for seasonal states determ(etsModel$states[,-c(1:2)]) ## s1 s2 s3 s4 s5 s6 s7 s8 ## 0.9999992 0.9999992 0.9999991 0.9999991 0.9999992 0.9999992 0.9999992 0.9999991 ## s9 s10 s11 s12 ## 0.9999991 0.9999991 0.9999992 0.9999992 As we see, the states of the model are almost perfectly correlated, but still, the model works and does not have the issue that the classical linear regression would have. This is because the state-space models are constructed and estimated differently than the conventional regression (see Section 10). References "],["ADAMSelection.html", "Chapter 16 Model selection and combinations in ADAM", " Chapter 16 Model selection and combinations in ADAM So far, we have managed to avoid discussing the topic of model selection and combinations. However, it is important to understand how to select the most appropriate model and capture the uncertainty around the selection (see discussion of sources of uncertainty in Section 1.3 of Svetunkov, 2021). There are several ways to decide which model to use, and there are several dimensions in which a decision needs to be made: Which of the models to use: ETS / ARIMA / ETS+ARIMA / Regression / ETSX / ARIMAX / ETSX+ARIMA? What components of the ETS model to select? What order of ARIMA model to select? Which of the explanatory variables to use? What distribution to use? Should we select the model or combine forecasts from different ones? Do we need all models in the pool? How should we do all the above? What about the demand occurrence part of the model? (luckily, this question has already been answered in Subsection 13.1.6). In this chapter, we discuss these questions. We start with principles based on information criteria (addressed in Chapter 13 of Svetunkov, 2021) for ETS and ARIMA. We then move to selecting explanatory variables and finish with topics related to the combination of models. Before we do that, we need to recall the distributional assumptions in ADAM, which play an essential role if the model is estimated via the maximisation of the likelihood function (Section 11.1). In this case, an information criterion (IC) can be calculated and used for the selection of the most appropriate model. Based on this, we can fit several ADAM models with different distributions and then select the one that leads to the lowest IC. Here is the list of the supported distributions in ADAM: Normal; Laplace; S; Generalised Normal; Log-Normal; Inverse Gaussian; Gamma. The function auto.adam() implements this automatic selection of distribution based on IC for the provided vector of distribution by a user. This selection procedure can be combined with other selection techniques for different elements of the ADAM model discussed in the following sections of the textbook. Here is an example of selection of distribution for a specific model, ETS(M,M,N) on Box-Jenkins data using auto.adam(): adamModel &lt;- auto.adam(BJsales, model=&quot;MMN&quot;, h=10, holdout=TRUE) adamModel ## Time elapsed: 0.21 seconds ## Model estimated using auto.adam() function: ETS(MMN) ## Distribution assumed in the model: Log-Normal ## Loss function type: likelihood; Loss function value: 245.3716 ## Persistence vector g: ## alpha beta ## 1.0000 0.2406 ## ## Sample size: 140 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 135 ## Information criteria: ## AIC AICc BIC BICc ## 500.7432 501.1909 515.4514 516.5577 ## ## Forecast errors: ## ME: 3.219; MAE: 3.332; RMSE: 3.786 ## sCE: 14.133%; Asymmetry: 91.6%; sMAE: 1.463%; sMSE: 0.028% ## MASE: 2.819; RMSSE: 2.484; rMAE: 0.925; rRMSE: 0.922 In this case, the function has applied one and the same model but with different distributions, estimated it using likelihood and selected the one that has the lowest AICc value. It looks like log Normal is the most appropriate distribution for ETS(M,M,N) on this data. References "],["ETSSelection.html", "16.1 ADAM ETS components selection", " 16.1 ADAM ETS components selection Having 30 ETS models to choose from, selecting the most appropriate one becomes challenging. Petropoulos et al. (2018b) show that human experts can do this task successfully if they need to decide which components to include in the time series. However, when you face the problem of fitting ETS to thousands of time series, the judgmental selection becomes infeasible. Using some automatic selection becomes critically important. The basic idea underlying the components selection in ETS is based on information criteria (Section 13.3 of Svetunkov, 2021). The general procedure consists of the following three steps: we define a pool of models, we fit those models, and we select the one that has the lowest information criterion. Using this approach in the ETS context was first proposed by Hyndman et al. (2002). Based on this, we can prepare a pool of models (e.g. based on our understanding of the problem) and then select the most appropriate one for our data. The adam() function in the smooth package supports the following options for the pools: Pool of all 30 models (Section 3.4), model=\"FFF\"; Pool of pure additive models (Section 5.1), model=\"XXX\". As an option, “X” can also be used to tell function to only try additive component on the selected place. e.g. model=\"MXM\" will tell function to only test ETS(M,N,M), ETS(M,A,M) and ETS(M,Ad,M) models; Pool of pure multiplicative models (Section 6.1), model=\"YYY\". Similarly to (2), we can tell adam() to only consider multiplicative component in a specific place. e.g. model=\"YNY\" will consider only ETS(M,N,N) and ETS(M,N,M); Pool of pure models only, model=\"PPP\" – this is a shortcut for doing (2) and (3) and then selecting the best between the two pools; Manual pool of models, which can be provided as a vector of models, for example: model=c(\"ANN\",\"MNN\",\"ANA\",\"AAN\"); model=\"ZZZ\", which triggers the selection among all possible models based on a branch-and-bound algorithm (see below). In the cases above, adam() will try different models and select the most appropriate one from the predefined pool. There is a trade-off when deciding which pool to use: if you provide the bigger one, it will take more time to find the appropriate model, and there is a risk of overfitting the data; if you provide the smaller pool, then the optimal model might be outside of it, giving you the sub-optimal one. Furthermore, in some situations, you might not need to go through all 30 models because, for example, the seasonal component is not required for the data. Trying out all the models would be just a waste of time. So, to address this issue, I have developed a branch-and-bound algorithm for the selection of the most appropriate ETS model, which is triggered via model=\"ZZZ\" (the same mechanism is used in the es() function). The idea of the algorithm is to drop the components that do not improve the model. Here is how it works: Apply ETS(A,N,N) to the data, calculate an information criterion (IC); Apply ETS(A,N,A) to the data, calculate IC. If it is lower than (1), then this means that there is some seasonal component in the data, move to step (3). Otherwise, go to (4); If (2) is lower than (1), then apply ETS(M,N,M) model and calculate IC. If it is lower, then the data exhibits multiplicative seasonality. Go to (4); Fit the model with the additive trend component and the seasonal component selected from previous steps, which can be either “N,” “A,” or “M.” Calculate IC for the new model and compare it with the best IC so far. If it is lower, there is some trend component in the data. If it is not, then the trend component is not needed. Based on these four steps, we can kick off the unneeded components and reduce the pool of models to check. For example, if the algorithm shows that seasonality is not needed, but there is a trend, then we only have 10 models to check overall instead of 30: ETS(A,N,N), ETS(A,A,N), ETS(A,Ad,N), ETS(M,N,N), ETS(M,M,N), ETS(M,Md,N), ETS(A,M,N), ETS(A,Md,N), ETS(M,A,N), ETS(M,Ad,N). In steps (2) and (3), if there is a trend in the data, the model will have a higher than needed smoothing parameter \\(\\alpha\\), but the seasonality will play an important role in reducing the value of IC. This is why the algorithm is, in general efficient. It might not guarantee that the optimal model will be selected all the time, but it substantially reduces the computational time. The branch-and-bound algorithm can be combined with different types of model pools and is also supported in model=\"XXX\" and model=\"YYY\", where the pool of models for steps (1) – (4) is restricted by the pure ones only. This would also work in the combinations of the style model=\"XYZ\", where the function would form the pool of the following models: ETS(A,N,N), ETS(A,M,N), ETS(A,Md,N), ETS(A,N,A), ETS(A,M,A), ETS(A,Md,A), ETS(A,N,M), ETS(A,M,M) and ETS(A,Md,M). Finally, while the branch-and-bound algorithm is efficient, it might end up providing a mixed model, which might not be very suitable for the data. So, it is recommended to think of the possible pool of models before applying it to the data. For example, in some cases, you might realise that additive seasonality is unnecessary and that the data can be either non-seasonal or with multiplicative seasonality. In this case, you can explore the model=\"YZY\" option, aligning the error term with the seasonal component. Here is an example with automatically selected ETS model using the branch-and-bound algorithm described above: adamETSModel &lt;- adam(AirPassengers, model=&quot;ZZZ&quot;, h=12, holdout=TRUE) adamETSModel ## Time elapsed: 0.94 seconds ## Model estimated using adam() function: ETS(MAM) ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 467.2981 ## Persistence vector g: ## alpha beta gamma ## 0.7691 0.0053 0.0000 ## ## Sample size: 132 ## Number of estimated parameters: 17 ## Number of degrees of freedom: 115 ## Information criteria: ## AIC AICc BIC BICc ## 968.5961 973.9646 1017.6038 1030.7102 ## ## Forecast errors: ## ME: 9.537; MAE: 20.784; RMSE: 26.106 ## sCE: 43.598%; Asymmetry: 64.8%; sMAE: 7.918%; sMSE: 0.989% ## MASE: 0.863; RMSSE: 0.833; rMAE: 0.273; rRMSE: 0.254 In this specific example, the optimal model will coincide with the one selected via model=\"FFF\" and model=\"ZXZ\" (the reader is encouraged to try these selection mechanisms on their own), although this does not necessarily hold universally. References "],["ARIMASelection.html", "16.2 ADAM ARIMA order selection", " 16.2 ADAM ARIMA order selection While ETS has 30 models to choose from, ARIMA has many more options. For example, selecting the non-seasonal ARIMA with / without constant restricting the orders with \\(p \\leq 3\\), \\(d \\leq 2\\) and \\(q \\leq 3\\) leads to the combination of \\(3 \\times 2 \\times 3 \\times 2 = 36\\) possible ARIMA models. If we increase the possible orders to 5 or even more, we will need to go through hundreds of models. Adding the seasonal part increases this number by order of magnitude. This means that we cannot just test all possible ARIMA models and select the most appropriate one. We need to be smart in the selection process. Hyndman and Khandakar (2008) developed an efficient mechanism of ARIMA order selection based on statistical tests (for stationarity and seasonality), reducing the number of models to test to a reasonable amount. Svetunkov and Boylan (2020b) developed an alternative mechanism, relying purely on information criteria, which works well on seasonal data, but potentially may lead to models that overfit the data (this is implemented in auto.ssarima() and auto.msarima() functions in smooth package). We also have the Box-Jenkins approach discussed in Section 8.3 for ARIMA orders selection, which relies on the analysis of ACF (Subsection 8.3.2) and PACF (Subsection 8.3.3). Still, we should not forget the limitations of that approach (Subsection 8.3.4). Finally, Sagaert and Svetunkov (2022) proposed the stepwise trace forward approach (discussed briefly in Section 16.3), which relies on partial correlations and uses the information criteria to test the model on each iteration. Building upon all of that, I have developed the following algorithm for order selection of ADAM ARIMA: Determine the order of differences by fitting all possible combinations of ARIMA models with \\(P_j=0\\) and \\(Q_j=0\\) for all lags \\(j\\). This includes trying the models with and without the constant term. The order \\(D_j\\) is then determined via the model with the lowest IC; Then iteratively, starting from the highest seasonal lag and moving to the lag of 1 do for every lag \\(m_j\\): Calculate ACF of residuals of the model; Find the highest value of autocorrelation coefficient that corresponds to the multiple of the respective seasonal lag \\(m_j\\); Define what should be the order of MA based on the lag of the autocorrelation coefficient on the previous step and include it in the ARIMA model; Calculate IC, and if it is lower than for the previous best model, keep the new MA order; Repeat (a) – (d) while there is an improvement in IC; Do steps (a) – (e) for AR order, substituting ACF with PACF of the residuals of the best model; Move to the next seasonal lag and go to step (a); Try out several restricted ARIMA models of the order \\(q=d\\) (this is based on (1) and the restrictions provided by the user). The motivation for this comes from the idea of the relation between ARIMA and ETS (Section 8.4). As you can see, this algorithm relies on the Box-Jenkins methodology but takes it with a pinch of salt, checking every time if the proposed order is improving the model or not. The motivation for doing MA orders before AR is based on understanding what the AR model implies for forecasting (Section 8.1.1). In a way, it is safer to have ARIMA(0,d,q) model than ARIMA(p,d,0) because the former is less prone to overfitting than the latter. Finally, the proposed algorithm is faster than the algorithm of Svetunkov and Boylan (2020b) and is more modest in the number of selected orders of the model. In R, in order to start the algorithm, you would need to provide a parameter select=TRUE in the orders. Here is an example with Box-Jenkins sales data: adamARIMAModel &lt;- adam(BJsales, model=&quot;NNN&quot;, orders=list(ar=3,i=2,ma=3,select=TRUE), h=10, holdout=TRUE) In this example, orders=list(ar=3,i=2,ma=3,select=TRUE) tells function that the maximum orders to check are \\(p\\leq 3\\), \\(d\\leq 2\\) \\(q\\leq 3\\). The resulting model is ARIMA(0,2,2), which has the fit shown in Figure 16.1. Figure 16.1: Actuals, fitted and forecast for the Box-Jenkins Sales data. Note that the resulting model will be parsimonious when optimal initials are used. If we want to have a more flexible model, we can use a different initialisation, and in some cases, the algorithm will select a model with higher orders of AR, I and MA. 16.2.1 ETS + ARIMA restrictions Based on the relation between ARIMA and ETS (see Section 8.4), we do not need to test some of the combinations of models when selecting ARIMA orders. For example, if we already consider ETS(A,N,N), we do not need to check the ARIMA(0,1,1) model. The recommendations for what to skip in different circumstances have been discussed in Section 9.4. Still, there are various ways to construct an ETS + ARIMA model, with different sequences between ETS selection / ARIMA selection. We suggest starting with ETS, then going to the selection of ARIMA orders. This way, we are building upon the robust forecasting model and see if it can be improved further by introducing elements that are not there. Note that given the complexity of the task of estimating all parameters for ETS and ARIMA, it is advised to use backcasting (see Section 11.4.1) for the initialisation of such model. Here is an example in R: adamETSARIMAModel &lt;- adam(AirPassengers, model=&quot;PPP&quot;, orders=list(ar=c(3,3),i=c(2,1),ma=c(3,3),select=TRUE), h=10, holdout=TRUE, initial=&quot;back&quot;) adamETSARIMAModel ## Time elapsed: 1.08 seconds ## Model estimated using auto.adam() function: ETS(MMM)+SARIMA(3,0,0)[1](1,0,0)[12] ## Distribution assumed in the model: Gamma ## Loss function type: likelihood; Loss function value: 468.391 ## Persistence vector g: ## alpha beta gamma ## 0.5109 0.0046 0.0000 ## ## ARMA parameters of the model: ## AR: ## phi1[1] phi2[1] phi3[1] phi1[12] ## 0.2154 0.2296 -0.0402 0.2084 ## ## Sample size: 134 ## Number of estimated parameters: 8 ## Number of degrees of freedom: 126 ## Information criteria: ## AIC AICc BIC BICc ## 952.7819 953.9339 975.9647 978.7858 ## ## Forecast errors: ## ME: 3.416; MAE: 16.347; RMSE: 20.154 ## sCE: 12.908%; Asymmetry: 31.9%; sMAE: 6.178%; sMSE: 0.58% ## MASE: 0.681; RMSSE: 0.646; rMAE: 0.164; rRMSE: 0.163 The resulting model is ETS(M,M,M) with AR elements: three non-seasonal and one seasonal AR, which improve the fit of the model and hopefully result in more accurate forecasts. References "],["ETSXSelection.html", "16.3 Explanatory variables selection", " 16.3 Explanatory variables selection There are different approaches for automatic variable selection, but not all of them are efficient in the context of dynamic models. For example, backward stepwise might be either not feasible in the case of small samples or may take too much time to converge to an optimal solution (it has polynomial computational time). This is because the ADAMX model needs to be refitted and reestimated repeatedly using recursive relations based on the state space model (10.4). The classical stepwise forward might also be too slow because it has polynomial computational time as well. So, there need to be some simplifications, which will make variables selection in ADAMX doable in a reasonable time. To make the mechanism efficient in a limited time, we rely on the Sagaert and Svetunkov (2022) approach of stepwise trace forward selection of variables. It is the approach that uses partial correlations between variables to identify which variables to include in each iteration. It has linear computational time instead of the polynomial. Still, doing that in the proper ADAMX would take more time than needed because of the fitting of the dynamic model. So one of the possible solutions is to make variables selection in ADAMX in the following steps: Estimate and fit the ADAM model; Extract the residuals of the ADAM model; Select the most suitable variables, explaining the residuals, based on the trace forward stepwise approach and the selected information criterion; Estimate the ADAMX model with the selected explanatory variables. The residuals in step (2) might vary from model to model, depending on the type of the error term and the selected distribution: Normal, Laplace, S, Generalised Normal or Asymmetric Laplace: \\(e_t\\); Additive error and Log Normal, Inverse Gaussian or Gamma: \\(\\left(1+\\frac{e_t}{\\hat{y}_t} \\right)\\); Multiplicative error and Log Normal, Inverse Gaussian or Gamma: \\(1+e_t\\). So, the extracted residuals should be formulated based on the distributional assumptions of each model. In R, step (3) is done using the stepwise() function from the greybox package, supporting all the distributions discussed in the previous chapters. The only thing that needs to be modified is the number of degrees of freedom: the function should consider all estimated parameters. This is done internally via the df parameter in stepwise(). While the suggested approach has obvious limitations (e.g. smoothing parameters can be higher than needed, explaining the variability otherwise explained by variables), it is efficient in terms of computational time. To see how it works, we use SeatBelt data: SeatbeltsData &lt;- Seatbelts[,c(&quot;drivers&quot;,&quot;kms&quot;,&quot;PetrolPrice&quot;,&quot;law&quot;)] We have already had a look at this data earlier in Section 10.6, so we can move directly to the selection part: adamModelETSXMNMSelect &lt;- adam(SeatbeltsData, &quot;MNM&quot;, h=12, holdout=TRUE, regressors=&quot;select&quot;) summary(adamModelETSXMNMSelect) ## Warning: Observed Fisher Information is not positive semi-definite, which means ## that the likelihood was not maximised properly. Consider reestimating the model, ## tuning the optimiser or using bootstrap via bootstrap=TRUE. ## ## Model estimated using adam() function: ETSX(MNM) ## Response variable: drivers ## Distribution used in the estimation: Gamma ## Loss function type: likelihood; Loss function value: 1117.189 ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## alpha 0.2877 0.0856 0.1186 0.4565 * ## gamma 0.0000 0.0414 0.0000 0.0816 ## level 1655.9759 91.0281 1476.2378 1835.4961 * ## seasonal_1 1.0099 0.0155 0.9808 1.0459 * ## seasonal_2 0.9053 0.0153 0.8762 0.9413 * ## seasonal_3 0.9352 0.0156 0.9061 0.9712 * ## seasonal_4 0.8696 0.0147 0.8405 0.9056 * ## seasonal_5 0.9465 0.0162 0.9174 0.9825 * ## seasonal_6 0.9152 0.0155 0.8861 0.9513 * ## seasonal_7 0.9623 0.0160 0.9332 0.9983 * ## seasonal_8 0.9706 0.0159 0.9416 1.0067 * ## seasonal_9 1.0026 0.0169 0.9735 1.0386 * ## seasonal_10 1.0824 0.0178 1.0533 1.1184 * ## seasonal_11 1.2012 0.0183 1.1721 1.2372 * ## law 0.0200 0.1050 -0.1873 0.2271 ## ## Error standard deviation: 0.0752 ## Sample size: 180 ## Number of estimated parameters: 16 ## Number of degrees of freedom: 164 ## Information criteria: ## AIC AICc BIC BICc ## 2266.378 2269.715 2317.465 2326.131 Note that the function might complain about the observed Fisher Information. This only means that the estimated variances of parameters might be lower than they should be in reality. This is discussed in Section 17.1. Based on the summary from the model, we can see that neither kms nor PetrolPrice improve the model in terms of AICc (they were not included in the model). We could check them manually to see if the selection worked out well in our case (construct sink regression as a benchmark): adamModelETSXMNMSink &lt;- adam(SeatbeltsData, &quot;MNM&quot;, h=12, holdout=TRUE) summary(adamModelETSXMNMSink) ## Warning: Observed Fisher Information is not positive semi-definite, which means ## that the likelihood was not maximised properly. Consider reestimating the model, ## tuning the optimiser or using bootstrap via bootstrap=TRUE. ## ## Model estimated using adam() function: ETSX(MNM) ## Response variable: drivers ## Distribution used in the estimation: Gamma ## Loss function type: likelihood; Loss function value: 1234.291 ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## alpha 0.9508 2.5779 0.0000 1.0000 ## gamma 0.0000 0.0132 0.0000 0.0260 ## level 23.2952 1.2746 20.7783 25.8086 * ## seasonal_1 1.1340 0.0621 1.0115 4.2959 * ## seasonal_2 0.9924 0.9574 0.8698 4.1542 * ## seasonal_3 0.9248 0.8608 0.8023 4.0867 * ## seasonal_4 0.8342 0.7952 0.7117 3.9960 * ## seasonal_5 0.9068 0.9038 0.7843 4.0686 * ## seasonal_6 0.8625 0.9196 0.7400 4.0243 * ## seasonal_7 0.8396 0.8211 0.7171 4.0014 * ## seasonal_8 0.8477 0.7802 0.7252 4.0095 * ## seasonal_9 0.9798 1.0017 0.8573 4.1417 * ## seasonal_10 1.1417 1.3043 1.0192 4.3036 * ## seasonal_11 1.3273 1.6033 1.2047 4.4891 * ## kms 0.0000 0.0000 -0.0001 0.0001 ## PetrolPrice -2.7216 3.1200 -8.8827 3.4312 ## law 0.0181 5.2239 -10.2976 10.3197 ## ## Error standard deviation: 0.1413 ## Sample size: 180 ## Number of estimated parameters: 18 ## Number of degrees of freedom: 162 ## Information criteria: ## AIC AICc BIC BICc ## 2504.581 2508.829 2562.054 2573.085 We can see that the sink regression model has a higher AICc value than the model with the selected variables, which means that the latter is closer to the “true model.” While adamModelETSXMNMSelect might not be the best possible model in terms of information criteria, it is still a reasonable one and can be used for further inference. References "],["ADAMCombinations.html", "16.4 Forecasts combinations in ADAM", " 16.4 Forecasts combinations in ADAM When it comes to achieving as accurate forecasts as possible in practice, the best and the most robust (in terms of not failing) approach is producing combined forecasts. The primary motivation for combining is that there is no one best forecasting method for everything – methods might perform very well in some conditions and fail in others. It is typically not possible to say which of the cases you face in practice. Furthermore, the model selected on one sample might differ from the model chosen for the same sample but with one more observation. Thus there is a model uncertainty (as defined by Chatfield, 1996), and the safer option is to produce forecasts from several models and then combine them to get the final forecast. This way, the potential damage from an inaccurate forecast hopefully will be reduced. There are many different techniques for combining forecasts. The non-exhaustive list includes: Simple average, which works fine as long as you do not have exceptionally poorly performing methods; Median, which produces good combinations when the pool of models is relatively small and might contain those that produce very different forecasts from the others (e.g. explosive trajectories). However, when a big pool of models is considered, the median might ignore vital information and decrease accuracy, as noted by Jose and Winkler (2008). Stock and Watson (2004) conducted an experiment on macroeconomic data, and medians performed poorer than the other approaches (probably because of the high number of forecasting methods), while median-based combination worked well for Petropoulos and Svetunkov (2020), who considered only four forecasting approaches; Trimmed and/or Winsorized mean, which drop extreme forecasts, when calculating the mean and, as was shown by Jose and Winkler (2008), work well in case of big pools of models, outperforming medians and simple average; Weighted mean, which assigns weights to each forecast and produces a combined forecast based on them. While this approach sounds more reasonable than the others, there is no guarantee that it will work better because the weights need to be estimated and might change with the sample size change or a pool of models. Claeskens et al. (2016) explain why the simple average approach outperforms weighted averages in many cases: it does not require estimation of weights and thus does not introduce as much uncertainty. However, when done smartly, combinations can be beneficial in terms of accuracy, as shown, for example, in Kolassa (2011) and Kourentzes et al. (2019a). The forecast combination approach implemented in ADAM is the weighted mean, based on Kolassa (2011), who used AIC weights as proposed by Burnham and Anderson (2004). This approach aims to estimate all models in the pool, calculate information criteria (IC) for each of them (see discussion in Section 13.4 in Svetunkov, 2021) and then calculate weights for each model. Those models with lower ICs will have higher weights, while the poorly performing ones will have higher ones. The only requirement of the approach is for the parameters of models to be estimated via likelihood maximisation (see Section 11.1). It is not important what model is used or what distribution is assumed, as long as the models are initialised (see discussion in Section 11.4) and constructed in the same way and the likelihood is used in the estimation. When it comes to prediction intervals, the correct way of calculating them for the combination is to consider the joint distribution of all forecasting models in the pool and take quantiles based on that. However, Lichtendahl et al. (2013) showed that a simpler approach of averaging the quantiles works well in practice. This approach implies producing prediction intervals for all the models in the pool and then averaging the obtained values. It is fast and efficient in terms of getting well-calibrated intervals. In R, adam() function supports the combination of ETS models via model=\"CCC\" or any other combination of letters, as long as the model contains “C” in its name. For example, the function will combine all non-seasonal models if model=\"CCN\" is provided. Consider the following example on Box-Jenkins sales series: adamETSCCN &lt;- adam(BJsales, &quot;CCN&quot;, h=10, holdout=TRUE, ic=&quot;AICc&quot;) In the code above, the function will estimate all non-seasonal models, extract AICc for each of them and then calculate weights, which we can extract for further analysis: round(adamETSCCN$ICw,3) ## ANN MAN AAdN MMN AMdN MNN AAN MAdN AMN MMdN ## 0.000 0.014 0.252 0.010 0.511 0.000 0.073 0.031 0.050 0.059 As can be seen from the output of weights, the level models ETS(A,N,N) and ETS(M,N,N) were further away from the best model and, as a result, got weights very close to zero. The fitted values are combined from all models, the residuals are equal to \\(e_t = y_t -\\hat{y}_t\\), where \\(\\hat{y}_t\\) is the combined value. The final forecast together with the prediction interval can be generated via the forecast() function (Figure 16.2): plot(forecast(adamETSCCN,h=10,interval=&quot;prediction&quot;)) Figure 16.2: An example of combination of ETS non-seasonal models on Box-Jenkins sale time series. What the function does, in this case, is produces forecasts and prediction intervals from each model and then uses original weights to combine them. Each model can be extracted and used separately if needed. Here is an example with ETS(A,Ad,N) model from the estimated pool: plot(forecast(adamETSCCN$models$AAdN,h=10,interval=&quot;prediction&quot;)) Figure 16.3: An example of the combination of ETS non-seasonal models on Box-Jenkins sale time series. As can be seen from the plots in Figures 16.2 and 16.3, due to the highest weight, ETS(A,Ad,N) and ETS(C,C,N) models have produced very similar point forecasts and prediction intervals. Alternatively, if we do not need to consider all ETS models, we can provide the pool of models, including a model with “C” in its name. Here is an example of how pure additive non seasonal models can be combined: adamETSCCNPureAdditive &lt;- adam(BJsales, c(&quot;CCN&quot;,&quot;ANN&quot;,&quot;AAN&quot;,&quot;AAdN&quot;), h=10, holdout=TRUE, ic=&quot;AICc&quot;) The main issue with the combined ETS approach is that it is computationally expensive due to the estimation of all models in the pool and can also result in high memory usage. As a result, it is recommended to be smart in deciding which models to include in the pool. 16.4.1 Other combination approaches While adam() supports IC weights combination of ETS models only, it is also possible to combine ARIMA, regression models and models with different distributions in the framework. Given that all models are initialised in the same way and that the likelihoods are calculated using similar principles, the weights can be calculated manually using the formula from Burnham and Anderson (2004): \\[\\begin{equation} w_i = \\frac{\\exp\\left(-\\frac{1}{2}\\Delta_i\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}\\Delta_j\\right)}, \\tag{16.1} \\end{equation}\\] where \\(\\Delta_i=\\mathrm{IC}_i -\\min_{i=1}^n \\left(\\mathrm{IC}_i\\right)\\) is the information criteria distance from the best performing model, \\(\\mathrm{IC}_i\\) is the value of information criterion of model \\(i\\) and \\(n\\) is the number of models in the pool. For example, here how we can combine the best ETS with the best ARIMA and the ETSX(M,M,N) model in the ADAM framework, based on BICc: # Prepare data with explanatory variables BJsalesData &lt;- cbind(as.data.frame(BJsales), xregExpander(BJsales.lead,c(-5:5))) # Apply models adamModelsPool &lt;- vector(&quot;list&quot;,3) adamModelsPool[[1]] &lt;- adam(BJsales, &quot;ZZN&quot;, h=10, holdout=TRUE, ic=&quot;BICc&quot;) adamModelsPool[[2]] &lt;- adam(BJsales, &quot;NNN&quot;, orders=list(ar=3,i=2,ma=3,select=TRUE), h=10, holdout=TRUE, ic=&quot;BICc&quot;) adamModelsPool[[3]] &lt;- adam(BJsalesData, &quot;MMN&quot;, h=10, holdout=TRUE, ic=&quot;BICc&quot;, regressors=&quot;select&quot;) # Extract BICc values adamModelsICs &lt;- sapply(adamModelsPool,BICc) # Calculate weights adamModelsICWeights &lt;- adamModelsICs - min(adamModelsICs) adamModelsICWeights[] &lt;- exp(-0.5*adamModelsICWeights) / sum(exp(-0.5*adamModelsICWeights)) names(adamModelsICWeights) &lt;- c(&quot;ETS&quot;,&quot;ARIMA&quot;,&quot;ETSX&quot;) round(adamModelsICWeights,3) ## ETS ARIMA ETSX ## 0.524 0.424 0.052 These weights can then be used for the combination of the fitted values, forecasts and prediction intervals: adamModelsPoolForecasts &lt;- vector(&quot;list&quot;,3) # Produce forecasts from the three models for(i in 1:3){ adamModelsPoolForecasts[[i]] &lt;- forecast(adamModelsPool[[i]], h=10, interval=&quot;pred&quot;) } # Produce combined conditional means and prediction intervals finalForecast &lt;- cbind(sapply(adamModelsPoolForecasts, &quot;[[&quot;,&quot;mean&quot;) %*% adamModelsICWeights, sapply(adamModelsPoolForecasts, &quot;[[&quot;,&quot;lower&quot;) %*% adamModelsICWeights, sapply(adamModelsPoolForecasts, &quot;[[&quot;,&quot;upper&quot;) %*% adamModelsICWeights) # Give the appropriate names colnames(finalForecast) &lt;- c(&quot;Mean&quot;, &quot;Lower bound (2.5%)&quot;, &quot;Upper bound (97.5%)&quot;) # Transform the table in the ts format (for convenience) finalForecast &lt;- ts(finalForecast, start=start(adamModelsPoolForecasts[[i]]$mean)) finalForecast ## Time Series: ## Start = 141 ## End = 150 ## Frequency = 1 ## Mean Lower bound (2.5%) Upper bound (97.5%) ## 141 257.6525 254.8779 260.4508 ## 142 257.7067 253.2954 262.0823 ## 143 257.7703 251.8166 263.7092 ## 144 257.8144 250.2603 265.2998 ## 145 257.8679 248.8403 266.9401 ## 146 257.9106 247.2729 268.6758 ## 147 257.9509 245.6800 270.2934 ## 148 257.9917 244.1489 272.0797 ## 149 258.0302 242.5430 273.8336 ## 150 258.0623 240.8738 275.6457 In order to see how the forecast looks like, we can plot it via graphmaker() function from greybox: graphmaker(BJsales, finalForecast[,1], lower=finalForecast[,2], upper=finalForecast[,3], level=0.95) Figure 16.4: Final forecast from the combination of ETS, ARIMA and ETSX models. Figure 16.4 demonstrates the slightly increasing trajectory with expanding prediction interval. The point forecast (conditional mean) corresponds to the trajectory observed on the last several observations, just before the forecast origin. The future values are inside the prediction interval, so overall, this can be considered as a reasonable forecast. References "],["ADAMUncertainty.html", "Chapter 17 Handling uncertainty in ADAM", " Chapter 17 Handling uncertainty in ADAM So far, when we discussed forecasts from ADAM models, we have assumed that the smoothing parameters and initial values are known, even though we have acknowledged in Chapter 11 that they are estimated. This is the conventional assumption of ETS models from Hyndman et al. (2008) (it also applies to ARIMA models). However, in reality, the parameters are never known and are always estimated in-sample. This means that the estimates of parameters will inevitably change with the change of sample size. This uncertainty will impact the model fit, the point forecasts and prediction intervals. To overcome this issue, Bergmeir et al. (2016) proposed bagging – the procedure that decomposes time series using STL (Cleveland et al., 1990), then recreates many time series by bootstrapping the remainder then fits best ETS models to each of the newly created time series and combines the forecasts from the models. This way (as was explained by Petropoulos et al., 2018a), the parameters of models will differ from one generated time series to another. Thus the final forecasts will handle the uncertainty about the parameters. In addition, this approach also covers the model uncertainty element, which was discussed in Section 16.4. The main issue with the approach is that it is computationally expensive and assumes that STL decomposition is appropriate for time series and that the residuals from this decomposition are independent. In this chapter, we focus on a discussion of uncertainty about the estimates of parameters of ADAM models, starting from dealing with confidence intervals for them and ending with propagating the parameters uncertainty to the states and fitted values of the model. We also discuss a method that allows capturing this uncertainty and use it for fitted values and forecasts of the model. References "],["ADAMUncertaintyVCOV.html", "17.1 Covariance matrix of parameters", " 17.1 Covariance matrix of parameters One of the basic conventional statistical ways of capturing uncertainty about estimates of parameters is via the calculation of the covariance matrix of parameters. The covariance matrix that is typically calculated in regression context is based on the assumption of normality and can be derived based on the maximum likelihood estimate of parameters. It relies on the “Fisher Information,” which in turn is calculated as a negative expectation of Hessian of parameters (the matrix of second derivatives of the likelihood function with respect to all the estimates of parameters). The idea of Hessian is to capture the curvatures of the likelihood function in its optimal point to understand what impact each of the parameters has on it. If we calculate the Hessian matrix and have Fisher Information, then using Cramer-Rao bound (Wikipedia, 2021n), the true variance of parameters will be greater or equal to the inverse of Fisher Information: \\[\\begin{equation} \\mathrm{V}(\\hat{\\theta_j}) \\geq \\frac{1}{\\mathrm{FI}(\\hat{\\theta_j})} , \\tag{17.1} \\end{equation}\\] where \\(\\theta_j\\) is the parameter under consideration. The property (17.1) can then be used for the calculation of the covariance matrix of parameters. While in case of regression this calculation has an analytical solution, in case of ETS and ARIMA, this can only be done via numeric methods, because the models rely on recursive relations. In R, an efficient calculation of Hessian can be done via the hessian() function from the pracma package. In smooth there is a method vcov() that does all the calculations, estimating the negative Hessian inside the adam() and then inverts the result. Here is an example of how this works: adamModel &lt;- adam(BJsales, h=10, holdout=TRUE) adamModelVcov &lt;- vcov(adamModel) round(adamModelVcov, 3) ## alpha beta phi level trend ## alpha 0.013 -0.010 0.007 -0.206 0.167 ## beta -0.010 0.019 -0.014 0.470 -0.376 ## phi 0.007 -0.014 0.016 -0.482 0.389 ## level -0.206 0.470 -0.482 22.287 -16.158 ## trend 0.167 -0.376 0.389 -16.158 12.704 The precision of the estimate will depend on the closeness of the likelihood function to its maximum in the estimated parameters. If the likelihood was not properly maximised and the function stopped prematurely, then the covariance matrix might be incorrect and contain errors. Out of curiosity, we could calculate the correlation matrix of the estimated parameters: round(adamModelVcov / sqrt(diag(adamModelVcov) %*% t(diag(adamModelVcov))), 3) ## alpha beta phi level trend ## alpha 1.000 -0.657 0.467 -0.381 0.410 ## beta -0.657 1.000 -0.803 0.714 -0.757 ## phi 0.467 -0.803 1.000 -0.813 0.870 ## level -0.381 0.714 -0.813 1.000 -0.960 ## trend 0.410 -0.757 0.870 -0.960 1.000 This matrix does not provide helpful analytical information but demonstrates that the estimates of the initial level and trend of the ETS(AAdN) model applied to this data are negatively correlated. The values from the covariance matrix can then be used for various purposes, including calculation of confidence intervals of parameters, construction of confidence interval for the fitted value and point forecasts and finally, the construction of more adequate prediction intervals. In some cases, the vcov() method would complain that the Fisher Information cannot be inverted. This typically means that the adam() failed to reach the maximum of the likelihood function. Re-estimating the model might resolve the problem. Note that this method only works when loss=\"likelihood\" or when the loss is aligned with the assumed distribution (e.g. loss=\"MSE\" and distribution=\"dnorm\"). In all the other cases, other approaches (such as bootstrap) would need to be used to estimate the covariance matrix of parameters. 17.1.1 Bootstrapped covariance matrix An alternative way of constructing the matrix is via the bootstrap. The one implemented in smooth is based on the coefboostrap() method from the greybox package, which implements the modified case resampling. It is less efficient than the Fisher Information method in terms of computational time and works only for larger samples. The algorithm implemented in the function uses a continuous sub-sample of the original data, starting from the initial point \\(t=1\\) (if backcasting is used, as discussed in Section 11.4.1, then the starting point will be allowed to vary). This sub-sample is then used to re-estimate adam() to get the empirical estimates of parameters. The procedure is repeated nsim times, which for adam() is by default equal to 100. This approach is far from ideal and will typically lead to the underestimated variance of initials because of the sample size restrictions. Still, it does not break the data structure and allows obtaining results relatively fast without imposing any additional assumptions on the model and the data. I recommend using it in case of the initialisation via backcasting. Here is an example of how the function works on the data above – it is possible to speed up the process by doing parallel calculations: adamModelBoot &lt;- coefbootstrap(adamModel, parallel=TRUE) adamModelBoot ## Bootstrap for the adam model with nsim=100 and size=70 ## Time elapsed: 0.56 seconds The covariance matrix can then be extracted from the result via adamModelBoot$vcov. The same procedure is used in vcov() method if bootstrap=TRUE: round(vcov(adamModel, bootstrap=TRUE, parallel=TRUE), 3) ## alpha beta phi level trend ## alpha 0.020 0.004 0.003 0.021 -0.026 ## beta 0.004 0.013 -0.001 -0.017 0.028 ## phi 0.003 -0.001 0.015 0.067 -0.071 ## level 0.021 -0.017 0.067 0.563 -0.726 ## trend -0.026 0.028 -0.071 -0.726 1.022 References "],["confidence-intervals-for-parameters.html", "17.2 Confidence intervals for parameters", " 17.2 Confidence intervals for parameters As discussed in Section 5.1 of Svetunkov (2021), if several vital assumptions (discussed in Section 15) are satisfied and CLT holds, then the distribution of estimates of parameters will follow the Normal one, which will allow us to construct confidence intervals for them. In the case of ETS and ARIMA models in the ADAM framework, the estimated parameters include smoothing, dampening and ARMA parameters together with the initial states’ values. In the case of explanatory variables, the pool of parameters is increased by the coefficients for those variables and their smoothing parameters (if the dynamic model from Section 10.3 is used). And in the case of the intermittent state space model, the parameters will also include the elements of the occurrence part of the model. The CLT should work if: Estimates of parameters are consistent (e.g. MSE or Likelihood is used in estimation, see Section 11), the parameters do not lie near the bounds, the model is correctly specified and moments of the distribution of error term are finite. In the case of ETS and ARIMA, the parameters are bounded, and the estimates might lie near the bounds. This means that the distribution of estimates might not be normal. However, given that the bounds of the parameters are typically fixed and are forced by the optimiser, the estimates of parameters will follow rectified normal distribution (Wikipedia, 2021o). This is important because knowing the distribution, we can derive the confidence intervals for the parameters. We would need to use t-statistics because we estimate the standard errors of parameters. The confidence intervals will be constructed in a conventional way in this case, using the formula (see Section 5.1 of Svetunkov, 2021): \\[\\begin{equation} \\theta_j \\in (\\hat{\\theta_j} + t_{\\alpha/2}(df) s_{\\theta_j}, \\hat{\\theta_j} + t_{1-\\alpha/2}(df) s_{\\theta_j}), \\tag{17.2} \\end{equation}\\] where \\(t_{\\alpha/2}(df)\\) is Student’s t-statistics for \\(df=T-k\\) degrees of freedom (\\(T\\) is the sample size and \\(k\\) is the number of estimated parameters) and \\(\\alpha\\) is the significance level. Then, after constructing the intervals, we can cut their values with the bounds of parameters, thus imposing rectified distribution (t distribution in this case). An example would be the ETS(A,N,N) model, for which the smoothing parameter is typically restricted by (0, 1) region and thus the confidence interval should not go beyond these bounds as well. To construct the interval, we need to know the standard errors of parameters. Luckily, they can be calculated as square roots of the diagonal of the covariance matrix of parameters (discussed in Section 17.1): sqrt(diag(adamModelVcov)) ## alpha beta phi level trend ## 0.1144386 0.1394695 0.1256504 4.7208625 3.5642472 Based on these values and the formula (17.2), we can produce confidence intervals for parameters of any ADAM model, which is done in R using the confint() method. For example, here are the intervals for the significance level of 1%: confint(adamModel, level=0.99) ## S.E. 0.5% 99.5% ## alpha 0.1144386 0.6517003 1.0000000 ## beta 0.1394695 0.0000000 0.6550654 ## phi 0.1256504 0.5348598 1.0000000 ## level 4.7208625 190.4171720 215.0739522 ## trend 3.5642472 -11.7130838 6.9027646 In the output above, the distributions for \\(\\alpha\\), \\(\\beta\\) and \\(\\phi\\) are rectified: \\(\\alpha\\) and \\(\\phi\\) are restricted with one from above, while \\(\\beta\\) is restricted with zero from below. To have the bigger picture, we can produce the summary of the model, which will include the table above: summary(adamModel, level=0.99) ## ## Model estimated using adam() function: ETS(AAdN) ## Response variable: BJsales ## Distribution used in the estimation: Normal ## Loss function type: likelihood; Loss function value: 241.078 ## Coefficients: ## Estimate Std. Error Lower 0.5% Upper 99.5% ## alpha 0.9507 0.1144 0.6517 1.0000 * ## beta 0.2911 0.1395 0.0000 0.6551 ## phi 0.8632 0.1257 0.5349 1.0000 * ## level 202.7529 4.7209 190.4172 215.0740 * ## trend -2.3996 3.5642 -11.7131 6.9028 ## ## Error standard deviation: 1.384 ## Sample size: 140 ## Number of estimated parameters: 6 ## Number of degrees of freedom: 134 ## Information criteria: ## AIC AICc BIC BICc ## 494.1560 494.7876 511.8058 513.3664 The output above shows the estimates of parameters and their 99% confidence intervals. For example, based on this output, we can conclude that the uncertainty about the initial trend estimate is considerable, and in the “true model,” it could be either positive or negative (or even close to zero). At the same time, the “true” parameter of the initial level will lie in 99% of the cases between 194.56 and 208.06. Just as a reminder, Figure 17.1 shows the model fit and point forecasts for the estimated ETS model on this data. Figure 17.1: Model fit and point forecasts of ETS(A,Ad,N) on Box-Jenkins Sales data. As another example, we can have a similar summary for ARIMA models in ADAM: adamModelARIMA &lt;- adam(BJsales, &quot;NNN&quot;, h=10, holdout=TRUE, order=list(ar=3,i=2,ma=3,select=TRUE)) summary(adamModelARIMA) ## ## Model estimated using auto.adam() function: ARIMA(0,2,2) ## Response variable: BJsales ## Distribution used in the estimation: Normal ## Loss function type: likelihood; Loss function value: 243.2819 ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## theta1[1] -0.7515 0.0830 -0.9156 -0.5875 * ## theta2[1] -0.0109 0.0956 -0.1998 0.1780 ## ARIMAState1 -200.1902 1.9521 -204.0508 -196.3321 * ## ARIMAState2 -200.2338 2.7554 -205.6830 -194.7879 * ## ## Error standard deviation: 1.4007 ## Sample size: 140 ## Number of estimated parameters: 5 ## Number of degrees of freedom: 135 ## Information criteria: ## AIC AICc BIC BICc ## 496.5638 497.0115 511.2720 512.3783 From the summary above, we can see that the parameter \\(\\theta_2\\) is close to zero, and the interval around it is wide. So, we can expect that it might change the sign if the sample size increases or become even closer to zero. Given that the model above was estimated with the optimisation of initial states, we also see the values for the ARIMA states and their confidence intervals in the summary above. If we used initial=\"backcasting\", the summary would not include them. This estimate of uncertainty via confidence intervals might also be helpful to see what can happen with the estimates of parameters if the sample size increases: will they change substantially or not. If they do, then the decisions made on Monday based on the available data might differ considerably from the decisions made on Tuesday. So, in the ideal world, we would want to have as narrow confidence intervals as possible. References "],["conditionalVarianceUncertainty.html", "17.3 Conditional variance with uncertain parameters", " 17.3 Conditional variance with uncertain parameters We now consider two special cases with pure additive state space models: When the values of the initial state vector are unknown; When the model parameters (e.g. smoothing or AR/MA parameters) are estimated on a sample of data. We discuss analytical formulae for the conditional variances for these cases. This variance can then be used to construct the confidence interval of the fitted line and/or for the confidence/prediction interval for the holdout period. We do not cover the more realistic case when both initials and parameters are estimated because there is no closed analytical form for this due to potential correlations between the estimates of parameters. 17.3.1 Estimated initial state First, we need to recall the recursive relations discussed in Section 5.4, specifically formula (5.9). Just to simplify all the derivations in this section, we consider the non-seasonal case, in which all elements of \\(\\mathbf{l}\\) are equal to one. This can be ETS(A,N,N), ETS(A,A,N), ETS(A,Ad,N) or several ARIMA models. The more general case is more complicated but is derivable using the same principles as discussed below. The recursive relation from the first observation till the end of the sample can be written as: \\[\\begin{equation} \\hat{\\mathbf{v}}_{t} = \\mathbf{D}^{t} \\hat{\\mathbf{v}}_{0} + \\sum_{j=0}^{t-1} \\mathbf{D}^{j} \\hat{\\mathbf{g}} y_{t-j} , \\tag{17.3} \\end{equation}\\] where \\(\\mathbf{D}=\\mathbf{F} -\\mathbf{g}\\mathbf{w}^\\prime\\). The formula (17.3) shows that the most recent value of the state vector depends on the initial value \\(\\hat{\\mathbf{v}}_{0}\\) and on the linear combination of actual values. Note that we assume in this part that the matrix \\(\\mathbf{D}\\) is known, i.e. the smoothing parameters are not estimated. Although this is an unrealistic assumption, it helps in showing how the variance of initial state would influence the conditional variance of actual values at the end of sample. If we now take the variance of state vector conditional on the previous actual values \\(y_{t-j}\\) for all \\(j=\\{0, \\dots, t-1 \\}\\), then we will have (due to independence of two terms in (17.3)): \\[\\begin{equation} \\mathrm{V}(\\hat{\\mathbf{v}}_{t} | y_1, y_2, \\dots y_t) = \\mathrm{V}\\left( \\mathbf{D}^{t} \\hat{\\mathbf{g}} \\hat{\\mathbf{v}}_{0} \\right) + \\mathrm{V}\\left(\\sum_{j=0}^{t-1} \\mathbf{D}^{j} y_{t-j} | y_1, y_2, \\dots y_t \\right) . \\tag{17.4} \\end{equation}\\] We condition the variance on actual values because they are given to us, and we want to see how different initial states would lead to the changes in the model fit given these values and thus how the uncertainty will propagate from \\(j=1\\) to \\(j=t\\). In the formula (17.4), the right-hand side is equal to zero because all actual values are known, and \\(\\mathbf{D}\\) does not have any uncertainty due to the assumption above. This leads to the following covariance matrix of states on observation \\(t\\): \\[\\begin{equation} \\mathrm{V}(\\hat{\\mathbf{v}}_{t} | y_1, y_2, \\dots y_t) = \\mathbf{D}^{t} \\mathrm{V}\\left( \\hat{\\mathbf{v}}_{0} \\right) \\left(\\mathbf{D}^{t}\\right)^\\prime . \\tag{17.5} \\end{equation}\\] Inserting the values of matrix \\(\\mathbf{D}\\) in (17.5), we can then get the variance of state vector. For example, for ETS(A,N,N), the conditional variance of the level on observation \\(t\\) is: \\[\\begin{equation} \\mathrm{V}(\\hat{l}_{t} | y_1, y_2, \\dots y_t) = (1-\\alpha)^{t} \\mathrm{V}\\left( \\hat{l}_{0} \\right) (1-\\alpha)^{t} . \\tag{17.6} \\end{equation}\\] As the formula above shows, if the smoothing parameter lies between zero and one, then the uncertainty of the initial level will not have a big impact on the uncertainty on observation \\(t\\). The closer \\(\\alpha\\) is to zero, the more impact the variance of the initial level will have on the variance of the final level. If we use admissible bounds (see Section 4.6), then the smoothing parameter might lie in the region (1, 2), and thus the impact of the variance of the initial state will increase with the increase of the sample size \\(t\\). Now that we have the variance of the state, we can also calculate the variance of the fitted values (or one step ahead in-sample forecast). In the pure additive model, the fitted values are calculated as: \\[\\begin{equation} \\hat{y}_t = \\mu_{y,t|t-1} = \\mathbf{w}^\\prime \\hat{\\mathbf{v}}_{t-1}. \\tag{17.7} \\end{equation}\\] The variance conditional on all actual observations will then be: \\[\\begin{equation} \\mathrm{V}(\\hat{y}_t | y_1, y_2, \\dots y_t) = \\mathrm{V}\\left( \\mathbf{w}^\\prime \\hat{\\mathbf{v}}_{t-1} \\right) , \\tag{17.8} \\end{equation}\\] which after inserting (17.5) in (17.8) leads to: \\[\\begin{equation} \\mathrm{V}(\\hat{y}_t | y_1, y_2, \\dots y_t) = \\mathbf{w}^\\prime \\mathbf{D}^{t-1} \\mathrm{V}\\left( \\hat{\\mathbf{v}}_{0} \\right) \\left(\\mathbf{D}^{t-1}\\right)^\\prime \\mathbf{w} . \\tag{17.9} \\end{equation}\\] This variance can then be used to calculate the confidence interval for the fitted values, assuming that the estimates of initials state follow a normal distribution (due to CLT). Finally, the variance of initial states will also impact the conditional h steps ahead variance of the model. This can be seen from the recursion (5.19), which in case of non-seasonal models simplifies to: \\[\\begin{equation} y_{t+h} = \\mathbf{w}^\\prime \\mathbf{F}^{h-1} \\hat{\\mathbf{v}}_{t} + \\mathbf{w}^\\prime \\sum_{j=1}^{h-1} \\mathbf{F}^{j-1} \\mathbf{g} e_{t+h-j} + e_{t+h} . \\tag{17.10} \\end{equation}\\] Taking the variance of \\(y_{t+h}\\) conditional on the all the information until the observation \\(t\\) (all actual values) leads to: \\[\\begin{equation} \\begin{aligned} \\mathrm{V}( y_{t+h} | y_1, y_2, \\dots y_t) = &amp; \\mathbf{w}^\\prime \\mathbf{F}^{h-1} \\mathbf{D}^{t-1} \\mathrm{V}\\left( \\hat{\\mathbf{v}}_{0} \\right) \\left(\\mathbf{D}^{t-1}\\right)^\\prime (\\mathbf{F}^\\prime)^{h-1} \\mathbf{w} + \\\\ &amp; \\left( \\left(\\mathbf{w}^\\prime \\sum_{j=1}^{h-1} \\mathbf{F}^{j-1} \\mathbf{g} \\mathbf{g}^\\prime (\\mathbf{F}^\\prime)^{j-1} \\mathbf{w} \\right) + 1 \\right) \\sigma^2 . \\end{aligned} \\tag{17.11} \\end{equation}\\] This formula can then be used for the construction of prediction intervals of the model, for example using formula (5.12). The topic of construction of prediction intervals will be discussed later in Section 18.2. As a final note, it is also possible to derive the variances for the seasonal models. The only thing that would change in this situation is that the matrices \\(\\mathbf{F}\\), \\(\\mathbf{w}\\) and \\(\\mathbf{g}\\) will need to be split into submatrices, similar to how it was done in Section 5.2. 17.3.2 Estimated parameters of ADAM model Now we discuss the case when the initial states are either known or not estimated directly. This, for example, corresponds to the situation with backcasted initials. Continuing our non-seasonal model example, we can use the following recursion (similar to (17.10)), keeping in mind that now the value of the initial state vector \\(\\mathbf{v}_0\\) is known: \\[\\begin{equation} \\mathbf{v}_{t+h-1} = \\hat{\\mathbf{F}}^{h-1} \\mathbf{v}_{t} + \\sum_{j=1}^{h-1} \\hat{\\mathbf{F}}^{j-1} \\hat{\\mathbf{g}} e_{t+h-j} . \\tag{17.12} \\end{equation}\\] The conditional variance of the state, given the values on observation \\(t\\) in (17.12) in general does not have a closed-form because of the exponentiation of the transition matrix \\(\\hat{\\mathbf{F}}\\). However, in a special case, when the matrix does not contain the parameters (e.g. non-damped trend ETS models or ARIMA without AR terms), there is an analytical solution to the variance. In this case, \\(\\mathbf{F}\\) is provided rather than being estimated, which simplifies the inference: \\[\\begin{equation} \\mathrm{V}(\\mathbf{v}_{t+h-1} | t) = \\mathrm{V}\\left(\\sum_{j=1}^{h-1} \\mathbf{F}^{j-1} \\hat{\\mathbf{g}} e_{t+h-j}\\right) \\tag{17.13} \\end{equation}\\] The variance of the sum in (17.13) can be expanded as: \\[\\begin{equation} \\mathrm{V} \\left(\\sum_{j=1}^{h-1} \\mathbf{F}^{j-1} \\hat{\\mathbf{g}} e_{t+h-j} \\right) = \\sum_{j=1}^{h-1} \\mathrm{V} \\left(\\mathbf{F}^{j-1} \\hat{\\mathbf{g}} e_{t+h-j}\\right) + 2 \\sum_{j=2}^{h-1} \\sum_{i=1}^{j-1} \\mathrm{cov}(\\mathbf{F}^{j-1} \\hat{\\mathbf{g}} e_{t+h-j},\\mathbf{F}^{i} \\hat{\\mathbf{g}} e_{t+h-i}). \\tag{17.14} \\end{equation}\\] Each variance in left-hand side of (17.14) can be expressed via: \\[\\begin{equation} \\mathrm{V} \\left(\\mathbf{F}^{j-1} \\hat{\\mathbf{g}} e_{t+h-j}\\right) = \\mathbf{F}^{j-1} \\left( \\mathrm{V} (\\hat{\\mathbf{g}}) \\mathrm{V}(e_{t+h-j}) + \\mathrm{V} (\\hat{\\mathbf{g}}) \\mathrm{E}(e_{t+h-j})^2 + \\mathrm{E} (\\hat{\\mathbf{g}}) \\mathrm{E} (\\hat{\\mathbf{g}})^\\prime \\mathrm{V}(e_{t+h-j})\\right) (\\mathbf{F}^{j-1})^\\prime. \\tag{17.15} \\end{equation}\\] Given that the expectation of error term is assumed to be zero, and substituting \\(\\mathrm{V}(e_{t+h-j})=\\sigma^2\\), this simplifies to: \\[\\begin{equation} \\mathrm{V} \\left(\\mathbf{F}^{j-1} \\hat{\\mathbf{g}} e_{t+h-j}\\right) = \\mathbf{F}^{j-1} \\left( \\mathrm{V} (\\hat{\\mathbf{g}}) + \\mathrm{E} (\\hat{\\mathbf{g}}) \\mathrm{E} (\\hat{\\mathbf{g}})^\\prime \\right) (\\mathbf{F}^{j})^\\prime \\sigma^2. \\tag{17.16} \\end{equation}\\] As for the covariances in (17.14), after the expansion it can be shown that each of them is equal to: \\[\\begin{equation} \\begin{aligned} \\mathrm{cov}(\\mathbf{F}^{j-1} \\hat{\\mathbf{g}} e_{t+h-j},\\mathbf{F}^{i} \\hat{\\mathbf{g}} e_{t+h-i}) = &amp; \\mathrm{V}(\\mathbf{F}^{j-1} \\hat{\\mathbf{g}}) \\mathrm{cov}(e_{t+h-i},e_{t+h-j}) \\\\ &amp; + \\left(\\mathbf{F}^{j-1} \\hat{\\mathbf{g}}\\right)^2 \\mathrm{cov}(e_{t+h-i},e_{t+h-j}) \\\\ &amp; + \\mathrm{E}(e_{t+h-i}) \\mathrm{E}(e_{t+h-j}) \\mathrm{V}(\\mathbf{F}^{j-1} \\hat{\\mathbf{g}}). \\end{aligned} \\tag{17.17} \\end{equation}\\] Given the assumptions of the model, the autocovariances of error terms should all be equal to zero, and the expectation of the error term should be equal to zero as well, which means that all the value in (17.17) will be equal to zero as well. Based on this, the conditional variance of states is equal to: \\[\\begin{equation} \\mathrm{V}(\\mathbf{v}_{t+h-1}|t) = \\sum_{j=1}^{h-1} \\mathbf{F}^{j-1} \\left( \\mathrm{V} (\\hat{\\mathbf{g}}) + \\mathrm{E} (\\hat{\\mathbf{g}}) \\mathrm{E} (\\hat{\\mathbf{g}})^\\prime \\right) (\\mathbf{F}^{j})^\\prime \\sigma^2 \\tag{17.18} \\end{equation}\\] As discussed in Section 5.3, the conditional variance of the actual value \\(h\\) steps ahead is: \\[\\begin{equation} \\mathrm{V}(y_{t+h}|t) = \\mathbf{w}^\\prime \\mathrm{V}(\\mathbf{v}_{t+h-1}|t) \\mathbf{w} + \\sigma^2 \\tag{17.19} \\end{equation}\\] Inserting (17.18) in (17.19), we get the final conditional h steps ahead variance of the model: \\[\\begin{equation} \\sigma^2_h = \\mathrm{V}(y_{t+h}|t) = \\left(\\mathbf{w}^\\prime \\sum_{j=1}^{h-1} \\mathbf{F}^{j-1} \\left( \\mathrm{V} (\\hat{\\mathbf{g}}) + \\mathrm{E} (\\hat{\\mathbf{g}}) \\mathrm{E} (\\hat{\\mathbf{g}})^\\prime \\right) (\\mathbf{F}^{j})^\\prime \\mathbf{w} + 1 \\right)\\sigma^2, \\tag{17.20} \\end{equation}\\] which looks similar to the one in formula (5.14) from Section 5.3, but now has the covariance of persistence vector in it. Unfortunately, the conditional variances for the other models are more complicated due to the introduction of convolutions of parameters. Furthermore, the formula (17.20) only focuses on the conditional variance given the known \\(\\mathbf{v}_t\\) but does not take into account the uncertainty of it for the fitted values in-sample. Given the complexity of the problem, in the next section, we introduce a technique that allows correctly propagating the uncertainty of parameters and initial values to the forecasts of any ADAM model. "],["adamRefitted.html", "17.4 Multi-scenarios for ADAM states", " 17.4 Multi-scenarios for ADAM states As discussed in Section 17.3, it is difficult to capture the impact of the uncertainty about the parameters on the states of the model and, as a result, difficult to take it into account on the forecasting stage. Furthermore, so far, we have only discussed pure additive models, for which it is at least possible to do some derivations. When it comes to models with multiplicative components, it becomes nearly impossible to demonstrate how the uncertainty propagates over time. To overcome these limitations, we develop a simulation-based approach that relies on the selected model form. The idea of the approach is to get the covariance matrix of the parameters of the selected model (see Section 17.1) and then generate \\(n\\) sets of parameters randomly from a rectified multivariate normal distribution using the matrix and the values of estimated parameters. After that, the model is applied to the data with each generated parameter combination to get the states, fitted values, and residuals. This way, we propagate the uncertainty about the parameters from the first observation to the last. The final states can also be used to produce point forecasts and prediction intervals based on each set of parameters. These scenarios allow creating more adequate prediction intervals from the model and/or confidence intervals for the fitted values, states and conditional expectations. All of this is done without additional assumptions (as it is done in bagging), relying entirely on the model. However, the approach is computationally expensive, as it requires fitting all the \\(n\\) models to the data, although no estimation is needed. If the uncertainty about the model needs to be taken into account, then the combination of models can be used, as described in Section 16.4. smooth package has the method reapply() that implements this approach for adam() models. This works with ADAM ETS, ARIMA, regression and any combination of the three. Here is an example in R with \\(n=1000\\): adamModelETS &lt;- adam(AirPassengers, &quot;MMM&quot;, h=10, holdout=TRUE) adamModelETSRefitted &lt;- reapply(adamModelETS, nsim=1000) ## Warning: The covariance matrix of parameters is not positive semi-definite. I ## will try fixing this, but it might make sense re-estimating adam(), tuning the ## optimiser. plot(adamModelETSRefitted) Figure 17.2: Refitted ADAM ETS(M,M,M) model on AirPassengers data. Figure 17.2 demonstrates how the approach works on the example of AirPassengers data with ETS(M,M,M) model. The grey areas around the fitted line show quantiles from the fitted values, forming confidence intervals of width 95%, 80%, 60%, 40% and 20%. They show how the fitted value would vary if the parameters would differ from the estimated ones. Note that there was a warning about the covariance matrix of parameters, which typically arises if the optimal value of the loss function was not reached. If this happens, I would recommend tuning the optimiser (see Section 11.4). For example, we could try more iterations via setting the maxeval parameter to a higher value or re-estimating the model, providing the estimates of parameters in B. If these fail and the bounds from the reapply() are too wide, it might make sense to consider backcasting for the initialisation. The adamModelETSRefitted object contains several variables, including: adamModelETSRefitted$states – the array of states of dimensions \\(k \\times (T+m) \\times n\\), where \\(m\\) is the maximum lag of the model, \\(k\\) is the number of components and \\(T\\) is the sample size; adamModelETSRefitted$refitted – distribution of fitted values of dimensions \\(T \\times n\\); adamModelETSRefitted$transition – the array of transition matrices of the size \\(k \\times k \\times n\\); adamModelETSRefitted$measurement – the array of measurement matrices of the size \\((T+m) \\times k \\times n\\); adamModelETSRefitted$persistence – the persistence matrix of the size \\(k \\times n\\); The last three will contain the random parameters (smoothing, damping and AR/MA parameters), which is why they are provided together with the other values. As mentioned earlier, ADAM ARIMA also supports this approach. Here is an example on artificial, non-seasonal data (see Figure 17.3): y &lt;- rnorm(200,100,10) adamModelARIMA1 &lt;- adam(y, &quot;NNN&quot;, h=10, holdout=TRUE, orders=c(0,1,1)) adamModelARIMARefitted1 &lt;- reapply(adamModelARIMA1) plot(adamModelARIMARefitted1) Figure 17.3: Refitted ADAM ARIMA(0,1,1) model on artificial data. Note that the more complicated the fitted model is, the more difficult it is to optimise it, and thus the more difficult it is to get accurate estimates of the covariance matrix of parameters. This might result in highly uncertain states and thus fitted values. The safer approach, in this case, is using bootstrap for the estimation of the covariance matrix, but this is more computationally expensive and would only work on longer time series. See example in R (and Figure 17.4): adamModelARIMA2 &lt;- adam(y, &quot;NNN&quot;, h=10, holdout=TRUE, orders=c(0,1,1)) adamModelARIMARefitted2 &lt;- reapply(adamModelARIMA2, bootstrap=TRUE, nsim=1000, parallel=TRUE) plot(adamModelARIMARefitted2) Figure 17.4: Refitted ADAM ARIMA(0,1,1) model on artificial data, bootstrapped covariance matrix. The approach described in this section is still a work in progress. While it works in theory, there are computational difficulties with calculating the Hessian matrix. If the covariance matrix is not estimated accurately, it might contain high variances, leading to the higher than needed uncertainty of the model. This will result in unreasonable confidence bounds and lead to extremely wide prediction intervals. "],["ADAMForecasting.html", "Chapter 18 Forecasting with ADAM", " Chapter 18 Forecasting with ADAM Finally, we come to the technicalities of producing forecasts using ADAM. We have already discussed how conditional expectations can be generated from some of the models (e.g. Sections 5.3, 6.3, 9.2.1, 10.2 and 10.3.1), but we have not discussed this in necessary detail. Furthermore, as discussed in Section 1.1, forecasts should align with specific decisions, but we have not discussed how to do that. In this chapter, we start the discussion with the principles behind calculating the conditional expectations from ADAM models (including ETS, ARIMA, regression and their combinations). We then discuss various methods for prediction intervals construction, starting from the basic parametric ones and ending with empirical and those that take the uncertainty of parameters into account (building upon Chapter 17). Finally, we discuss prediction intervals for the intermittent state space model (Chapter 13), one-sided intervals and cumulative forecasts over the forecast horizon, which is useful in practice, especially when inventory decisions need to be made. We also discuss the confidence interval for the conditional mean, which is not as important as the other topics mentioned above but is still worth discussing. "],["ADAMForecastingExpectation.html", "18.1 Conditional expectation", " 18.1 Conditional expectation As discussed in sections 5.3 and 6.3, the conditional h steps ahead expectation is in general available only for the pure additive models. In the cases of pure multiplicative ones, the point forecasts would correspond to conditional expectations only for \\(h \\leq m\\) for seasonal models and \\(h=1\\) for models with the trend. The one exception is the ETS(M,N,N) model, where the point forecast corresponds to the conditional expectation for any horizon as long as the expectation of the error term \\(1+\\epsilon_t\\) is equal to one. When it comes to the mixed models (Section 7.2), the situation would depend on the specific model, but in general, the same logic as for the pure multiplicative ones applies. In this case, the conditional expectations need to be obtained via other means. This is when the simulations come into play. 18.1.1 Simulating demand trajectories The general idea of this approach is to use the estimated parameters, last obtained state vector (level, trend, seasonal, ARIMA components etc.) and the estimate of the scale of distribution to generate the possible paths of the data. The simulation itself is done in several steps after obtaining parameters, states and scale: Generate \\(n \\times h\\) (where \\(n\\) is the number of time series to produce) random variables for the error term, \\(\\epsilon_t\\) or \\(1+\\epsilon_t\\) – depending on the type of error and assumed distribution in the model (the latter was discussed in Sections 5.5 and 6.5); Generate actual values from \\(n\\) models with the provided state vector, transition, measurement and persistence matrices and the generated error terms for the subsequent \\(h\\) observations. In a general case, this is done using the model (7.1); Take expectations for each horizon from 1 to \\(h\\). A thing to note is that in the case of multiplicative trend or multiplicative seasonality, it makes sense to take trimmed mean instead of the basic arithmetic one. This is because the models with these components might exhibit explosive behaviour, and thus the expectation might become unrealistic. I suggest using 1% trimming, although this does not have any scientific merit and is only based on my personal expertise. The simulation-based approach is universal, no matter what model is used, and can be applied to any ETS, ARIMA, regression model or combination (including dynamic ETSX, intermittent demand and multiple frequency models). Furthermore, instead of taking expectations on step 3, one can take geometric means, medians or any desired quantile. This is discussed later in Subsection 18.2.2. The main issue with this approach is that the conditional expectation and any other statistics calculated based on this will differ with every new simulation run. If \\(n\\) is small, these values will be less stable (vary more with the new runs). But they will reach some asymptotic values with the increase of \\(n\\), staying random nonetheless. However, this is a good thing because this randomness reflects the uncertain nature of these statistics in the sample: the true values are never known, and the estimates will inevitably change with the sample size change. Another limitation is the computational time and memory usage: the more iterations we want to produce, the more calculations will need to be done, and more memory will be used. Luckily, time complexity in this situation should be linear: \\(O(h \\times n)\\). 18.1.2 Explanatory variables If the model contains explanatory variables, then the h steps ahead conditional expectations should use them in the calculation. The main challenge in this situation is that the future values might not be known in some cases. This has been discussed in Section 10.2. Practically speaking, if the user provides the holdout sample values of explanatory variables, the forecast.adam() method will use them in forecasting. If they are not provided, the function will produce forecasts for each of the explanatory variables via the adam() function and use the conditional h steps ahead expectations in forecasting. 18.1.3 Demonstration in R In order to demonstrate how the approach works, we consider an artificial case of ETS(M,M,N) model with \\(l_t=1000\\), \\(b_t=0.95\\), \\(\\alpha=0.1\\), \\(\\beta=0.01\\), and Gamma distribution for error term with scale \\(s=0.05\\). We generate 1000 scenarios from this model for the horizon of \\(h=10\\) using sim.es() function from smooth package: nsim &lt;- 1000 h &lt;- 10 s &lt;- 0.1 initial &lt;- c(1000,0.95) persistence &lt;- c(0.1,0.01) y &lt;- sim.es(&quot;MMN&quot;, obs=h, nsim=nsim, persistence=persistence, initial=initial, randomizer=&quot;rgamma&quot;, shape=1/s, scale=s) After running the code above, we will obtain an object y that will contain several variables, including y$data with all the 1000 possible future trajectories. We can plot them to get an impression of what we are dealing with (see Figure 18.1): plot(y$data[,1], ylab=&quot;Sales&quot;, ylim=range(y$data), col=rgb(0.8,0.8,0.8,0.4), xlab=&quot;Horizon&quot;) # Plot all the generated lines for(i in 2:nsim){ lines(y$data[,i], col=rgb(0.8,0.8,0.8,0.4)) } # Add conditional mean and quantiles lines(apply(y$data,1,mean)) lines(apply(y$data,1,quantile,0.025), col=&quot;grey&quot;, lwd=2, lty=2) lines(apply(y$data,1,quantile,0.975), col=&quot;grey&quot;, lwd=2, lty=2) Figure 18.1: Data generated from 1000 ETS(M,M,N) models. Based on the plot in Figure 18.1, we can see what the conditional h steps ahead expectation (black line) and what the 95% prediction interval will be for the data based on the ETS(M,M,N) model with the parameters mentioned above. "],["ADAMForecastingPI.html", "18.2 Prediction intervals", " 18.2 Prediction intervals As discussed in Section 5.2 of Svetunkov (2021), a prediction interval is needed to reflect the uncertainty about the data. In theory, the 95% prediction interval will cover the actual values in 95% of the cases if the model is correctly specified. The specific formula for prediction interval will vary with the assumed distribution. For example, for the Normal distribution (assuming that \\(y_{t+j} \\sim \\mathcal{N}(\\mu_{y, t+j}, \\sigma_j^2)\\)) we will have the classical one: \\[\\begin{equation} y_{t+j} \\in (\\hat{y}_{t+j} + z_{\\alpha/2} \\hat{\\sigma}_j^2, \\hat{y}_{t+j} + z_{1-\\alpha/2} \\hat{\\sigma}_j^2), \\tag{18.1} \\end{equation}\\] where \\(\\hat{y}_{t+j}\\) is the estimate of \\(\\mu_{y,t+j}\\), \\(j\\) steps ahead conditional expectation, \\(\\hat{\\sigma}_j^2\\) is the estimate of \\({\\sigma}_j^2\\), \\(j\\) steps ahead variance of the error term (for example, calculated via the formula (5.11)) and \\(z\\) is z-statistics (quantile of standard normal distribution) for the selected significance level \\(\\alpha\\). Note that \\(\\alpha\\) has nothing to do with the smoothing parameters for the level of ETS models. This type of prediction interval can be called parametric. It assumes a specific distribution and relies on the other assumptions about the constructed model (such as residuals are i.i.d., see Section 15). Most importantly, it assumes that the \\(j\\) steps ahead value follows a specific distribution, related to the one for the error term. In case of normal distribution, the assumption \\(\\epsilon_t\\sim\\mathcal{N}(0,\\sigma^2)\\) implies that \\(y_{t+1}\\sim\\mathcal{N}(\\mu_{y,t+1}, \\sigma_1^2)\\) and due to convolution of random variables (the sum of random variables follows the same distribution as individual variables, but with different parameters), that \\(y_{t+j}\\sim\\mathcal{N}(\\mu_{y,t+j},\\sigma_j^2)\\) for all \\(j\\) from 1 to \\(h\\). The interval produced via (18.1) corresponds to two quantiles from normal distribution and can be written in a more general form as: \\[\\begin{equation} y_{t+j} \\in \\left(q \\left(\\hat{y}_{t+j},\\hat{\\sigma}_j^2,\\frac{\\alpha}{2}\\right), q\\left(\\hat{y}_{t+j},\\hat{\\sigma}_j^2,1-\\frac{\\alpha}{2}\\right)\\right), \\tag{18.2} \\end{equation}\\] where \\(q(\\cdot)\\) is a quantile function of an assumed distribution, \\(\\hat{y}_{t+j}\\) acts as a location and \\(\\hat{\\sigma}^2\\) acts as a scale of distribution. Using this general formula (18.2) for prediction interval, we can construct them for other distributions as long as they support convolution. In ADAM framework, this works for all pure additive models that have error term that follows one of the following distributions: Normal: \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\), thus \\(y_{t+j} \\sim \\mathcal{N}(\\mu_{y, t+j}, \\sigma_j^2)\\); Laplace: \\(\\epsilon_t \\sim \\mathcal{Laplace}(0, s)\\) and \\(y_{t+j} \\sim \\mathcal{Laplace}(\\mu_{y, t+j}, s_j)\\); S: \\(\\epsilon_t \\sim \\mathcal{S}(0, s)\\), \\(y_{t+j} \\sim \\mathcal{S}(\\mu_{y, t+j}, s_j)\\); Generalised Normal: \\(\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)\\), so that \\(y_{t+j} \\sim \\mathcal{GN}(\\mu_{y, t+j}, s_j, \\beta)\\). If a model has multiplicative components or relies on a different distribution, then the several steps ahead actual value will not necessarily follow the assumed distribution and the formula (18.2) will produce incorrect intervals. For example, if we work with a pure multiplicative ETS model, ETS(M,N,N), assuming that \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\), the two-steps-ahead actual value will be expressed in terms of the values on observation \\(t\\): \\[\\begin{equation} y_{t+2} = l_{t+1} (1+\\epsilon_{t+2}) = l_{t} (1+\\alpha \\epsilon_{t+1}) (1+\\epsilon_{t+2}) , \\tag{18.3} \\end{equation}\\] which introduces the product of normal distributions and thus \\(y_{t+2}\\) does not follow Normal distribution any more. In such cases, we might have several options of what to use in order to produce intervals. They are discussed in the subsections below. 18.2.1 Approximate intervals Even if the actual multistep value does not follow the assumed distribution, we can use approximations in some cases: the produced prediction interval will not be too far from the correct one. The main idea behind the approximate intervals is to rely on the same distribution for \\(y_{t+j}\\) as for the error term, even though we know that the variable will not follow it. In the case of multiplicative error models, the limit (6.5) can be used to motivate the usage of that assumption. For example, in the case of the ETS(M,N,N) model, we know that \\(y_{t+2}\\) will not follow the normal distribution, but if the variance of the error term is low (e.g. \\(\\sigma^2 &lt; 0.05\\)) and the smoothing parameter \\(\\alpha\\) is close to zero, then the normal distribution would be a satisfactory approximation of the real one. This becomes clear if we expand the brackets in (18.3): \\[\\begin{equation} y_{t+2} = l_{t} (1 + \\alpha \\epsilon_{t+1} + \\epsilon_{t+2} + \\alpha \\epsilon_{t+1} \\epsilon_{t+2}) . \\tag{18.4} \\end{equation}\\] With the conditions discussed above (low \\(\\alpha\\), low variance) the term \\(\\alpha \\epsilon_{t+1} \\epsilon_{t+2}\\) will be close to zero, thus making the sum of normal distributions dominate in the formula (18.4). The advantage of this approach is in its speed: you only need to know the scale parameter of the error term and the conditional expectation. The disadvantage of the approach is that it becomes inaccurate with the increase of parameters and scale of the model. The rule of thumb, when to use this approach: if the smoothing parameters are all below 0.1 (in case of ARIMA, this is equivalent to MA terms being negative and AR terms being close to zero) or the scale of distribution is below 0.05, the differences between the proper interval and the approximate one should be negligible. 18.2.2 Simulated intervals This approach relies on the idea discussed in Subsection 18.1.1. It is universal and supports any distribution because it only assumes that the error term follows the selected distribution (no need for the actual value to do that as well). The simulated paths are then produced based on the generated values and the assumed model. After generating \\(n\\) paths, one can take the desired quantiles to get the bounds of the interval. The main issue of the approach is that it is time-consuming (slower than the approximate intervals) and might be highly inaccurate if the number of iterations \\(n\\) is low. This approach is used as a default in adam() for the non-additive models. 18.2.3 Semiparametric intervals The three approaches above assume that the residuals of the applied model are i.i.d. (see discussion in Chapter 15). If this assumption is violated (for example, the residuals are autocorrelated), then the intervals might be miscalibrated (i.e. producing wrong values). In this case, we might need to use different approaches. One of these is the construction of semiparametric prediction intervals (see, for example, Lee and Scholtes, 2014). This approach relies on the in-sample multistep forecast errors, discussed in Subsection 15.7.1. After producing \\(e_{t+j|t}\\) for all in sample values of \\(t\\) and for \\(j=1,\\dots,h\\), we can use these errors to calculate the respective h steps ahead conditional variances \\(\\sigma_j^2\\) for \\(j=1,\\dots,h\\). These values can then be inserted in the formula (18.2) to get the desired prediction intervals. The approach works well in the case of pure additive models, as it relies on specific assumed distribution. However, it might have limitations similar to those discussed earlier for the mixed models and the models with positively defined distributions (such as Log-Normal, Gamma and Inverse Gaussian). It can be considered a semiparametric alternative to the approximate method discussed above. 18.2.4 Nonparametric intervals When some of the assumptions might be violated, and when we cannot rely on the parametric distributions, we can use the nonparametric approach, proposed by Taylor and Bunn (1999). The authors proposed using the multistep forecast errors to construct the following quantile regression model: \\[\\begin{equation} \\hat{e}_{t+j} = a_0 + a_1 j + a_2 j^2, \\tag{18.5} \\end{equation}\\] for each of the bounds of the interval. The motivation behind the polynomial in (18.5) is because typically, the multistep conditional variance will involve the square of the forecast horizon. The main issue with this approach is that the polynomial function has an extremum, which might appear sometime in the future. For example, the upper bound of the interval would increase until that point and then start declining. To overcome this limitation, I propose using the power function instead: \\[\\begin{equation} \\hat{e}_{t+j} = a_0 j^a_1 . \\tag{18.6} \\end{equation}\\] This way, the bounds will always change monotonically, and the parameter \\(a_1\\) will control the speed of expansion of the interval. The model (18.6) is estimated using quantile regression for the upper and the lower bounds separately as Taylor and Bunn (1999) recommend. This approach does not require any assumptions about the model and works as long as there are enough observations in-sample (so that the matrix of forecast errors contains more rows than columns). The main limitation of this approach is that it relies on quantile regression and thus will have the same issues as, for example, pinball score has (see discussion in Section 2.2): the quantiles are not always uniquely defined. Another limitation is that we assume that the quantiles will follow the model (18.6), which might be violated in real-life scenarios. 18.2.5 Empirical intervals Another alternative to the parametric intervals uses the same matrix of multistep forecast errors as discussed earlier. The empirical approach is more straightforward than the approaches discussed above and does not rely on any assumptions (it was discussed in Lee and Scholtes, 2014). The idea behind it is just to take quantiles of the forecast errors for each forecast horizon \\(j=1,\\dots,h\\). These quantiles are then added to the point forecast if the error term is additive or are multiplied by it in the case of the multiplicative one. (Kourentzes2021TBA?) show that the empirical prediction intervals perform on average better than the other approaches. So, in general, I would recommend producing empirical intervals if it was not for the computational difficulties related to the multistep forecast errors. If you have an additive model and believe that the assumptions are satisfied, then the parametric interval will be as accurate but faster. Furthermore, the approach will be unreliable on small samples due to the same problem with quantiles discussed earlier. 18.2.6 Complete parametric intervals So far, all the intervals discussed above relied on an unrealistic assumption that the parameters of the model are known. This is one of the reasons why the intervals produced for ARIMA and ETS are typically narrower than expected (see, for example, results of tourism competition, Athanasopoulos et al., 2011). But as we discussed in Section 17, there are ways of capturing the uncertainty of estimated parameters of the model and propagating it to the future uncertainty (i.e. to conditional h steps ahead variance). As discussed in Section 17.4, the more general approach is to create many in-sample model paths based on randomly generated parameters of the model. This way, we can obtain a variety of states for the final in-sample observation \\(T\\) and then use those values to construct final prediction intervals. The simplest and most general way of producing intervals, in this case, is using simulations (as discussed earlier in Subsection 18.2.2). The intervals produced via this approach will be wider than the conventional ones, and their width will be proportional to the uncertainty around the parameters. This also means that the intervals might become too wide if the uncertainty is not captured correctly (see discussion in Section 17.4). One of the main limitations of the approach is its computational time: it will be proportional to the number of simulation paths for both refitted model and prediction intervals. It is also theoretically possible to use other approaches for the intervals construction in case of the complete uncertainty (e.g. “empirical” one for each of the set of parameters of the model), but they would be even more computationally expensive than the approach described above and will have the limitations similar to the discussed above (i.e. non-uniqueness of quantiles, sample size requirements). 18.2.7 Explanatory variables In all the cases described above, when constructing prediction intervals for the model with explanatory variables, we assume that their values are known in the future. Even if they are not provided by the user and need to be forecasted, the produced conditional expectations will be used for all the calculations. This is not an entirely correct approach, as was shown in Subsection 10.2.2, but it speeds up the calculation process and typically produces adequate results. The more theoretically correct approach is to take the multistep variance of explanatory variables into account. This would work for pure additive models for explanatory and response variables but imply more complicated formulae for other models. 18.2.8 Example in R All the types of intervals discussed in this Section are implemented for the adam() models in smooth package. In order to demonstrate how they work and how they differ, we consider the example with ETS model on BJSales data: adamModel &lt;- adam(BJsales, h=10, holdout=TRUE) modelType(adamModel) ## [1] &quot;AAdN&quot; The model selected above is ETS(A,Ad,N). In order to make sure that the parametric intervals are suitable, we can do model diagnostics (see Section 15), producing plots shown in Figure 18.2. par(mfcol=c(2,3)) plot(adamModel,c(2,4,6,8,10:11)) Figure 18.2: Diagnostics of the ADAM model on BJSales data. The model’s residuals do not exhibit any serious issues. Given that this is a pure additive model, we can conclude that the parametric intervals would be appropriate for this situation and produce the plot with them. The only thing that these intervals do not take into account is the uncertainty about the parameters, so we can construct the respective intervals either via the reforecast() function or using the same forecast(), but with the option interval=\"complete\". Note that this is a computationally expensive operation (both in terms of time and memory), so the more iterations you set up, the longer it will take and the more memory it will consume. The two intervals are shown next to each other in Figure 18.3: par(mfcol=c(1,2)) plot(forecast(adamModel,h=10,interval=&quot;parametric&quot;), main=&quot;Parametric prediction interval&quot;, ylim=c(200,280)) plot(forecast(adamModel,h=10,interval=&quot;complete&quot;,nsim=100), main=&quot;Complete prediction interval&quot;, ylim=c(200,280)) Figure 18.3: Prediction intervals for ADAM on BJSales data. The resulting complete parametric interval shown in Figure 18.3 will not be substantially different from the parametric one, maybe slightly wider. This is because the smoothing parameters of the model are high, thus the model forgets the initial states fast, and the uncertainty does not propagate to the last observation as much as in the case of lower values of parameters: summary(adamModel) ## ## Model estimated using adam() function: ETS(AAdN) ## Response variable: BJsales ## Distribution used in the estimation: Normal ## Loss function type: likelihood; Loss function value: 241.078 ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## alpha 0.9507 0.1144 0.7244 1.0000 * ## beta 0.2911 0.1395 0.0152 0.5667 * ## phi 0.8632 0.1257 0.6147 1.0000 * ## level 202.7529 4.7209 193.4158 212.0829 * ## trend -2.3996 3.5642 -9.4491 4.6445 ## ## Error standard deviation: 1.384 ## Sample size: 140 ## Number of estimated parameters: 6 ## Number of degrees of freedom: 134 ## Information criteria: ## AIC AICc BIC BICc ## 494.1560 494.7876 511.8058 513.3664 Figure 18.4 demonstrates what happens with the fitted values when we take the uncertainty into account. plot(reapply(adamModel)) Figure 18.4: Refitted values for ADAM on BJSales data. As we see from Figure 18.4, the uncertainty around the line is narrow at the end of the sample, so the prediction intervals are similar between the two methods. To make things more complicated and exciting, we introduce explanatory variable with lags and leads of the indicator BJsales.lead, automatically selecting the model and explanatory variables using information criteria (see discussion in Chapter 16). BJsalesData &lt;- cbind(as.data.frame(BJsales), xregExpander(BJsales.lead,c(-3:3))) colnames(BJsalesData)[1] &lt;- &quot;y&quot; adamModelX &lt;- adam(BJsalesData, &quot;YYY&quot;, h=10, holdout=TRUE, regressors=&quot;select&quot;) I have asked the function specifically to do the selection between pure multiplicative models (see Section 16.1). We will then construct several types of prediction intervals and compare them: adamModelPI &lt;- vector(&quot;list&quot;,6) intervalType &lt;- c(&quot;approximate&quot;, &quot;semiparametric&quot;, &quot;nonparametric&quot;, &quot;simulated&quot;, &quot;empirical&quot;, &quot;complete&quot;) for(i in 1:6){ adamModelPI[[i]] &lt;- forecast(adamModelX, h=10, interval=intervalType[i]) } These can be plotted in Figure 18.5. par(mfcol=c(3,2), mar=c(2,2,2,1)) for(i in 1:6){ plot(adamModelPI[[i]],main=paste0(intervalType[i],&quot; interval&quot;)) } Figure 18.5: Different prediction intervals for ADAM ETS(M,N,N) on BJSales data The thing to notice is how the width and shape of intervals change depending on the used approach. The approximate and simulated intervals look very similar because the selected model is ETSX(M,N,N) with a standard error of 0.005 (thus, the approximation works well). The complete interval is similar because the estimated smoothing parameter \\(\\alpha=1\\) (hence, the forgetting happens instantaneously). However, it has a slightly different shape because the number of iterations for the interval was low (nsim=100 for interval=\"complete\" by default). The semiparametric interval is the widest as it calculates the forecast errors directly but still uses the normal approximation. Both nonparametric and empirical are skewed because the in-sample forecast errors followed skewed distributions, which can be seen via the plot (see Figure 18.6): adamModelXForecastErrors &lt;- rmultistep(adamModelX,h=10) boxplot(1+adamModelXForecastErrors) abline(h=1,col=&quot;red&quot;) points(apply(1+adamModelXForecastErrors,2,mean), col=&quot;red&quot;,pch=16) Figure 18.6: Distribution of in-sample multistep forecast errors from ADAM ETSX(M,N,N) model on BJSales data. Red point correspond to mean values. Analysing the plot 18.5, it might be challenging to select the most appropriate type of prediction interval. But the model diagnostics (Section 15) might help in this situation: If the residuals look i.i.d. and the model does not omit important variables, then choose between parametric, approximate and simulated intervals, choose: “parametric” in case of the pure additive model, “approximate” in other cases, when the standard error is lower than 0.05 or smoothing parameters are close to zero, “simulated” if you deal with a non-additive model with high values of standard error and smoothing parameters, “complete parametric” when the smoothing parameters of the model are close to zero, and you want to take the uncertainty about the parameters into account; If residuals seem to follow the assumed distribution but are not i.i.d., then the “semiparametric” approach might help. Note that this only works on samples of \\(T&gt;&gt;h\\); If residuals do not follow the assumed distribution, but your sample is still longer than the forecast horizon, then use either “empirical” or “nonparametric” intervals. Finally, forecast.adam() will automatically select between “parametric,” “approximate” and “simulated” if you ask for interval=\"prediction\". References "],["forecastingADAMOther.html", "18.3 Other aspects of forecast uncertainty", " 18.3 Other aspects of forecast uncertainty There are other elements related to forecasting and taking uncertainty into account that we have not discussed in the previous sections. Here we discuss several special cases where forecasting approaches might differ from the conventional ones. 18.3.1 Constructing intervals for intermittent demand model When it comes to constructing prediction intervals for the intermittent state space model (see Section 13), then there is an important aspect that should be taken into account. Given that the model consists of two parts: demand sizes and demand occurrence, the prediction interval should take the uncertainty from both parts into account. In this case, we should first predict the probability of occurrence of demand for the h steps ahead and then decide what the width of the interval should be based on this probability. For example, if we estimate that the demand will occur with probability \\(\\hat{p}_{t+h|t} = 0.8\\), then this means that we expect that in 20% of the cases, we will observe zeroes. This should reduce the confidence level for the demand sizes. Formally speaking, this comes to the following equation: \\[\\begin{equation} F_y(y_{t+h} \\leq q) = \\hat{p}_{t+h|t} F_z(z_{t+h} \\leq q) +(1 -\\hat{p}_{t+h|t}), \\tag{18.7} \\end{equation}\\] where \\(F_y(\\cdot)\\) is the cumulative distribution function of demand, \\(F_z(\\cdot)\\) is the cumulative distribution function of the demand sizes, \\(\\hat{p}_{t+h|t}\\) is the h steps ahead expected probability of occurrence and \\(q\\) is the quantile of distribution. In the formula (18.7), we know the expected probability and we know the confidence level \\(F_y(y_{t+h} \\leq q)\\). The unknown element is the \\(1-\\alpha = F_z(z_{t+h} \\leq q)\\). So after regrouping elements we get: \\[\\begin{equation} F_z(z_{t+h} \\leq q) = \\frac{F_y(y_{t+h} \\leq q) -(1 -\\hat{p}_{t+h|t})}{\\hat{p}_{t+h|t}}. \\tag{18.8} \\end{equation}\\] For example, if the confidence level is 0.95 and the expected probability of occurrence is 0.8, then \\(F_z(z_{t+h} \\leq q) = \\frac{0.95 -0.2}{0.8} = 0.9375\\). Assuming a distribution for the demand sizes, we can use formula (18.2) to construct the parametric prediction interval. Alternatively, we can use any other approach discussed in Section 18.2 to generate the intervals. 18.3.2 One-sided prediction interval In some cases, we might not need both bounds of the interval. For example, when we deal with intermittent demand, we know that the lower bound will be equal to zero in many cases. In fact, this will always happen when the significance level is lower than the probability of inoccurrence \\(1-p_{t+h|t}\\): we will have the quantile equal to zero because the probability of having zeroes is higher than the significance level. Another example is the safety stock calculation: we only need the upper bound of the interval, and we need to make sure that the specific proportion of demand is satisfied (e.g. 95% of it). In these cases, we can just focus on the particular bound of the interval and drop the other one. Statistically speaking, this means that we cut only one tail of the assumed distribution. This has its implications and issues in several scenarios: When we are interested in upper bound only and deal with positive distribution of demand (for example, Gamma, Log-Normal or Inverse Gaussian), we know that the demand will always lie between zero and the constructed bound. In cases of low volume (or even intermittent) data, this makes sense because the original data might contain zeroes or have values close to it. The upper bound in this case will be lower than in the case of the two-sided prediction interval because we would not be splitting the probability into two parts (for the left and the right tails); The combination of lower bound and positive distribution implies that the demand will be greater than the specified bound in the pre-selected number of cases (defined by confidence level). There is no natural bound from above, so from a theoretical point of view, this implies that the demand can be infinite; Upper or lower bound with real-valued distribution (such as Normal, Laplace, S or Generalised Normal) implies that the demand is either below or above the specified level, respectively, without any natural limit on the other side. If Normal distribution is used on positive low volume data, there is a natural lower bound, but the model itself will not be aware of it and will not restrict the space with the specific value. 18.3.3 Cumulative over the horizon forecast Another related thing to consider when producing forecasts in practice is that the point forecast is not needed in some contexts. Instead, the cumulative over the forecast horizon (or over the lead time) might be more suitable. The classic example is the safety stock calculation based on the lead time (time between the order of product and its delivery). In this situation, we need to make sure that while the product is being delivered, we do not run out of stock, thus still satisfying the selected level of demand (e.g. 95%), but now over the whole period of time rather than on every separate observation. In the case of pure additive ADAM models, there are analytical formulae for the conditional expectations and conditional variance for this case that can be used in forecasting. These formulae come directly from the recursive relation (5.9) Svetunkov and Petropoulos (2018): \\[\\begin{equation} \\begin{aligned} \\mu_{Y,t,h} = \\text{E}(Y_{c,t,h}|t) = &amp; \\sum_{j=1}^h \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i}&#39; \\mathbf{F}_{m_i}^{\\lceil\\frac{j}{m_i}\\rceil-1} \\right) \\mathbf{v}_{t} \\\\ \\sigma^2_{Y,h} = \\text{V}(Y_{c,t,h}|t) = &amp; \\left(1 + \\sum_{k=1}^{h-1} \\left(1+ (h-k) \\sum_{i=1}^d \\left(\\mathbf{w}_{m_i}&#39; \\sum_{j=1}^{\\lceil\\frac{k}{m_i}\\rceil-1} \\mathbf{F}_{m_i}^{j-1} \\mathbf{g}_{m_i} \\mathbf{g}&#39;_{m_i} (\\mathbf{F}_{m_i}&#39;)^{j-1} \\mathbf{w}_{m_i} \\right) \\right) \\right) \\sigma^2 \\end{aligned}, \\tag{18.9} \\end{equation}\\] where \\(Y_{c,t,h}=\\sum_{j=1}^h y_{t+j}\\) is the cumulative actual value and all the other variables have been defined in Section 5.2. Based on these expectation and variance, we can construct prediction interval as discussed in Section 18.2. In cases of multiplicative and mixed ADAM models, there are no closed forms for the conditional expectation and variance. As a result, simulations similar to the one discussed in Subsection 18.1.1 are needed to produce all possible paths for the next \\(h\\) steps ahead. The main difference would be that before taking the expectation or quantiles, the paths would need to be aggregated over the forecast horizon \\(h\\). This approach, together with the idea of a one-sided prediction interval, can be directly used to calculate the safety stock over the lead time. 18.3.4 Example in R For the demonstration purposes, we consider an artificial intermittent demand example, similar to the one from Section 13.4: y &lt;- ts(c(rpois(20,0.25), rpois(20,0.5), rpois(20,1), rpois(20,2), rpois(20,3))) For simplicity, we apply iETS(M,Md,N) model with odds ratio occurrence: adamModeliETS &lt;- adam(y, &quot;MMdN&quot;, occurrence=&quot;odds-ratio&quot;, h=7, holdout=TRUE) plot(adamModeliETS,7) To make this setting closer to a possible real-life situation, we assume that the lead time is seven days, and we need to satisfy the 99% of demand for the last seven observations based on our model. Thus we produce the upper bound for the cumulative values for the confidence level of 99%: adamModeliETSForecast &lt;- forecast(adamModeliETS, h=7, cumulative=TRUE, interval=&quot;prediction&quot;, side=&quot;upper&quot;) Given that we deal with cumulative values, the basic plot will not be helpful, we should produce a different one (see Figure 18.7): plot(sum(adamModeliETS$holdout), ylab=&quot;Cumulative demand&quot;, xlab=&quot;&quot;, pch=16, ylim=range(c(0, sum(adamModeliETS$holdout), adamModeliETSForecast$upper))) abline(h=adamModeliETSForecast$mean, col=&quot;blue&quot;, lwd=2) abline(h=adamModeliETSForecast$upper, col=&quot;grey&quot;, lwd=2) Figure 18.7: Cumulative demand, expectation and the upper bound based on iETS model. What Figure 18.7 demonstrates is that for the holdout period of 7 days, the cumulative demand was around 27 units, while the upper bound of the interval was approximately 75. Based on that upper bound, we could place an order (based on what we already have in stock) and have an appropriate safety stock. To see if the approach is suitable, we would need to apply it in either a rolling origin fashion (Section 2.4) or to a set of products to collect the distribution of related error measures. 18.3.5 Confidence interval Finally, we can construct a confidence interval for some statistics. As discussed in Section 5.2 of Svetunkov (2021), it can be built for the mean, a parameter, fitted values, etc. In our context, we might be interested in the confidence interval for the conditional h steps ahead expectation. This implies that we are interested in the uncertainty of the line, not of the actual values, which can only be constructed for the model that takes the uncertainty of parameters into account (as discussed in Section 17). The construction of confidence interval, in this case, relies on the normal distribution, based on Central Limit Theorem, as long as the basic assumptions for the model and CLT are satisfied (see Section 4.2 and Chapter 12 of Svetunkov, 2021). Technically speaking, the construction of confidence interval comes to capturing the model uncertainty discussed in Chapter 17. 18.3.5.1 Example in R The only way how the confidence interval can be constructed for ADAM models is via the reforecast() function. Consider the example with ADAM ETS(A,Ad,N) on BJSales data as in Section 18.2.8: adamModel &lt;- adam(BJsales, h=10, holdout=TRUE) The confidence interval for this model can be produced either directly via reforecast() or via forecast(), which will call it for you: adamModelConfidence &lt;- forecast(adamModel, h=10, interval=&quot;confidence&quot;, nsim=1000) plot(adamModelConfidence) Note that I have increased the number of iterations for the simulation to get a more accurate confidence interval around the conditional expectation. This will consume more memory, as the operation involves creating 1000 sample paths for the fitted values and another 1000 for the holdout sample forecasts. Figure 18.8: Confidence interval for the point forecast from ADAM ETS(A,Ad,N) model. Figure 18.8 shows the uncertainty around the point forecast based on the uncertainty of parameters of the model. As can be seen, the interval is narrow, demonstrating that the conditional expectation would not change much if the model’s parameters would vary slightly. The fact that the actual values are systematically above the forecast does not mean anything because the confidence interval does not consider the uncertainty of actual values. References "],["conclusions.html", "Chapter 19 What’s next?", " Chapter 19 What’s next? Having discussed the main aspects of ADAM and how to use it in different circumstances, I want to pause to look back at what we have covered and what is left behind. The reason why I did not call this textbook “Forecasting with ETS” or “Forecasting with State space models” is because the framework proposed here is not the same as for ETS, and it does not rely on the standard state space model. The combination of ETS, ARIMA and Regression in one unified model has not been discussed in the literature before. This is then extended by introducing a variety of distributions: typically, dynamic models rely on Normality, which is not realistic in real life, but ADAM supports several real-valued and several positive distributions. Furthermore, the model that can be applied to both regular and intermittent demand has been developed only by Svetunkov and Boylan (2019) but has not been published yet (and god knows when it will be). This is included in ADAM. In addition, the model then can be extended with multiple seasonal components, making it applicable to high-frequency data. All of the aspects mentioned above are united in one approach, giving immense flexibility to the analyst. But what’s next? While we have discussed the important aspects of ADAM, there are still several things left that I did not have time to make work yet. The first one is the scale model for ADAM. This is already implemented for the alm() function in the greybox package and will be discussed in Svetunkov (2021) in one of the subsequent online editions. For ADAM, this would mean that the scale parameter (e.g. variance in Normal distribution) can be modelled explicitly using the combination of ETS, ARIMA and regression models. This would have some similarities with GARCH and/or GAMLSS, but (again) would be united in one framework. The second one is the ADAM with asymmetric Laplace and similar distributions with the non-zero mean (the related to this is the estimation of ADAM with non-standard loss functions, such as pinball). While it is possible to use such distributions, in theory, they do not work as intended in the ADAM framework. They work perfectly in the case of the regression model (e.g. see how the alm() from the greybox works with the Asymmetric Laplace) but fail when a model has MA-related terms. This is because the model becomes more adaptive to the changes, pulls to the centre and cannot maintain the desired quantile. An introduction of such distributions would imply changing the architecture of the state space model. Third, model combination and selection literature has seen several bright additions to the field, such as a stellar paper by Kourentzes et al. (2019a) on pooling. This is neither implemented in the adam() function nor discussed in the textbook. Yes, one can use ADAM to do pooling, but it would make sense to introduce it as a part of the ADAM approach. Fourth, we have not discussed multiple frequency models in detail that they deserve. For example, we have not mentioned how to diagnose such models when the sample includes thousands of observations. The classical statistical approaches discussed in Section 15 typically fail in this situation, and other tools can be used in this context. Fifth, adam() has a built-in missing values approach that relies on interpolation and intermittent state space model (from Section 13). While this already works in practice, there are some aspects of this that are worth discussing that have been left outside this textbook. Finally, while I tried to introduce examples of application of ADAM, case studies for several contexts would be helpful. This would show how ADAM can be used for decisions in inventory management (we have touched the topic in Subsection 18.3.4), scheduling, staff allocation etc. All of this will hopefully come in the next editions of this textbook. References "],["about-the-author.html", "About the author", " About the author Ivan Svetunkov is a Lecturer of Marketing Analytics at Lancaster University, UK and a Marketing Director of Centre for Marketing Analytics and Forecasting. He has PhD in Management Science from Lancaster University and a candidate degree in economics from Saint Petersburg State University of Economics and Finance. His areas of interest include statistical methods of analytics and forecasting, focusing on demand forecasting in supply-chain and retail. He is a creator and maintainer of several forecasting- and analytics-related R packages, such as Svetunkov (2022a), Svetunkov (2022b) and Svetunkov and Pritularga (2022). LinkedIn, Twitter, Open Forecasting website, SoundCloud, ResearchGate, StackExchange, StackOverflow and GitHub. References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
