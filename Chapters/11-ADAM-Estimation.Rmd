# Estimation of ADAM {#ADAMETSEstimation}
Now that we have discussed the properties of ETS, ARIMA, ETSX and ARIMAX models, we need to understand how to estimate them. As mentioned earlier, when we apply a model to the data, we assume that it is suitable and we need to see how it fits the data and produces forecasts to assess this suitability. In this case, all the model parameters are substituted by their estimates (observed in the sample), and the error term becomes an estimate of the true one. In general, this means that the state space model \@ref(eq:ETSADAMStateSpace) is substituted by:
\begin{equation}
  \begin{aligned}
    {y}_{t} = &w(\hat{\mathbf{v}}_{t-\boldsymbol{l}}) + r(\hat{\mathbf{v}}_{t-\boldsymbol{l}}) e_t \\
    \hat{\mathbf{v}}_{t} = &f(\hat{\mathbf{v}}_{t-\boldsymbol{l}}) + \hat{g}(\hat{\mathbf{v}}_{t-\boldsymbol{l}}) e_t
  \end{aligned},
  (\#eq:ETSADAMStateSpaceEstimatedAgain)
\end{equation}
implying that the initial values of components and the smoothing parameters of the model are estimated. An example is the ETS(A,A,A) model applied to the data is:
\begin{equation}
  \begin{aligned}
    \hat{y}_{t} = & \hat{l}_{t-1} + \hat{b}_{t-1} + \hat{s}_{t-m} \\
    e_t = & y_t - \hat{y}_{t} \\
    \hat{l}_t = & \hat{l}_{t-1} + \hat{b}_{t-1} + \hat{\alpha} e_t \\
    \hat{b}_t = & \hat{b}_{t-1} + \hat{\beta} e_t \\
    \hat{s}_t = & \hat{s}_{t-m} + \hat{\gamma} e_t
  \end{aligned},
  (\#eq:ETSADAMAAAEstimated)
\end{equation}
where the initial values $\hat{l}_0, \hat{b}_0$ and $\hat{s}_{-m+2}, ... \hat{s}_0$ are estimated and then influence all the future values of components via the recursion \@ref(eq:ETSADAMAAAEstimated) and $e_t = y_t -\hat{y}_t$ is the one step ahead in sample forecast error, also known in statistics as the residual of the model. The set of equations \@ref(eq:ETSADAMAAAEstimated) allows constructing the model by applying equations one by one (you can even do that in MS Excel y creating five columns for the respective five equations).

The estimation itself does not happen on its own, a complicated process of minimisation/maximisation of the pre-selected loss function by changing the values of parameters is involved. Typically, there is no analytical solution for estimates of ADAM parameters because of the model's recursive nature. As a result, numerical optimisation is used to obtain the estimates of parameters. The results of the estimation will differ depending on:

1. the assumed distribution,
2. the used loss function,
3. the initial values of parameters that are fed to the optimiser,
4. the parameters of the optimiser (such as sensitivity, number of iterations etc.),
5. the sample size,
6. the number of parameters to estimate and,
7. the restrictions imposed on parameters.

The aspects above are covered in this chapter.


## Maximum Likelihood Estimation {#ADAMETSEstimationLikelihood}
Maximum Likelihood Estimation (MLE) is one of the popular approaches for estimating parameters of a statistical model. It relies on assumptions about the distribution of the response variable and uses either a probability density or a cumulative distribution functions, depending on the assumed model and aims to maximise the likelihood that the observations can be described by the model with specific parameters. An interested reader can find a detailed explanation of likelihood approach in Chapter 16 of @SvetunkovSBA.

MLE of the ADAM relies on the distributional assumptions of each specific model and might differ from one model to another (see assumptions in Sections \@ref(ADAMETSAdditiveDistributions), \@ref(ADAMETSMultiplicativeDistributions) and \@ref(ADAMARIMADistributions)). There are several options for the `distribution` supported by the `adam()` function in the `smooth` package, here we briefly discuss how the estimation is done for each one of them. We start with the additive error model, for which the assumptions, log-likelihoods and MLE of scales are provided in Table \@ref(tab:additiveErrorLikelihoods).

```{r echo=FALSE}
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\begin{landscape}')
}
# Assumption | log-likelihood \ell(\boldsymbol{\theta}, {\sigma}_\epsilon^2 | \mathbf{y}) | Scale
# Normal distribution
distributionsTable <- c("$\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$",
                        "$-\\frac{T}{2} \\log(2 \\pi \\sigma^2) -\\frac{1}{2} \\sum_{t=1}^T \\frac{\\epsilon_t^2}{\\sigma^2}$",
                        "$\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T e_t^2$",
# Laplace distribution
                        "$\\epsilon_t \\sim \\mathcal{L}(0, s)$",
                        "$-T \\log(2 s) -\\sum_{t=1}^T \\frac{|\\epsilon_t|}{s}$",
                        "$\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T |e_t|$",
# S distribution
                        "$\\epsilon_t \\sim \\mathcal{S}(0, s)$",
                        "$-T \\log(4 s^2) -\\sum_{t=1}^T \\frac{\\sqrt{|\\epsilon_t|}}{s}$",
                        "$\\hat{s} = \\frac{1}{2T} \\sum_{t=1}^T \\sqrt{|e_t|}$",
# GN distribution
                        "$\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)$",
                        "$\\begin{aligned} &T\\log\\beta -T \\log(2 s \\Gamma\\left(\\beta^{-1}\\right)) -\\\\
                                         &\\sum_{t=1}^T \\frac{\\left|\\epsilon_t\\right|^\\beta}{s}\\end{aligned}$",
                        "$\\hat{s} = \\sqrt[^{\\beta}]{\\frac{\\beta}{T} \\sum_{t=1}^T\\left| e_t \\right|^{\\beta}}$",
# Asymmetric Laplace distribution
                        # "$\\epsilon_t \\sim \\mathcal{ALaplace}(0, s, \\alpha)$",
                        # "$\\begin{aligned} &T\\log\\left(\\alpha(1-\\alpha)\\right) -T \\log(s) -\\\\
                        #                  &\\sum_{t=1}^T \\frac{\\epsilon_t (\\alpha -I(\\epsilon_t \\leq 0))}{s} \\end{aligned}$",
                        # "$\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T e_t(\\alpha -I(e_t \\leq 0))$",
# IG distribution
                        "$1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\sim \\mathcal{IG}(1, \\sigma^2)$",
                        "$\\begin{aligned} &-\\frac{T}{2} \\log \\left(2 \\pi \\sigma^2 \\right) -\\frac{1}{2} \\sum_{t=1}^T \\left( \\left(\\frac{y_t}{\\mu_{y,t}}\\right)^3 \\right) -\\\\
                                         &\\frac{3}{2}\\sum_{t=1}^T \\log y_t -\\frac{1}{2\\sigma^2} \\sum_{t=1}^{T} \\frac{\\epsilon_t^2}{\\mu_{y,t}y_t}\\end{aligned}$",
                        "$\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{e_t^2}{\\hat{\\mu}_{y,t} y_t}$",
# Gamma distribution
                        "$1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\sim \\mathcal{\\Gamma}(\\sigma^{-2}, \\sigma^2)$",
                        "$\\begin{aligned} &-T \\log \\Gamma \\left(\\sigma^{-2}\\right) -\\frac{T}{\\sigma^2} \\log \\sigma^2 + \\\\
                                         &\\frac{1}{\\sigma^2} \\sum_{t=1}^T \\log \\left(\\frac{y_t/\\mu_{y,t}}{ \\exp(y_t/\\mu_{y,t})}\\right) -\\sum_{t=1}^T \\log y_t\\end{aligned}$",
                        "$\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T \\left(\\frac{e_t}{\\mu_{y,t}}\\right)^2$ *",
# logN distribution
                        "$1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)$",
                        "$\\begin{aligned} &-\\frac{T}{2} \\log \\left(2 \\pi \\sigma^2\\right) -\\sum_{t=1}^T \\log y_t -\\\\
                                         &\\frac{1}{2\\sigma^2} \\sum_{t=1}^{T} \\left(\\log \\left(\\frac{y_t}{\\mu_{y,t}}\\right)+\\frac{\\sigma^2}{2}\\right)^2 \\end{aligned}$",
                        "$\\hat{\\sigma}^2 = 2\\left(1-\\sqrt{ 1-\\frac{1}{T} \\sum_{t=1}^{T} \\log^2\\left(\\frac{y_t}{\\hat{\\mu}_{y,t}}\\right)}\\right)$"
                        )
distributionsTable <- matrix(distributionsTable, 7, 3, byrow=TRUE,
                           dimnames=list(c("\\textbf{Normal}","\\textbf{Laplace}","\\textbf{S}",
                                           "\\textbf{Generalised Normal}",
                                           # "\\textbf{Asymmetric Laplace}",
                                           "\\textbf{Inverse Gaussian}","\\textbf{Gamma}","\\textbf{Log-Normal}"),
                                         c("\\textbf{Assumption}","\\textbf{log-likelihood}","\\textbf{MLE of scale}")))
kableTable <- kableExtra::kable(distributionsTable, escape=FALSE, caption="Likelihood approach for additive error models. $\\mathcal{N}$ is the Normal, $\\mathcal{L}$ is the Laplace, $\\mathcal{S}$ is the S, $\\mathcal{GN}$ is the Generalised Normal, $\\mathcal{IG}$ is the Inverse Gaussian, $\\Gamma$ is the Gamma, and $\\mathrm{log}\\mathcal{N}$ is the Log-Normal distribution.",
                                col.names=c("Assumption","log-likelihood","MLE of scale"), label="additiveErrorLikelihoods")
kableExtra::kable_styling(kableTable, font_size=12, protect_latex=TRUE, latex_options="scale_down")
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\end{landscape}')
}
```

The likelihoods are derived based on the probability density functions, by taking the logarithms of their products for all in-sample observations. For example, this is what we get for the Normal distribution:
\begin{equation}
  \begin{aligned}
    \mathcal{L}(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y}) = & \prod_{t=1}^T \left(\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{\left(y_t -\mu_t \right)^2}{2 \sigma^2} \right)\right) \\
    \mathcal{L}(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y}) = & \frac{1}{\left(\sqrt{2 \pi \sigma^2}\right)^T} \exp \left( \sum_{t=1}^T -\frac{\epsilon_t^2}{2 \sigma^2} \right) \\
    \ell(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y}) = & \log \mathcal{L}(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y}) \\
    \ell(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y}) = & -\frac{T}{2} \log(2 \pi \sigma^2) -\frac{1}{2} \sum_{t=1}^T \frac{\epsilon_t^2}{\sigma^2}
  \end{aligned},
  (\#eq:ETSADAMNormalDistributionExample01)
\end{equation}
where $\mathbf{y}$ is the vector of all in-sample actual values, $\mathcal{L}(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y})$ is the likelihood value, while $\ell(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y})$ is the log-likelihood. As for the scale, it is obtained by solving the equation after taking derivative of the log-likelihood \@ref(eq:ETSADAMNormalDistributionExample01) with respect to $\sigma^2$ and setting it equal to zero. We do not discuss the concentrated log-likelihoods (obtained after inserting the estimated scale in the respective log-likelihood function), because they are not useful in understanding of how the model is estimated, but knowing how to calculate scale helps, because it simplifies the model estimation process.

::: remark
MLE of scale parameter for Gamma distribution (formulated in ADAM) does not have a closed-form. While there are no proofs for it, it seems that the maximum of the likelihood of Gamma distribution is achieved, when $\hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^T \left(\frac{e_t}{\mu_{y,t}}\right)^2$, which corresponds to the estimate based on method of moments.
:::

While other distributions can be used in ADAM ETS (for example, Logistic distribution or Student's t), we do not discuss them here. In all cases in Table \@ref(tab:additiveErrorLikelihoods), the assumptions imply that the actual value follows the same distributionn as the error term (due to additivity of the model) but with a different location and/or scale. For example, for the Normal distribution, we have:
\begin{equation}
  \begin{aligned}
    & \epsilon_t \sim \mathcal{N}(0, \sigma^2) \\
    & \mathrm{or} \\
    & y_t = \mu_{y,t}+\epsilon_t \sim \mathcal{N}(\mu_{y,t}, \sigma^2)
  \end{aligned}.
  (\#eq:ETSADAMNormalDistributionExample02)
\end{equation}

When it comes to the multiplicative error models, the likelihoods become slightly different. For example, when we assume that $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$ in the multiplicative error model, this implies that:
\begin{equation}
    y_t = \mu_{y,t}(1+\epsilon_t) \sim \mathcal{N}(\mu_{y,t}, \mu_{y,t}^2 \sigma^2) .
  (\#eq:ETSADAMNormalDistributionExample03)
\end{equation}
As a result, the log-likelihoods would have the $\mu_{y,t}$ part in the formulae:
\begin{equation}
    \ell(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y}) = -\frac{1}{2} \sum_{t=1}^T \log(2 \pi \mu_{y,t}^2 \sigma^2) -\frac{1}{2} \sum_{t=1}^T \frac{\epsilon_t^2}{\sigma^2} .
  (\#eq:ETSADAMNormalDistributionExample04)
\end{equation}

::: remark
The part $\frac{1}{2} \sum_{t=1}^T \frac{\epsilon_t^2}{\sigma^2}$ does not contain $\mu_{y,t}$ because when inserted in the formula of Normal distribution, the numerator is: $y_t - \mu_{y,t}$, which is based on \@ref(eq:ETSADAMNormalDistributionExample03) is $\mu_{y,t} \epsilon_t$. After inserting it in the formula for the Normal distribution that part becomes: $\frac{1}{2} \sum_{t=1}^T \frac{(\mu_{y,t} \epsilon_t)^2}{\mu_{y,t}^2\sigma^2}$ which after cancellations leads to \@ref(eq:ETSADAMNormalDistributionExample04).
:::

Similar logic is applicable to Laplace ($\mathcal{L}$), Generalised Normal ($\mathcal{GN}$) and S ($\mathcal{S}$) distributions. From the practical point of view, these assumptions imply that the scale (and variance) of the distribution of $y_t$ changes together with the level of the series. When it comes to the Inverse Gaussian ($\mathcal{IG}$), Gamma ($\Gamma$) and Log-Normal (log$\mathcal{N}$), the assumptions are imposed on the $1+\epsilon_t$ part of the model, the respective likelihoods do not involve the expectation $\mu_{y,t}$, but the formulation still implies that the variance of the data increases together with the increase of the level of data.

All the likelihoods for the multiplicative error models are summarised in Table \@ref(tab:multiplicativeErrorLikelihoods).

```{r echo=FALSE}
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\begin{landscape}')
}
# Assumption | log-likelihood \ell(\boldsymbol{\theta}, {\sigma}_\epsilon^2 | \mathbf{y}) | Scale
# Normal distribution
distributionsTable <- c("$\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$",
                        "$\\begin{aligned} &-\\frac{T}{2} \\log(2 \\pi \\sigma^2) -\\frac{1}{2} \\sum_{t=1}^T \\frac{\\epsilon_t^2}{\\sigma^2} -\\\\
                                         &\\sum_{t=1}^T \\log |\\mu_{y,t}|\\end{aligned}$",
                        "$\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T e_t^2$",
# Laplace distribution
                        "$\\epsilon_t \\sim \\mathcal{L}(0, s)$",
                        "$\\begin{aligned} &-T \\log(2 s) -\\sum_{t=1}^T \\frac{|\\epsilon_t|}{s} -\\\\
                                         &\\sum_{t=1}^T \\log \\mu_{y,t}\\end{aligned}$",
                        "$\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T |e_t|$",
# S distribution
                        "$\\epsilon_t \\sim \\mathcal{S}(0, s)$",
                        "$\\begin{aligned} &-T \\log(4 s^2) -\\sum_{t=1}^T \\frac{\\sqrt{|\\epsilon_t|}}{s} -\\\\
                                         &\\sum_{t=1}^T \\log \\mu_{y,t}\\end{aligned}$",
                        "$\\hat{s} = \\frac{1}{2T} \\sum_{t=1}^T \\sqrt{|e_t|}$",
# GN distribution
                        "$\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)$",
                        "$\\begin{aligned} &T\\log\\beta -T \\log(2 s \\Gamma\\left(\\beta^{-1}\\right)) -\\\\
                                         &\\sum_{t=1}^T \\frac{\\left|\\epsilon_t\\right|^\\beta}{s} -\\sum_{t=1}^T \\log \\mu_{y,t}\\end{aligned}$",
                        "$\\hat{s} = \\sqrt[^{\\beta}]{\\frac{\\beta}{T} \\sum_{t=1}^T\\left| e_t \\right|^{\\beta}}$",
# Asymmetric Laplace distribution
                        # "$\\epsilon_t \\sim \\mathcal{ALaplace}(0, s, \\alpha)$",
                        # "$\\begin{aligned} &T\\log\\left(\\alpha(1-\\alpha)\\right) -T \\log(s) -\\\\
                        #                  &\\sum_{t=1}^T \\frac{\\epsilon_t (\\alpha -I(\\epsilon_t \\leq 0))}{s} -\\sum_{t=1}^T \\log \\mu_{y,t}\\end{aligned}$",
                        # "$\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T e_t(\\alpha -I(e_t \\leq 0))$",
# IG distribution
                        "$1+\\epsilon_t \\sim \\mathcal{IG}(1, \\sigma^2)$",
                        "$\\begin{aligned} &-\\frac{T}{2} \\log \\left(2 \\pi \\sigma^2 \\right) -\\frac{1}{2}\\sum_{t=1}^T \\left(1+\\epsilon_{t}\\right)^3 -\\\\
                                         &\\frac{3}{2}\\sum_{t=1}^T \\log y_t -\\frac{1}{2\\sigma^2} \\sum_{t=1}^{T} \\frac{\\epsilon_t^2}{1+\\epsilon_t}\\end{aligned}$",
                        "$\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{e_t^2}{1+e_t}$",
# Gamma distribution
                        "$1+\\epsilon_t \\sim \\mathcal{\\Gamma}(\\sigma^{-2}, \\sigma)$",
                        "$\\begin{aligned} &-T \\log \\Gamma \\left(\\sigma^{-2}\\right) -\\frac{T}{\\sigma^2} \\log \\sigma^2 + \\\\
                                         &\\frac{1}{\\sigma^2} \\sum_{t=1}^T \\log \\left(\\frac{1+\\epsilon_t}{\\exp(1+\\epsilon_t)}\\right) -\\sum_{t=1}^T \\log y_t\\end{aligned}$",
                        "$\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T e_t^2$ *",
# logN distribution
                        "$1+\\epsilon_t \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)$",
                        "$\\begin{aligned} &-\\frac{T}{2} \\log \\left(2 \\pi \\sigma^2\\right) - \\sum_{t=1}^T \\log y_t -\\\\
                                         &\\frac{1}{2\\sigma^2} \\sum_{t=1}^{T} \\left(\\log \\left(1+\\epsilon_t\\right)+\\frac{\\sigma^2}{2}\\right)^2 \\end{aligned}$",
                        "$\\hat{\\sigma}^2 = 2\\left(1-\\sqrt{ 1-\\frac{1}{T} \\sum_{t=1}^{T} \\log^2\\left(1+e_t\\right)}\\right)$"
                        )
distributionsTable <- matrix(distributionsTable, 7, 3, byrow=TRUE,
                           dimnames=list(c("\\textbf{Normal}","\\textbf{Laplace}","\\textbf{S}",
                                           "\\textbf{Generalised Normal}",
                                           # "\\textbf{Asymmetric Laplace}",
                                           "\\textbf{Inverse Gaussian}","\\textbf{Gamma}","\\textbf{Log-Normal}"),
                                         c("\\textbf{Assumption}","\\textbf{log-likelihood}","\\textbf{MLE of scale}")))
kableTable <- kableExtra::kable(distributionsTable, escape=FALSE, caption="Likelihood approach for multiplicative error models. $\\mathcal{N}$ is the Normal, $\\mathcal{L}$ is the Laplace, $\\mathcal{S}$ is the S, $\\mathcal{GN}$ is the Generalised Normal, $\\mathcal{IG}$ is the Inverse Gaussian, $\\Gamma$ is the Gamma, and $\\mathrm{log}\\mathcal{N}$ is the Log-Normal distribution.",
                                col.names=c("Assumption","log-likelihood","MLE of scale"), label="multiplicativeErrorLikelihoods")
kableExtra::kable_styling(kableTable, font_size=12, protect_latex=TRUE, latex_options="scale_down")
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\end{landscape}')
}
```

When it comes to practicalities, the optimiser in `adam()` function calculates the scale from Tables \@ref(tab:additiveErrorLikelihoods) and \@ref(tab:multiplicativeErrorLikelihoods) on each iteration and then uses it in the log-likelihood based on the respective distribution function. For additive error models:

1. Normal, $\mathcal{N}$ -- `dnorm(x=actuals, mean=fitted, sd=scale,  log=TRUE)` from `stats` package;
2. Laplace, $\mathcal{L}$ -- `dlaplace(q=actuals, mu=fitted, scale=scale, log=TRUE)` from `greybox` package;
3. S, $\mathcal{S}$ -- `ds(q=actuals, mu=fitted, scale=scale, log=TRUE)` from `greybox` package;
4. Generalised Normal, $\mathcal{GN}$ -- `dgnorm(x=actuals, mu=fitted, alpha=scale, beta=beta, log=TRUE)` implemented in `greybox` package based on `gnorm` package (the version on CRAN is outdated);
<!-- 5. $\mathcal{ALaplace}$ -- `dalaplace(q=actuals, mu=fitted, scale=scale,  alpha=alpha, log=TRUE)` from `greybox` package; -->
5. Inverse Gaussian, $\mathcal{IG}$ -- `dinvgauss(x=actuals, mean=fitted, dispersion=scale/fitted, log=TRUE)` from `statmod` package;
6. Log-Normal, log$\mathcal{N}$ -- `dlnorm(x=actuals, meanlog=fitted-scale^2/2, sdlog=scale, log=TRUE)` from `stats` package;
7. Gamma, $\Gamma$ -- `dgamma(x=actuals, shape=1/scale, scale=scale*fitted, log=TRUE)` from `stats` package.

and for multiplicative error models:

1. Normal, $\mathcal{N}$ -- `dnorm(x=actuals, mean=fitted, sd=scale*fitted,  log=TRUE)`;
2. Laplace, $\mathcal{L}$ -- `dlaplace(q=actuals, mu=fitted, scale=scale*fitted, log=TRUE)`;
3. S, $\mathcal{S}$ -- `ds(q=actuals, mu=fitted, scale=scale*sqrt(fitted), log=TRUE)`;
4. Generalised Normal, $\mathcal{GN}$ -- `dgnorm(x=actuals, mu=fitted, alpha=scale*fitted^beta, beta=beta, log=TRUE)`;
<!-- 5. $\mathcal{ALaplace}$ -- `dalaplace(q=actuals, mu=fitted, scale=scale*fitted,  alpha=alpha, log=TRUE)`; -->
5. Inverse Gaussian, $\mathcal{IG}$ -- `dinvgauss(x=actuals, mean=fitted, dispersion=scale/fitted, log=TRUE)`;
6. Log-Normal, log$\mathcal{N}$ -- `dlnorm(x=actuals, meanlog=fitted-scale^2/2, sdlog=scale, log=TRUE)`;
7. Gamma, $\Gamma$ -- `dgamma(x=actuals, shape=1/scale, scale=scale*fitted, log=TRUE)`.

::: remark
In cases of $\mathcal{GN}$, additional parameter (namely $\beta$) is needed. If the user does not provide it, it will be estimated together with the other parameters via the maximisation of respective likelihoods. Note however that the estimate of $\beta$ can be inefficient if its true value is lower than 1.
:::

The MLE of ADAM makes different models comparable with each other irrespective of the types of components and distributional assumptions. As a result, model selection based on information criteria can be done using the `auto.adam()` function from the `smooth`, which will select the most appropriate distribution for ADAM.


### An example in R
`adam()` function in `smooth` package has the parameter `distribution`, which allows selecting between several options discussed in this chapter, based on the respective density functions. Here is a brief example in R with ADAM ETS(M,A,M) applied to the `AirPassengers` data with several distributions:
```{r}
adamETSMAMAir <- vector("list",5)
adamETSMAMAir[[1]] <- adam(AirPassengers, "MAM", h=12, holdout=TRUE,
                           distribution="dnorm")
adamETSMAMAir[[2]] <- adam(AirPassengers, "MAM", h=12, holdout=TRUE,
                           distribution="dlaplace")
adamETSMAMAir[[3]] <- adam(AirPassengers, "MAM", h=12, holdout=TRUE,
                           distribution="dgnorm")
adamETSMAMAir[[4]] <- adam(AirPassengers, "MAM", h=12, holdout=TRUE,
                           distribution="dinvgauss")
adamETSMAMAir[[5]] <- adam(AirPassengers, "MAM", h=12, holdout=TRUE,
                           distribution="dgamma")
```

In this case, the function will select the most appropriate ETS model for each distribution. We can see what was selected in each case and compare the models using information criteria:

```{r}
sapply(adamETSMAMAir, AICc) |>
    setNames(c("dnorm","dlaplace","dgnorm","dinvgauss","dgamma"))
```
We could compare the performance of models in detail, but for demonstration purpose, it should suffice to say that among the four models considered above, based on the AICc value, the model with the Normal distribution should be selected. This process of fit and selection can be automated using `auto.adam()` function, which accepts the vector of distributions to test and by default would consider `distribution=c("default", "dnorm", "dlaplace", "ds", "dgnorm", "dlnorm", "dinvgauss", "dgamma")` (where `default` is Normal for additive error and Gamma for the multiplicative error model):
```{r eval=FALSE}
auto.adam(AirPassengers, "MAM", h=12, holdout=TRUE)
```
This command should return the ADAM ETS(M,A,M) model with the most appropriate distribution, selected based on the AICc.


## Non MLE-based loss functions
An alternative approach for estimating ADAM is using the conventional loss functions, such as MSE, MAE etc. In this case, the model selection using information criteria would not work, but this might not matter when you have already decided what model to use and want to improve it. Alternatively, you can use cross validation (e.g. rolling origin from Section \@ref(rollingOrigin)) to select a model estimated non MLE-based loss function. But in some special cases, the minimisation of these losses would give the same results as the maximisation of some likelihood functions:

- MSE is minimised by mean, and its minimum corresponds to the maximum likelihood of Normal distribution [see discussion in @Kolassa2016];
- MAE is minimised by median, and its minimum corresponds to the maximum likelihood of Laplace distribution [@Schwertman1990];
- RMSLE is minimised by the geometric mean, and its minimum corresponds to the maximum likelihood of Log-Normal distribution (this is based on the fact that the maximum of the distribution coincides with the Normal one applied to the log-transformed data).
<!-- 3. Minimum of pinball function [@Koenker1978, quantile regression] corresponds to the maximum of the likelihood of Asymmetric Laplace distribution [@Yu2005]; -->

The main difference between using these losses and maximising respective likelihoods is in the number of estimated parameters: the latter implies that the scale is estimated together with the other parameters, while the former does not consider it and in a way provides it for free.

The assumed distribution does not necessarily depend on the used loss and vice versa. For example, we can assume that the actuals follow the Inverse Gaussian distribution, but estimate model via minimisation of MAE. The estimates of parameters in this case might not be as efficient as in the case of MLE, but it is still possible to do.

When it comes to different types of models, the forecast error (which is used in respective losses) depends on the error type:

- Additive error: $e_t = y_t -\hat{\mu}_{y,t}$;
- Multiplicative error: $e_t = \frac{y_t -\hat{\mu}_{y,t}}{\hat{\mu}_{y,t}}$.

This follows directly from the respective ETS models.


### MSE and MAE {#MSEandMAEEstimators}
MSE and MAE have been discussed in Section \@ref(errorMeasures), but in the context of forecasts evaluation rather than estimation. If they are used for the latter, then the formulae \@ref(eq:RMSE) and \@ref(eq:MAE) will be amended to:
\begin{equation}
    \mathrm{MSE} = \sqrt{\frac{1}{T} \sum_{j=1}^T \left( e_t \right)^2 }
    (\#eq:MSEInSample)
\end{equation}
and 
\begin{equation}
    \mathrm{MAE} = \frac{1}{T} \sum_{j=1}^T \left| e_t \right| ,
    (\#eq:MAEInSample)
\end{equation}
where the specific formula for $e_t$ would depend on the error type as shown above. The main difference between the two estimators is in what they are minimised by: MSE is minimised by mean, while MAE is minimised by the median. This means that models estimated using MAE will typically be more conservative (i.e. in the case of ETS, have lower smoothing parameters). @Gardner2006 recommended using MAE in cases of outliers in the data, as the ETS model is supposed to become less reactive than in case of MSE. Note that in case of intermittent demand, MAE should be in general avoided due to the issues discussed in Section \@ref(errorMeasures).


### HAM
Along with the discussed MSE and MAE, there is also HAM -- "Half Absolute Moment":
\begin{equation}
  \mathrm{HAM} = \frac{1}{T} \sum_{j=1}^T \sqrt{\left|e_t\right|},
  (\#eq:HAM)
\end{equation}
the minimum of which corresponds to the maximum likelihood of S distribution. The idea of this estimator is to minimise the errors that happen very often, close to the location of the distribution. It will typically ignore the outliers and focus on the most frequently appearing values. As a result, if used for the integer values, the minimum of HAM would correspond to the mode of that distribution if it is unimodal. I do not have a proof of this property, but it becomes apparent, given that the square root in \@ref(eq:HAM) would reduce the influence of all values lying above one and increase the values of everything that lies between (0, 1) (e.g. $\sqrt{0.16}=0.4$, but $\sqrt{16}=4$).

Similar to HAM, one can calculate other fractional losses, which would be even less sensitive to outliers and more focused on the frequently appearing values, e.g. by using the $\sqrt[^\alpha]{\left|e_t\right|}$ with $\alpha>1$. This would then correspond to the maximum of Generalised Normal distribution with shape parameter $\beta=\frac{1}{\alpha}$.


### LASSO and RIDGE
It is also possible to use LASSO [@Tibshirani1996] and RIDGE for the estimation of ADAM [@James2017 give a good overview of these losses with examples in R]. This was studied in detail for ETS by @Pritularga2022. The losses can be formulated in ADAM as:
\begin{equation}
  \begin{aligned}
    \mathrm{LASSO} = &(1-\lambda) \sqrt{\frac{1}{T} \sum_{j=1}^T e_t^2} + \lambda \sum |\hat{\theta}| \\
    \mathrm{RIDGE} = &(1-\lambda) \sqrt{\frac{1}{T} \sum_{j=1}^T e_t^2} + \lambda \sqrt{\sum \hat{\theta}^2},
  \end{aligned}
  (\#eq:Regularisation)
\end{equation}
where $\theta$ is the vector of all parameters in the model except for initial states of ETS and ARIMA components (thus this includes all smoothing parameters, dampening parameter, AR, MA and the initial parameters for the explanatory variables) and $\lambda$ is the regularisation parameter. The idea of these losses is in the shrinkage of parameters. If $\lambda=0$, then the losses become equivalent to the MSE, when $\lambda=1$, the optimiser would minimise the values of parameters, ignoring the MSE part. @Pritularga2022 argues that the initial states of the model do not need to be shrunk (they will be handled by the optimiser automatically) and that the dampening parameter should shrink to 1 instead of zero (thus enforcing local trend model). Following the same idea, we introduce the following modifications of parameters in the loss in ADAM:

1. The smoothing parameters should shrink towards zero, implying that the respective states will change slower over time (the model becomes less stochastic);
2. Damping parameter is modified to shrink towards one, enforcing no dampening of the trend via $\hat{\phi}^\prime=1-\hat{\phi}$;
3. All AR parameters are forced to shrink to one: $\hat{\phi}_i^\prime=1-\hat{\phi}_i$ for all $i$. This means that ARIMA shrinks towards non-stationarity (this idea comes from the connection of ETS and ARIMA, discussed in Section \@ref(ARIMAandETS));
4. All MA parameters shrink towards zero, removing their impact on the model;
5. If there are explanatory variables and the error term of the model is *additive*, then the respective parameters are divided by the standard deviations of the respective variables. In the case of the *multiplicative* error term, nothing is done because the parameters would typically be close to zero anyway (see a Chapter \@ref(ADAMXFormulation)).
6. Finally, in order to make $\lambda$ slightly more meaningful, in case of *additive* error model, we also divide the MSE part of the loss by $\mathrm{V}\left({\Delta}y_t\right)$, where ${\Delta}y_t=y_t-y_{t-1}$. This sort of scaling helps in cases when there is a trend in the data. We do not do anything for the *multiplicative* error models because, typically, the error is already small.

::: remark
There is no good theory behind the shrinkage of AR and MA parameters. More research is required to understand how to shrink them properly.
:::

::: remark
The `adam()` function does not select the most appropriate `lambda` and will set it equal to zero if the user does not provide it. It is up to a user to try out different `lambda` and select the one that minimises a chosen error measure. This can be done, for example, using rolling origin procedure (Section \@ref(rollingOrigin)).
:::


### Custom losses
It is also possible to use other non-standard loss functions for ADAM estimation. `adam()` function allows doing that via the parameter `loss`. For example, we could estimate an ETS(A,A,N) model on the `BJsales` data using an absolute cubic loss (note that the parameters `actual`, `fitted` and `B` are compulsory for the function):
```{r eval=FALSE}
lossFunction <- function(actual, fitted, B){
    return(mean(abs(actual-fitted)^3));
}
adam(BJsales, "AAN", loss=lossFunction, h=10, holdout=TRUE)
```

where `actual` is the vector of actual values $y_t$, `fitted` is the estimate of the one-step-ahead point forecast $\hat{y}_t$, and $B$ is the vector of all estimated parameters, $\hat{\theta}$. The syntax above allows using more advanced estimators, such as, for example, M-estimators [@Barrow2020].


### Examples in R
`adam()` has two parameters, one regulating the assumed `distribution`, and another one, regulating how the model will be estimated, what `loss` will be used for these purposes. Here are examples with combinations of different losses and the Inverse Gaussian distribution for ETS(M,A,M) on `AirPassengers` data. We start with likelihood, MSE, MAE and HAM:
```{r}
adamETSMAMAir <- vector("list",6)
names(adamETSMAMAir) <-
    c("likelihood", "MSE", "MAE", "HAM", "LASSO", "Huber")
adamETSMAMAir[[1]] <- adam(AirPassengers, "MAM",h=12, holdout=TRUE,
                           distribution="dinvgauss",
                           loss="likelihood")
adamETSMAMAir[[2]] <- adam(AirPassengers, "MAM", h=12, holdout=TRUE,
                           distribution="dinvgauss",
                           loss="MSE")
adamETSMAMAir[[3]] <- adam(AirPassengers, "MAM", h=12, holdout=TRUE,
                           distribution="dinvgauss",
                           loss="MAE")
adamETSMAMAir[[4]] <- adam(AirPassengers, "MAM", h=12, holdout=TRUE,
                           distribution="dinvgauss",
                           loss="HAM")
```

In these cases, the models assuming the same distribution for the error term are estimated using likelihood, MSE, MAE and HAM. Their smoothing parameters should differ, with MSE producing fitted values closer to the mean, MAE -- closer to the median and HAM -- closer to the mode (but not exactly the mode) of the distribution.

In addition, we introduce ADAM ETS(M,A,M) estimated using LASSO with arbitrarily selected $\lambda=0.9$:

```{r}
adamETSMAMAir[[5]] <- adam(AirPassengers, "MAM", h=12, holdout=TRUE,
                           distribution="dinvgauss",
                           loss="LASSO", lambda=0.9)
```

And, finally, we estimate the same model using a custom loss, which in this case is Huber loss [@Huber1992] with the threshold of 1.345:

```{r}
# Huber loss with a threshold of 1.345
lossFunction <- function(actual, fitted, B){
    errors <- actual-fitted;
    return(sum(errors[errors<=1.345]^2) +
               sum(abs(errors)[errors>1.345]));
}
adamETSMAMAir[[6]] <- adam(AirPassengers, "MAM", h=12, holdout=TRUE,
                           distribution="dinvgauss",
                           loss=lossFunction)
```

Now we can compare the performance of the six models. First, we can compare the smoothing parameters:

```{r}
sapply(adamETSMAMAir,"[[","persistence") |>
    round(3)
```

What we observe in this case is that LASSO has the lowest smoothing parameter $\alpha$ because it is shrunk directly in the model estimation. Also note that Likelihood and MSE give similar values. They both rely on squared errors, but not in the same way because the likelihood of Inverse Gaussian distribution has some additional elements (see Section \@ref(ADAMETSEstimationLikelihood)).

Unfortunately, we do not have information criteria for models 2 -- 6 in this case because the likelihood function is not maximised with these losses, so it's not possible to compare them via the in-sample statistics. But we can compare their holdout sample performance:

```{r}
round(sapply(adamETSMAMAir,"[[","accuracy"),
      3)[c("ME","MAE","MSE"),]
```

And we can also produce forecasts and plot them for the visual inspection (Figure \@ref(fig:adamModelsForecasts)):

```{r adamModelsForecasts, fig.width=8, fig.height=10, fig.cap="Forecasts from ETS(M,A,M) model estimated using different loss functions."}
adamETSMAMAirForecasts <- lapply(adamETSMAMAir, forecast,
                                 h=12, interval="empirical")
layout(matrix(c(1:6),3,2,byrow=TRUE))
for(i in 1:6){
    plot(adamETSMAMAirForecasts[[i]],
         main=paste0("ETS(MAM) estimated using ",
                     names(adamETSMAMAir)[i]))
}
```

What we observe in Figure \@ref(fig:adamModelsForecasts) is that different losses led to different forecasts and prediction intervals (we used the empirical ones, discussed in Subsection \@ref(ADAMForecastingPIEmpirical)). What can be done to make this practical is the rolling origin evaluation (Section \@ref(rollingOrigin)) for different losses and then comparing forecast errors between them to select the most accurate approach.


## Multistep losses {#multistepLosses}
Another family of losses that can be used to estimate ADAM is the multistep losses. The idea behind them is to produce the point forecast for $h$ steps ahead from each observation in-sample and then calculate a measure based on that, which the optimiser will then minimise to find the most suitable values of parameters. There is a lot of literature on this topic, @Svetunkov2020Multistep studied them in detail, showing that their usage implies shrinkage of smoothing parameters in ETS models. In this section, we will discuss the most popular multistep losses, see what they imply and make a connection between them and predictive likelihoods from the ADAM.


### $\mathrm{MSE}_h$ -- MSE for h steps ahead
One of the simplest estimators is the $\mathrm{MSE}_h$ -- mean squared $h$-steps ahead error:
\begin{equation}
	\mathrm{MSE}_h = \frac{1}{T-h} \sum_{t=1}^{T-h} e_{t+h|t}^2 ,
  (\#eq:hstepsMSE)
\end{equation}
where $e_{t+h|t}$ is the conditional $h$ steps ahead forecast error on the observation $t+h$ produced from the point at time $t$. In case of additive error model, it is calculated as $e_{t+h|t}=y_{t+h}-\hat{y}_{t+h}$, while in case of the multiplicative one it is $e_{t+h|t}=\frac{y_{t+h}-\hat{y}_{t+h}}{\hat{y}_{t+h}}$. This estimator is sometimes used to fit a model several times, for each horizon from 1 to $h$ steps ahead, resulting in $h$ different values of parameters for each $j=1, \ldots, h$. The estimation process, in this case, becomes at least $h$ times more complicated than estimating one model but is reported to result in increased accuracy [see for example @kourentzes2019unconstraining]. @Svetunkov2020Multistep show that MSE$_h$ is proportional to the h steps ahead forecast error variance $V(y_{t+h}|t)=\sigma^2_{h}$. This implies that the minimisation of \@ref(eq:hstepsMSE) leads to the minimisation of the variance $\sigma^2_{h}$ and in turn to the minimisation of both one step ahead MSE and a combination of smoothing parameters of a model. This becomes more obvious in the case of pure additive ETS (Section \@ref(ADAMETSPureAdditive)), where the analytical formulae for variance(from Section \@ref(pureAdditiveExpectationAndVariance)) are available. In the case of ETS, the parameters are shrunk towards zero, making the model deterministic. The effect is softened on large samples when the ratio $\frac{T-h}{T-1}$ becomes close to one. In the case of ARIMA, the shrinkage mechanism is similar, making models closer to the deterministic ones. However, shrinkage direction in ARIMA is more complicated than in ETS and differs from one model to another. The shrinkage strength is proportional to the forecast horizon $h$ and is weakened with the increase of the sample size.

@Svetunkov2020Multistep demonstrate that the minimum of MSE$_h$ corresponds to the maximum of the predictive likelihood based on the Normal distribution, assuming that $\epsilon_t \sim N(0,\sigma^2)$. The log-likelihood in this case is:
\begin{equation}
	\ell_{\mathrm{MSE}_h}(\boldsymbol{\theta}, {\sigma^2_h} | \mathbf{y}) = -\frac{T-h}{2} \left( \log(2 \pi) + \log \sigma^2_h \right) -\frac{1}{2} \sum_{t=1}^{T-h} \left( \frac{\eta_{t+h|t}^2}{\sigma^2_h} \right) ,
  (\#eq:LikelihoodMSEh)
\end{equation}
where $\eta_{t+h|t} \sim N(0, \sigma_h^2)$ is the h steps ahead forecast error, conditional on the information available at time $t$, $\boldsymbol{\theta}$ is the vector of all estimated parameters of the model and $\mathbf{y}$ is the vector of $y_{t+h}$ for all $t=1,..,T-h$. The MLE of the scale parameter in \@ref(eq:LikelihoodMSEh) coincides with the MSE$_h$:
\begin{equation}
	\hat{\sigma}_h^2 = \frac{1}{T-h} \sum_{t=1}^{T-h} e_{t+h|t}^2 ,
  (\#eq:hstepsSigma)
\end{equation}
where $e_{t+h|t}$ is the in sample estimate of the $\eta_{t+h}$. The formula \@ref(eq:LikelihoodMSEh) can be used for the calculation of information criteria and in turn for the model selection in cases, when MSE$_h$ is used for the model estimation.

@Svetunkov2020Multistep demonstrate that \@ref(eq:hstepsMSE) is more efficient than the conventional MSE$_1$ when the true smoothing parameters are close to zero and is less efficient otherwise. On smaller samples, MSE$_h$ produces biased estimates of parameters due to shrinkage. This can still be considered an advantage if you are interested in forecasting and do not want the smoothing parameters to vary substantially from one sample to another.


### TMSE -- Trace MSE {#multistepLossesTMSE}
An alternative to MSE$_h$ is to in-sample produce 1 to $h$ steps ahead forecasts and calculate the respective forecast errors. Then, based on that, we can calculate the overall measure, which we will call "Trace MSE":
\begin{equation}
	\mathrm{TMSE} = \sum_{j=1}^h \frac{1}{T-h} \sum_{t=1}^{T-h} e_{t+j|t}^2 = \sum_{j=1}^h \mathrm{MSE}_j.
  (\#eq:TMSE)
\end{equation}
The benefit of this estimator is in minimising the error for the whole 1 to $h$ steps ahead in one model -- there is no need to construct $h$ models, minimising MSE$_j$ for $j=1,...,h$. However, this comes with a cost: typically, short-term forecast errors have lower MSE than the longer-term ones, so if we sum their squares up, we are mixing different values, and the minimisation will be done mainly for the ones on the longer horizons.

TMSE does not have a related predictive likelihood, so it is difficult to study its properties. Still, the simulations show that it tends to produce less biased and more efficient estimates of parameters than MSE$_h$, especially in cases of higher smoothing parameters. @Kourentzes2019 showed that TMSE performs well compared to the conventional MSE$_1$ and MSE$_h$ in terms of forecasting accuracy and does not take as much computational time as the estimation of $h$ models using MSE$_h$.


### GTMSE -- Geometric Trace MSE {#multistepLossesGTMSE}
An estimator that addresses the issues of TMSE, is the GTMSE, which is derived from a so called General Predictive Likelihood [GPL by @Clements1998; @Svetunkov2020Multistep]. The word "Geometric" sums up how the value is calculated:
\begin{equation}
	\mathrm{GTMSE} =  \sum_{j=1}^h \log \left(\frac{1}{T-h} \sum_{t=1}^{T-h} e_{t+j|t}^2 \right) = \sum_{j=1}^h \log \mathrm{MSE}_j.
  (\#eq:GTMSE)
\end{equation}
Logarithms in the formula \@ref(eq:GTMSE) bring the MSEs on different horizons to the similar level so that both short term and long term errors are minimised with similar power. As a result, the shrinkage effect in this estimator is milder than in MSE$_h$ and TMSE, and the estimates of parameters are less biased and more efficient on smaller samples. It still has the benefits of other multistep estimators, shrinking the parameters towards zero. Although it is possible to derive a predictive likelihood that would be maximised when GTMSE is minimised, it relies on unrealistic assumptions of independence of multistep forecast errors [they are always correlated as long as smoothing parameters are not zero, @Svetunkov2020Multistep].


### MSCE -- Mean Squared Cumulative Error
This estimator aligns the loss function with a specific inventory decision: ordering based on the lead time $h$:
\begin{equation}
	\mathrm{MSCE} = \frac{1}{T-h} \sum_{t=1}^{T-h} \left( \sum_{j=1}^h e_{t+j|t} \right)^2 .
  (\#eq:MSCE)
\end{equation}
@kourentzes2019unconstraining demonstrated that it produced more accurate forecasts in cases of intermittent demand and leads to fewer revenue losses. @Svetunkov2020Multistep showed that the shrinkage effect is much stronger in this estimator than in the others discussed in this section. In addition, it is possible to derive a predictive log-likelihood related to this estimator (assuming normality of the error term):
\begin{equation}
	\ell_{\mathrm{MSCE}}(\theta, {\varsigma^2_h} | \mathbf{y}_c) = -\frac{T-h}{2} \left( \log(2 \pi) + \log {\varsigma^2_h} \right) -\frac{1}{2} \sum_{t=1}^{T-h} \left( \frac{\left(\sum_{j=1}^h \eta_{t+j|t}\right)^2}{2 {\varsigma^2_h}} \right) ,
  (\#eq:LikelihoodMSCE)
\end{equation}
where $\mathbf{y}_c$ is the cumulative sum of actual values, the vector of $y_{c,t}=\sum_{j=1}^h y_{t+j}$ for all $t=1, \ldots, T-h$ and ${\varsigma^2_h}$ is the variance of the cumulative error term, the MLE of which is equal to \@ref(eq:MSCE). Having the likelihood \@ref(eq:LikelihoodMSCE), permits the model selection and combination using information criteria [Section 16.4 @SvetunkovSBA] and also means that the parameters estimated using MSCE will be asymptotically consistent and efficient.


### GPL -- General Predictive Likelihood {#multistepLossesGPL}
Finally, @Svetunkov2020Multistep studied the General Predictive Likelihood for normally distributed variable from @Clements1998, p.77, logarithmic version of which can be written as:
\begin{equation}
	\ell_{\mathrm{GPL}_h}(\theta, \boldsymbol{\Sigma} | \mathbf{Y}) = -\frac{T-h}{2} \left( h \log(2 \pi) + \log | \mathbf{\Sigma}| \right) -\frac{1}{2} \sum_{t=1}^T \left( \mathbf{E_t^\prime} \mathbf{\Sigma}^{-1} \mathbf{E_t} \right) ,
  (\#eq:logGPL)
\end{equation}
where $\boldsymbol{\Sigma}$ is the conditional covariance matrix for the vector of variables $\mathbf{y}_t=\begin{pmatrix} y_{t+1|t} & y_{t+2|t} & \ldots & y_{t+h|t} \end{pmatrix}$, $\mathbf{Y}$ is the matrix consisting of $\mathbf{y}_t$ for all $t=1, \ldots, T-h$ and $\mathbf{E_t}^{\prime} = \begin{pmatrix} \eta_{t+1|t} & \eta_{t+2|t} & \ldots & \eta_{t+h|t} \end{pmatrix}$ is the vector of 1 to $h$ steps ahead forecast errors. @Svetunkov2020Multistep showed that the maximisation of the likelihood \@ref(eq:logGPL) is equivalent to minimisation of the generalised variance of the error term, $|\hat{\boldsymbol{\Sigma}}|$, where:
\begin{equation}
	\hat{\boldsymbol{\Sigma}} = \frac{1}{T-h} \sum_{t=1}^{T-h} \mathbf{E_t} \mathbf{E_t^\prime} =
	\begin{pmatrix}
		\hat{\sigma}_1^2 & \hat{\sigma}_{1,2} & \dots & \hat{\sigma}_{1,h} \\
		\hat{\sigma}_{1,2} & \hat{\sigma}_2^2 & \dots & \hat{\sigma}_{2,h} \\
		\vdots & \vdots & \ddots & \vdots \\
		\hat{\sigma}_{1,h} & \hat{\sigma}_{2,h} & \dots & \hat{\sigma}_h^2
	\end{pmatrix} ,
	(\#eq:Sigmaest)
\end{equation}
where $\hat{\sigma}_{i,j}$ is the covariance between $i$-th and $j$-th steps ahead forecast errors. @Svetunkov2020Multistep show that this estimator encompassess all the other estimators discussed in this section: minimising MSE$_h$ is equivalent to minimising the $\hat{\sigma}^2_{h}$; minimising TMSE is equivalent to minimising the trace of the matrix $\hat{\boldsymbol{\Sigma}}$; minimising GTMSE is the same as minimising the determinant of $\hat{\boldsymbol{\Sigma}}$ but with the restriction that all off-diagonal elements are equal to zero; minimising MSCE produces the same results as minimising the sum of all elements in $\hat{\boldsymbol{\Sigma}}$. However, maximum of GPL is equivalent to the maximum of the conventional one step ahead likelihood for a Normal model in case when all the basic assumptions (discussed in Subsection \@ref(assumptions)) hold. In other cases, they would be different, but it is still not clear, whether the difference would be favouring the conventional likelihood of the GPL. Nonetheless, GPL, being the likelihood, guarantees that the estimates of parameters will be efficient and consistent and permits model selection and combination via information criteria.

When it comes to models with multiplicative error term, the formula of GPL \@ref(eq:logGPL) will need to be amended by analogy with the log-likelihood of Normal distribution in the same situation (Table \@ref(tab:multiplicativeErrorLikelihoods)):
\begin{equation}
    \begin{aligned}
	\ell_{\mathrm{GPL}_h}(\theta, \boldsymbol{\Sigma} | \mathbf{Y}) = & -\frac{T-h}{2} \left( h \log(2 \pi) + \log | \boldsymbol{\Sigma}| \right) -\frac{1}{2} \sum_{t=1}^T \left( \mathbf{E_t^\prime} \boldsymbol{\Sigma}^{-1} \mathbf{E_t} \right) \\
	                    & -\sum_{t=1}^{T-h} \sum_{j=1}^h \log |\mu_{y,t+j|t}|,
	\end{aligned}
  (\#eq:logGPLMultiplicative)
\end{equation}
where the term $\sum_{t=1}^{T-h} \sum_{j=1}^h \log |\mu_{y,t+j|t}|$ appears because we assume that the actual value $h$ steps ahead follows multivariate Normal distribution with the conditional expectation $\mu_{y,t+j|t}$. Note that this is only a very crude approximation, as the conditional distribution for $h$ steps ahead is not defined for multiplicative error models. So, when dealing with GPL, it is recommended to use pure additive models only.


### Other multistep estimators
It is also possible to derive multistep estimators based on MAE, HAM and other error measures. `adam()` unofficially supports the following multistep losses by analogy with MSE$_h$, TMSE and MSCE discussed in this section:

1. MAE$_h$;
2. TMAE;
3. MACE;
4. HAM$_h$;
5. THAM;
6. CHAM.

When calculating likelihoods based on these losses, `adam()` will assume Laplace distribution for (1) -- (3) and S distribution for (4) -- (6) if the user does not specify the `distribution` parameter.


### An example in R
In order to see how different estimators perform, we will apply ETS(A,A,N) model to Box-Jenkins Sales data, setting forecast horizon $h=10$:
```{r}
adamETSAANBJ <- vector("list",6)
names(adamETSAANBJ) <- c("MSE","MSEh","TMSE","GTMSE","MSCE","GPL")
for(i in 1:length(adamETSAANBJ)){
    adamETSAANBJ[[i]] <- adam(BJsales, "AAN", h=10, holdout=TRUE,
                              loss=names(adamETSAANBJ)[i])
}
```

We can compare the smoothing parameters of these models to see how the shrinkage effect worked in different estimators:
```{r}
round(sapply(adamETSAANBJ,"[[","persistence"),5)
```

The table above shows that $\beta$ is close to zero for the estimators that impose harder shrinkage on parameters: MSE$_h$, TMSE and MSCE. MSE does not shrink the parameters, while GTMSE has a mild shrinkage effect. While the models estimated using these losses are in general not comparable in-sample (although MSE, MSE$_h$, MSCE and GPL could be compared via information criteria if they are scaled appropriately), they are comparable on the holdout via the error measures:
```{r}
round(sapply(adamETSAANBJ,"[[","accuracy"),5)[c("ME","MAE","MSE"),]
```

In this case, ETS(A,A,N) estimated using GPL produced a more accurate forecast than the other estimators. Repeating the experiment on many samples and selecting the approach that produces more accurate forecasts would allow choosing the most appropriate one for the specific model on specific data.


## Initialisation of ADAM {#ADAMInitialisation}
To construct a model, we need to initialise it, defining the values of initial states of the model. In case of non-seasonal model, this means estimating values of $\mathbf{v}_{0}$. In case of the seasonal one, it is $\mathbf{v}_{-m+1}, \dots, \mathbf{v}_0$, where $m$ is the seasonal lag. There are different ways of doing that, but here we only discuss the following three:

1. Optimisation of initials,
2. Backcasting,
3. Provided values.

The first option implies that the values of initial states are found in the same procedure as the other parameters of the model. This is what @Hyndman2002 suggested doing. (2) means that the initials are refined iteratively when the model is fit to the data from observation $t=1$ to $t=T$ and then going backwards to get values for $t=0$. Finally, (3) is when a user knows initials and provides them to the model.

As a side note, we assume in ADAM that the model is initialised at the moment just before $t=1$. We do not believe that it was initialised at some point before the Big Bang (as ARIMA typically does), and we do not initialise it at the start of the sample. This way, we make all models in ADAM comparable, making them work on the same sample, no matter how many differences are taken or how many seasonal components we define.


### Optimisation vs backcasting {#ADAMInitialisationOptAndBack}
In the case of **optimisation**, all the model parameters are estimated together. This includes (depending on the type of model, in the order used in optimiser):

- Smoothing parameters of ETS,
- Smoothing parameters for the regression part of the model (from Section \@ref(ADAMXDynamic)),
- Dampening parameter of ETS,
- Parameters of ARIMA: first AR(p), then MA(q),
- Initials of ETS,
- Initials of ARIMA,
- Initial values for parameters of explanatory variables,
- Constant / drift for ARIMA,
- Other additional parameters that are needed by assumed distributions.

The more complex the selected model is, the more parameters we will need to estimate, and all of this will happen in one and the same iterative process in the optimiser:

1. Choose parameters,
2. Fit the model,
3. Calculate loss function,
4. Compare the loss with the previous one,
5. Update the parameters based on (4),
6. Go to (2) and repeat until a specific criterion is met.

The user can specify the stopping criteria. There are several options accepted by the optimiser of `adam()`:

1. Maximum number of iterations (`maxeval`), which by default is equal to $40\times k$ in case of ETS / ARIMA and $100 \times k$ for the model with explanatory variables, where $k$ is the number of all estimated parameters;
2. The relative precision of the optimiser (`xtol_rel`) with the default value of $10^{-6}$, which regulates the relative change of parameters;
3. The absolute precision of the optimiser (`xtol_abs`) with the default value of $10^{-8}$, which regulates the absolute change of parameters;
4. The stopping criterion in case of the relative change in the loss function (`ftol_rel`) with the default value of $10^{-8}$;

All these parameters are explained in more detail in the documentation of the `nloptr()` function from the `nloptr` package for R [@nlopt], which handles the estimation of ADAM. `adam()` accepts several other stopping criteria, which can be found in the documentation of the function.

::: remark
`adam()` can also print the results of the optimisation via the `print_level` parameter, which is defined in the same way as in the `nloptr()` function, but with an additional option of `print_level=41`, which will print the results of the optimisation, without producing step-by-step outputs.
:::

The mechanism explained above can slow down substantially if a complex model is constructed and might take a lot of time and manual tuning of parameters to get to the optimum. In some cases, reducing the number of estimated parameters is worth considering, and one way of doing that is backcasting.

In case of **backcasting** we do not need to estimate initials of ETS, ARIMA and regression. What model does in this case is goes through the series from $t=1$ to $t=T$, fitting to the data and then reverses and goes back from $t=T$ to $t=1$ based on the following state space model:
\begin{equation}
  \begin{aligned}
  {y}_{t} = &w(\mathbf{v}_{t+\boldsymbol{l}}) + r(\mathbf{v}_{t+\boldsymbol{l}}) \epsilon_t \\
  \mathbf{v}_{t} = &f(\mathbf{v}_{t+\boldsymbol{l}}) + g(\mathbf{v}_{t+\boldsymbol{l}}) \epsilon_t
  \end{aligned}.
  (\#eq:ETSADAMStateSpaceBackwards)
\end{equation}
The new values of $\mathbf{v}_t$ for $t<1$ are then used to fit the model to the data again. The procedure can be repeated several times for the initial states to converge to more reasonable values. `adam()` does that only two times.

The backcasting procedure implies the extended fitting process for the model, removing the need to estimate all the initials. It works exceptionally well on large samples of data (thousands of observations) and with models with several seasonal components. The bigger your model is, the more time the optimisation will take, and the more likely backcasting would do better. On the other hand, you might also prefer backcasting to optimisation on small samples when you do not have more than two seasons of data -- estimation of initial seasonal components might become challenging and can lead to overfitting.

When discussing specific models, ADAM ARIMA works better (faster and more accurate) with backcasting than with optimisation because it does not need to estimate as many parameters as in the latter case. On the other hand, ADAM ETS typically works quite well in case of optimisation, when there is enough data to train the model on. Last but not least, if you introduce explanatory variables, backcasting mechanism implemented in `adam()` will optimise their parameters instead of backcasting. In case of a dynamic ETSX / ARIMAX (Section \@ref(ADAMXDynamic)) this will mean that the initial values of the part of the state vector for explanatory variables will be estimated as well. If you want them not to be optimised you can use `initial="complete"` to do full backcasting of all elements of the state vector.

It is also important to note that **the information criteria of models with backcasting are typically lower than in the case of the optimised initials**. This is because the difference in the number of estimated parameters is substantial in these two cases, and the models are initialised differently. So, it is advised not to mix the model selection between the two initialisation techniques, although there is no theoretical ground for forbid it.

Nonetheless, no matter what initialisation method is used, we need to start the fitting process from $t=1$. This cannot be done unless we provide some starting (pre-initialised) values of parameters to the optimiser. The better we guess the starting values, the faster the optimiser will converge to the optimum. `adam()` uses several heuristics at this stage, explained in more detail in the following subsections.


### Starting optimisation of parameters
::: remark
In this subsection, we discuss how the values of smoothing parameters, damping parameters and coefficients of ARIMA are preset before the initialisation. All the things discussed here are heuristics, developed based on my experience and many experiments with ADAM.
:::

Depending on the model type, the vector of estimated parameters will have different lengths. We start with smoothing parameters of ETS:

1. For the unsafe mixed models ETS(A,A,M), ETS(A,M,A), ETS(M,A,A) and ETS(M,A,M): $\hat{\alpha}=0.01$, $\hat{\beta}=0$ and $\hat{\gamma}=0$. This is needed because the models listed above are susceptible to the changes in smoothing parameters and might fail for time series with actual values close to zero;
2. For the one of the most complicated and sensitive models ETS(M,M,A) $\hat{\alpha}=\hat{\beta}=\hat{\gamma}=0$. The combination of additive seasonality and the multiplicative trend is the most difficult one. The multiplicative error makes estimation even more challenging in cases of low-level data. So starting from the deterministic model, that will work for sure is a safe option;
3. ETS(M,A,N) is slightly easier to estimate than ETS(M,A,M) and ETS(M,A,A), so $\hat{\alpha}=0.2$, $\hat{\beta}=0.01$. The low value for the trend is needed to avoid the difficult situations with low level data, when the fitted values become negative;
4. ETS(M,M,N) and ETS(M,M,M) have $\hat{\alpha}=0.1$, $\hat{\beta}=0.05$ and $\hat{\gamma}=0.01$, making the trend and seasonal components a bit more conservative. The high values are typically not needed in this model as they might lead to explosive trends;
5. Other models with multiplicative components (ETS(M,N,N), ETS(M,N,A), ETS(M,N,M), ETS(A,N,M), ETS(A,M,N) and ETS(A,M,M)) are slightly easier to estimate and harder to break, so their parameters are set to $\hat{\alpha}=0.1$, $\hat{\beta}=0.05$ and $\hat{\gamma}=0.05$;
6. Finally, pure additive models are initialised with $\hat{\alpha}=0.1$, $\hat{\beta}=0.05$ and $\hat{\gamma}=0.11$. Their parameter space is the widest, and the models do not break on any data.

The smoothing parameter for the explanatory variables (Section \@ref(ADAMXDynamic)) is set to $\hat{\delta}=0.01$ in case of additive error and $\hat{\delta}=0$ in case of the multiplicative one. The latter is done because the model might break if some ETS components are additive.

If a dampening parameter is needed in the model, then its starting value is $\hat{\phi}=0.95$.

In case of ARIMA, the parameters are pre-initialised based on ACF and PACF (Sections \@ref(ACF) and \@ref(PACF)). First, the in-sample actual values are differenced, according to the selected order $D_j$ for all $j=0,\dots,n$, after which the ACF and PACF are calculated. Then the initials for AR parameters are taken from the PACF, while the initials for MA parameters are taken from ACF, making sure that the sum of parameters is not greater than one in both cases. If it is, then the parameters are renormalised to satisfy the condition. This mechanism aims to get a potentially correct direction towards the optimal parameters of the model and make sure that the initial values meet the basic stationarity and invertibility conditions. In cases when it is not possible to calculate ACF and PACF for the specified lags and orders, AR parameters are set to -0.1, while the MA parameters are set to 0.1, making sure that the conditions mentioned above hold.

In case of Generalised Normal distribution, the shape parameter is set to 2 (if it is estimated), making the optimiser start from the conventional Normal distribution.
<!-- If the skewness parameter of Asymmetric Laplace distribution is not provided, then its pre-initial value is set to 0.5, corresponding to the median of the data. -->

The pre-initialisations described above guarantee that the model is estimable for a wide variety of time series and that the optimiser will reach the optimum in a limited time. If it does not work for a specific case, a user can provide their vector of pre-initialised parameters via the parameter `B` in the ellipsis of the model. Furthermore, the typical bounds for the parameters can be tuned as well. For example, the bounds for smoothing parameters in ADAM ETS are (-5, 5), and they are needed only to simplify the optimisation procedure. The function will check the violation of either `usual` or `admissible` bounds inside the optimiser, but having some ideas of where to search for optimal parameters, helps. A user can provide their vector for the lower bound via `lb` and the upper one via `ub`.


### Starting optimisation of ETS states
After defining the pre-initial parameters, we need to provide similar values for the initial state vector $\mathbf{v}_t$. The steps explained below are based on my experience and typically lead to a robust model. The pre-initialisation of states of ADAM ETS differs depending on whether the model is seasonal or not. If it is **seasonal**, then the multiple seasonal decomposition is done using the `msdecompose()` function from the `smooth` package with the seasonality set to "multiplicative" if either error or seasonal component of ETS is multiplicative. After that:

- Initial level is set to be equal to the first initial value from the function (which is the back forecasted de-seasonalised series);
- The value is corrected if regressors are included to remove their impact on the value (either by subtracting the fitted of the regression part or by dividing by them -- depending on the error type);
- If the trend is additive and seasonality is multiplicative, then the trend component is obtained by multiplying the initial level and trend from the decomposition (remember, the assumed model is multiplicative in this case) and then subtracting the previous level from the resulting value;
- If the trend is multiplicative and seasonality is additive, then the initials are added and then divided by the previous level to get the initial multiplicative trend component;
- If there is no seasonality and the trend is multiplicative, then the initial trend is set to 1. This is done to avoid the potentially explosive behaviour in the model;
- If the trend is multiplicative and the level is negative, then the level is substituted by the first actual value. This might happen in some weird cases of time series with low values;
- When it comes to seasonal components, if we have a pure additive or a pure multiplicative ETS model or ETS(A,Z,M), we use the seasonal indices obtained from the `msdecompose()` function (discussed in Subsection \@ref(ClassicalDecompositionMS)), making sure that they are normalised. The type of seasonality in `msdecompose()` corresponds to the seasonal component of ETS in this case, and nothing additional needs to be done;
- The situation is more challenging with ETS(M,Z,A), for which the decomposition would return the multiplicative seasonal components. To convert them to the additive ones, we take their logarithm and multiply them by the minimum value of the actual time series. This way, we guarantee that the seasonal components are closer to the optimal ones.

In the case of the **non-seasonal** model, the algorithm is more straightforward:

- The initial level is equal to either arithmetic or geometric mean (depending on the type of trend component) of the first $\max(m_1,\dots,m_n)$ observations, where $m_j$ is the model lag (e.g. in case of ARIMA(1,1,2), the components will have lags of 1 and 2). If the length of this mean is smaller than 20% of the sample, then the arithmetic mean of the first 20% of actual values is used;
- If regressors are included, then the value is modified, similarly to how it is done in the seasonal case discussed above;
- If the model has an additive trend, then its initial value is equal to the mean difference between first $\max(m_1,\dots,m_n)$ observations;
- In the case of multiplicative trend, the initial value is equal to the geometric mean of ratios between first $\max(m_1,\dots,m_n)$ observations;

In cases of the small samples (less than two seasonal periods), the procedure is similar to the one above. However, the seasonal indices are obtained by taking the actual values and either subtracting arithmetic mean from them or dividing them by the geometric one of the first $m_j$ observations, depending on the seasonality type, normalising them afterwards.

Finally, to ensure that the safe initials were provided, for the ETS(M,Z,Z) models, if the initial level contains a negative value, it is substituted by the global mean of the series.

The pre-initialisation described here is not simple, but it guarantees that any ETS model can be constructed and estimated almost to any data. Yes, there might still be some issues with mixed ETS models, but the mechanism used in ADAM is quite robust.


### Starting optimisation of ARIMA states
ADAM ARIMA models have as many states as the number of polynomials $K$ (see Subsection \@ref(StateSpaceARIMAAdditive)). Each state $v_{i,t}$ needs to be initialised with $i$ values (e.g. 1 for the first state, 2 for the second etc). This leads in general to more initial values for states than the SSARIMA from @Svetunkov2019: $\frac{K(K+1)}{2}$ instead of $K$. However, we can reduce the number of initial seeds to estimate either by using a different initialisation procedure (e.g. backcasting) or using the following trick. First, we take the conditional expectations of all ARIMA states, which leads to:
\begin{equation}
  \mathrm{E}(v_{i,t} | t) = \eta_i y_{t} \text{ for } t=\{-K+1, -K+2, \dots, 0\},
  (\#eq:MSARIMAInitialisation00)
\end{equation}
and then we use these expectations for the initialisation of ARIMA states. This still implies calculating a lot of initials, but we can further reduce their number. We can express the actual value in terms of the state and error from \@ref(eq:MSARIMAState) for the last state $K$:
\begin{equation}
  y_{t} = \frac{v_{K,t} -\theta_K \epsilon_{t}}{\eta_K}.
  (\#eq:MSARIMAInitialisation01)
\end{equation}
We select the last state $K$ because it has the highest number of initials to estimate among all states. We can then insert the value \@ref(eq:MSARIMAInitialisation01) in each formula \@ref(eq:MSARIMAInitialisation00) for each state for $i=\{1, 2, \dots, K-1\}$ and take their expectations:
\begin{equation}
  \mathrm{E}(v_{i,t}|t) = \frac{\eta_i}{\eta_K} \mathrm{E}(v_{K,t}|t)  \text{ for } t=\{-i+1, -i+2, \dots, 0\}.
  (\#eq:MSARIMAInitialisation02)
\end{equation}
This formula shows how the expectation of each state $i$ depends on the expectation of the state $K$. We can use it to propagate the values of the last state to the previous ones. However, this strategy will only work for the states corresponding to the ARI elements of the model. In the case of MA, using the same principle of initialisation via the conditional expectation, we can set the initial MA states to zero and estimate only ARI states. This is a crude but relatively simple way to pre-initialise ADAM ARIMA.

Having said all that, we need to point out that it is advised to use backcasting in the ADAM ARIMA model -- this is a more reliable and faster procedure for initialisation of ARIMA than the optimisation.


### Starting optimisation of regressor states and constant
When it comes to the initials for the regressors, they are obtained from the parameters of the `alm()` model based on the rules below:

- The model with the logarithm of the response variable is constructed, if the **error term is multiplicative** and one of the following distributions has been selected: Normal, Laplace, S or Generalised Normal;
- Otherwise, the model is constructed based on the provided formula and selected distribution;
- In any case, the global trend is added to the formula to make sure that its effect on the values of parameters is reduced;
- If the data contains categorical variables (aka "factors" in R), then they are expanded to set of dummy variables, adding the baseline value to the mix (i.e. not dropping any categories). While the classical multiple regression would not be estimable in this situation, dynamic models like ETSX and ARIMAX can work with the complete set of levels of categorical variables. To get the missing level, the intercept is added to the parameters of dummy variables, after which the obtained vector is normalised. This way, we can get, for example, all seasonal components if we want to model seasonality via X part of the model, not merging one of the components with level (see discussion in Section \@ref(ETSXDynamicCategories)).

Finally, the initialisation of constant (if needed in the model) depends on the selected model. In case of ARIMA with all $D_j=0$, the mean of the data is used. In all other cases, the arithmetic mean of difference or the geometric mean of ratios of all actual values is used depending on the error type. This is because the constant acts as a drift in the model in the case of non-zero differences. In case of ETS, the impact of the constant is removed from the level in ETS and the states of ARIMA by either subtraction or division, again depending on the error term type.


### Example in R
All the details discussed above allow us telling ADAM what values to start from if we want to help it in optimisation. For demonstration purposes, we consider the ETS(A,A,N)+ARIMA(2,0,0) applied to Box-Jenkins data:

```{r}
adamETSBJ <- adam(BJsales, "AAN", orders=c(2,0,0))
```

The function will return the vector of parameters in the form it was used by the optimiser:
```{r}
adamETSBJ$B
```

It can be amended to help optimiser if we have an idea of what values we should have or just reused again to get to better set of values:

```{r}
adamETSBJ <- adam(BJsales, "AAN", orders=c(2,0,0),
                  B=adamETSBJ$B)
```

If we are dissatisfied with the result, we can print the solution found by the optimiser to understand why it stopped (we do not provide the output here):
```{r eval=FALSE}
adamETSBJ <- adam(BJsales, "AAN", orders=c(2,0,0),
                  B=adamETSBJ$B, print_level=41)
```

But hopefully after several iterations, we will get a better estimates of parameters of the model:

```{r}
adamETSBJ
```

Given that we are trying the ETS+ARIMA model, we can use backcasting, which should help optimiser by reducing the number of estimated parameters:

```{r}
adam(BJsales, "AAN", orders=c(2,0,0),
     initial="backcasting")
```

Hopefully this gives an idea how the estimation of parameters in `adam()` can be fine-tuned.
