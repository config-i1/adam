# Pure multiplicative ADAM ETS {#ADAMETSPureMultiplicativeChapter}
There is a reason why we discuss pure multiplicative ADAM ETS models separately: they are suitable for the positive data, especially when the level is low, yet they do not rely on prior data transformations (such as taking logarithms or applying a power transform). However, the models discussed in this chapter are not easy to work with -- they typically do not have closed forms for the conditional h steps ahead mean and variance and do not have well-defined parameters space. Furthermore, they make more sense in conjunction with positive-valued distributions, although they also work with the normal one. All these aspects are discussed in this chapter.


## Model formulation {#ADAMETSPureMultiplicative}
The pure multiplicative ETS implemented in ADAM framework can be formulated using logarithms, similar to how the pure additive ADAM ETS is formulated in \@ref(eq:ETSADAMStateSpacePureAdditive):
\begin{equation}
  \begin{aligned}
		\log y_t = & \mathbf{w}' \log(\mathbf{v}_{t-\mathbf{l}}) + \log(1 + \epsilon_{t}) \\
		\log \mathbf{v}_{t} = & \mathbf{F} \log \mathbf{v}_{t-\mathbf{l}} + \log(\mathbf{1}_k + \mathbf{g} \epsilon_t)
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureMultiplicative)
\end{equation}
where $\mathbf{1}_k$ is the vector of ones, containing $k$ elements (number of components in the model), $\log$ is the natural logarithm, applied element-wise to the vectors, and all the other values have been discussed in the previous sections. An example of a pure multiplicative model is ETS(M,M,M), for which we have the following values:
\begin{equation}
  \begin{aligned}
    \mathbf{w} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}, & \mathbf{F} = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}, & \mathbf{g} = \begin{pmatrix} \alpha \\ \beta \\ \gamma \end{pmatrix}, \\
    \mathbf{v}_{t} = \begin{pmatrix} l_t \\ b_t \\ s_t \end{pmatrix}, & \mathbf{l} = \begin{pmatrix} 1 \\ 1 \\ m \end{pmatrix}, & \mathbf{1}_k = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}
  \end{aligned}.
  (\#eq:ETSADAMMMMMatrices)
\end{equation}
By inserting these values in the equation \@ref(eq:ETSADAMStateSpacePureMultiplicative), we obtain model in logarithms:
\begin{equation}
	\begin{aligned}
		\log y_t = & \log l_{t-1} + \log b_{t-1} + \log s_{t-m} + \log \left(1 + \epsilon_{t} \right) \\
		\log l_{t} = & \log l_{t-1} + \log b_{t-1} + \log( 1  + \alpha \epsilon_{t}) \\ 
		\log b_{t} = & \log b_{t-1} + \log( 1  + \beta \epsilon_{t}) \\
		\log s_{t} = & \log s_{t-m} + \log( 1  + \gamma \epsilon_{t}) \\
	\end{aligned} ,
	(\#eq:ETSADAMMMMLogs)
\end{equation}
which after exponentiation becomes equal to the one, discussed in Section \@ref(ETSTaxonomyMaths):
\begin{equation}
  \begin{aligned}
    y_{t} = & l_{t-1} b_{t-1} s_{t-m} (1 + \epsilon_t) \\
    l_t = & l_{t-1} b_{t-1} (1 + \alpha \epsilon_t) \\
    b_t = & b_{t-1} (1 + \beta \epsilon_t) \\
    s_t = & s_{t-m} (1 + \gamma \epsilon_t) 
  \end{aligned}.
  (\#eq:ETSADAMMMM)
\end{equation}
An interesting observation is that the model \@ref(eq:ETSADAMMMMLogs) will produce values similar to the model ETS(A,A,A) applied to the data in logarithms, when the values of smoothing parameters are close to zero. This becomes apparent, when recalling the limit:
\begin{equation}
  \lim\limits_{x \to 0}\log(1+x) = x .
  (\#eq:limitOf1x)
\end{equation}
Based on that, the model will become close to the following one in cases of small values of smoothing parameters:
\begin{equation}
	\begin{aligned}
		\log y_t = & \log l_{t-1} + \log b_{t-1} + \log s_{t-m} + \epsilon_{t} \\
		\log l_{t} = & \log l_{t-1} + \log b_{t-1} + \alpha \epsilon_{t} \\ 
		\log b_{t} = & \log b_{t-1} + \beta \epsilon_{t} \\
		\log s_{t} = & \log s_{t-m} + \gamma \epsilon_{t} \\
	\end{aligned} ,
	(\#eq:ETSADAMMMMLogsEquivalent)
\end{equation}
which is the ETS(A,A,A) applied to the data in the logarithms. In many cases, the smoothing parameters will be small enough for the limit \@ref(eq:limitOf1x) to hold, so the two models will produce similar forecasts. The main benefit of \@ref(eq:ETSADAMMMMLogsEquivalent) is that it has closed forms for the conditional mean and variance, so the model \@ref(eq:ETSADAMMMMLogsEquivalent) can be used instead of \@ref(eq:ETSADAMMMMLogs) when the smoothing parameters are close to zero, and the variance of the error term is small to get conditional moments and quantiles of distribution. However, the form \@ref(eq:ETSADAMMMMLogsEquivalent) does not permit mixed components -- it only supports the multiplicative ones, making it detached from the other ETS models.


## Recursive relation {#adamETSPuremultiplicativeRecursive}
Similarly to how it was done for the pure additive model in Section \@ref(adamETSPureAdditiveRecursive), we can show what the recursive relation will look like for the pure multiplicative one (the logic here is the same, the main difference is in working with logarithms instead of the original values):
\begin{equation}
  \begin{aligned}
    \log y_{t+h} = & \mathbf{w}_{m_1}' \mathbf{F}_{m_1}^{\lceil\frac{h}{m_1}\rceil-1} \log \mathbf{v}_{t} + \mathbf{w}_{m_1}' \sum_{j=1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{F}_{m_1}^{j-1} \log \left(\mathbf{1}_k + \mathbf{g}_{m_1} \epsilon_{t+m_1\lceil\frac{h}{m_1}\rceil-j}\right) + \\
    & \mathbf{w}_{m_2}' \mathbf{F}_{m_2}^{\lceil\frac{h}{m_2}\rceil-1} \log \mathbf{v}_{t} + \mathbf{w}_{m_2}' \sum_{j=1}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{F}_{m_2}^{j-1} \log \left(\mathbf{1}_k + \mathbf{g}_{m_2} \epsilon_{t+m_2\lceil\frac{h}{m_2}\rceil-j}\right) + \\
    & \dots \\
    & \mathbf{w}_{m_d}' \mathbf{F}_{m_d}^{\lceil\frac{h}{m_d}\rceil-1} \log \mathbf{v}_{t} + \mathbf{w}_{m_d}' \sum_{j=1}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{F}_{m_d}^{j-1} \log \left(\mathbf{1}_k + \mathbf{g}_{m_d} \epsilon_{t+m_d\lceil\frac{h}{m_d}\rceil-j}\right) + \\
    & \log \left(1 + \epsilon_{t+h}\right)
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureMultiplicativeRecursion)
\end{equation}
In order to see how this recursion works, we can take the example of ETS(M,N,N), for which $m_1=1$ and all the other frequencies are equal to zero:
\begin{equation}
    y_{t+h} = \exp\left(\mathbf{w}_{1}' \mathbf{F}_{1}^{h-1} \log\mathbf{v}_{t} + \mathbf{w}_{1}' \sum_{j=1}^{h-1} \mathbf{F}_{1}^{j-1} \log \left(\mathbf{1}_k + \mathbf{g}_{1} \epsilon_{t+h-j}\right) +\log \left(1 + \epsilon_{t+h}\right)\right) ,
  (\#eq:ETSMNNADAMStateSpacePureMultiplicativeRecursion01)
\end{equation}
or after inserting $\mathbf{w}_{1}=1$, $\mathbf{F}_{1}=1$, $\mathbf{v}_{t}=l_t$, $\mathbf{g}_{1}=\alpha$ and $\mathbf{1}_k=1$:
\begin{equation}
    y_{t+h} = l_t \prod_{j=1}^{h-1} \left(1 + \alpha \epsilon_{t+h-j}\right) \left(1 + \epsilon_{t+h}\right) .
  (\#eq:ETSMNNADAMStateSpacePureMultiplicativeRecursion02)
\end{equation}

This recursion is useful to understand how the states evolve, and in the case of ETS(M,N,N), it allows obtaining the conditional expectation and variance. But in general, for models with trend and/or seasonality, it cannot be used to calculate moments, like the one for the pure additive ADAM ETS. This is discussed in Section \@ref(pureMultiplicativeExpectationAndVariance)).


## The problem with moments in pure multiplicative ETS {#pureMultiplicativeExpectationAndVariance}
The recursion \@ref(eq:ETSADAMStateSpacePureMultiplicativeRecursion) obtained in the previous subsection shows how the previous values influence the logarithms of states. While it is possible to calculate the expectation of the logarithm of the variable $y_{t+h}$, in general, this does not allow deriving the expectation of the variable in the original scale. This is because of the convolution of terms $\log(\mathbf{1}_k + \mathbf{g}_{m_i} \epsilon_{t+j})$ for different $j$. To better understand this issue, we consider this persistence part of the equation for the ETS(M,N,N) model:
\begin{equation}
    \log(1+\alpha\epsilon_t) = \log(1-\alpha + \alpha(1+\epsilon_t)).
  (\#eq:ETSMNNADAMPersistenceIssue)
\end{equation}
Whatever we assume about the distribution of the variable $(1+\epsilon_t)$, the distribution of \@ref(eq:ETSMNNADAMPersistenceIssue) will be more complicated. For example, if we assume that $(1+\epsilon_t)\sim\mathrm{log}\mathcal{N}(0,\sigma^2)$, then the distribution of \@ref(eq:ETSMNNADAMPersistenceIssue) is something like exp three-parameter log normal distribution [@Sangal1970]. The convolution of \@ref(eq:ETSMNNADAMPersistenceIssue) for different $t$ does not follow a known distribution, so it is not possible to calculate the conditional expectation and variance based on \@ref(eq:ETSADAMStateSpacePureMultiplicativeRecursion). Similar issues arrise if we assume any other distribution. The problem is worsened in case of multiplicative trend and / or multiplicative seasonality models, because then the recursion \@ref(eq:ETSADAMStateSpacePureMultiplicativeRecursion) contains several errors on the same observation (e.g. $\log(1+\alpha\epsilon_t)$ and $\log(1+\beta\epsilon_t)$).

The only way to derive the conditional expectation and variance for the pure multiplicative models is to use the formulae from tables \@ref(tab:ETSAdditiveError) and \@ref(tab:ETSMultiplicativeError) in Section \@ref(ETSTaxonomyMaths) and manually derive the values in the original scale. This works well only for the ETS(M,N,N) model, for which it is possible to take conditional expectation and variance of the recursion \@ref(eq:ETSMNNADAMStateSpacePureMultiplicativeRecursion02) to obtain:
\begin{equation}
    \begin{aligned}
	    \mu_{y,t+h} = \mathrm{E}(y_{t+h}|t) = & l_{t} \\
	    \mathrm{V}(y_{t+h}|t) = & l_{t}^2 \left(  \left(1+ \alpha^2 \sigma^2 \right)^{h-1} (1 + \sigma^2) -1 \right),
	\end{aligned}
	(\#eq:ETSMNNADAMConditionalValues)
\end{equation}
where $\sigma^2$ is the variance of the error term. For the other models, the conditional moments do not have a general closed forms because of the product of $\log(1+\alpha\epsilon_t)$, $\log(1+\beta\epsilon_t)$ and $\log(1+\gamma\epsilon_t)$. It is still possible to derive the moments for special cases of $h$, but this is a tedious process. In order to see that, we demonstrate here how the recursion looks for ETS(M,Md,M) model:
\begin{equation}
	\begin{aligned}
	    & y_{t+h} = l_{t+h-1} b_{t+h-1}^\phi s_{t+h-m} \left(1 + \epsilon_{t+h} \right) = \\
	    & l_{t} b_{t}^{\sum_{j=1}^h{\phi^j}} s_{t+h-m\lceil\frac{h}{m}\rceil} \prod_{j=1}^{h-1} \left( (1 + \alpha \epsilon_{t+j}) \prod_{i=1}^{j} (1 + \beta \epsilon_{t+i})^{\phi^{j-i}} \right) \prod_{j=1}^{\lceil\frac{h}{m}\rceil} \left(1 + \gamma \epsilon_{t+j}\right) \left(1 + \epsilon_{t+h} \right) .
	\end{aligned}
	(\#eq:ETSMMdMADAMRecursion)
\end{equation}
The conditional expectation of the recursion \@ref(eq:ETSMMdMADAMRecursion) does not have a simple form, because of the difficulties in calculating the expectation of $(1 + \alpha \epsilon_{t+j})(1 + \beta \epsilon_{t+i})^{\phi^{j-i}}(1 + \gamma \epsilon_{t+j})$. In a simple example of $h=2$ and $m>h$ the conditional expectation based on \@ref(eq:ETSMMdMADAMRecursion) can be simplified to:
\begin{equation}
	\mu_{y,t+2} = l_{t} b_{t}^{\phi+\phi^2} \left(1 + \alpha \beta \sigma^2 \right),
	(\#eq:ETSMMdMADAMRecursionHorizon2)
\end{equation}
introducing the second moment, the variance of the error term $\sigma^2$. The case of $h=3$ implies the appearance of the third moment, the $h=4$ -- the fourth etc. This is why there are no closed forms for the conditional moments for the pure multiplicative models with trend and/or seasonality. In some special cases, when smoothing parameters and the variance of error term are all low, it is possible to use approximate formulae for some of the multiplicative models. These are discussed in Chapter 6 of @Hyndman2008b. In a special case, when all smoothing parameters are equal to zero or when $h=1$, the conditional expectation will coincide with the point forecast from the Tables \@ref(tab:ETSAdditiveError) and \@ref(tab:ETSMultiplicativeError) in Section \@ref(ETSTaxonomyMaths). But in general, the best thing that can be done in this case is the simulation of possible paths (using the formulae from the tables mentioned above) and then the calculation of mean and variance based on them. Finally, it can be shown for pure multiplicative models that:
\begin{equation}
    \hat{y}_{t+h} \leq \check{y}_{t+h} \leq \mu_{y,t+h} ,
    (\#eq:ETSADAMpointValueInequality)
\end{equation}
where $\mu_{y,t+h}$ is the conditional h steps ahead expectation, $\check{y}_{t+h}$ is the conditional h steps ahead geometric expectation (expectation in logarithms) and $\hat{y}_{t+h}$ is the point forecast [@Svetunkov2020ETS].


## Smoothing parameters bounds {#stabilityConditionMultiplicativeError}
Similar to the pure additive ADAM ETS, it is possible to have different restrictions on smoothing parameters for pure multiplicative models. However, in this case, the classical and the usual restrictions become more reasonable from the model's point of view. In contrast, the derivation of admissible bounds becomes a challenging task. Consider the ETS(M,N,N) model, for which the level is updated using the following relation:
\begin{equation}
    l_t = l_{t-1} (1 + \alpha\epsilon_t) = l_{t-1} (1-\alpha + \alpha(1+\epsilon_t)).
  (\#eq:ETSMNNADAMLevelUpdate)
\end{equation}
As discussed previously, the main benefit of pure multiplicative models is in dealing with positive data. So, it is reasonable to assume that $(1 + \epsilon_t)>0$, which implies that the actual values will always be positive and that each model component should also be positive. This means that $\alpha(1 + \epsilon_t)>0$, which implies that $(1-\alpha + \alpha(1+\epsilon_t))>1-\alpha$ or equivalently based on \@ref(eq:ETSMNNADAMPersistenceIssue) $(1 + \alpha\epsilon_t)>1-\alpha$ should always hold. In order for the model to make sense, the condition $(1 + \alpha\epsilon_t)>0$ should hold as well, ensuring that the level is always positive. Connecting the two inequalities, this can be achieved when $1-\alpha \geq 0$, meaning that $\alpha \leq 1$. Furthermore, for the level to be positive irrespective of the specific error on observation $t$, the smoothing parameter should be non-negative. So, in general, the bounds $[0, 1]$ guarantee that the model ETS(M,N,N) will produce positive values only. The two special cases $\alpha=0$ and $\alpha=1$ make sense because the level in \@ref(eq:ETSMNNADAMLevelUpdate) will be positive in both of them, implying that for the former, the model becomes equivalent to the global level, while for the latter the model is equivalent to Random Walk. Using similar logic, it can be shown that the **classical restriction** $\alpha, \beta, \gamma \in [0, 1]$ guarantees that the model will always produce positive values.

The more restrictive condition of the **usual bounds**, discussed in Section \@ref(ETSParametersBounds), makes sense as well, although it might be more restrictive than needed. But it has a different idea: guaranteeing that the model exhibits averaging properties.

Finally, the **admissible bounds** might still make sense for the pure multiplicative models, but the condition for parameters bounds becomes more complicated and implies that the distribution of the error term becomes trimmed from below to satisfy the classical restrictions discussed above. Very crudely, the conventional restriction from pure additive models can be used to approximate the proper admissible bounds, given the limit \@ref(eq:limitOf1x), but this should be used with care, given the discussion above.

From the practical point of view, the pure multiplicative models typically have low smoothing parameters, close to zero, because they rely on multiplication of components rather than on addition, so even the classical restriction might seem broad in many situations.


## Distributional assumptions in pure multiplicative ETS {#ADAMETSMultiplicativeDistributions}
The conventional assumption for the error term in ETS is that $\epsilon_t\sim\mathcal{N}(0,\sigma^2)$, which guarantees that the conditional expectation of the model will be equal to the point forecasts when the trend and seasonal components are not multiplicative. In general, ETS works well in many cases with this assumption, mainly when the data is strictly positive, and the level of series is high (e.g. thousands of units). However, this assumption might become unhelpful when dealing with lower-level data because the models may start generating non-positive values, which contradicts the idea of pure multiplicative ETS models. @Akram2009 studied the ETS models with multiplicative error and suggested that applying ETS on data in logarithms is a better approach than just using ETS(M,Y,Y) models (here "Y" stands for non-additive components). However, this approach sidesteps the ETS taxonomy, creating a new group of models. An alternative (also discussed in @Akram2009) is to assume that the error term $1+\epsilon_t$ follows some distribution for positive data. The authors mentioned log Normal, truncated and Gamma distributions but never explored them further.

@Svetunkov2020ETS discussed several options for the distribution of $1+\epsilon_t$ in ETS, including log Normal, Gamma and Inverse Gaussian. Other distributions for positive data can be applied as well, but their usage might become complicated, because they need to meet condition $\mathrm{E}(1+\epsilon_t)=1$ in order for the expectation to coincide with the point forecasts for models with non-multiplicative trend and seasonality. For example, if the error term follows log Normal distribution, then this restriction implies that the location of the distribution should be non-zero: $1+\epsilon_t\sim\mathrm{log}\mathcal{N}\left(-\frac{\sigma^2}{2},\sigma^2\right)$. Using this principle the following distributions can be used for ADAM ETS:

1. Inverse Gaussian: $\left(1+\epsilon_t \right) \sim \mathcal{IG}(1, s)$, so that $y_t = \mu_t (1+\epsilon_t) \sim \mathcal{IG}(\mu_t, s)$;
2. Gamma: $\left(1+\epsilon_t \right) \sim \Gamma (s^{-1}, s)$, so that $y_t = \mu_t (1+\epsilon_t) \sim \Gamma (s^{-1}, s \mu_t)$;
3. Log Normal: $\left(1+\epsilon_t \right) \sim \mathrm{log}\mathcal{N}\left(-\frac{\sigma^2}{2}, \sigma^2\right)$ so that $y_t = \mu_t (1+\epsilon_t) \sim \mathrm{log}\mathcal{N}(\log \mu_t -\frac{\sigma^2}{2}, \sigma^2)$.

The MLE of $s$ in $\mathcal{IG}$ is straightforward and is:
\begin{equation}
	\hat{s} = \frac{1}{T} \sum_{t=1}^{T} \frac{e_{t}^2}{1+e_t} ,
	(\#eq:ETSMultiplicativeErrorMLESigmaIG)
\end{equation}
where $e_t$ is the estimate of the error term $\epsilon_t$. However, when it comes to the MLE of scale parameter for the log Normal distribution with the aforementioned restrictions, it is more complicated and is [@Svetunkov2020ETS]:
\begin{equation}
	\hat{\sigma}^2 = 2\left(1-\sqrt{ 1-\frac{1}{T} \sum_{t=1}^{T} \log^2(1+e_{t})}\right).
	(\#eq:ETSMultiplicativeErrorMLESigmaLogN)
\end{equation}
Finally, MLE of $s$ in $\mathcal{\Gamma}$ does not have a closed form. Luckily, method of moments can be used to obtain its value [@Svetunkov2020ETS]:
\begin{equation}
	\hat{s} = \frac{1}{T} \sum_{t=1}^{T} e_{t}^2 .
	(\#eq:ETSMultiplicativeErrorMLESigmaGamma)
\end{equation}

Even if we deal with strictly positive high level data, it is not necessary to limit the distribution with Normal only. The following distributions can be applied as well:

1. Normal: $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$, implying that $y_t = \mu_t (1+\epsilon_t) \sim \mathcal{N}(\mu_t, \mu_t^2 \sigma^2)$;
2. Laplace: $\epsilon_t \sim \mathcal{Laplace}(0, s)$, meaning that $y_t = \mu_t (1+\epsilon_t) \sim \mathcal{Laplace}(\mu_t, \mu_t s)$;
3. S: $\epsilon_t \sim \mathcal{S}(0, s)$, so that $y_t = \mu_t (1+\epsilon_t) \sim \mathcal{S}(\mu_t, \sqrt{\mu_t} s)$;
4. Generalised Normal: $\epsilon_t \sim \mathcal{GN}(0, s, \beta)$ and $y_t = \mu_t (1+\epsilon_t) \sim \mathcal{GN}(\mu_t, \mu_t^\beta s)$;
<!-- 5. Logistic: $\epsilon_t \sim \mathcal{Logis}(0, s)$; -->
<!-- 6. Student's t: $\epsilon_t \sim \mathcal{t}(\nu)$; -->
<!-- 5. Asymmetric Laplace: $\epsilon_t \sim \mathcal{ALaplace}(0, s, \alpha)$ with $y_t = \mu_t (1+\epsilon_t) \sim \mathcal{ALaplace}(\mu_t, \mu_t s, \alpha)$. -->

Note that the MLE of scale parameters for these distributions will be calculated differently than in the case of pure additive models. For example, for the normal distribution it is:
\begin{equation}
	\hat{\sigma}^2 = \frac{1}{T}\sum_{t=1}^T \frac{y_t-\hat{\mu}_t}{\hat{\mu}_t} ,
	(\#eq:ETSMultiplicativeErrorMLESigmaNormal)
\end{equation}
where the main difference from the additive error case arises from the measurement equation of the multiplicative error models:
\begin{equation}
	y_t = \mu_t (1+\epsilon_t),
	(\#eq:ETSMultiplicativeErrorMeasurement)
\end{equation}
implying that 
\begin{equation}
	e_t = \frac{y_t-\hat{\mu}_t}{\hat{\mu}_t}.
	(\#eq:ETSMultiplicativeErrorFormula)
\end{equation}
The estimates of scale can then be used in the next phase, when parameters are optimised via the maximisation of respective log-likelihood function. The maximum likelihood approach is in case of ADAM models is discussed in detail in Section \@ref(ADAMETSEstimationLikelihood).

The distributional assumptions impact both the estimation of models and the prediction intervals. In the case of asymmetric distributions (such as log Normal, Gamma and Inverse Gaussian), the intervals will typically be asymmetric, with the upper bound being further away from the point forecast than the lower one. Furthermore, even with the comparable estimates of scales of distributions, Inverse Gaussian distribution will typically produce wider bounds than Log-Normal and Gamma. The width of intervals relates to the kurtosis of distributions, which is discussed in Chapter 3 of @SvetunkovSBA.


## Examples of application {#ADAMETSMultiplicativeExamples}
### Non-seasonal data
We continue our examples with the same Box-Jenkins sales data by fitting the ETS(M,M,N) model, but this time with a holdout of 10 observations:

```{r}
adamModel <- adam(BJsales, "MMN", h=10, holdout=TRUE)
adamModel
```

The output above is similar to the one we discussed in Section \@ref(ADAMETSPureAdditiveExamples), so we can compare the two models using various criteria and select the most appropriate. Even though the default distribution for the multiplicative error models in ADAM is $\Gamma$, we can compare this model with the ETS(A,A,N) via information criteria. For example, here are the AICc for the two models:
```{r}
# ETS(M,M,N)
AICc(adamModel)
# ETS(A,A,N)
AICc(adam(BJsales, "AAN", h=10, holdout=TRUE))
```
The comparison is fair because both models were estimated via likelihood, and both likelihoods are formulated correctly, without omitting any terms (e.g. the `ets()` function from the `forecast` package omits the $-\frac{T}{2} \log\left(2\pi e \frac{1}{T}\right)$ for convenience, which makes it incomparable with other models). In this example, the pure additive model is more suitable for the data than the pure multiplicative one.

Figure \@ref(fig:BJSalesadamETSMMN) shows how the model fits the data and what forecast it produces. Note that the function produces the **point forecast** in this case, which is not equivalent to the conditional expectation! The point forecast undershoots the actual values in the holdout.

```{r BJSalesadamETSMMN, fig.cap="Model fit for Box-Jenkins Sales data from ETS(M,M,N).", echo=FALSE}
plot(adamModel,7,main="")
```

If we want to produce the forecasts (conditional expectation and prediction interval) from the model, we can do it, using the same command as in Section \@ref(ADAMETSPureAdditiveExamples):

```{r BJSalesadamETSMMNForecast, fig.cap="Forecast for Box-Jenkins Sales data from ETS(M,M,N).", echo=FALSE}
plot(forecast(adamModel, h=10,
              interval="prediction", level=0.95))
```

Note that, when we ask for "prediction" interval, the `forecast()` function will automatically decide what to use based on the estimated model: in case of pure additive one, it will use analytical solutions, while in the other cases, it will use simulations (see Section \@ref(ADAMForecastingPI)). The point forecast obtained from the forecast function corresponds to the conditional expectation and is calculated based on the simulations. This also means that it will differ slightly from one run of the function to another (reflecting the uncertainty in the error term). Still, the difference, in general, should be negligible for a large number of simulation paths.

The forecast with prediction interval is shown in Figure \@ref(fig:BJSalesadamETSMMNForecast). The conditional expectation is not very different from the point forecast in this example. This is because the variance of the error term is close to zero, thus bringing the two close to each other:
```{r}
sigma(adamModel)^2
```

We can also compare the performance of ETS(M,M,N) with $\Gamma$ distribution and the conventional ETS(M,M,N), assuming normality:

```{r}
adamModelNormal <- adam(BJsales, "MMN", h=10, holdout=TRUE,
                        distribution="dnorm")
adamModelNormal
```
In this specific example, the two distributions produce very similar results with almost indistinguishable estimates of parameters.


### Seasonal data
The `AirPassengers` data used in Section \@ref(ADAMETSPureAdditiveExamples) has (as we discussed) multiplicative seasonality. So, the ETS(M,M,M) model might be more suitable than the pure additive one that we used previously:

```{r}
adamModel <- adam(AirPassengers, "MMM", h=12, holdout=TRUE)
adamForecast <- forecast(adamModel, h=12, interval="prediction")
adamModel
```

Notice that the smoothing parameter $\gamma=0$, which implies that we deal with the data with deterministic multiplicative seasonality. Comparing the information criteria (e.g. AICc) with the ETS(A,A,A) (discussed in Section \@ref(ADAMETSPureAdditiveExamplesETSAAA)), the pure multiplicative model does a better job at fitting the data than the additive one:
```{r}
adamModelAdditive <- adam(AirPassengers, "AAA", lags=12,
                          h=12, holdout=TRUE)
AICc(adamModelAdditive)
```
The conditional expectation and prediction interval from this model are mroe adequate as well (Figure \@ref(fig:AirPassengersMMMForecast)):

```{r AirPassengersMMMForecast, fig.cap="Forecast for air passengers data using ETS(M,M,M) model.", echo=FALSE}
plot(adamForecast, main="")
```

If we want to calculate the error measures based on the conditional expectation, we can use the `measures()` function from `greybox` package the following way:
```{r}
measures(adamModel$holdout,adamForecast$mean,actuals(adamModel))
```
These can be compared with the measures from the ETS(A,A,A) model:
```{r}
measures(adamModel$holdout,adamModelAdditive$forecast,actuals(adamModel))
```

Comparing, for example, MSE from the two models, we can conclude that the pure multiplicative model is more accurate than the pure additive one.

We can also produce the plot of the time series decomposition according to ETS(M,M,M) (see Figure \@ref(fig:AirPassengersMMMDecomposition)).

```{r AirPassengersMMMDecomposition, fig.cap="Decomposition of air passengers data using ETS(M,M,M) model.", echo=FALSE}
plot(adamModel,12)
```

The plot in Figure \@ref(fig:AirPassengersMMMDecomposition) shows that the residuals are more random for the pure multiplicative model than for the ETS(A,A,A), but there still might be some structure left. The autocorrelation and partial autocorrelation functions (discussed in Section \@ref(BJApproach)) might help in understanding this better:

```{r AirPassengersMMMACFPACF, fig.cap="ACF and PACF of residuals of ETS(M,M,M) model."}
par(mfcol=c(2,1), mar=c(2,4,2,1))
plot(adamModel,10:11)
```

The plot in Figure \@ref(fig:AirPassengersMMMDecomposition) shows that there is still some correlation left in the residuals, which could be either due to pure randomness or imperfect estimation of the model. Tuning the parameters of the optimiser or selecting a different model might solve the problem.
