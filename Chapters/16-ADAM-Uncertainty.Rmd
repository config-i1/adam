# Handling uncertainty in ADAM {#ADAMUncertainty}
So far, when we discussed forecasts from ADAM models, we have assumed that the smoothing parameters and initial values are known. This is the conventional assumption of ETS models from @Hyndman2008b. However, in reality they are never known and are always estimated in sample. This means that with the change of sample size, the estimates of parameters will change as well. This uncertainty will impact the model fit, the point forecasts and prediction intervals. In order to overcome this issue @Bergmeir2016 proposed bagging - the procedure that decomposes time series using STL [@Cleveland1990], then recreates many time series by bootstrapping the remainder then fits best ETS models to each of the newly created time series and combines the forecasts from the models. This way [as was explained by @Petropoulos2018] the parameters of models will differ from one generated time series to another, thus the final forecasts will handle the uncertainty about the parameters. In addition, this approach also covers the model uncertainty element, which was discussed in Section \@ref(ADAMCombinations). The main issue with the approach is that it is computationally expensive and assumes that STL decomposition is appropriate for time series and that the residuals from this decomposition are independent.

In this chapter we focus on discussion of uncertainty about the estimates of parameters of ADAM models, starting from dealing with confidence intervals for them and ending with producing forecasts that take that uncertainty into account. We start with the discussion of covariance matrix of parameters, then move to the confidence intervals construction and to how the uncertainty around the estimates of parameters is propagated in the model. We then discuss a method that allows capturing this uncertainty and use it for fitted values and forecasts of the model.


## Covariance matrix of parameters {#ADAMUncertaintyVCOV}
One of the basic conventional statistical ways of capturing uncertainty about estimates of parameters is via the calculation of covariance matrix of parameters. The covariance matrix that is typically calculated in regression context is based on the assumption of normality and can be derived base don maximum likelihood estimates of parameters. It relies on the "Fisher Information", which in turn is calculated as a negative expectation of Hessian of parameters (the matrix of second derivatives of likelihood function with respect to all the estimates of parameters). The idea of Hessian is to capture the curvatures of the likelihood function in its optimal point in order to understand what impact each of parameters has on it. If we calculate the Hessian matrix and have Fisher Information, then using Cramer-Rao bound [@WikipediaCramerRaoBound], the true variance of parameters will be greater or equal to the inverse of Fisher Information:
\begin{equation}
    \mathrm{V}(\hat{\theta_j}) \geq \frac{1}{\mathrm{FI}(\hat{\theta_j})} ,
    (\#eq:FICovariance)
\end{equation}
where $\theta_j$ is the parameter under consideration. The property \@ref(eq:FICovariance) can then be used for the calculation of the covariance matrix of parameters. While in case with regression this calculation has an analytical solution, in case of ETS and ARIMA, this can only be done via numeric methods, because the models rely on recursive relations.

In R, an efficient calculation of Hessian can be done via `hessian()` function from `pracma` package. There is a method `vcov()` that does all the calculations, estimating the negative Hessian inside `adam()` and then inverts the result. Here is an example of how this works:
```{r}
adamModel <- adam(BJsales, h=10, holdout=TRUE)
adamModelVcov <- vcov(adamModel)
adamModelVcov
```
The precision of estimate will depend on the closeness of the likelihood function to its maximum in the estimated parameters. If the likelihood was not properly maximised, and the function stopped prematurely, then the covariance matrix might be incorrect and contain errors. Our of curiosity, we could calculate the correlation matrix of the estimated parameters:
```{r}
adamModelVcov / sqrt(diag(adamModelVcov) %*% t(diag(adamModelVcov)))
```
This matrix does not provide much useful analytical information, but demonstrates that the estimates of initial level and trend of the `r adamModel$model` model applied to this data are negatively correlated. The values from this matrix can then be used for a variety of purposes, including calculation of confidence intervals of parameters, construction of confidence interval for the fitted value and point forecasts and finally the construction of more adequate prediction intervals.

In some cases the `vcov()` method would complain that the Fisher Information cannot be inverted. This typically means that the `adam()` failed to reach the maximum of the likelihood function

Note that this method only works, when `loss="likelihood"` or when the loss is aligned with the assumed distribution (e.g. `loss="MSE"` and `distribution="dnorm"`). In all the other cases, other approaches (such as bootstrapt) would need to be used for the estimation of the covariance matrix of parameters.


### Bootstrapped covariance matrix
An alternative way of constructing the matrix is via the bootstrap. The one that is implemented in `smooth` is based on `coefboostrap()` method from `greybox` package, which implements the modified case resampling. It is less efficient than the Fisher Information method in terms of computational time and works only for larger samples. The algorithm implemented in the function uses a continuous sub-sample of the original data, starting from the original point $t=1$ (if backcasting is used (see Section \@ref(ADAMInitialisationOptAndBack)), then the starting point will be allowed to differ). This sub-sample is then used for the re-estimation of `adam()` in order to get the empirical estimates of parameters. The procedure is repeated `nsim` times, which for `adam()` is by default equal to 100. This approach is far from ideal and will typically lead to underestimated variance of initials, but it does not break the structure of the data and allows obtaining relatively fast results without imposing any additional assumptions on the model and the data. I personally recommend using it in case of the initialisation via backcasting.

Here is an example of how the function work on the data above - it is possible to speed up the process by doing parallel calculations:
```{r}
adamModelBoot <- coefbootstrap(adamModel,parallel=TRUE)
adamModelBoot
```
The covariance matrix can then be extracted from the result via `adamModelBoot$vcov`. The same procedure is used in `vcov()` method if `bootstrap=TRUE`:
```{r}
vcov(adamModel, bootstrap=TRUE, parallel=TRUE)
```


## Confidence intervals for parameters
As discussed in Section 5.1 of @SvetunkovSBA, if several important [assumptions](#diagnostics) are satisfied and CLT holds, then the distribution of estimates of parameters will follow the Normal one, which will allow us constructing confidence intervals for them. In case of ETS and ARIMA models in ADAM framework, the estimated parameters include smoothing parameters, ARMA parameters and initial values. In case of explanatory variables, the pool of parameters is increased by the coefficients for those variables. And in case of intermittent state space model, the parameters will also include the elements of the occurrence part of the model. The CLT should work if consistent [estimators](#ADAMETSEstimation) are used (e.g. MSE or Likelihood), if the parameters do not lie near the bounds, the model is correctly specified and moments of the distribution of error term are finite.

In case of ETS and ARIMA, the parameters are bounded and the estimates might lie near the bounds. This means that the distribution of estimates might not be normal. However, given that the bounds are typically fixed and are forced by the optimiser, the estimates of parameters will follow rectified normal distribution [@WikipediaRectifiedNormal]. This is important because knowing the distribution, we can derive the confidence intervals for the parameters. First, we would need to use t-statistics for this purposes, because we would need to estimate the standard errors of parameters. The confidence intervals will be constructed in the conventional way in this case, using the formula (see Section 5.1 of @SvetunkovSBA):
\begin{equation}
    \mu \in (\hat{\theta_j} + t_{\alpha/2}(df) s_{\theta_j}, \hat{\theta_j} + t_{1-\alpha/2}(df) s_{\theta_j}), 
    (\#eq:confidenceInterval)
\end{equation}
where $t_{\alpha/2}(df)$ is Student's t-statistics for $df=T-k$ degrees of freedom ($T$ is the sample size and $k$ is the number of estimated parameters) and $\alpha$ is the significance level. Second, after constructing the intervals, we can cut their values with the bounds of parameters, thus imposing rectified distribution. An example would be the ETS(A,N,N) model, for which the smoothing parameter is typically restricted by (0, 1) region and thus the confidence interval should not go beyond these bounds as well.

In order to construct the interval, we need to know the standard errors of parameters. Luckily, they can be calculated as square roots of the diagonal of the covariance matrix of parameters (discussed in Section \@ref(ADAMUncertaintyVCOV)):
```{r}
sqrt(diag(adamModelVcov))
```
Based on these values and the formula \@ref(eq:confidenceInterval) we can produce confidence intervals for parameters of any ADAM model:
```{r}
confint(adamModel, level=0.99)
```
In order to have the bigger picture, we can produce the summary of the model, which will include the table above:
```{r}
summary(adamModel, level=0.99)
```
The output above shows the estimates of parameters and their 99% confidence intervals. For example, based on this output we can conclude that the uncertainty about the estimate of the initial trend is very big, and in the "true model" it could be either positive or negative (or even close to zero). At the same time, the "true" parameter of the initial level will lie in 99% of the cases between 194.56 and 208.06. Just as a reminder, here how the model fit looks for ADAM on this data:

```{r}
plot(adamModel,7)
```

As another example, we can have a similar summary for ARIMA models in ADAM:
```{r}
adamModelARIMA <- adam(BJsales, "NNN", h=10, holdout=TRUE,
                       order=list(ar=3,i=2,ma=3,select=TRUE))
summary(adamModelARIMA)
```
From the summary above, we can see that the parameter $\theta_2$ is close to zero, and the interval around it is wide. So, we can expect that it might change sign if the sample size increases or become even closer to zero. Given that the model above was estimated with the optimisation of initial states, we see in the summary above the values for the ARIMA states and their confidence intervals. If we used `initial="backcasting"`, the summary would not include them.

This estimate of uncertainty via confidence intervals might also be useful to see what can happen with the estimates of parameters if the sample size increases: will they change substantially or not. If they do, then the decisions made on Monday based on the available data might differ seriously from the decisions made on Tuesday. So, in the ideal world we would want to have as lower confidence intervals as possible.


## Conditional variance with uncertain parameters
We now consider two special cases with pure additive state space models: (1) When the values of the initial state vector are unknown; (2) When the parameters of the model (e.g. smoothing or AR/MA parameters) are estimated on a sample of data - and discuss analytical formulae for the conditional variance for the two cases. This variance can then be used for the construction of confidence interval of the fitted line and / or for the confidence / prediction interval for the holdout period. We do not cover the more realistic case, when both initials and parameters are estimated, because there is no closed analytical form for this due to potential correlations between the estimates of parameters.

### Estimated initial state
First, we need to recall the recursive relations discussed in Section \@ref(stabilityConditionAdditiveError), specifically formula \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04). Just to simplify all the derivations in this section, we consider the non-seasonal case, for which all elements of $\boldsymbol{l}$ are equal to one. This can be ETS(A,N,N), ETS(A,A,N), ETS(A,Ad,N) or ARIMA models. The recursive relation from the observation first observation till the end of sample can be written as:
\begin{equation}
    \hat{\mathbf{v}}_{t} = \mathbf{D}^{t} \hat{\mathbf{v}}_{0} + \sum_{j=0}^{t-1} \mathbf{D}^{j} y_{t - j} ,
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal01)
\end{equation}
where $\mathbf{D}=\mathbf{F} - \mathbf{g}\mathbf{w}'$. The formula \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal01) shows that the most recent value of the state vector depends on the initial value $\hat{\mathbf{v}}_{0}$ and on the linear combination of actual values. **Note** that we assume in this part that the matrix $\mathbf{D}$ is known, i.e. the smoothing parameters are not estimated. Although this is an unrealistic assumption, it helps in showing how the variance of initial state would influence the variance at the end of sample. If we now take the variance conditional on the actual values $y_{t - j}$ for all $j=\{0, \dots, t-1 \}$, then we will have (due to independence of two terms in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal01)):
\begin{equation}
    \mathrm{V}(\hat{\mathbf{v}}_{t} | y_1, y_2, \dots y_t) = \mathrm{V}\left( \mathbf{D}^{t} \hat{\mathbf{v}}_{0} \right) + \mathrm{V}\left(\sum_{j=0}^{t-1} \mathbf{D}^{j} y_{t - j} | y_1, y_2, \dots y_t \right) .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal02)
\end{equation}
The reason why we condition the variance on actual values, is because they are given to us and we want to see how different initial states would lead to the changes in the model fit given these values and thus how the uncertainty will propagate from $j=1$ to $j=t$. In the formula \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal02), the right-hand side is equal to zero, because all actual values are known and $\mathbf{D}$ does not have any uncertainty due to the assumption above. This leads to the following covariance matrix of states on observation $t$:
\begin{equation}
    \mathrm{V}(\hat{\mathbf{v}}_{t} | y_1, y_2, \dots y_t) = \mathbf{D}^{t} \mathrm{V}\left( \hat{\mathbf{v}}_{0} \right) \left(\mathbf{D}^{t}\right)' .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal03)
\end{equation}
Inserting the values of matrix $\mathbf{D}$, we can then get the variance \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal03). For example, for ETS(A,N,N), the conditional variance of the state on observation $t$ is:
\begin{equation}
    \mathrm{V}(\hat{l}_{t} | y_1, y_2, \dots y_t) = (1-\alpha)^{t} \mathrm{V}\left( \hat{l}_{0} \right) (1-\alpha)^{t} .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionANN)
\end{equation}
As the formula above shows, if the smoothing parameter lies between zero and one, then the uncertainty of the initial level will not have a big impact on the uncertainty on observation $t$. If we use admissible bounds (see Section \@ref(ETSParametersBounds)), then the smoothing parameters might lie in the region (1, 2) and thus the impact of the variance of the initial state will increase with the increase of the sample size $t$.

Now that we have the variance of the state, we can also calculate the variance of the fitted values (or one step ahead in-sample forecast). In the pure additive model, the fitted values are calculated as:
\begin{equation}
    \hat{y}_t = \mu_{t|t-1} = \mathbf{w}' \hat{\mathbf{v}}_{t-1}.
  (\#eq:ETSADAMStateSpacePureAdditiveFitted)
\end{equation}
The variance conditional on all actual observations will then be:
\begin{equation}
    \mathrm{V}(\hat{y}_t | y_1, y_2, \dots y_t) = \mathrm{V}\left( \mathbf{w}' \hat{\mathbf{v}}_{t-1} \right) ,
  (\#eq:ETSADAMStateSpacePureAdditiveFittedVariance01)
\end{equation}
which after inserting \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal03) in \@ref(eq:ETSADAMStateSpacePureAdditiveFittedVariance01) leads to:
\begin{equation}
    \mathrm{V}(\hat{y}_t | y_1, y_2, \dots y_t) = \mathbf{w}' \mathbf{D}^{t-1} \mathrm{V}\left( \hat{\mathbf{v}}_{0} \right) \left(\mathbf{D}^{t-1}\right)' \mathbf{w} .
  (\#eq:ETSADAMStateSpacePureAdditiveFittedVariance02)
\end{equation}
This variance can then be used in the calculation of the confidence interval for the fitted values, assuming that the estimates of initials state follow normal distribution (CLT).

Finally, the variance of initial states will also impact the conditional h steps ahead variance from the model. This can be seen from the recursion \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion05), which in case of non-seasonal models simplifies to:
\begin{equation}
    y_{t+h} = \mathbf{w}' \mathbf{F}^{h-1} \hat{\mathbf{v}}_{t} + \mathbf{w}' \sum_{j=1}^{h-1} \mathbf{F}^{j-1} \mathbf{g} \epsilon_{t+h-j} + \epsilon_{t+h} .
  (\#eq:ETSADAMStateSpacePureAdditiveForecastVariance01)
\end{equation}
Taking the variance of $y_{t+h}$ conditional on the all the information until the observation $t$ (all actual values) leads to:
\begin{equation}
    \begin{aligned}
    \mathrm{V}( y_{t+h} | y_1, y_2, \dots y_t) = & \mathbf{w}' \mathbf{F}^{h-1} \mathbf{D}^{t-1} \mathrm{V}\left( \hat{\mathbf{v}}_{0} \right) \left(\mathbf{D}^{t-1}\right)' (\mathbf{F}')^{h-1} \mathbf{w} + \\
                                                 & \left( \left(\mathbf{w}' \sum_{j=1}^{h-1} \mathbf{F}^{j-1} \mathbf{g} \mathbf{g}' (\mathbf{F}')^{j-1} \mathbf{w} \right) + 1 \right) \sigma^2 .
    \end{aligned}
  (\#eq:ETSADAMStateSpacePureAdditiveForecastVariance02)
\end{equation}
This formula can then be used for the construction of prediction intervals of the model, for example using formula \@ref(eq:ETSADAMStateSpacePureAdditivePredictionInterval). Note that the construction of prediction intervals will be discussed later in Section \@ref(predictionIntervalsConstruction).

As a final note, it is also possible to derive the variances for the seasonal models, the only thing that would change in this situation is that the matrices $\mathbf{F}$, $\mathbf{w}$ and $\mathbf{g}$ will be split into submatrices, similar to how it was done in Section \@ref(adamETSPureAdditiveRecursive).


### Estimated parameters of ADAM model
Now we discuss the case, when the initial states are either known or not estimated directly. This, for example, corresponds to the situation with backcasted initials. Continuing our non-seasonal model example, we can use the same recursion \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal01), keeping in mind that now the value of the initial state vector $\mathbf{v}_0$ is known. This results in the following variance:
\begin{equation}
    \mathrm{V}(\mathbf{v}_{t} | y_1, y_2, \dots y_t) = \mathrm{V}\left( \hat{\mathbf{D}}^{t} \mathbf{v}_{0} + \sum_{j=0}^{t-1} \hat{\mathbf{D}}^{j} y_{t - j} | y_1, y_2, \dots y_t \right) .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal04)
\end{equation}
Unfortunately, there is no closed form for the formula \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal04) for a general state space model, because the uncertainty comes from $\hat{\mathbf{D}}$, which is then exponentiated. The variance of $\hat{\mathbf{D}}^{j}$ does not have analytical solution in a general case. Furthermore, we cannot assume that $\hat{\mathbf{D}}^{j}$ is independent of $\hat{\mathbf{D}}^{i}$ for any $i$ and $j$. However, what we can calculate is the conditional h steps ahead variance of $y_{t+h}$, given values of $\mathbf{v}_t$ for a special case, ETS(A,N,N) model. This is based on the recursion \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion05):
\begin{equation}
    \mathrm{V}(y_{t+h} | l_t) = \sigma^2_h = \mathrm{V} \left(\sum_{j=1}^{h-1} \hat{\alpha} \epsilon_{t+h-j} \right) + \sigma^2.
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionANN02)
\end{equation}
The variance of the sum in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionANN02) can be expanded as:
\begin{equation}
    \mathrm{V} \left(\sum_{j=1}^{h-1} \hat{\alpha} \epsilon_{t+h-j} \right) = \sum_{j=1}^h \mathrm{V} \left(\hat{\alpha} \epsilon_{t+h-j}\right) + 2 \sum_{j=2}^{h-1} \sum_{i=1}^{j-1} \mathrm{cov}(\hat{\alpha} \epsilon_{t+h-j},\hat{\alpha} \epsilon_{t+h-i}).
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionANN03)
\end{equation}
Each variance in left-hand side of \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionANN03) can be expressed via:
\begin{equation}
    \mathrm{V} \left(\hat{\alpha} \epsilon_{t+h-j}\right) = \mathrm{V} (\hat{\alpha}) \mathrm{V}(\epsilon_{t+h-j}) + \mathrm{V} (\hat{\alpha}) \mathrm{E}(\epsilon_{t+h-j})^2 + \mathrm{E} (\hat{\alpha})^2 \mathrm{V}(\epsilon_{t+h-j}).
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionANN04)
\end{equation}
Given that the expectation of error term is assumed to be zero, this simplifies to:
\begin{equation}
    \mathrm{V} \left(\hat{\alpha} \epsilon_{t+h-j}\right) = \left(\mathrm{V} (\hat{\alpha}) + \hat{\alpha}^2 \right) \sigma^2.
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionANN05)
\end{equation}
As for the covariances in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionANN03), after the expansion it can be shown that each of them is equal to:
\begin{equation}
    \begin{aligned}
    \sum_{i=1}^{j-1} \mathrm{cov}(\hat{\alpha} \epsilon_{t+j},\hat{\alpha} \epsilon_{t+i}) = & \mathrm{V}(\hat{\alpha}) \mathrm{cov}(\epsilon_{t+h-i},\epsilon_{t+h-j}) + \hat{\alpha}^2 \mathrm{cov}(\epsilon_{t+h-i},\epsilon_{t+h-j}) \\
    & + \mathrm{E}(\epsilon_{t+h-i}) \mathrm{E}(\epsilon_{t+h-j}) \mathrm{V}(\hat{\alpha}).
    \end{aligned}
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionANN06)
\end{equation}
Given the assumptions of the model, the autocovariances of error terms should all be equal to zero, and the expectation of the error term should be equal to zero as well, which means that the value in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionANN06) will be equal to zero as well. Based on that, each variance in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionANN03) can be represented as:
\begin{equation}
    \mathrm{V} \left(\sum_{j=1}^{h-1} \hat{\alpha} \epsilon_{t+h-j} \right) = (h-1) (\mathrm{V} \left(\hat{\alpha}) + \hat{\alpha}^2 \right) \sigma^2 .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionANN07)
\end{equation}
Inserting \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionANN07) in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionANN02), we get the final conditional h steps ahead variance of the model:
\begin{equation}
    \sigma^2_h = \left((h-1) (\mathrm{V} \left(\hat{\alpha}) + \hat{\alpha}^2 \right) + 1\right) \sigma^2,
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionANNFinal)
\end{equation}
which looks similar to the one in formula \@ref(eq:ETSADAMStateSpaceANNRecursionMeanAndVariance) from Section \@ref(pureAdditiveExpectationAndVariance), but now has the variance of the smoothing parameters in it.

Unfortunately, the conditional variances for the other models are more complicated due to the introduction of convolutions of parameters. Furthermore, the formula \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionANNFinal) only focuses on the conditional variance given the known $l_t$, but does not take into account the uncertainty of $l_t$ for the fitted values in sample. Given the complexity of the problem, in the next section, we introduce a technique that allows correctly propagating the uncertainty of parameters and initial values to the forecasts of the model.


## Multi-scenarios for ADAM states {#adamRefitted}
As discussed in the previous sections, it is difficult to capture the impact of the uncertainty about the parameters on the states of the model and as a result difficult to take it into account on the forecasting stage. Furthermore, so far we have only discussed pure additive models, for which it is at least possible to do some derivations. When it comes to models with multiplicative components, it becomes close to impossible to demonstrate how the uncertainty propagates over time. In order to overcome these limitations, we develop a simulation-based approach that relies on the selected model form.

The idea of the approach is to get the covariance matrix of the parameters of the selected model (see Section \@ref(ADAMUncertaintyVCOV)) and then generate $n$ sets of parameters randomly from rectified multivariate normal distribution using the matrix and the values of estimated parameters. After that the model is applied to the data with each combination of generated parameters, states, fitted values and residuals are extracted from it to get their distribution. This way we propagate the uncertainty about the parameters from the first observation to the last. The final states can also be used to produce point forecasts and prediction intervals based each set of parameters. These scenarios allow producing more adequate prediction intervals from the model and / or confidence intervals for the fitted values, states and conditional expectations. All of this is done without additional assumptions (as it is done in bagging), relying fully on the model. However, the approach is computationally expensive, as it requires fitting all the $n$ models to the data, although no estimation is needed for them. If the uncertainty about the model needs to be taken into account, then the combination of models can be used, as described in Section \@ref(ADAMCombinations).

`smooth` package has the method `refit()` that implements this approach for `adam()` models. This works with ADAM ETS, ARIMA, regression and any combination between them. Here is an example in R:

```{r adamETSRefitted, fig.cap="Refitted ADAM ETS(M,M,M) model on AirPassengers data."}
adamModelETS <- adam(AirPassengers, "MMM", h=10, holdout=TRUE)
adamModelETSRefitted <- refit(adamModelETS)
plot(adamModelETSRefitted)
```

Figure \@ref(fig:adamETSRefitted) demonstrates how the approach works on the example of `AirPassengers` data and ETS(M,M,M) model. The grey areas around the fitted line show quantiles from the fitted values, forming confidence intervals of width 95%, 80%, 60%, 40% and 20%. They show how the fitted value would vary, if the estimates of parameters would different from the estimated ones. Note that there was a warning about the covariance matrix of parameters, which typically arises if the optimal value of the loss function was not reached. If this happens, I would recommend tuning the optimiser (see Section \@ref(ADAMInitialisation)). For example, we could try more iterations via setting `maxeval` parameter or reestimating the model, providing the estimates of parameters in `B`. If these fail and the bounds from the `refit()` are too wide, then it might make sense considering backcasting for the initialisation.

As mentioned earlier, ADAM ARIMA also supports this approach. Here is an example on artificial, non-seasonal data (see Figure \@ref(fig:adamARIMARefitted):
```{r eval=FALSE}
y <- rnorm(200,100,10)
adamModelARIMA1 <- adam(y, "NNN", h=10, holdout=TRUE,
                       orders=c(0,1,1))
adamModelARIMARefitted1 <- refit(adamModelARIMA1)
plot(adamModelARIMARefitted1)
```

```{r adamARIMARefitted, fig.cap="Refitted ADAM ARIMA(0,1,1) model on artificial data.", echo=FALSE}
load("data/adamRefitted.Rdata")
plot(adamModelARIMARefitted1)
```

Note that the more complicated the fitted model is, the more difficult it is to optimise it and thus the more difficult it is to get accurate estimates of covariance matrix of parameters. This might result in highly uncertain states and thus fitted values. The safer approach in this case is using bootstrap for the estimation of covariance matrix, but this is more computationally expensive and would only work on longer time series. See example in R (and Figure \@ref(fig:adamARIMARefitted200)):

```{r eval=FALSE}
adamModelARIMA2 <- adam(y, "NNN", h=10, holdout=TRUE,
                       orders=c(0,1,1))
adamModelARIMARefitted2 <- refit(adamModelARIMA2, bootstrap=TRUE,
                                nsim=1000, parallel=TRUE)
plot(adamModelARIMARefitted2)
```

```{r adamARIMARefitted200, fig.cap="Refitted ADAM ARIMA(0,1,1) model on artificial data, bootstrapped covariance matrix.", echo=FALSE}
plot(adamModelARIMARefitted2)
```

The approach described in this section is still a work in progress. While it works in theory, there are still computational difficulties with the calculation of Hessian matrix. If the covariance matrix is not estimated accurately, it might contain high variances, leading to the higher than needed uncertainty of the model. This will then result in unreasonable confidence bounds and finally lead to extremely wide prediction intervals.
