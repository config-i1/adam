# Handling uncertainty in ADAM {#ADAMUncertainty}
So far, when we discussed forecasts from ADAM, we have assumed that the smoothing parameters and initial values are known, even though we have acknowledged in Chapter \@ref(ADAMETSEstimation) that they are estimated. This is the conventional assumption of ETS models from @Hyndman2008b, which also applies to ARIMA models. However, in reality, the parameters are never known and are always estimated in-sample. This means that the estimates of parameters will inevitably change with the change of sample size. This uncertainty will impact the model fit, the point forecasts and prediction intervals. To overcome this issue, @Bergmeir2016 proposed bagging ETS -- the procedure that decomposes time series using STL [@Cleveland1990] then recreates many time series by bootstrapping the remainder then fits best ETS model to each of the newly created time series and combines the forecasts from the models. This way [as was explained by @Petropoulos2018], the parameters of models will differ from one generated time series to another. Thus the final forecasts will handle the uncertainty about the parameters. In addition, this approach also mitigates to some extent the model uncertainty, which was discussed in Section \@ref(ADAMCombinations), because models are selected automatically on each bootstrapped series. The main issue with the approach is that it is computationally expensive and assumes that STL decomposition is appropriate for time series. Furthermore, it assumes that the residuals from this decomposition do not contain any information and are independent.

In this chapter, we focus on a discussion of uncertainty in ADAM, specifically about the estimates of parameters. We start with a discussion of how the data can be simulated from an estimated ADAM, then move to how to deal with confidence intervals for the parameters and after that -- how the parameters uncertainty can be propagated to the states and fitted values of the model.


## Simulating data from ADAM {#ADAMUncertaintySimulation}
Before we move to the discussion of parameters uncertainty and how it propagates to the states, fitted values and final forecasts, it makes sense to understand how data can be generated based on an ADAM with some parameters. The data generation in this case is done in the following steps:

1. Decide what structure will be used for data generation. This depends on the type and order of the specific ADAM. For example, if it a pure additive ETS(A,N,A)+ARIMA(2,0,0) model then the set of equations \@ref(eq:ADAMETSARIMAANA100) will be used with matrices and vectors defined by \@ref(eq:ADAMETSARIMAANN100Matrices);
2. Generate error term for the values of $t=1 \dots T$, where $T$ is the desired sample size. The error term can be generated using any known distribution as long as its mean equals to zero in case of additive error ($\mathrm{E}(\epsilon_t)=0$) or equals to one in case of the multiplicative one ($\mathrm{E}(1+\epsilon_t)=1$);
3. Set values of the persistence vector $\mathbf{g}$ and the matrices $\mathbf{w}$ and $\mathbf{F}$. In case of ARIMA, the values are defined based on the order of the model and the values of its parameters (see discussion in Section \@ref(StateSpaceARIMA)). The specific values will change depending on the type of model and the elements it has;
4. Define the initial value of the state vector $\mathbf{v}_t$ for $t<1$;
5. Apply recursively the formula of the state space model \@ref(eq:ETSADAMStateSpace) (discussed in Section \@ref(ADAMETSGeneral)), collecting the values of the state vector $\mathbf{v}_{t}$ and the generated actual values $y_t$ for all $t=1,\dots,T$.

Note that the simulation process allows using distributions that are not officially supported by ADAM on the step (2). In fact, in some instances the error term can be provided by a user, so it would not be random.

In the `smooth` package for R, there are several functions that implement this simulation procedure:

- `sim.es()` allows generating data from an arbitrary ETS model;
- `sim.ssarima()` supports data generation from a State Space ARIMA model with parameters and initial states provided by user;
- `sim.sma()` generates data from Simple Moving Average, discussed in Subsection \@ref(SMA);
- `simulate()` method for `adam` and `smooth` classes allows generating the data using the parameters of an estimated model, thus creating time series similar to the original one.

As an example, consider estimating ADAM ETS(M,M,M) on `AirPassengers` data and then generating several time series from it (see Figure \@ref(fig:adamSimulated)) using the following R code:

```{r echo=FALSE}
set.seed(41)
```

```{r adamSimulated, fig.cap="Data generated by ADAM using the parameters of the estimated ETS model."}
adam(AirPassengers, "MMM") |>
    simulate() |>
    plot(main="")
```

You might notice that the trend in Figure \@ref(fig:adamSimulated) differs from the one in the original data. This is because the ETS model captured some changes in the trend in the data, which was then reflected in the generated series. In some cases, the error term The resulting simulated data can be used for the experiments in a controlled environment.

In some cases, some specific data might be required with specific parameters and values for error term. In those cases, the `sim.es()`, `sim.ssarima()` and other similar function from the `smooth` package can be used. Here is an example of a code for `sim.es()` with an arbitrary function for generation of the error term:

```{r eval=FALSE}
# A function to generate error term
randomizer <- function(n, mu=0, s=1){
    return(mu + s * rlogis(n, 0, 1))
}
# Generation of data from ETS(A,N,N)
x <- sim.es("ANN", obs=120, nsim=10,
            persistence=0.1, initial=1000,
            randomizer="randomizer", mu=0, s=1)
# Plot a random series from the 10 generated ones
plot(x)
```


## Covariance matrix of parameters {#ADAMUncertaintyVCOV}
One of the basic conventional statistical ways of capturing uncertainty about estimates of parameters is via the calculation of the covariance matrix of parameters. The covariance matrix that is typically calculated in regression context can be based either on OLS estimates of parameters, or on MLE assuming that the residuals of the model follow Normal distribution. In the latter case, it relies on the "Fisher Information", which in turn is calculated as a negative expectation of Hessian of parameters (the matrix of second derivatives of the likelihood function with respect to all the estimates of parameters). The idea of Hessian is to capture the curvatures of the likelihood function in its optimal point to understand what impact each of the parameters has on the likelihood. If we calculate the Hessian matrix and have the Fisher Information, then using Cramer-Rao bound [@Rao1945], the true variance of parameters will be greater or equal to the inverse of the Fisher Information:
\begin{equation}
    \mathrm{V}(\hat{\theta_j}) \geq \frac{1}{\mathrm{FI}(\hat{\theta_j})} ,
    (\#eq:FICovariance)
\end{equation}
where $\theta_j$ is the parameter under consideration. The property \@ref(eq:FICovariance) can then be used for the calculation of the covariance matrix of parameters. While in case of the linear regression this calculation has an analytical solution, in case of ETS and ARIMA, this can only be done via numeric methods, because the models rely on recursive relations and there is no closed analytical solution for the parameters of these models.

In R, an efficient calculation of Hessian can be done via the `hessian()` function from the `pracma` package [@R-pracma]. In `smooth` there is a method `vcov()` that does all the calculations, estimating the negative Hessian inside the `adam()` and then inverting the result. Here is an example of how this works:
```{r}
adamETSBJ <- adam(BJsales, h=10, holdout=TRUE)
adamETSBJVcov <- vcov(adamETSBJ)
round(adamETSBJVcov, 3)
```
The precision of the estimate will depend on the closeness of the likelihood function to its maximum in the estimated parameters. If the likelihood was not properly maximised and the function stopped prematurely, then the covariance matrix might be incorrect and contain errors. In that case, `vcov()` will produce a warning, saying that the resulting matrix might not be estimated correctly. The covariance matrix itself shows the variability of each of the parameters and whether they have any relations between them or not. In order to get a clearer picture about the latter, we could calculate the correlation matrix of the estimated parameters:
```{r}
cov2cor(adamETSBJVcov) |>
    round(3)
```
This matrix demonstrates that the estimates of the initial level and trend of the ETS(A,Ad,N) model applied to this data are strongly negatively correlated. This means that on average with the increase of the initial level, the initial trend tends to be lower, which makes sense as a starting point of a model. The values from the covariance matrix can also be used for example, for calculation of confidence intervals of parameters (Section \@ref(ADAMUncertaintyConfidenceInterval)), construction of confidence interval for the fitted values and point forecasts (Section \@ref(adamRefitted)), and for the construction of more adequate prediction intervals (i.e. taking the uncertainty of estimates of parameters into account, see Subsection \@ref(ADAMForecastingPIComplete)).

In some cases, the `vcov()` method would complain that the Fisher Information cannot be inverted. This typically means that the `adam()` failed to reach the maximum of the likelihood function. Re-estimating the model with different initial values and/or optimiser settings might resolve the problem (see Section \@ref(ADAMInitialisation)).

::: remark
This method only works when `loss="likelihood"` or when the loss is aligned with the assumed distribution (e.g. `loss="MSE"` and `distribution="dnorm"`). In all the other cases, other approaches (such as bootstrap) would need to be used to estimate the covariance matrix of parameters.
:::


### Bootstrapped covariance matrix
An alternative way of constructing the matrix is via the bootstrap. The one implemented in `smooth` is based on the `coefbootstrap()` method from the `greybox` package, which implements the modified case resampling. It is less efficient than the Fisher Information method in terms of computational time and works only for larger samples. The algorithm implemented in the function creates continuous sub-samples of the original data, starting from the initial point $t=1$ (if backcasting is used, as discussed in Section \@ref(ADAMInitialisationOptAndBack), then the starting point will be allowed to vary). These sub-samples are then used for re-estimation of `adam()` to get the empirical estimates of parameters. The procedure is repeated `nsim` times, which for `adam()` is by default equal to 100. This approach is far from ideal and will typically lead to the underestimated variance of initials because of the sample size restrictions. Still, it does not break the data structure and allows obtaining results relatively fast without imposing any additional assumptions on the model and the data. I personally recommend using it in case of the initialisation via backcasting.

Here is an example of how the function works on the data above -- it is possible to speed up the process by doing parallel calculations:
```{r}
adamETSBJBoot <- coefbootstrap(adamETSBJ, parallel=TRUE)
adamETSBJBoot
```
The `size` in the output above refers to the sub-sample size, which by default is 75% of the original data length. The covariance matrix can then be extracted from the result via `adamETSBJBoot$vcov`. The same procedure is used in `vcov()` method if `bootstrap=TRUE`:
```{r}
adamETSBJ |>
    vcov(bootstrap=TRUE, parallel=TRUE) |>
    round(3)
```


## Confidence intervals for parameters {#ADAMUncertaintyConfidenceInterval}
As it is well known in statistics [e.g. see Section 6.4 of @SvetunkovSBA], if several vital model assumptions (discussed in Section \@ref(diagnostics)) are satisfied and CLT holds, then the distribution of estimates of parameters will follow the Normal one, which will allow us to construct confidence intervals to capture the uncertainty around the parameters. In case of ETS and ARIMA models in the ADAM framework, the estimated parameters include smoothing, dampening and ARMA parameters together with the initial states. In the case of explanatory variables, the pool of parameters is increased by the coefficients for those variables and their smoothing parameters (if the dynamic model from Section \@ref(ADAMXDynamic) is used). Furthermore, in case of the intermittent state space model, the parameters will also include the elements of the occurrence part of the model. The CLT should hold for all of them if:

1. Estimates of parameters are consistent (e.g. MSE or Likelihood is used in estimation, see Section \@ref(ADAMETSEstimation));
2. The parameters do not lie near the bounds;
3. The model is correctly specified;
4. Moments of the distribution of error term are finite.

In case of ETS and ARIMA, some of parameters are bounded (e.g. to satisfy stability condition from Section \@ref(stabilityConditionAdditiveError)), and the estimates might lie near the bounds. This means that the distribution of estimates of parameters might not be Normal. However, given that the bounds of the parameters are typically fixed, and all estimates that exceed them are set to the boundary values in the optimisation routine, the estimates of parameters will follow Rectified Normal distribution [@Socci1997]. This is important because knowing the distribution, we can derive the confidence intervals for the parameters. However, given that we estimate the standard errors of parameters in sample, we need to use t-statistics to correctly capture the uncertainty. The confidence intervals will be constructed in a conventional way in this case, using the formula [see Section 6.4 of @SvetunkovSBA]:
\begin{equation}
    \theta_j \in (\hat{\theta_j} + t_{\alpha/2}(df) s_{\theta_j}, \hat{\theta_j} + t_{1-\alpha/2}(df) s_{\theta_j}), 
    (\#eq:confidenceInterval)
\end{equation}
where $t_{\alpha/2}(df)$ is Student's t-statistics for $df=T-k$ degrees of freedom ($T$ is the sample size and $k$ is the number of all estimated parameters) and $\alpha$ is the significance level. Then, after constructing the intervals, we can cut their values with the bounds of parameters, thus rectifying the distribution.
<!-- An example would be the ETS(A,N,N) model, for which the smoothing parameter is typically restricted by (0, 1) region and thus the confidence interval should not go beyond these bounds as well. -->

To construct the interval, we need to know the standard errors of parameters. Luckily, they can be calculated as square roots of the diagonal of the covariance matrix of parameters (discussed in Section \@ref(ADAMUncertaintyVCOV)):
```{r}
diag(adamETSBJVcov) |>
    sqrt()
```
Based on these values and the formula \@ref(eq:confidenceInterval), we can produce confidence intervals for parameters of any ADAM, which is done in R using the `confint()` method. For example, here are the intervals for the model estimated before with the significance level of 1% (confidence level of 99%):
```{r}
confint(adamETSBJ, level=0.99)
```
In the output above, the distributions for $\alpha$, $\beta$ and $\phi$ are rectified: $\alpha$ and $\phi$ are restricted with the region (0, 1) and thus are rectified from above, while $\beta \in (0, \alpha)$ and as a result is rectified from below.

::: remark
We do not rectify the distribution of $\beta$ from above, because $\hat{\alpha} \approx$ `r round(adamETSBJ$persistence[1],4)`.
:::

To have the bigger picture, we can produce the summary of the model, which will include the table above:
```{r}
summary(adamETSBJ, level=0.99)
```
The output above shows the estimates of parameters and their 99% confidence intervals. Based on this output, for example, we can conclude that the uncertainty about the initial trend estimate is large, and in the "true model", it could be either positive or negative (or even close to zero). At the same time, the "true" parameter of the initial level will lie in 99% of the cases between 190.4172 and 215.0740. Just as a reminder, Figure \@ref(fig:adamModelBJAAdN) shows the model fit and point forecasts for the estimated ETS model on this data.

```{r adamModelBJAAdN, fig.cap="Model fit and point forecasts of ETS(A,Ad,N) on Box-Jenkins Sales data.", echo=FALSE}
plot(adamETSBJ,7)
```

As another example, we can have a similar summary for ARIMA models in ADAM:

```{r}
adam(BJsales, "NNN", h=10, holdout=TRUE,
     order=list(ar=3,i=2,ma=3,select=TRUE)) |>
    summary()
```

From the summary above, we can see that the parameter $\theta_2$ is close to zero, and the interval around it is wide. So, we can expect that it might change the sign if the sample size increases or become even closer to zero. Given that the model above was estimated with the optimisation of initial states, we also see the values for the ARIMA states and their confidence intervals in the summary above. If we used `initial="backcasting"`, the summary would not include them.

::: remark
If we faced difficulties estimating the covariance matrix of parameters using the standard Hessian-based approach, we could try bootstrap via `summary(adamETSBJ, bootstrap=TRUE)`.
:::

This estimate of uncertainty via confidence intervals might also be helpful to see what can happen with the estimates of parameters if the sample size increases: will they change substantially or not. If they do, then the decisions made on Monday based on the available data might differ considerably from the decisions made on Tuesday. So, in the ideal world, we would want to have as narrow confidence intervals of parameters as possible.


## Conditional variance with uncertain parameters {#conditionalVarianceUncertainty}
Now that we have discussed how the covariance matrix of parameters and confidence intervals for parameters can be generated in ADAM, we can move to the discussion of propagating the effect of uncertainty of parameters to the states, fitted values and forecasts. I consider two special cases with pure additive state space models:

(1) When the values of the initial state vector are estimated;
(2) When the model parameters (e.g. smoothing or AR/MA parameters) are estimated.

I discuss analytical formulae for the conditional variance for these cases. This variance can then be used to construct the confidence interval of the fitted line and/or for the confidence/prediction interval for the holdout period. I do not cover the more realistic case when both initials and parameters are estimated because there is no closed analytical form for this due to potential correlations between the estimates of parameters. Furthermore, there are no closed forms for the conditional variance for the multiplicative and mixed models, which is why I focus my explanation on the pure additive ones only.


### Estimated initial state
First, we need to recall the recursive relations discussed in Section \@ref(stabilityConditionAdditiveError), specifically formula \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04). Just to simplify all the derivations in this section, we consider the non-seasonal case, in which all elements of $\boldsymbol{l}$ are equal to one. This can be ETS(A,N,N), ETS(A,A,N), ETS(A,Ad,N) or some ARIMA models. 

::: remark
The more general case is more complicated but is derivable using the same principles as discussed below.
:::

The recursive relation from the first observation till some observation $t$ can be written as:
\begin{equation}
    \hat{\mathbf{v}}_{t} = \mathbf{D}^{t} \hat{\mathbf{v}}_{0} + \sum_{j=0}^{t-1} \mathbf{D}^{j} {\mathbf{g}} y_{t-j} ,
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal01)
\end{equation}
where $\mathbf{D}=\mathbf{F} -\mathbf{g}\mathbf{w}^\prime$. The formula \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal01) shows that the most recent value of the state vector depends on the initial value $\hat{\mathbf{v}}_{0}$ and on the linear combination of actual values.

::: remark
We assume in this part that the matrix $\mathbf{D}$ is known, i.e. the smoothing parameters are not estimated. Although this is an unrealistic assumption, it helps in showing how the variance of initial state would influence the conditional variance of actual values at the end of sample.
:::

If we now take the variance of state vector conditional on the previous actual values $y_{t-j}$ for all $j=\{0, \dots, t-1 \}$, then we will have (due to independence of two terms in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal01)):
\begin{equation}
    \mathrm{V}(\hat{\mathbf{v}}_{t} | y_1, y_2, \dots y_t) = \mathrm{V}\left( \mathbf{D}^{t} {\mathbf{g}} \hat{\mathbf{v}}_{0} \right) + \mathrm{V}\left(\sum_{j=0}^{t-1} \mathbf{D}^{j} y_{t-j} | y_1, y_2, \dots y_t \right) .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal02)
\end{equation}
We condition the variance on actual values in the formula above because they are given to us, and we want to see how different initial states would lead to the changes in the model fit given these values and thus how the uncertainty will propagate from $j=1$ to $j=t$. In the formula \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal02), the right-hand side is equal to zero because all actual values are known, and $\mathbf{D}$ does not have any uncertainty due to the assumption above. This leads to the following covariance matrix of states on observation $t$:
\begin{equation}
    \mathrm{V}(\hat{\mathbf{v}}_{t} | y_1, y_2, \dots y_t) = \mathbf{D}^{t} \mathrm{V}\left( \hat{\mathbf{v}}_{0} \right) \left(\mathbf{D}^{t}\right)^\prime .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal03)
\end{equation}
Inserting the values of matrix $\mathbf{D}$ in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal03), we can then get the variance of the state vector on the observation $t$ given the uncertainty of the initial state. For example, for ETS(A,N,N), the conditional variance of the level on observation $t$ is:
\begin{equation}
    \mathrm{V}(\hat{l}_{t} | y_1, y_2, \dots y_t) = (1-\alpha)^{t} \mathrm{V}\left( \hat{l}_{0} \right) (1-\alpha)^{t} .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionANN)
\end{equation}
As the formula above shows, if the smoothing parameter lies between zero and one, then the impact of the uncertainty of the initial level on the current one will be diminished with the increase of $t$. The closer $\alpha$ is to zero, the more impact the variance of the initial level will have on the variance of the current level. If we use admissible bounds (see Section \@ref(ETSParametersBounds)), then the smoothing parameter might lie in the region (1, 2), and the impact of the variance of the initial state on the current one will be higher the close $\alpha$ is to 2.

Now that we have the variance of the state, we can also calculate the variance of the fitted values (or one step ahead in-sample forecast). In the pure additive model, the fitted values are calculated as:
\begin{equation}
    \hat{y}_t = \mu_{y,t|t-1} = \mathbf{w}^\prime \hat{\mathbf{v}}_{t-1}.
  (\#eq:ETSADAMStateSpacePureAdditiveFitted)
\end{equation}
The variance of the fitted value conditional on all actual observations will then be:
\begin{equation}
    \mathrm{V}(\hat{y}_t | y_1, y_2, \dots y_t) = \mathrm{V}\left( \mathbf{w}^\prime \hat{\mathbf{v}}_{t-1} \right) ,
  (\#eq:ETSADAMStateSpacePureAdditiveFittedVariance01)
\end{equation}
which after inserting \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal03) in \@ref(eq:ETSADAMStateSpacePureAdditiveFittedVariance01) leads to:
\begin{equation}
    \mathrm{V}(\hat{y}_t | y_1, y_2, \dots y_t) = \mathbf{w}^\prime \mathbf{D}^{t-1} \mathrm{V}\left( \hat{\mathbf{v}}_{0} \right) \left(\mathbf{D}^{t-1}\right)^\prime \mathbf{w} .
  (\#eq:ETSADAMStateSpacePureAdditiveFittedVariance02)
\end{equation}
This variance can then be used to calculate the confidence interval for the fitted values, assuming that the estimates of the initial state follow a Normal distribution (due to CLT). In case of ETS(A,N,N), this equals to:
\begin{equation}
    \mathrm{V}(\hat{y}_t | y_1, y_2, \dots y_t) = (1-\alpha)^{t-1} \mathrm{V}\left( \hat{l}_{0} \right) (1-\alpha)^{t-1} .
  (\#eq:ETSADAMStateSpacePureAdditiveFittedVarianceANN)
\end{equation}

Finally, the variance of the initial states will also impact the conditional h steps ahead variance of the model. This can be seen from the recursion \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion05), which in case of non-seasonal models simplifies to:
\begin{equation}
    y_{t+h} = \mathbf{w}^\prime \mathbf{F}^{h-1} \hat{\mathbf{v}}_{t} + \mathbf{w}^\prime \sum_{j=1}^{h-1} \mathbf{F}^{j-1} \mathbf{g} e_{t+h-j} + e_{t+h} .
  (\#eq:ETSADAMStateSpacePureAdditiveForecastVariance01)
\end{equation}
Taking the variance of $y_{t+h}$ conditional on the all the information until the observation $t$ (all actual values) with $h>1$ leads to:
\begin{equation}
    \begin{aligned}
    \mathrm{V}( y_{t+h} | y_1, y_2, \dots y_t) = & \mathbf{w}^\prime \mathbf{F}^{h-1} \mathbf{D}^{t-1} \mathrm{V}\left( \hat{\mathbf{v}}_{0} \right) \left(\mathbf{D}^{t-1}\right)^\prime (\mathbf{F}^\prime)^{h-1} \mathbf{w} + \\
                                                 & \left( \left(\mathbf{w}^\prime \sum_{j=1}^{h-1} \mathbf{F}^{j-1} \mathbf{g} \mathbf{g}^\prime (\mathbf{F}^\prime)^{j-1} \mathbf{w} \right) + 1 \right) \sigma^2 .
    \end{aligned}
  (\#eq:ETSADAMStateSpacePureAdditiveForecastVariance02)
\end{equation}
This formula can then be used for the construction of prediction intervals of the model, for example using formula \@ref(eq:ETSADAMStateSpacePureAdditivePredictionInterval). The topic of construction of prediction intervals will be discussed later in Section \@ref(ADAMForecastingPI). In case of the ETS(A,N,N) model this simplifies to:
\begin{equation}
    \begin{aligned}
    \mathrm{V}( y_{t+h} | y_1, y_2, \dots y_t) = & (1-\alpha)^{t-1} \mathrm{V}\left( \hat{l}_{0} \right) (1-\alpha)^{t-1} + \\
                                                 & \left(1 + (h-1) \alpha^2 \right) \sigma^2 .
    \end{aligned}
  (\#eq:ETSADAMStateSpacePureAdditiveForecastVarianceANN)
\end{equation}

::: remark
It is also possible to derive the variances for the seasonal models. The only thing that would change in comparison with the formulae above is that the matrices $\mathbf{F}$, $\mathbf{w}$ and $\mathbf{g}$ will need to be split into sub-matrices, similar to how it was done in Section \@ref(adamETSPureAdditiveRecursive).
:::


### Estimated parameters of ADAM
Now we discuss the case when the initial states are either known or not estimated directly. This, for example, corresponds to the situation with backcasted initials. Continuing our non-seasonal model example, we can use the following recursion (similar to \@ref(eq:ETSADAMStateSpacePureAdditiveForecastVariance01)), keeping in mind that now the value of the initial state vector $\mathbf{v}_0$ is known:
\begin{equation}
    \mathbf{v}_{t+h-1} = \hat{\mathbf{F}}^{h-1} \mathbf{v}_{t} + \sum_{j=1}^{h-1} \hat{\mathbf{F}}^{j-1} \hat{\mathbf{g}} e_{t+h-j} .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal04)
\end{equation}
The conditional variance of the state, given the values on observation $t$ in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal04) in general does not have a closed-form because of the exponentiation of the transition matrix $\hat{\mathbf{F}}$. However, in a special case, when the matrix does not contain the parameters (e.g. non-damped trend ETS models or ARIMA without AR terms), there is an analytical solution to the variance. In this case, $\mathbf{F}$ is provided rather than being estimated, which simplifies the inference:
\begin{equation}
    \mathrm{V}(\mathbf{v}_{t+h-1} | t) = \mathrm{V}\left(\sum_{j=1}^{h-1} \mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j}\right)
  (\#eq:ETSADAMStateSpacePureAdditiveVariance01)
\end{equation}

The variance of the sum in \@ref(eq:ETSADAMStateSpacePureAdditiveVariance01) can be expanded as:
\begin{equation}
    \begin{aligned}
    \mathrm{V} \left(\sum_{j=1}^{h-1} \mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j} \right) = & \sum_{j=1}^{h-1} \mathrm{V} \left(\mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j}\right) + \\
    & 2 \sum_{j=2}^{h-1} \sum_{i=1}^{j-1} \mathrm{cov}(\mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j},\mathbf{F}^{i} \hat{\mathbf{g}} e_{t+h-i}).
    \end{aligned}
  (\#eq:ETSADAMStateSpacePureAdditiveVariance02)
\end{equation}
Each variance in the left-hand side of \@ref(eq:ETSADAMStateSpacePureAdditiveVariance02) can be expressed via:
\begin{equation}
    \begin{aligned}
    \mathrm{V} \left(\mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j}\right) = \mathbf{F}^{j-1} \left( \right. & \mathrm{V} (\hat{\mathbf{g}}) \mathrm{V}(e_{t+h-j}) + \mathrm{V} (\hat{\mathbf{g}}) \mathrm{E}(e_{t+h-j})^2 + \\
    & \left. \mathrm{E} (\hat{\mathbf{g}}) \mathrm{E} (\hat{\mathbf{g}})^\prime \mathrm{V}(e_{t+h-j})\right) (\mathbf{F}^{j-1})^\prime.
    \end{aligned}
  (\#eq:ETSADAMStateSpacePureAdditiveVariance03)
\end{equation}
Given that the expectation of error term is assumed to be zero, and substituting $\mathrm{V}(e_{t+h-j})=\sigma^2$ (assuming that the error term is homoscedastic), this simplifies to:
\begin{equation}
    \mathrm{V} \left(\mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j}\right) = \mathbf{F}^{j-1} \left( \mathrm{V} (\hat{\mathbf{g}}) + \mathrm{E} (\hat{\mathbf{g}}) \mathrm{E} (\hat{\mathbf{g}})^\prime \right) (\mathbf{F}^{j})^\prime \sigma^2.
  (\#eq:ETSADAMStateSpacePureAdditiveVariance04)
\end{equation}
As for the covariances in \@ref(eq:ETSADAMStateSpacePureAdditiveVariance02), after the expansion it can be shown that each of them is equal to:
\begin{equation}
    \begin{aligned}
    \mathrm{cov}(\mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j},\mathbf{F}^{i} \hat{\mathbf{g}} e_{t+h-i}) = & \mathrm{V}(\mathbf{F}^{j-1} \hat{\mathbf{g}}) \mathrm{cov}(e_{t+h-i},e_{t+h-j}) \\
     & + \left(\mathbf{F}^{j-1} \hat{\mathbf{g}}\right)^2 \mathrm{cov}(e_{t+h-i},e_{t+h-j}) \\
    & + \mathrm{E}(e_{t+h-i}) \mathrm{E}(e_{t+h-j}) \mathrm{V}(\mathbf{F}^{j-1} \hat{\mathbf{g}}).
    \end{aligned}
  (\#eq:ETSADAMStateSpacePureAdditiveVariance05)
\end{equation}
Given the assumptions of the model, the autocovariances of error terms should all be equal to zero, and the expectation of the error term should be equal to zero as well, which means that all the value in \@ref(eq:ETSADAMStateSpacePureAdditiveVariance05) will be equal to zero as well. Based on this, the conditional variance of states equals to:
\begin{equation}
    \mathrm{V}(\mathbf{v}_{t+h-1}|t) = \sum_{j=1}^{h-1} \mathbf{F}^{j-1} \left( \mathrm{V} (\hat{\mathbf{g}}) + \mathrm{E} (\hat{\mathbf{g}}) \mathrm{E} (\hat{\mathbf{g}})^\prime \right) (\mathbf{F}^{j})^\prime \sigma^2
  (\#eq:ETSADAMStateSpacePureAdditiveVariance06)
\end{equation}
As discussed in Section \@ref(pureAdditiveExpectationAndVariance), the conditional variance of the actual value $h$ steps ahead is:
\begin{equation}
    \mathrm{V}(y_{t+h}|t) = \mathbf{w}^\prime \mathrm{V}(\mathbf{v}_{t+h-1}|t) \mathbf{w} + \sigma^2
  (\#eq:ETSADAMStateSpacePureAdditiveVariance07)
\end{equation}
Inserting \@ref(eq:ETSADAMStateSpacePureAdditiveVariance06) in \@ref(eq:ETSADAMStateSpacePureAdditiveVariance07), we get the final conditional h steps ahead variance of the model:
\begin{equation}
    \sigma^2_h = \mathrm{V}(y_{t+h}|t) = \left(\mathbf{w}^\prime \sum_{j=1}^{h-1} \mathbf{F}^{j-1} \left( \mathrm{V} (\hat{\mathbf{g}}) + \mathrm{E} (\hat{\mathbf{g}}) \mathrm{E} (\hat{\mathbf{g}})^\prime \right) (\mathbf{F}^{j})^\prime \mathbf{w} + 1 \right)\sigma^2,
  (\#eq:ETSADAMStateSpacePureAdditiveVarianceFinal)
\end{equation}
which looks similar to the formula \@ref(eq:ETSADAMStateSpaceANNRecursionMeanAndVariance) from Section \@ref(pureAdditiveExpectationAndVariance), but now has the covariance of persistence vector in it. For a special case of ETS(A,N,N) this simplifies to:
\begin{equation}
    \sigma^2_h = \mathrm{V}(y_{t+h}|t) = \left((h-1) \left(\mathrm{V}(\hat{\alpha}) + \hat{\alpha}^2 \right) + 1 \right) \sigma^2,
  (\#eq:ETSADAMStateSpacePureAdditiveVarianceFinalANN)
\end{equation}
which as can be seen differs from the conventional variance by the value of the variance of the smoothing parameter $\mathrm{V}(\hat{\alpha})$. Similarly, the conditional variances for ETS(A,A,N), ETS(A,N,A) and ETS(A,A,A) can be produced using the formula \@ref(eq:ETSADAMStateSpacePureAdditiveVarianceFinal).

Unfortunately, the conditional variances for the other models are more complicated due to the introduction of convolutions of parameters. Furthermore, the formula \@ref(eq:ETSADAMStateSpacePureAdditiveVarianceFinal) only focuses on the conditional variance given the known $\mathbf{v}_t$ but does not take into account the uncertainty of it for the fitted values in-sample. Given the complexity of the problem, in the next section, we introduce a technique that allows correctly propagating the uncertainty of parameters and initial values to the forecasts of any ADAM.


## Multi-scenarios for ADAM states {#adamRefitted}
As discussed in Section \@ref(conditionalVarianceUncertainty), it is difficult to capture the impact of the uncertainty about the parameters on the states of the model and, as a result, difficult to take it into account on the forecasting stage. Furthermore, so far, we have only discussed pure additive models, for which it is at least possible to do some derivations. When it comes to models with multiplicative components, it becomes nearly impossible to demonstrate how the uncertainty propagates over time. To overcome these limitations, we develop a simulation-based approach (similar to the one discussed in Section \@ref(ADAMUncertaintySimulation)) that relies on the selected model form.

The idea of the approach is to get the covariance matrix of the parameters of the selected model (see Section \@ref(ADAMUncertaintyVCOV)) and then generate $n$ sets of parameters randomly from a Rectified Multivariate Normal distribution using the matrix and the values of estimated parameters. After that, the model is applied to the data with each generated parameters combination to get the states, fitted values, and residuals (simulation is done using the principles from Section \@ref(ADAMUncertaintySimulation)). This way, we propagate the uncertainty about the parameters from the first observation to the last one. The final states can then be used to produce point forecasts and prediction intervals based on each set of parameters. These scenarios allow creating more adequate prediction intervals from the model and/or confidence intervals for the fitted values, states and conditional expectations. All of this is done without any additional model assumptions and without any additional modelling steps, relying entirely on the estimated ADAM. This approach is computationally expensive, as it requires fitting all the $n$ models to the data, however no estimation is needed. Furthermore, if the uncertainty about the model needs to be taken into account, then the combination of models can be used, as described in Section \@ref(ADAMCombinations), where the approach from this Section would be applied for each of the models in the pool before combining the point forecasts and prediction intervals with IC weights.

`smooth` package has the method `reapply()` that implements this approach for `adam()` models. This works with ADAM ETS, ARIMA, regression and any combination of the three. Here is an example in R with $n=1000$:

```{r warning=FALSE}
# Estimate the model
adamETSAir <- adam(AirPassengers, "MMM", h=10, holdout=TRUE, maxeval=16*100)
# Produce the multiple scenarios
adamETSAirReapply <- reapply(adamETSAir, nsim=1000)
```

::: remark
In the code above I have increased the number of iterations in optimiser to $k \times 100$, because I noticed that the default value does not allow reaching the maximum of the likelihood, and as a result the variances of parameters become too large. At the moment, there is no automated solution to this problem and $k \times 100$ is a heuristic that can be used if a more precise optimum is required.
:::

After producing the scenarios, we can plot them (see Figure \@ref(fig:adamETSRefitted)).

```{r adamETSRefitted, fig.cap="Refitted ADAM ETS(M,M,M) model on AirPassengers data."}
plot(adamETSAirReapply)
```

Figure \@ref(fig:adamETSRefitted) demonstrates how the approach works on the example of `AirPassengers` data with ETS(M,M,M) model. The grey areas around the fitted line show quantiles from the fitted values, forming confidence intervals of width 95%, 80%, 60%, 40% and 20%. They show how the fitted value would vary if the parameters would differ from the estimated ones. Notice that there was a warning about the covariance matrix of parameters, which typically appears if the optimal value of the loss function was not reached. If this happens, I would recommend tuning the optimiser (see Section \@ref(ADAMInitialisation)).

The `adamETSAirReapply` object contains several variables, including:

- `adamETSAirReapply$states` -- the array of states of dimensions $k \times (T+m) \times n$, where $m$ is the maximum lag of the model, $k$ is the number of components and $T$ is the sample size;
- `adamETSAirReapply$refitted` -- fitted values produced from different parameters, dimensions $T \times n$;
- `adamETSAirReapply$transition` -- the array of transition matrices of the size $k \times k \times n$;
- `adamETSAirReapply$measurement` -- the array of measurement matrices of the size $(T+m) \times k \times n$;
- `adamETSAirReapply$persistence` -- the persistence matrix of the size $k \times n$;

The last three will contain the random parameters (smoothing, damping and AR/MA parameters), which is why they are provided together with the other values.

As mentioned earlier, ADAM ARIMA also supports this approach. Here is an example on an artificial, non-seasonal data, generated from ARIMA(0,1,1) (see Figure \@ref(fig:adamARIMARefitted)):

```{r eval=FALSE}
# Generate the data
y <- sim.ssarima(orders=list(i=1,ma=1), obs=120, MA=-0.7)
# Apply ADAM, then refit it and plot
adam(y$data, "NNN", h=10, holdout=TRUE,
     orders=c(0,1,1)) |>
    reapply() |>
    plot()
```

```{r adamARIMARefitted, fig.cap="Refitted ADAM ARIMA(0,1,1) model on artificial data.", echo=FALSE}
load("data/adamRefitted.Rdata")
plot(adamModelARIMARefitted1)
```

Note that the more complicated the fitted model is, the more difficult it is to optimise, and thus the more difficult it is to get accurate estimates of the covariance matrix of parameters. This might result in highly uncertain states and thus fitted values. The safer approach, in this case, is using bootstrap for the estimation of the covariance matrix, but this is more computationally expensive and would only work on longer time series. Here how the bootstrap can be used for the multi-scenarios in R (and Figure \@ref(fig:adamARIMARefitted200)):

```{r eval=FALSE}
adam(y$data, "NNN", h=10, holdout=TRUE,
     orders=c(0,1,1)) |>
    reapply(bootstrap=TRUE, parallel=TRUE) |>
    plot()
```

```{r adamARIMARefitted200, fig.cap="Refitted ADAM ARIMA(0,1,1) model on artificial data, bootstrapped covariance matrix.", echo=FALSE}
plot(adamModelARIMARefitted2)
```

The approach described in this section is still a work in progress. While it works in theory, there are computational difficulties with calculating the Hessian matrix in some situations. If the covariance matrix is not estimated accurately, it might contain high variances, leading to the higher than needed uncertainty of the model. This might result in unreasonable confidence bounds and lead to extremely wide prediction intervals.
