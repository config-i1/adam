# Forecasts evaluation {#forecastsEvaluation}
As discussed in Section \@ref(forecastingPlanningAnalytics), forecasts should serve a specific purpose. They should not be made "just because" but to help make decisions. The decision then dictates the kind of forecast that should be made -- its form and its time horizon(s). It also dictates how the forecast should be evaluated -- a forecast only being as good as the quality of the decisions it enables.

When you understand how your system works and what sort of forecasts you should produce, you can start an evaluation process, measuring the performance of different forecasting models/methods and selecting the most appropriate for your data.

This chapter discusses the most common approaches, focusing on evaluating point forecasts, then moving towards prediction intervals and quantile forecasts. After that, we discuss how to choose the appropriate error measure and, finally, ensure that the model performs consistently on the available data via rolling origin evaluation and statistical tests.


## Measuring accuracy of point forecasts {#errorMeasures}
We start with a setting in which we are interested in point forecasts only. In this case, we typically begin by splitting the available data into training and test sets, applying the models under consideration to the former, and producing forecasts on the latter, hiding it from the models. This is called the "fixed origin" approach: we fix the point in time from which to produce forecasts, produce them, calculate some appropriate error measure and compare the models.

Different error measures can be used in this case. Which one to use depends on the specific need. Here we briefly discuss the most important measures and refer reader to @Davydenko2013, @SvetunkovAccuracy2019 and @SvetunkovAPEs2017 for the gory details.

The majority of point forecast measures relies on the following two popular metrics:

**Root Mean Squared Error** (RMSE):
\begin{equation}
    \mathrm{RMSE} = \sqrt{\frac{1}{h} \sum_{j=1}^h \left( y_{t+j} -\hat{y}_{t+j} \right)^2 },
    (\#eq:RMSE)
\end{equation}
and **Mean Absolute Error** (MAE):
\begin{equation}
    \mathrm{MAE} = \frac{1}{h} \sum_{j=1}^h \left| y_{t+j} -\hat{y}_{t+j} \right| ,
    (\#eq:MAE)
\end{equation}
where $y_{t+j}$ is the actual value $j$ steps ahead (values in the test set), $\hat{y}_{t+j}$ is the $j$ steps ahead point forecast, and $h$ is the forecast horizon. As you see, these error measures aggregate the performance of competing forecasting methods across the forecasting horizon, averaging out the specific performances on each $j$. If this information needs to be retained, the summation can be dropped to obtain "SE" and "AE" values.

In a variety of cases RMSE and MAE might recommend different models, and a logical question would be which of the two to prefer. It is well-known [see, for example, @Kolassa2016] that the **mean value of distribution minimises RMSE**, and **the median value minimises MAE**. So, when selecting between the two, you should consider this property. It also implies, for example, that MAE-based error measures should not be used for the evaluation of models on intermittent demand (see Chapter \@ref(ADAMIntermittent) for the discussion of this topic) because zero forecast will minimise MAE, when the sample contains more than 50% of zeroes [see, for example, @Wallstrom2010].

Another error measure that has been used in some cases is Root Mean Squared Logarithmic Error [RMSLE, see discussion in @Tofallis2015]:
\begin{equation}
    \mathrm{RMSLE} = \exp\left(\sqrt{\frac{1}{h} \sum_{j=1}^h \left( \log y_{t+j} -\log \hat{y}_{t+j} \right)^2} \right).
    (\#eq:RMSLE)
\end{equation}
It assumes that the actual values and the forecasts are positive, and it is **minimised by the geometric mean**. I have added the exponentiation in the formula \@ref(eq:RMSLE), which is sometimes omitted, bringing the metric to the original scale to have the same units as the actual values $y_t$.

The main difference in the three measures arises when the data we deal with is not symmetric -- in that case, the arithmetic, geometric means, and median will all be different. Thus, the error measures might recommend different methods depending on what specifically is produced as a point forecast from the model (see discussion in Section \@ref(typesOfForecastsPoint)).

### An example in R
In order to see how the error measures work, we consider the following example based on a couple of forecasting functions from the `smooth` package for R [@Hyndman2008b; @Svetunkov2015] and measures from `greybox`:
```{r eval=FALSE}
# Generate the data
y <- rnorm(100,100,10)
# Apply two models to the data
model1 <- es(y,h=10,holdout=TRUE)
model2 <- ces(y,h=10,holdout=TRUE)
# Calculate RMSE
setNames(sqrt(c(MSE(model1$holdout, model1$forecast),
                MSE(model2$holdout, model2$forecast))),
         c("ETS","CES"))
# Calculate MAE
setNames(c(MAE(model1$holdout, model1$forecast),
           MAE(model2$holdout, model2$forecast)),
         c("ETS","CES"))
# Calculate RMSLE
setNames(exp(sqrt(c(MSE(log(model1$holdout),
                        log(model1$forecast)),
                    MSE(log(model2$holdout),
                        log(model2$forecast))))),
         c("ETS","CES"))
```

```{r echo=FALSE}
load("data/errorMeasures.Rdata")

# RMSE
setNames(sqrt(c(MSE(model1$holdout, model1$forecast),
                MSE(model2$holdout, model2$forecast))),
         c("ETS","CES"))
# MAE
setNames(c(MAE(model1$holdout, model1$forecast),
           MAE(model2$holdout, model2$forecast)),
         c("ETS","CES"))
# RMSLE
setNames(exp(sqrt(c(MSE(log(model1$holdout),
                        log(model1$forecast)),
                    MSE(log(model2$holdout),
                        log(model2$forecast))))),
         c("ETS","CES"))
```

::: remark
The point forecasts produced by ETS and CES correspond to conditional means, so ideally we should focus the evaluation on RMSE-based measures.
:::

Given that the distribution of the original data is symmetric, all three error measures should generally recommend the same model. But also, given that the data we generated for the example are stationary, the two models will produce very similar forecasts. The values above demonstrate the latter point -- the accuracy between the two models is roughly the same. Note that we have evaluated the same point forecasts from the models using different error measures, which would be wrong if the distribution of the data was skewed. In our case, the model relies on Normal distribution so that the point forecast would coincide with arithmetic mean, geometric mean and median.


### Aggregating error measures {#errorMeasuresAggregate}
The main advantage of the error measures discussed in the previous subsection is that they are straightforward and have a clear interpretation: they reflect the "average" distances between the point forecasts and the observed values. They are perfect for the work with only one time series. However, they are not suitable when we consider a set of time series, and a forecasting method needs to be selected across all of them. This is because they are scale-dependent and contain specific units: if you measure sales of apples in units, then MAE, RMSE and RMSLE will show the errors in units as well. And, as we know, you should not add apples to oranges -- the result might not make sense.

To tackle this issue, different error scaling techniques have been proposed, resulting in a zoo of error measures:

1. MAPE -- Mean Absolute Percentage Error:
\begin{equation}
    \mathrm{MAPE} = \frac{1}{h} \sum_{j=1}^h \frac{|y_{t+j} -\hat{y}_{t+j}|}{y_{t+j}},
    (\#eq:MAPE)
\end{equation}
2. MASE -- Mean Absolute Scaled Error [@Hyndman2006]:
\begin{equation}
    \mathrm{MASE} = \frac{1}{h} \sum_{j=1}^h \frac{|y_{t+j} -\hat{y}_{t+j}|}{\bar{\Delta}_y},
    (\#eq:MASE)
\end{equation}
where $\bar{\Delta}_y = \frac{1}{t-1}\sum_{j=2}^t |\Delta y_{j}|$ is the mean absolute value of the first differences $\Delta y_{j}=y_j-y_{j-1}$ of the in-sample data;
3. rMAE -- Relative Mean Absolute Error [@Davydenko2013]:
\begin{equation}
    \mathrm{rMAE} = \frac{\mathrm{MAE}_a}{\mathrm{MAE}_b},
    (\#eq:rMAE)
\end{equation}
where $\mathrm{MAE}_a$ is the mean absolute error of the model under consideration and $\mathrm{MAE}_b$ is the MAE of the benchmark model;
4. sMAE -- scaled Mean Absolute Error [@Petropoulos2015]:
\begin{equation}
    \mathrm{sMAE} = \frac{\mathrm{MAE}}{\bar{y}},
    (\#eq:sMAE)
\end{equation}
where $\bar{y}$ is the mean of the in-sample data.
5. and others.

::: remark
MAPE and sMAE are typically multiplied by 100% to get the percentages, which are easier to work with.
:::

There is no "best" error measure. All have advantages and disadvantages, but some are more suitable in some circumstances than others. For example:

1. MAPE is scale sensitive (if the actual values are measured in thousands of units, the resulting error will be much lower than in the case of hundreds of units) and cannot be estimated on data with zeroes. Furthermore, this error measure is biased, preferring when models underforecast the data [see, for example, @Makridakis1993] and is not minimised by either mean or median, but by an unknown quantity. Accidentally, in the case of Log-Normal distribution, it is minimised by the mode [see discussion in @Kolassa2016]. Despite all the limitations, MAPE has a simple interpretation as it shows the percentage error (as the name suggests);
2. MASE avoids the disadvantages of MAPE but does so at the cost of loosing a simple interpretation. This is because of the division by the first differences of the data (some interpret this as an in-sample one-step-ahead NaÃ¯ve forecast, which does not simplify the interpretation);
3. rMAE avoids the disadvantages of MAPE, has a simple interpretation (it shows by how much one model is better than the other), but fails, when either $\mathrm{MAE}_a$ or $\mathrm{MAE}_b$ for a specific time series is equal to zero. In practice, this happens more often than desired and can be considered a severe error measure limitation. Furthermore, the increase of rMAE (for example, with the increase of sample size) might mean that either the method A is performing better than before or that the method B is performing worse than before -- it is not possible to tell the difference unless the denominator in the formula \@ref(eq:rMAE) is fixed;
4. sMAE avoids the disadvantages of MAPE, has an interpretation close to it but breaks down when the data is non-stationary (e.g. has a trend).

When comparing different forecasting methods, it might make sense to calculate several error measures for comparison. The choice of metric might depend on the specific needs of the forecaster. Here are a few rules of thumb:

- You should typically avoid MAPE and other percentage error measures because the actual values highly influence them in the holdout.
- If you want a robust measure that works consistently, but you do not care about the interpretation, then go with MASE.
- If you want an interpretation, go with rMAE or sMAE. Just keep in mind that if you decide to use rMAE or any other relative measure for research purposes, you might get in an unnecessary dispute with its creator, who might blame you of stealing his creation (even if you reference his work).
- If the data does not exhibit trends (stationary), you can use sMAE.

Furthermore, similarly to the measures above, there have been proposed RMSE-based scaled and relative error metrics, which measure the performance of methods in terms of means rather than medians. Here is a brief list of some of them:

1. RMSSE -- Root Mean Squared Scaled Error [@Makridakis2020a]:
\begin{equation}
    \mathrm{RMSSE} = \sqrt{\frac{1}{h} \sum_{j=1}^h \frac{(y_{t+j} -\hat{y}_{t+j})^2}{\bar{\Delta}_y^2}} ;
    (\#eq:RMSSE)
\end{equation}
2. rRMSE -- Relative Root Mean Squared Error [@Stock2004]:
\begin{equation}
    \mathrm{rRMSE} = \frac{\mathrm{RMSE}_a}{\mathrm{RMSE}_b} ;
    (\#eq:rRMSE)
\end{equation}
3. sRMSE -- scaled Root Mean Squared Error [@Petropoulos2015]:
\begin{equation}
    \mathrm{sRMSE} = \frac{\mathrm{RMSE}}{\bar{y}} .
    (\#eq:sRMSE)
\end{equation}

Similarly, RMSSLE, rRMSLE and sRMSLE can be proposed, using the same principles as in \@ref(eq:RMSSE), \@ref(eq:rRMSE) and \@ref(eq:sRMSE) to assess performance of models in terms of geometric means across time series.

Finally, when aggregating the performance of forecasting methods across several time series, sometimes it makes sense to look at the distribution of errors -- this way, you will know which of the methods fails seriously and which does a consistently good job. If only an aggregate measure is needed then I recommend using **both mean and median of the chosen metric**. The mean might be non-finite for some error measures, especially when a method performs exceptionally poorly on a time series (an outlier). Still, it will give you information about the average performance of the method and might flag extreme cases. The median at the same time is robust to outliers and is always calculable, no matter what the distribution of the error term is. Furthermore, comparing mean and median might provide additional information about the tail of distribution without reverting to histograms or the calculation of quantiles. @Davydenko2013 argue for the use of geometric mean for relative and scaled measures. Still, as discussed earlier, it might become equal to zero or infinity if the data contains outliers (e.g. two cases, when one of the methods produced a perfect forecast, or the benchmark in rMAE produced a perfect forecast). At the same time, if the distribution of errors in logarithms is symmetric [which is the main argument of @Davydenko2013], then the geometric mean will be similar to the median, so there is no point in calculating the geometric mean at all.


### Demonstration in R {#errorMeasuresExampleBig}
In R, there is a variety of functions that calculate the error measures discussed above, including the `accuracy()` function from `forecast` package and `measures()` from `greybox`. Here is an example of how the measures can be calculated based on a couple of forecasting functions from `smooth` package for R and a set of generated time series:

```{r eval=FALSE}
# Apply a model to a test data to get names of error measures
y <- rnorm(100,100,10)
test <- es(y,h=10,holdout=TRUE)
# Define number of iterations
nsim <- 100
# Create an array for nsim time series,
# 2 models and a set of error measures
errorMeasures <- array(NA, c(nsim,2,length(test$accuracy)),
                       dimnames=list(NULL,c("ETS","CES"),
                                     names(test$accuracy)))
# Start a loop for nsim iterations
for(i in 1:nsim){
  # Generate a time series
  y <- rnorm(100,100,10)
  # Apply ETS
  testModel1 <- es(y,"ANN",h=10,holdout=TRUE)
  errorMeasures[i,1,] <- measures(testModel1$holdout,
                                  testModel1$forecast,
                                  actuals(testModel1))
  # Apply CES
  testModel2 <- ces(y,h=10,holdout=TRUE)
  errorMeasures[i,2,] <- measures(testModel2$holdout,
                                  testModel2$forecast,
                                  actuals(testModel2))
}
```

The default benchmark method for the relative measures above is NaÃ¯ve. To see how the distribution of error measures would look like, we can produce violinplots via the `vioplot()` function from the `vioplot` package. We will focus on rRMSE measure (see Figure \@ref(fig:errorMeasuresrRMSEDistLog)).

```{r errorMeasuresrRMSEDist, fig.cap="Distribution of rRMSE on the original scale."}
vioplot::vioplot(errorMeasures[,,"rRMSE"])
```

The distributions in Figure \@ref(fig:errorMeasuresrRMSEDistLog) look similar, and it is hard to tell which one performs better. Besides, they do not look symmetric, so we will take logarithms to see if this fixes the issue with the skewness (Figure \@ref(fig:errorMeasuresrRMSEDistLog)).

```{r errorMeasuresrRMSEDistLog, fig.cap="Distribution of rRMSE on the log scale."}
vioplot::vioplot(log(errorMeasures[,,"rRMSE"]))
```

Figure \@ref(fig:errorMeasuresrRMSEDistLog) demonstrates that the distribution in logarithms is still skewed, so the geometric mean would not be suitable and might provide a misleading information (being influenced by the tail of distribution). So, we calculate mean and median rRMSE to check the overall performance of the two models:

```{r}
# Calculate mean rRMSE
apply(errorMeasures[,,"rRMSE"],2,mean)
# Calculate median rRMSE
apply(errorMeasures[,,"rRMSE"],2,median)
```

Based on the values above, we cannot make any solid conclusion about the performance of the two models: in terms of both mean and median rRMSE, CES is doing slightly better, but the difference between the two models is not substantial, so we can probably choose the one that is easier to work with.


## Measuring uncertainty {#uncertainty}
As discussed in Section \@ref(typesOfForecastsInterval), point forecasts are not sufficient for adequate decision making -- prediction intervals and quantiles are needed to capture the uncertainty of demand. As with point forecasts, multiple measures can be used to evaluate them. There are several useful measures for the evaluation of intervals. We start with the simplest of them, coverage.

1. **Coverage** shows the percentage of observations lying inside the interval:
\begin{equation}
    \mathrm{Coverage} = \frac{1}{h} \sum_{j=1}^h \left( \mathbbm{1}(y_{t+j} > l_{t+j}) \times \mathbbm{1}(y_{t+j} < u_{t+j}) \right),
    (\#eq:coverage)
\end{equation}
where $l_{t+j}$ is the lower bound and $u_{t+j}$ is the upper bound of the interval and $\mathbbm{1}(\cdot)$ is the indicator function, returning one, when the condition is true and zero otherwise. Ideally, the coverage should be equal to the confidence level of the interval, but in reality, this can only be observed asymptotically (with the increase of the sample size) due to the inherited randomness of any sample estimates of parameters;
2. **Range** shows the width of the prediction interval:
\begin{equation}
    \mathrm{Range} = \frac{1}{h} \sum_{j=1}^h (u_{t+j} -l_{t+j});
    (\#eq:range)
\end{equation}
If the range of interval from one model is lower than the range of the other one, then the uncertainty about the future values is lower for the first one. However, the narrower interval might not include as many actual values in the holdout sample, leading to lower coverage. So, there is a natural trade-off between the two measures.
3. **Mean Interval Score** [@Gneiting2007] combines the properties of the previous two measures:
\begin{equation}
    \begin{aligned}
    \mathrm{MIS} = & \frac{1}{h} \sum_{j=1}^h \left( (u_{t+j} -l_{t+j}) + \frac{2}{\alpha} (l_{t+j} -y_{t+j}) \mathbbm{1}(y_{t+j} < l_{t+j}) +\right. \\
    & \left. \frac{2}{\alpha} (y_{t+j} -u_{t+j}) \mathbbm{1}(y_{t+j} > u_{t+j}) \right) ,
    \end{aligned}
    (\#eq:MIS)
\end{equation}
where $\alpha$ is the significance level. If the actual values lie outside of the interval, they get penalised with a ratio of $\frac{2}{\alpha}$, proportional to the distance from the interval bound. At the same time the width of the interval positively influences the value of the measure: the wider the interval, the higher the score. The ideal model with $\mathrm{MIS}=0$ should have all the actual values in the holdout lying on the bounds of the interval and $u_{t+j}=l_{t+j}$, implying that the bounds coincide with each other and that there is no uncertainty about the future (which is not possible in real life).
4. **Pinball Score** [@Koenker1978] measures the accuracy of models in terms of specific quantiles (this is usually applied to different quantiles produced from the model, not just to the lower and upper bounds of 95% interval):
\begin{equation}
    \mathrm{PS} = (1 -\alpha) \sum_{y_{t+j} < q_{t+j}, j=1,\dots,h } |y_{t+j} -q_{t+j}| + \alpha \sum_{y_{t+j} \geq q_{t+j} , j=1,\dots,h } |y_{t+j} -q_{t+j}|,
    (\#eq:pinball)
\end{equation}
where $q_{t+j}$ is the value of the specific quantile of the distribution. PS shows how well we capture the specific quantile in the data. The lower the value of pinball is, the closer the bound is to the specific quantile of the holdout distribution. If the PS is equal to zero, then we have done the perfect job in hitting that specific quantile. The main issue with PS is that it is very difficult to assess the quantiles correctly on small samples. For example, in order to get a better idea of how the 0.975 quantile performs, we would need to have at least 40 observations, so that 39 of them would be expected to lie below this bound $\left(\frac{39}{40} = 0.975\right)$. In fact, quantiles are not always uniquely defined [see, for example, @Taylor2020], which makes the measurement challenging.

Similar to the pinball function, it is possible to propose the expectile-based score, but while expectiles have good statistical properties [@Taylor2020], they are more difficult to interpret.

Range, MIS and PS are unit-dependent. To aggregate them over several time series, they need to be scaled either via division by either the in-sample mean or in-sample mean absolute differences to obtain the scaled counterparts of the measures or via division by the values from the benchmark model to get the relative one. The idea here would be similar to what we discussed for MAE and RMSE in Section \@ref(errorMeasures).

If you are interested in the model's overall performance, then MIS provides this information. However, it does not show what happens specifically (is there an issue in the distance from the bound or the width of the interval?) and it is difficult to interpret. Coverage and range are easier to interpret, but they only give information about the specific prediction interval. They typically must be traded off against each other (i.e. one can either cover more or have a narrower interval). Academics prefer pinball for uncertainty assessment, as it shows more detailed information about the predictive distribution from each model. However, while it is easier to interpret than MIS, it is still not as straightforward as coverage and range. So, the selection of the measure depends on the specific situation and the understanding of statistics by decision-makers.


### Example in R
Continuing the example from Section \@ref(errorMeasures), we could produce prediction intervals from the two models and compare them using MIS and pinball:

```{r}
model1Forecast <- forecast(model1,h=10,interval="p",level=0.95)
model2Forecast <- forecast(model2,h=10,interval="p",level=0.95)

# Mean Interval Score
setNames(c(MIS(model1$holdout, model1Forecast$lower,
               model1Forecast$upper, 0.95),
           MIS(model2$holdout, model2Forecast$lower,
               model2Forecast$upper, 0.95)),
         c("Model 1", "Model 2"))

# Pinball for the upper bound
setNames(c(pinball(model1$holdout, model1Forecast$upper, 0.975),
           pinball(model2$holdout, model2Forecast$upper, 0.975)),
         c("Model 1", "Model 2"))


# Pinball for the lower bound
setNames(c(pinball(model1$holdout, model1Forecast$lower, 0.025),
           pinball(model2$holdout, model2Forecast$lower, 0.025)),
         c("Model 1", "Model 2"))

# Coverage
setNames(c(mean(model1$holdout > model1Forecast$lower &
                model1$holdout < model1Forecast$upper),
           mean(model2$holdout > model2Forecast$lower &
                model2$holdout < model2Forecast$upper)),
         c("Model 1", "Model 2"))
```

The values above imply that the first model (ETS) performed better than the second one in terms of MIS and pinball loss (the interval was narrower). However, these measures do not tell much in terms of the performance of models when only applied to one time series. To see more solid results, we need to apply models to a set of time series, produce prediction intervals, calculate measures and then look at their aggregate performance, e.g. via mean / median or quantiles. The loop and the analysis would be similar to the one discussed in Section \@ref(errorMeasuresExampleBig), so we do not repeat it here.


## How to choose appropriate error measure {#errorMeasuresSelection}
While, in general, the selection of error measures should be dictated by the specific problem at hand, some guidelines might be helpful in the process. I have summarised them in the flowchart in Figure \@ref(fig:errorMeasuresFlowChart).

```{r errorMeasuresFlowChart, out.width="100%", fig.cap="Error measures selection flowchart.", echo=FALSE}
if (knitr:::is_latex_output()) {
    knitr::include_graphics("./images/errorMeasuresFlowChart.pdf")
    # knitr::asis_output('\\includegraphics{./images/errorMeasuresFlowChart.pdf}')
} else {
    knitr::include_graphics("./images/errorMeasuresFlowChart.gif")
}
```

The flowchart does not provide excessive options and simplifies the possible process. It does not discuss the quantile and interval measures in detail, as there are many options for them. The idea of the flowchart is to list the most important measures, and its aim is to provide a guideline for selection based on:

1. Number of time series under consideration. If there are several of them and you need to aggregate the error measure, you need to use either scaled or relative ones. In the case of just one time series, you do not need to scale the error measure;
2. What specifically you want to measure: point forecasts, quantiles, prediction interval or something else;
3. Whether the interpretability of the error measure is essential or not. If not, then scaled measures similar to @Hyndman2006 can be used. If yes, then the choice is between relative and scaled using mean measures;
4. Whether the data is stationary or not. If it is, then it is safe to use scaled measures similar to @Petropoulos2015 because the division by in-sample mean would be meaningful. Otherwise, you should either use @Hyndman2006 scaling or relative measures;
5. Whether the data is intermittent or not. If it is and you are interested in point forecasts, then you should use RMSE based measures -- other measures might recommend zero forecast as the best one;
6. Symmetry of distribution of demand. If it is symmetric (which does not happen very often), then the median will coincide with the mean and geometric mean, and it would not be important whether to use RMSE-, MAE- or RMSLE- based measure. In that case, just use a MAE-based one (for simplicity reasons);
7. What you need (denoted as "What do you like?" in the flowchart). If you are interested in mean performance, then use RMSE based measures. The median minimises MAE, and the geometric mean minimises RMSLE. This relates to the discussion in Section \@ref(typesOfForecasts).

The point forecast related error measures have been discussed in Section \@ref(errorMeasures), while the interval and quantile ones -- in Section \@ref(uncertainty).

::: remark
I personally do not recommend using MAPE and SMAPE (symmetric MAPE) for the reasons discussed by @Goodwin1999 and @Hyndman2006. In fact, any percentage-based error measure has severe limitations and should be avoided if possible.
:::

You can also download this flowchart in pdf format from the [https://www.openforecast.org/adam/](https://www.openforecast.org/adam/images/errorMeasuresFlowChart-v2.pdf) website.


## Rolling origin {#rollingOrigin}
::: remark
The text in this section is based on the vignette for the [greybox package](https://cran.r-project.org/package=greybox), written by the author of this monograph.
:::

When there is a need to select the most appropriate forecasting model or method for the data, the forecaster usually splits the sample into two parts: in-sample (aka "training set") and holdout sample (aka out-sample or "test set"). The model is estimated on the in-sample, and its forecasting performance is evaluated [using some error measure](#errorMeasures) on the holdout sample.

Using this procedure only once is known as "fixed origin" evaluation. However, this might give a misleading impression of the accuracy of forecasting methods. If, for example, the time series contains outliers or level shifts, a poor model might perform better in fixed origin evaluation than a more appropriate one just by chance. So it makes sense to have a more robust evaluation technique, where the model's performance is evaluated several times, not just once. An alternative procedure known as "rolling origin" evaluation is one such technique.

In rolling origin evaluation, the forecasting origin is repeatedly moved forward by a fixed number of observations, and forecasts are produced from each origin [@Tashman2000]. This technique allows obtaining several forecast errors for time series, which gives a better understanding of how the models perform. This can be considered a time series analogue to cross-validation techniques [see Chapter 5 of @James2017]. Here is a simple graphical representation, courtesy of [Nikos Kourentzes](https://kourentzes.com/forecasting/).

```{r ROProcessAnimation, out.width="75%", fig.cap="Visualisation of rolling origin by Nikos Kourentzes", echo=FALSE}
if (knitr:::is_latex_output()) {
    # knitr::asis_output('\\includegraphics{./images/03-ROAnimation.jpg}')
    knitr::include_graphics("./images/03-ROAnimation.jpg")
} else {
    knitr::include_graphics("./images/03-ROAnimation.gif")
}
```

The plot in Figure \@ref(fig:ROProcessAnimation) shows how the origin moves forward and the point and interval forecasts of the model change. As a result, this procedure gives information about the performance of the model over a set of observations, not on a random one. There are different options of how this can be done, here we discuss the main principles behind it.

### Principles of Rolling origin

Figure \@ref(fig:ROProcessCO) [@Svetunkov2017] illustrates the basic idea behind rolling origin. White cells correspond to the in-sample data, while the light grey cells correspond to the three-steps-ahead forecasts. The time series in the figure has 25 observations, and forecasts are produced for eight origins starting from observation 15. In the first step, the model is estimated on the first in-sample set, and forecasts are created for the holdout. Next, another observation is added to the end of the in-sample set, the test set is advanced, and the procedure is repeated. The process stops when there is no more data left. This is a rolling origin with a **constant holdout** sample size. As a result of this procedure, eight one to three steps ahead forecasts are produced. Based on them, we can calculate the preferred error measures and choose the best performing model (see Section \@ref(errorMeasuresAggregate)).

```{r ROProcessCO, out.width="75%", echo=FALSE, fig.cap="Rolling origin with constant holdout size from @Svetunkov2017"}
if (knitr:::is_latex_output()) {
    # knitr::asis_output('\\includegraphics{./images/03-ROProcessCO.jpg}')
    knitr::include_graphics("./images/03-ROProcessCO.jpg")
} else {
    knitr::include_graphics("./images/03-ROProcessCO.gif")
}
```

Another option for producing forecasts via rolling origin would be to continue with rolling origin even when the test sample is smaller than the forecast horizon, as shown in Figure \@ref(fig:ROProcessNoCO). In this case, the procedure continues until origin 22, when the last complete set of three-steps-ahead forecasts can be produced, and then continues with a decreasing forecasting horizon. So the two-steps-ahead forecast is produced from origin 23, and only a one-step-ahead forecast is produced from origin 24. As a result, we obtain ten one-step-ahead forecasts, nine two-steps-ahead forecasts and eight three-steps-ahead forecasts. This is a rolling origin with a **non-constant holdout** sample size, which can be helpful with small samples when not enough observations are available.

```{r ROProcessNoCO, out.width="75%", echo=FALSE, fig.cap="Rolling origin with non-constant holdout size"}
if (knitr:::is_latex_output()) {
    # knitr::asis_output('\\includegraphics{./images/03-ROProcessNoCO.jpg}')
    knitr::include_graphics("./images/03-ROProcessNoCO.jpg")
} else {
    knitr::include_graphics("./images/03-ROProcessNoCO.gif")
}
```

Finally, in both cases above, we had the **increasing in-sample** size. However, we might need a **constant in-sample** for some research purposes. Figure \@ref(fig:ROProcessCOCI) demonstrates such a setup. In this case, in each iteration, we add an observation to the end of the in-sample series and remove one from the beginning (dark grey cells).

```{r ROProcessCOCI, out.width="75%", echo=FALSE, fig.cap="Rolling origin with constant in-sample size"}
if (knitr:::is_latex_output()) {
    # knitr::asis_output('\\includegraphics{./images/03-ROProcessCOCI.jpg}')
    knitr::include_graphics("./images/03-ROProcessCOCI.jpg")
} else {
    knitr::include_graphics("./images/03-ROProcessCOCI.gif")
}
```


### Implementing rolling origin in R
Now that we discussed the main idea of rolling origin, we can see how it can be implemented in R. In this section, we will implement rolling origin with a fixed holdout sample size and a changing in-sample. This aligns with what is typically done in practice when new data arrives, the model is re-estimated, and forecasts are produced for the next $h$ steps ahead. For this example, we will use artificially created data and apply a Simple Moving Average (discussed Subsection \@ref(SMA)) implemented in the `smooth` package.

```{r}
# Set sample to 100 observations
obs <- 100
# Generate the data
y <- rnorm(obs,100,10)
```

We will produce forecasts for the horizon of 10 steps ahead, $h=10$ from 5 origins.

```{r}
h <- 10
origins <- 5
```

We will create a list containing several objects of interest:

- `actuals` will contain all the actual values,
- `holdout` will be a matrix containing the actual values for the holdout. It will have `h` rows and `origins` columns,
- `mean` will contain point forecasts from our model. This will also be a matrix with the same dimensions as the `holdout` one.

```{r}
returnedValues1 <- setNames(vector("list",3),
                            c("actuals","holdout","mean"))
returnedValues1$actuals <- y
returnedValues1$holdout <-
  returnedValues1$mean <-
  matrix(NA,h,origins,
         dimnames=list(paste0("h",1:h),
                       paste0("origin",1:origins)))
```

Finally, we write a simple loop that repeats the model fit and forecasting for the horizon `h` several times. The trickiest part is understanding how to define the train and test samples. In our example, the former should have `obs+1-origins-h` observations in the first step and `obs-h` in the last one so that we can have `h` observations in the test set throughout all origins, and we can repeat this `origins` times. One way of doing this is via the following loop:
```{r}
for(i in 1:origins){
    # Fit the model
    testModel <- sma(y[1:(obs+i-origins-h)])
    # Drop the in-sample observations and extract the first h observations from the rest
    returnedValues1$holdout[,i] <- head(y[-c(1:(obs-origins+i-h))], h)
    # Produce forecasts and write down the mean values
    returnedValues1$mean[,i] <- forecast(testModel, h=h)$mean
}
```

This basic loop can be amended to include anything else we want from the function or by changing the parameters of the rolling origin. After filling in the object `returnedValues1`, we can analyse the residuals of the model over the horizon and several origins in various ways. For example, Figure \@ref(fig:roOrigins) shows boxplots across the horizon of 10 for different origins.

```{r roOrigins, fig.cap="Boxplots of forecast errors over several origins."}
boxplot(returnedValues1$holdout-returnedValues1$mean)
```

In the ideal situation, the boxplots in Figure \@ref(fig:roOrigins) should be similar, meaning that the model performs consistently over different origins. We do not see this in our cae, observing that the distribution of errors changes from one origin to another.

While the example above already gives some information about performance of a model, the more useful information would be obtained if the performance of one model is compared to the others in the rolling origin experiment. This can be done manually for several models using the code above or it can be done using the function `ro()` from the `greybox` package.


### Rolling origin function in R
In R, there are several packages and functions that implement rolling origin. One of those is the function `ro()` from the `greybox` package (written by Yves Sagaert and Ivan Svetunkov in 2016 on their way to the International Symposium on Forecasting in Riverside, US). It implements the rolling origin evaluation for any function you like with a predefined `call` and returns the desired `value`. It heavily relies on the two variables: `call` and `value` -- so it is pretty important to understand how to formulate them to get the desired results. `ro()` is a very flexible function, but as a result, it is not very simple. In this subsection, we will see how it works on a couple of examples.

We start with a simple example, generating a series from Normal distribution:
```{r}
y <- rnorm(100,100,10)
```

We use an ARIMA(0,1,1) model implemented in the `stats` package (this model is discussed in Section \@ref(ARIMA)). Given that we are interested in forecasts from the model, we need to use the `predict()` function to get the desired values:
```{r}
ourCall <- "predict(arima(x=data,order=c(0,1,1)),n.ahead=h)"
```

The call that we specify includes two important elements: `data` and `h`. `data` specifies where the in-sample values are located in the function that we want to use, and **it needs to be called "data"** in the call; `h` will tell our function, where the forecasting horizon is specified in the provided line of code. Note that in this example we use `arima(x=data,order=c(0,1,1))`, which produces a desired ARIMA(0,1,1) model and then we use `predict(..., n.ahead=h)`, which produces an h steps ahead forecast from that model.

Having the call, we also need to specify what the function should return. This can be the conditional mean (point forecasts), prediction intervals, the parameters of a model, or, in fact, anything that the model returns (e.g. name of the fitted model and its likelihood). However, there are some differences in what `ro()` returns depending on what the function returns. If it is a vector, then `ro()` will produce a matrix (with values for each origin in columns). If it is a matrix, then an array is returned. Finally, if it is a list, then a list of lists is returned.

In order not to overcomplicate things, we start from collecting the conditional mean from the `predict()` function:
```{r}
ourValue <- c("pred")
```

::: remark
If you do not specify the value to return, the function will try to return everything, but it might fail, especially if many values are returned. So, to be on the safe side, **always provide the `value` when possible**.
:::

Now that we have specified `ourCall` and `ourValue`, we can produce forecasts from the model using rolling origin. Let's say that we want three-steps-ahead forecasts and eight origins with the default values of all the other parameters:
```{r}
returnedValues1 <- ro(y, h=3, origins=8,
                      call=ourCall, value=ourValue)
```

The function returns a list with all the values that we asked for plus the actual values and the holdout sample. We can calculate some basic error measure based on those values, for example, scaled Absolute Error [@Petropoulos2015]:
```{r}
apply(abs(returnedValues1$holdout - returnedValues1$pred),
      1, mean, na.rm=TRUE) /
  mean(returnedValues1$actuals)
```

In this example, we use the `apply()` function to distinguish between the different forecasting horizons and have an idea of how the model performs for each of them. These numbers do not tell us much on their own, but if we compare the performance of this model with an alternative one, we could infer if one model is more appropriate for the data than the other one. For example, applying ARIMA(0,2,2) to the same data, we will get:
```{r}
ourCall <- "predict(arima(x=data,order=c(0,2,2)),n.ahead=h)"
returnedValues2 <- ro(y, h=3, origins=8,
                      call=ourCall, value=ourValue)
apply(abs(returnedValues2$holdout - returnedValues2$pred),
      1, mean, na.rm=TRUE) /
  mean(returnedValues2$actuals)
```
Comparing these errors with the ones from the previous model, we can conclude which of the approaches is more suitable for the data.

We can also plot the forecasts from the rolling origin, which shows how the models behave:
```{r roExample01, fig.cap="Rolling origin performance of two forecasting methods", fig.width=8, fig.height=6}
par(mfcol=c(2,1), mar=c(4,4,3,1))
plot(returnedValues1, main="ARIMA(0,1,1)")
plot(returnedValues2, main="ARIMA(0,2,2)")
```

In Figure \@ref(fig:roExample01), the forecasts from different origins are close to each other. This is because the data is stationary, and both models produce flat lines as forecasts. The second model, however, has a slightly higher variability because it has more parameters than the first one (bias-variance trade-off in action).

The rolling origin function from the `greybox` package also allows working with explanatory variables and returning prediction intervals if needed. Some further examples are discussed in the vignette of the package. Just run the command `vignette("ro","greybox")` in R to see it.

Practically speaking, if we have a set of forecasts from different models we can analyse the distribution of error measures and come to conclusions about performance of models. Here is an example with analysis of performance for $h=1$ based on absolute errors:

```{r roExample02, fig.cap="Boxplots of error measures of two methods."}
aeValuesh1 <- cbind(abs(returnedValues1$holdout -
                          returnedValues1$pred)[1,],
                    abs(returnedValues1$holdout -
                          returnedValues2$pred)[1,])
colnames(aeValuesh1) <- c("ARIMA(0,1,1)","ARIMA(0,2,2)")
boxplot(aeValuesh1)
points(apply(aeValuesh1,2,mean),pch=16,col="red")
```

The boxplots in Figure \@ref(fig:roExample02) can be interpreted as any other boxplot applied to random variables [see, for example, discussion in Section 5.2 of @SvetunkovSBA].

::: remark
When it comes to applying `ro()` to models with explanatory variables, one can use the internal parameters `counti`, `counto` and `countf`, which define the size of the in-sample, the holdout and the full sample, respectively. An example of the code in this situation is shown below with a function `alm()` from `greybox` package being used for fitting a simple linear regression model.

```{r eval=FALSE}
# Generate the data
x <- rnorm(100, 100, 10)
xreg <- cbind(y=100+1.5*x+rnorm(100, 0, 10), x=x)
# Predict values from the model.
# counti and counto determine sizes for the in-sample and the holdout
ourCall <- "predict(alm(y~x, data=xreg[counti,,drop=FALSE]), newdata=xreg[counto,,drop=FALSE])"
# Extract the mean only
ourValue <- "mean"
# Run rolling origin
testRO <- ro(xreg[,1],h=5,origins=5,ourCall,ourValue)
# plot the result
plot(testRO)
```
:::


## Statistical comparison of forecasts {#statisticalTests}
After applying several competing models to the data and obtaining a distribution of forecast errors, we might find that some approaches performed very similarly. In this case, there might be a question, whether the difference between them is significant and which of the forecasting models we should select. If they produce similar forecasts then it might make sense to select the one that is less computationally expensive or easier to work with.

Consider the following artificial example, where we have four competing models and measure their performance in terms of RMSSE:
```{r eval=FALSE}
smallCompetition <- matrix(NA, 100, 4,
                           dimnames=list(NULL,
                                         paste0("Method",c(1:4))))
smallCompetition[,1] <- rnorm(100,1,0.35)
smallCompetition[,2] <- rnorm(100,1.2,0.2)
smallCompetition[,3] <- runif(100,0.5,1.5)
smallCompetition[,4] <- rlnorm(100,0,0.3)
```

```{r echo=FALSE}
load("data/smallCompetition.Rdata")
```
We can check the mean and median error measures in this example in order to see, how the methods perform overall:
```{r}
overalResults <-
  matrix(c(colMeans(smallCompetition), 
           apply(smallCompetition, 2, median)),
         4, 2, dimnames=list(colnames(smallCompetition),
                             c("Mean","Median")))
round(overalResults,5)
```
In this artificial example, it looks like the most accurate method in terms of mean and median RMSSE is Method 4, and the least accurate one is Method 2. However, the difference in terms of accuracy between methods 1, 3 and 4 does not look substantial. So, should we conclude that Method 4 is the best? Let's first look at the distribution of errors using `vioplot()` function from `vioplot` package (Figure \@ref(fig:smallCompetitionBoxplot)).

```{r smallCompetitionBoxplot, fig.cap="Boxplot of RMSE for the artificial example"}
vioplot::vioplot(smallCompetition)
points(colMeans(smallCompetition), col="red", pch=16)
```

The violin plots in Figure \@ref(fig:smallCompetitionBoxplot) show that the distribution of errors for Method 2 is shifted higher than the distributions of other methods. It also looks like Method 2 is working more consistently, meaning that the variability of the errors is lower (the size of the box on the graph). It is difficult to tell whether Method 1 is better than Methods 3 and 4 or not -- their boxes intersect and roughly look similar, with Method 4 having a slightly shorter box and Method 3 having the box slightly lower positioned.

This is all the basics of descriptive statistics, which allows concluding that in general, Methods 1, 3 and 4 do a better job than Method 2. This is also reflected in the mean and median error measures discussed above. So, what should we conclude?

Well, we should not make hasty decisions, and we should remember that we are dealing with a sample of data (100 time series), so inevitably, the performance of methods will change if we try them on different data sets. If we had a population of all the time series in the world, we could run our methods and make a more solid conclusion about their performances. But here, we deal with a sample. So it might make sense to see whether the difference in performance of the methods is significant. How can we do that?

We can **compare means** of distributions of errors using a parametric statistical test. We can try F-test [see, for example Section 10.4 of @Newbold2020], which will tell us whether the mean performance of methods is similar or not. Unfortunately, this will not tell us how the methods compare. But t-test [see Chapter 10 of @Newbold2020] could be used to do that instead for pairwise comparison. One could also use a regression model with dummy variables for methods, giving us parameters and their confidence intervals (based on t-statistics), telling us how the means of methods compare. However, F-test, t-test and t-statistics from regression rely on strong assumptions related to the distribution of the means of error measures [that it is symmetric, so that Central Limit Theorem works, see discussion in Section 6.2 of @SvetunkovSBA]. If we had a large sample (e.g. a thousand of series) and well-behaved distribution, we could try it, hoping that the central limit theorem would work and might get something relatively meaningful. However, on 100 observations, this still could be an issue, especially given that the distribution of error measures is typically asymmetric (the estimate of the mean might be biased, which leads to many issues).

Alternatively, we can **compare medians** of distributions of errors. They are robust to outliers, so their estimates should not be too biased in case of skewed distributions on smaller samples. To have a general understanding of performance (is everything the same or is there at least one method that performs differently), we could try the Friedman test [@Friedman1937], which could be considered a nonparametric alternative of the F-test. This should work in our case but will not tell us how specifically the methods compare. We could try the Wilcoxon signed-ranks test [@Wilcoxon1945], which could be considered a nonparametric counterpart of the t-test. However, it only applies to two variables, while we want to compare four.

Luckily, there is the Nemenyi/MCB test [@Demsar2006; @KourentzesWeb2012; @Koning2005]. What the test does, is it ranks the performance of methods for each time series and then takes the mean of those ranks and produces confidence bounds for those means. The means of ranks correspond to medians, so by using this test, we compare medians of errors of different methods. If the confidence bounds for different methods intersect, we can conclude that the medians are not different from a statistical point of view. Otherwise, we can see which of the methods has a higher rank and which has the lower one. There are different ways to present the test results, and there are several R functions that implement it, including `nemenyi()` from the `tsutils` package. However, we will use the function `rmcb()` from the `greybox`, which has more flexible plotting capabilities, supporting all the default parameters for the `plot()` method.

```{r eval=FALSE}
rmcb(smallCompetition, outplot="none") |>
    plot(outplot="mcb", main="")
```

```{r mcbForCompetition, fig.cap="MCB test results for small competition.", echo=FALSE}
plot(smallCompetitionTest, outplot="mcb", main="")
```

Figure \@ref(fig:mcbForCompetition) shows that Methods 1, 3 and 4 are not statistically different on 5% level -- their intervals intersect, so we cannot tell the difference between them, even though the mean rank of Method 4 is lower than for the other methods. Method 2, on the other hand, is significantly worse than the other methods on 5% level: it has the highest mean rank of all, and its interval does not intersect with the intervals of other methods.

Note that while this is a good way of presenting the results, all the MCB test does is a comparison of mean ranks. It does not tell much about the distribution of errors and neglects the distances between values (i.e. 0.1 is lower than 0.11, so the first method has a lower rank, which is precisely the same result as with comparing 0.1 and 100). This happens because by doing the test, we move from a numerical scale to the ordinal one [see Section 1.2 of @SvetunkovSBA]. Finally, like any other statistical test, it will become more powerful when the sample increases. We know that the null hypothesis "variables are equal to each other" in reality is always wrong [see Section 7.1 of @SvetunkovSBA], so the increase of sample size will lead at some point to the correct conclusion: methods are statistically different. Here is a demonstration of this assertion:

```{r eval=FALSE}
largeCompetition <- 
  matrix(NA, 100000, 4,
         dimnames=list(NULL, paste0("Method",c(1:4))))
# Generate data
largeCompetition[,1] <- rnorm(100000,1,0.35)
largeCompetition[,2] <- rnorm(100000,1.2,0.2)
largeCompetition[,3] <- runif(100000,0.5,1.5)
largeCompetition[,4] <- rlnorm(100000,0,0.3)
# Run the test
rmcb(largeCompetition, outplot="none") |>
    plot(outplot="mcb", main="")
```

```{r mcbForCompetitionLarge, fig.cap="MCB test results for large competition.", echo=FALSE}
plot(largeCompetitionTest, outplot="mcb", main="")
```

In the plot in Figure \@ref(fig:mcbForCompetitionLarge), Method 4 has become significantly worse than Methods 1 and 3 in terms of mean ranks on 5% level (note that it was winning in the small competition). The difference between Methods 1 and 3 is still not significant on 5%, but it would become if we continue increasing the sample size. This example tells us that we need to be careful when selecting the best method, as this might change under different circumstances. At least we knew from the start that Method 2 was not suitable.
