# General ADAM ETS model {#ADAMETSOther}
Now that we have discussed two special cases of ADAM ETS models (namely pure additive from Chapter \@ref(ADAMETSIntroduction) and pure multiplicative from Chapter \@ref(ADAMETSPureMultiplicativeChapter) ADAM ETS), we can move to the discussion of the general model and special cases of it, including some of the mixed models. We will consider two important groups of mixed ADAM ETS models: with non-multiplicative and with multiplicative trends. They have very different properties that are worth mentioning. Finally, we will discuss the question of the normalisation of seasonal indices. This topic was studied in the literature back in the 80s when we still did not have a proper ETS model, but it has not been thoroughly studied since then. In Section \@ref(ADAMETSSeasonalNormalisation), we take a more critical view towards the topic.


## Model formulation {#ADAMETSGeneral}
Based on the discussion in previous chapters, we can summarise the general ADAM ETS model. It is built upon the conventional model discussed in Section \@ref(ETSConventionalModel) but has several significant differences, the most important of which is that it is formulated using lags of components rather than the transition of them over time (this was discussed in Chapter \@ref(ADAMETSPureAdditive) for the pure additive model). The general ADAM ETS model is formulated in the following way:
\begin{equation}
  \begin{aligned}
    {y}_{t} = &w(\mathbf{v}_{t-\boldsymbol{l}}) + r(\mathbf{v}_{t-\boldsymbol{l}}) \epsilon_t \\
    \mathbf{v}_{t} = &f(\mathbf{v}_{t-\boldsymbol{l}}) + g(\mathbf{v}_{t-\boldsymbol{l}}) \epsilon_t
  \end{aligned},
  (\#eq:ETSADAMStateSpace)
\end{equation}
where $\mathbf{v}_{t-\boldsymbol{l}}$ is the vector of lagged components and $\boldsymbol{l}$ is the vector of lags, while all the other functions correspond to the ones used in Section \@ref(ETSConventionalModel), equation \@ref(eq:ETSConventionalStateSpace). This model form is mainly useful for the formulation, rather than for inference. Not only it encompasses any pure models, it also allows formulating any of the mixed ones. For example, the ETS(M,A,M) will have the following values:
\begin{equation*}
  \begin{aligned}
    w(\mathbf{v}_{t-\boldsymbol{l}}) = (l_{t-1}+b_{t-1}) s_{t-m}\text{, } &
    r(\mathbf{v}_{t-\boldsymbol{l}}) = w(\mathbf{v}_{t-\boldsymbol{l}}), \\
    f(\mathbf{v}_{t-\boldsymbol{l}}) = \begin{pmatrix} l_{t-1} + b_{t-1} \\ b_{t-1} \\ s_{t-m} \end{pmatrix}\text{, } &
    g(\mathbf{v}_{t-\boldsymbol{l}}) = \begin{pmatrix} \alpha (l_{t-1} + b_{t-1}) \\ \beta (l_{t-1} + b_{t-1}) \\ \gamma s_{t-m} \end{pmatrix}, \\
    \mathbf{v}_{t} = \begin{pmatrix} l_t \\ b_t \\ s_t \end{pmatrix}\text{, } &
    \boldsymbol{l} = \begin{pmatrix} 1 \\ 1 \\ m \end{pmatrix}, \\
    \mathbf{v}_{t-\boldsymbol{l}} = \begin{pmatrix} l_{t-1} \\ b_{t-1} \\ s_{t-m} \end{pmatrix}
  \end{aligned}.
\end{equation*}
By inserting these values in \@ref(eq:ETSADAMStateSpace) we will get the classical ETS(M,A,M) model, mentioned in Section \@ref(ETSTaxonomyMaths):
\begin{equation}
  \begin{aligned}
    y_{t} = & (l_{t-1} + b_{t-1}) s_{t-m}(1 + \epsilon_t) \\
    l_t = & (l_{t-1} + b_{t-1})(1 + \alpha \epsilon_t) \\
    b_t = & b_{t-1} + (l_{t-1} + b_{t-1}) \beta \epsilon_t \\
    s_t = & s_{t-m} (1 + \gamma \epsilon_t) 
  \end{aligned}.
  (\#eq:ETSADAMMAM)
\end{equation}
The model \@ref(eq:ETSADAMStateSpace) with different values for the functions is the basis of `adam()` function from `smooth` package. It is used in the C++ code to generate fitted values and/or simulate data from any ETS model.


## Mixed ADAM ETS models {#ADAMETSMixedModels}
@Hyndman2008b proposed five classes of ETS models, based on the types of their components:

1. ANN; AAN; AAdN; ANA; AAA; AAdA;
2. MNN; MAN; MAdN; MNA; MAA; MAdA;
3. MNM; MAM; MAdM;
4. MMN; MMdN; MMM; MMdM;
5. ANM; AAM; AAdM; MMA; MMdA; AMN; AMdN; AMA; AMdA; AMM; AMdM

The idea behind this split is to distinguish the models by their complexity and the availability of analytical expressions for conditional moments. Class 1 models were discussed in Chapter \@ref(ADAMETSPureAdditive). They have analytical expressions for conditional mean and variance; they can be applied to any data; they have simple formulae for prediction intervals.

@Hyndman2008b demonstrate that models from Class 2 have closed forms for conditional expectation and variance, with the former corresponding to the point forecasts. However, the conditional distribution from these models is not Gaussian, so there are no formulae for the prediction intervals from these models. Yes, in some cases, Normal distribution might be used as a satisfactory approximation for the real one, but simulations should generally be preferred.

Class 3 models suffer from similar issues, but the situation worsens: there are no analytical solutions for the conditional mean and variance, and there are only approximations to these statistics.

Class 4 models were discussed in Chapter \@ref(ADAMETSPureMultiplicative). They do not have analytical expressions for the moments, their conditional h steps ahead distributions represent a complex convolution of products of the basic ones, but they are appropriate for the positive data and become more valuable when the level of series is low, as already discussed in Chapter \@ref(ADAMETSPureMultiplicative).

Finally, Class 5 models might have infinite variances, specifically on long horizons and when the data has low values. Indeed, when the level in one of these models becomes close to zero, there is an increased chance of breaking the model due to the appearance of negative values. Consider an example of the ETS(A,A,M) model, which might have a negative trend, leading to negative values, which are then multiplied by the positive seasonal indices. This would lead to unreasonable values of states in the model. That is why in practice, these models should only be used when the level of the series is high. Furthermore, some Class 5 models are very difficult to estimate and are very sensitive to the smoothing parameter values. This mainly applies to the multiplicative trend models.

The `ets()` function from the `forecast` package by default supports only Classes 1 -- 4 for the reasons explained above, although it is possible to switch on the Class 5 models by setting parameter `restrict=FALSE`.

To be fair, any mixed model can potentially break when the level of the series is close to zero. For example, ETS(M,A,N) can have a negative trend, which might lead to the negative level and, as a result, to the multiplication of pure positive error term by the negative components. Estimating such model on real data becomes a non-trivial task.

In addition, as discussed above, simulations are typically needed to get prediction intervals for models of Classes 2 -- 5 and conditional mean and variance for models for Classes 4 -- 5. All of this, in my opinion, means that the more useful classification of ETS models is the following [it was first proposed by @Akram2009]:

A) Pure additive models (Chapter \@ref(ADAMETSPureAdditive)): ANN; AAN; AAdN; ANA; AAA; AAdA;
B) Pure multiplicative models (Chapter \@ref(ADAMETSPureMultiplicative)): MNN; MMN; MMdN; MNM; MMM; MMdM;
C) Mixed models with non-multiplicative trend (Section \@ref(ADAMETSMixedModelsGroup3)): MAN; MAdN; MNA; MAA; MAdA; MAM; MAdM; ANM; AAM; AAdM;
D) Mixed models with multiplicative trend (Section \@ref(ADAMETSMixedModelsGroup4): MMA; MMdA; AMN; AMdN; AMA; AMdA; AMM; AMdM;

The main idea behind the split to (C) and (D) is that the multiplicative trend makes it almost impossible to derive the formulae for the conditional moments of the distribution. So this class of models can be considered as the most challenging one.

`adam()` function supports all 30 ETS models, but you should keep in mind the limitations of some of them discussed in this section. `es()` function from `smooth` is a wrapper of `adam()` and as a result supports the same set of models as well.


## Mixed models with non-multiplicative trend {#ADAMETSMixedModelsGroup3}
There are two subclasses in this class of models:

1. With a non-multiplicative seasonal component (MAN, MAdN, MNA, MAA, MAdA)
2. With the multiplicative one (MAM; MAdM; ANM; AAM; AAdM).

The conditional mean for the former models coincides with the point forecasts, while the conditional variance can be calculated using the following recursive formula [@Hyndman2008b, page 84]:
\begin{equation}
	\begin{aligned}
	& \mathrm{V}(y_{t+h}|t) = (1+\sigma^2) \xi_h -\mu_{t+h|t}^2 \\
	& \text{where } \xi_{1} = \mu_{t+1|t}^2 \text{ and } \xi_h = \mu_{t+h|t}^2 + \sigma^2 \sum_{j=1}^{h-1} c_{j}^2 \xi_{h-j}
	\end{aligned} ,
	(\#eq:ETSADAMMixedModels31Variance)
\end{equation}
where $\sigma^2$ is the variance of the error term. Still, the predictive distribution from these models does not have a closed form, and as a result, in general the simulations need to be used to get the correct quantiles.

As for the second subgroup, the conditional mean corresponds to the point forecasts, when the forecast horizon is less than or equal to the seasonal frequency of the component (i.e. $h\leq m$), and there is a cumbersome formula for calculating the conditional mean to some of models in this subgroup for the $h>m$. When it comes to the conditional variance, there exists a formula for some of models in the second subgroup, but they are cumbersome as well. For all of these, the interested reader is directed to @Hyndman2008b, page 85.

When it comes to the parameters bounds for the models in this group, the first subgroup of models has bounds similar to the ones for the respective additive error models (Section \@ref(stabilityConditionAdditiveError)) because they both underlie the same exponential smoothing methods, but with additional restrictions, coming from the multiplicative error (Section \@ref(stabilityConditionMultiplicativeError)).

1. The *traditional bounds* (aka "usual") should work fine for these models for the same reasons they work for the pure additive ones, although they might be too restrictive in some cases;
2. The *admissible bounds* for smoothing parameters for the models in this group might be too wide and violate the condition of $(1+ \alpha \epsilon_t)>0$, which is important in order not to break the models.

The second subgroup is more challenging in terms of parameters bounds because of the multiplication of states by the seasonal components. In general, to be on the safe side, the bounds should not be wider than [0, 1] for the smoothing parameters $\alpha$ and $\gamma$ in these models.

Finally, some models in this group are difficult to motivate from the application point of view. For example, ETS(M,A,A) assumes that the trend and seasonal components change in units (e.g. sales increase for several units in January), while the error term reflects the percentage changes. Similarly, ETS(A,A,M) has a difficult explanation because it assumes unit changes due to the error term and percentage changes due to the seasonality.


## Mixed models with multiplicative trend {#ADAMETSMixedModelsGroup4}
This is the most challenging class of models (MMA; MMdA; AMN; AMdN; AMA; AMdA; AMM; AMdM). They do not have analytical formulae for conditional moments, and they are very sensitive to smoothing parameter values and may lead to explosive forecasting trajectories. So, to obtain conditional expectation, variance and prediction interval from the models of these classes, simulations should be used.

One of the issues encountered when using these models is the explosive trajectories because of the multiplicative trend. As a result, when these models are estimated, it makes sense to set the initial value of the trend to 1 or a lower value so that the optimiser does not encounter difficulties in the calculations because of the explosive behaviour in-sample.

Furthermore, the combinations of components for the models in this class make even less sense than the combinations for Class C (discussed in Section \@ref(ADAMETSMixedModelsGroup3)). For example, the multiplicative trend assumes either explosive growth or decay, as shown in Figure \@ref(fig:plotsOfExponent).

```{r plotsOfExponent, echo=FALSE, fig.cap="Plots of exponential increase and exponential decline."}
par(mfcol=c(1,2))
plot(exp(0.05*c(1:100)),type="l",ylab="Demand",xlab="Time")
plot(exp(-0.05*c(1:100)),type="l",ylab="Demand",xlab="Time")
```

However, assuming that either seasonal component, the error term, or both will have precisely the same impact on the final value irrespective of the point of time (thus being additive) is unrealistic for this situation. The more reasonable would be for the amplitude of seasonality to decrease together with the exponential decay of the trend and for the variance of the error term to do the same. But this means that we are talking about the pure multiplicative models (Chapter \@ref(ADAMETSPureMultiplicative)), not the mixed ones. There is only one situation where such mixed models could make sense: when the speed of change of the exponential trajectory is close to zero (i.e. the trend behaves similar to the linear one) and when the volume of the data is high. In this case, the mixed models might perform well and even produce more accurate forecasts than the models from the other classes.

When it comes to the bounds of the parameters, this is a mystery for the mixed models of this class. This is because the recursive relations are complicated, and calculating the discount matrix or anything like that becomes challenging. The usual bounds should still be okay, but keep in mind that these mixed models are typically not very stable and might exhibit explosive behaviour even, for example, with $\beta=0.1$. So from my experience, the smoothing parameters in these models should be as low as possible, assuming that the initial values guarantee a working model (not breaking on some of the observations).


## Normalisation of seasonal indices in ETS models {#ADAMETSSeasonalNormalisation}
One of the ideas arising from time series decomposition (Section \@ref(ClassicalDecomposition)), inherited by the conventional ETS, is the renormalisation of seasonal indices. It comes to one of the two principles, depending on the type of seasonality:

1. If the model has additive seasonality, then the seasonal indices should add up to zero in a specific period of time, e.g. monthly indices should add up to 0 over the yearly period;
2. If the model has multiplicative seasonality, then the geometric mean of seasonal indices over a specific period should be equal to one.

Condition (2) in the conventional ETS is substituted by "the arithmetic mean of multiplicative indices should be equal to one", which does not have reasonable grounds behind it. If we deal with the multiplicative effect, the geometric mean should be used, not the arithmetic one. Otherwise, we introduce bias in the model by multiplying components by indices.

While the normalisation is a natural element of the time series decomposition and works fine for the initial seasonal indices, renormalising the seasonal indices over time might not be natural for the ETS.

@Hyndman2008b discuss different mechanisms for the renormalisation of seasonal indices, which, as the authors claim, are needed to make the principles (1) and (2) hold from period to period in the data. However, I argue that this is an unnatural idea for the ETS models. The indices should only be normalised during the initialisation of the model (at the moment $t=0$), and that they should vary independently for the rest of the sample. The rationale for this comes from the model itself. To illustrate it, I will use ETS(A,N,A), but the idea can be easily applied to any other seasonal ETS model with any types of components and any number of seasonal frequencies. Just a reminder, ETS(A,N,A) is formulated as:
\begin{equation}
  \begin{aligned}
  y_t = &l_{t-1} + s_{t-m} + \epsilon_t \\
  {l}_{t} = &l_{t-1} + \alpha\epsilon_t \\
  s_t = &s_{t-m} + \gamma\epsilon_t
  \end{aligned}.
  (\#eq:ETSANAExampleNormalisation)
\end{equation}
Let's assume that this is the "true model" (as discussed in Section \@ref(modelsMethods)) for whatever data we have for whatever reason. In this case, the set of equations \@ref(eq:ETSANAExampleNormalisation) tells us that the seasonal indices change over time, depending on the value of smoothing parameter $\gamma$ and each specific value of $\epsilon_t$, which is assumed to be i.i.d. All seasonal indices $s_{t+i}$ in a particular period (e.g. monthly indices in a year) can be written down explicitly based on \@ref(eq:ETSANAExampleNormalisation):
\begin{equation}
  \begin{aligned}
  s_{t+1} = &s_{t+1-m} + \gamma\epsilon_{t+1} \\
  s_{t+2} = &s_{t+2-m} + \gamma\epsilon_{t+2} \\
  \vdots \\
  s_{t+m} = &s_{t} + \gamma\epsilon_{t+m}
  \end{aligned}.
  (\#eq:ETSANAExampleNormalisationIndices01)
\end{equation}
If this is how the data is "generated" and the seasonality evolves over time, then there is only one possibility, for the indices $s_{t+1}, s_{t+2}, \dots, s_{t+m}$ to add up to zero:
\begin{equation}
  s_{t+1}+ s_{t+2}+ \dots+ s_{t+m} = 0
  (\#eq:ETSANAExampleNormalisationIndices02a)
\end{equation}
or after inserting \@ref(eq:ETSANAExampleNormalisationIndices01) in \@ref(eq:ETSANAExampleNormalisationIndices02a):
\begin{equation}
  s_{t+1-m}+ s_{t+2-m}+ \dots+ s_{t} + \gamma \left(\epsilon_{t+1}+ \epsilon_{t+2}+ \dots+ \epsilon_{t+m}\right) = 0 ,
  (\#eq:ETSANAExampleNormalisationIndices02b)
\end{equation}
meaning that:

1. the previous indices $s_{t+1-m}, s_{t+2-m}, \dots, s_{t}$ add up to zero **and**
2. either:
    a. $\gamma=0$,
    b. the sum of error terms $\epsilon_{t+1}, \epsilon_{t+2}, \dots, \epsilon_{t+m}$ is zero.

Note that we do not consider the situation $s_{t+1-m}+ \dots+ s_{t} = -\gamma \left(\epsilon_{t+1}+ \dots+ \epsilon_{t+m}\right)$ as it does not make sense. The condition (1) can be considered reasonable if the previous indices are normalised. (2.a) means that the seasonal indices do not evolve over time. However, (2.b) implies that the error term is not independent, because $\epsilon_{t+m} = -\epsilon_{t+1}- \epsilon_{t+2}- \dots- \epsilon_{t+m-1}$, which violates one of the basic assumptions of the model from Subsection \@ref(assumptions), meaning that \@ref(eq:ETSANAExampleNormalisationIndices01) cannot be considered as the "true" model any more, as it omits some important elements. Thus renormalisation is unnatural for ETS from the "true" model point of view.

Alternatively each seasonal index could be updated on each observation $t$ (to make sure that the indices are renormalised). In this situation we have:
\begin{equation*}
  \begin{aligned}
  &s_t = s_{t-m} + \gamma\epsilon_t \\
  &s_{t-m+1}+ s_{t-m+2}+ \dots+ s_{t-1} + s_{t} = 0
  \end{aligned},
\end{equation*}
which can be rewritten as $s_{t-m} + \gamma\epsilon_t = -s_{t+1-m}- s_{t+2-m}- \dots- s_{t-1}$, meaning that:
\begin{equation*}
  \begin{aligned}
  s_{t-m}+ s_{t+1-m}+ s_{t+2-m}+ \dots+ s_{t-1} = -\gamma\epsilon_t
  \end{aligned},
\end{equation*}
But due to the renormalisation, the sum on the left-hand side should be equal to zero, implying that either $\gamma=0$ or $\epsilon_t=0$. While the former might hold in some cases (deterministic seasonality), the latter cannot hold for all $t=1,\dots,T$ and again violates the model's assumptions. The renormalisation is thus impossible without changing the structure of the model. @Hyndman2008b acknowledge that and propose in Chapter 8 some modifications for the seasonal ETS model (i.e. introducing new models), which we do not aim to discuss in this Chapter.

The discussion in this section demonstrates that the renormalisation of seasonal indices is unnatural for the conventional ETS model and should not be used. This does not mean that the initial seasonal indices (corresponding to the observation $t=0$) cannot be normalised. On the contrary, this is the desired approach as it reduces the number of estimated parameters in the model. But this means that there is no need to implement an additional mechanism of renormalisation of indices in sample for $t>0$.
