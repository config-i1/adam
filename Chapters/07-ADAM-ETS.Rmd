# ADAM ETS

The ETS models implemented in ADAM framework is built upon the conventional one but has several important differences. First it is formulated using lags of components rather than the transition of them over time, so the original model \@ref{eq:ETSConventionalStateSpace} is written in the following way:
\begin{equation}
  \begin{aligned}
  {y}_{t} = &w(\mathbf{v}_{t-\mathbf{l}}) + r(\mathbf{v}_{t-\mathbf{l}}) \epsilon_t \\
  \mathbf{v}_{t} = &f(\mathbf{v}_{t-\mathbf{l}}) + g(\mathbf{v}_{t-\mathbf{l}}) \epsilon_t
  \end{aligned},
  (\#eq:ETSADAMStateSpace)
\end{equation}
where $\mathbf{v}_{t-\mathbf{l}}$ is the vector of lagged components and $\mathbf{l}$ is the vector of lags, and all the other functions corresponds to the ones used in \@ref{eq:ETSConventionalStateSpace}. So, for example, for the ETS(A,A,A) model the lags will be $\mathbf{l}'=\begin{pmatrix}1 & 1 & m\end{pmatrix}$, where $m$ is the seasonal periodicity of the data, leading to $\mathbf{v}_{t-\mathbf{l}}'=\begin{pmatrix} l_{t-1} & b_{t-1} & s_{t-m}\end{pmatrix}$. The model \@ref{eq:ETSADAMStateSpace} updates the states exactly in the same way as \@ref{eq:ETSConventionalStateSpace} and produces exactly the same values. The main benefit of doing that is that the transition matrix becomes smaller, containing $3\times 3$ elements in case of ETS(A,A,A) instead of $(2+m)\times (2+m)$ as for the conventional model. The main disadvantage of this approach is in the complications arrising in the derivation of conditional expectation and mean, which still have closed forms, but are more cumbersome. They are discussed later in this chapter for the example of [pure additive ETS](#ADAMETSPureAdditive).

Based on this formulation, in this chapter, we will discuss several special cases of the ADAM ETS model and explain how they can be extended.


## ADAM: Pure additive ETS {#ADAMETSPureAdditive}
### Model formulation
The pure additive case is interesting, because this is the group of models that has closed forms for both conditional mean and variance. It is formulated in the following way:
\begin{equation}
  \begin{aligned}
    {y}_{t} = &\mathbf{w}' \mathbf{v}_{t-\mathbf{l}} + \epsilon_t \\
    \mathbf{v}_{t} = &\mathbf{F} \mathbf{v}_{t-\mathbf{l}} + \mathbf{g} \epsilon_t
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditive)
\end{equation}
where $\mathbf{w}$ is the measurement vector, $\mathbf{F}$ is the transition matrix and $\mathbf{g}$ is the persistence vector. An example of a pure additive model is ETS(A,A,A), for which we have the following values:
\begin{equation}
  \begin{aligned}
    \mathbf{w} = & \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} \\
    \mathbf{F} = & \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \\
    \mathbf{g} = & \begin{pmatrix} \alpha \\ \beta \\ \gamma \end{pmatrix} \\
    \mathbf{v}_{t} = & \begin{pmatrix} l_t \\ b_t \\ s_t \end{pmatrix} \\
    \mathbf{l} = & \begin{pmatrix} 1 \\ 1 \\ m \end{pmatrix}
  \end{aligned}.
  (\#eq:ETSADAMAAAMatrices)
\end{equation}
By inserting these values in the equation \@ref{eq:ETSADAMStateSpacePureAdditive}, we will obtain the model discussed in the [ETS Taxonomy](#ETSTaxonomyMaths) section:
\begin{equation}
  \begin{aligned}
    y_{t} = & l_{t-1} + b_{t-1} + s_{t-m} + \epsilon_t \\
    l_t = & l_{t-1} + b_{t-1} + \alpha \epsilon_t \\
    b_t = & b_{t-1} + \beta \epsilon_t \\
    s_t = & s_{t-m} + \gamma \epsilon_t 
  \end{aligned}.
  (\#eq:ETSADAMAAA)
\end{equation}
Just to compare, the conventinal ETS(A,A,A), formulated according to \@ref{eq:ETSConventionalStateSpace} would have the following transition matrix:
\begin{equation}
  \mathbf{F} = & \begin{pmatrix} 1 & 1 \mathbf{0}'_{m-1} & 0 \\ 0 & 1 & \mathbf{0}'_{m-1} & 0 \\ 0 & 0 & \mathbf{0}'_{m-1} & 1 \\ \mathbf{0}_{m-1} & \mathbf{0}_{m-1} & \mathbf{I}_m & \mathbf{0}_{m-1} \end{pmatrix},
  (\#eq:ETSADAMAAAMatrices)
\end{equation}
where $\mathbf{I}_m$ is the identity matrix of the size $m \times m$ and $\mathbf{0}_{m-1}$ is the vector of zeroes of size $m-1$. The model \@ref{eq:ETSADAMStateSpacePureAdditive} is more parsimonious and simplifies some of the calculations, making it realistic, for example, to apply models to data with large frequency $m$ (e.g. 24, 48, 52, 365).


### Recursive relation
A useful thing that can be derived from the pure additive model \@ref{eq:ETSADAMStateSpacePureAdditive} is the recursive value, which can be used in several important aspects. First, when we produce forecast for $h$ steps ahead, it is important to understand what the actual value $h$ steps ahead might be, given all the information we have on the observation $t$:
\begin{equation}
  \begin{aligned}
    {y}_{t+h} = &\mathbf{w}' \mathbf{v}_{t-h_\mathbf{l}} + \epsilon_{t+h} \\
    \mathbf{v}_{t+h} = &\mathbf{F} \mathbf{v}_{t-h_\mathbf{l}} + \mathbf{g} \epsilon_{t+h}
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion01)
\end{equation}
where $\mathbf{v}_{t-h_\mathbf{l}}$ is the vector of previous states, given the lagged values $\mathbf{l}$. In order to obtain the recursion, we need to split the measurement and persisitence vectors together with the transition matrix into parts for the same lags of components, leading to the following transition equation in \@ref{eq:ETSADAMStateSpacePureAdditiveRecursion01}:
\begin{equation}
  \begin{aligned}
    {y}_{t+h} = &(\mathbf{w}_{m_1}' + \mathbf{w}_{m_2}' + dots + \mathbf{w}_{m_d}') \mathbf{v}_{t-h_\mathbf{l}} + \epsilon_{t+h} \\
    \mathbf{v}_{t+h} = &(\mathbf{F}_{m_1} + \mathbf{F}_{m_2} + \dots + \mathbf{F}_{m_d}) \mathbf{v}_{t-h_\mathbf{l}} + (\mathbf{g}_{m_1} + \mathbf{g}_{m_2} + \dots \mathbf{g}_{m_d}) \epsilon_{t+h}
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion02)
\end{equation}
where $m_1, m_2, \dots, m_d$ are the distinct seasonal frequencies. So, for example, in case of ETS(A,A,A) model on quarterly data (periodicity is equal to four), $m_1=1$, $m_d=4$, leading to $\mathbf{F}_{m_1} = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ and $\mathbf{F}_{m_2} = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix}$. This split of matrices and vectors into distinct sub matrices and subvectors is needed in order to get the correct recursion and obtain the correct conditional expectation and variance. By substituting the values in the transition equation of \@ref{eq:ETSADAMStateSpacePureAdditiveRecursion02} by their previous values until we reach $t$, we get:
\begin{equation}
  \begin{aligned}
    \mathbf{v}_{t-h_\mathbf{l}} = & \mathbf{F}_{m_1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{v}_{t} + \sum_{j=1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{F}_{m_1}^{j-1} \mathbf{g}_{m_1} \epsilon_{t+m_1\lceil\frac{h}{m_1}\rceil-j} + \\
    & \mathbf{F}_{m_2}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{v}_{t} + \sum_{j=1}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{F}_{m_2}^{j-1} \mathbf{g}_{m_2} \epsilon_{t+m_2\lceil\frac{h}{m_2}\rceil-j} + \\
    & \dots \\
    & \mathbf{F}_{m_d}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{v}_{t} + \sum_{j=1}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{F}_{m_d}^{j-1} \mathbf{g}_{m_d} \epsilon_{t+m_d\lceil\frac{h}{m_d}\rceil-j}
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion03)
\end{equation}
Inserting \@ref{eq:ETSADAMStateSpacePureAdditiveRecursion03} in the measurement equation of \@ref{eq:ETSADAMStateSpacePureAdditiveRecursion02}, we will get:
\begin{equation}
  \begin{aligned}
    y_{t+h} = & \mathbf{w}_{m_1}' \mathbf{F}_{m_1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{v}_{t} + \mathbf{w}_{m_1}' \sum_{j=1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{F}_{m_1}^{j-1} \mathbf{g}_{m_1} \epsilon_{t+m_1\lceil\frac{h}{m_1}\rceil-j} + \\
    & \mathbf{w}_{m_2}' \mathbf{F}_{m_2}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{v}_{t} + \mathbf{w}_{m_2}' \sum_{j=1}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{F}_{m_2}^{j-1} \mathbf{g}_{m_2} \epsilon_{t+m_2\lceil\frac{h}{m_2}\rceil-j} + \\
    & \dots \\
    & \mathbf{w}_{m_d}' \mathbf{F}_{m_d}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{v}_{t} + \mathbf{w}_{m_d}' \sum_{j=1}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{F}_{m_d}^{j-1} \mathbf{g}_{m_d} \epsilon_{t+m_d\lceil\frac{h}{m_d}\rceil-j} + \\
    & \epsilon_{t+h}
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion04)
\end{equation}
Substituting the specific values of $m_1, m_2, \dots, m_d$ in \@ref{eq:ETSADAMStateSpacePureAdditiveRecursion04} will simplify the equation and make it easier to understand. For example, for ETS(A,N,N) $m_1=1$ and all the other frequencies are equal to zero, so the recursion \@ref{eq:ETSADAMStateSpacePureAdditiveRecursion04} simplifies to:
\begin{equation}
  \begin{aligned}
    y_{t+h} = & \mathbf{w}_{1}' \mathbf{F}_{1}^{h-1} \mathbf{v}_{t} + \mathbf{w}_{1}' \sum_{j=1}^{h-1} \mathbf{F}_{1}^{j-1} \mathbf{g}_{1} \epsilon_{t+h-j} + \\
    & \epsilon_{t+h}
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion05)
\end{equation}
which is the recursion obtained by @Hyndman2008b.


### Conditional expectation and variance {#pureAdditiveExpectationAndVariance}
Now, why is the recursion \@ref{eq:ETSADAMStateSpacePureAdditiveRecursion04} important? This is because we can take the expectation and variance of \@ref{eq:ETSADAMStateSpacePureAdditiveRecursion04} conditional on the values of the state vector $\mathbf{v}_{t}$ on the observation $t$ (assuming that the error term is homoscedastic, uncorrelated and has the expectation of zero) in order to get:
\begin{equation}
  \begin{aligned}
    \mu_{t+h} = \text{E}(y_{t+h}|t) = & \sum_{i=1}^d \left(\mathbf{w}_{m_i}' \mathbf{F}_{m_i}^{\lceil\frac{h}{m_i}\rceil-1} \mathbf{v}_{t} \right) \\
    \text{V}(y_{t+h}|t) = & \left( \sum_{i=1}^d \left(\mathbf{w}_{m_i}' \sum_{j=1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{F}_{m_i}^{j-1} \mathbf{g}_{m_i} \mathbf{g}'_{m_i} \mathbf{F}'_{m_i}^{j-1} \mathbf{w}_{m_i} \right) + 1 \right) \sigma^2
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionMeanAndVariance)
\end{equation}
These two formulae are cumbersome, but they give the analytical solutions to the two statistics. Having obtained both of them, we can construct prediction intervals, assuming, for example, that the error term follows normal distribution:
\begin{equation}
    y_{t+h} \in \text{E}(y_{t+h}|t) \pm z_{\frac{\alpha}{2}} \sqrt{\text{V}(y_{t+h}|t)} ,
  (\#eq:ETSADAMStateSpacePureAdditivePredictionInterval)
\end{equation}
where $z_{\frac{\alpha}{2}}$ is quantile of standardised normal distribution for the level $\alpha$.


#### Example with ETS(A,N,N)
For example, for the ETS(A,N,N) model, discussed above, we get:
\begin{equation}
  \begin{aligned}
    \text{E}(y_{t+h}|t) = & \mathbf{w}_{1}' \mathbf{F}_{1}^{h-1} \mathbf{v}_{t} \\
    \text{V}(y_{t+h}|t) = & \left(\mathbf{w}_{1}' \sum_{j=1}^{h-1} \mathbf{F}_{1}^{j-1} \mathbf{g}_{1} \mathbf{g}'_{1} \mathbf{F}'_{1}^{j-1} \mathbf{w}_{1} + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSADAMStateSpaceAANRecursionMeanAndVariance)
\end{equation}
or by substituting $\mathbf{F}=1$, $\mathbf{w}=1$, $\mathbf{g}=\alpha$ and $\mathbf{v}_t=l_t$:
\begin{equation}
  \begin{aligned}
    \mu_{t+h} = \text{E}(y_{t+h}|t) = & l_{t} \\
    \text{V}(y_{t+h}|t) = & \left((h-1) \alpha^2 + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSADAMStateSpaceAANRecursionMeanAndVariance)
\end{equation}
which is the same conditional expectation and variance as in the [ETS Taxonomy section](#ETSTaxonomyMaths) and in the @Hyndman2008b textbook.


### Stability and forecastability conditions
Another important aspect of the pure additive model \@ref{eq:ETSADAMStateSpacePureAdditive} is the restriction on the smoothing parameters. This is related to the stability and forecastability conditions of the model. The **stability** implies that [the weights for observations decay](#whyExponential), guaranteeing that the newer ones will have higher weights than the older ones. If this condition holds, then the model behaves "steadily", forgetting eventually the past values. The **forecastability** does not guarantee that the weights will decay, but it guarantees that the initial value of the state vector will have a constant impact on forecasts, i.e. will not increase in weight with the increase of the forecast horizon. An example of the non-stable, but forecastable model is ETS(A,N,N) with $\alpha=0$. In this case it reverts to the global level model, where the initial value impacts the forecast, but does not change with the increase of the forecast horizon.

In order to obtain both conditions, we need to use a reduced form of ETS by inserting the measurement equation in the transition equation via $\epsilon_t= {y}_{t} - \mathbf{w}' \mathbf{v}_{t-\mathbf{l}}$:
\begin{equation}
  \begin{aligned}
    \mathbf{v}_{t} = &\mathbf{F} \mathbf{v}_{t-\mathbf{l}} + \mathbf{g} \left({y}_{t} - \mathbf{w}' \mathbf{v}_{t-\mathbf{l}} \right)\\
    & \left(\mathbf{F} - \mathbf{g}\mathbf{w}' \right) \mathbf{v}_{t-\mathbf{l}} + \mathbf{g} {y}_{t} \\
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureAdditiveBackRecursion01)
\end{equation}
The matrix $\mathbf{D}=\mathbf{F} - \mathbf{g}\mathbf{w}'$ is called the discount matrix and it shows how the weights diminish over time.


#### Example with ETS(A,N,N)
In order to better understand what we plan to discuss later, we can take **an example of ETS(A,N,N) model**, for which $\mathbf{F}=1$, $\mathbf{w}=1$, $\mathbf{g}=\alpha$, $\mathbf{v}_t=l_t$ and $\mathbf{l}=1$:
\begin{equation}
  \begin{aligned}
    l_{t} = & \left(1 - \alpha \right) {l}_{t-1} + \alpha {y}_{t},
  \end{aligned}.
  (\#eq:ETSADAMStateSpaceANNBackRecursion01)
\end{equation}
which corresponds to the formula of Simple Exponential Smoothing \@ref{eq:BrownMethod}. The discount matrix in this case is $\mathbf{D}=1-\alpha$. If we now substitute the values for the level on the right hand side of the equation \@ref{eq:ETSADAMStateSpaceANNBackRecursion01} by the previous values of the level, we will obtain the recursion that we have already discussed in [a previous section](#whyExponential), but now in terms of the "true" components and parameters:
\begin{equation}
  \begin{aligned}
    l_{t} = & {\alpha} \sum_{j=0}^{t-1} (1 -{\alpha})^j {y}_{t-j} + (1 -{\alpha})^t l_0,
  \end{aligned}.
  (\#eq:ETSADAMStateSpaceANNBackRecursion02)
\end{equation}
The *stability* condition for ETS(A,N,N) is that the discount matrix $\mathbf{D}=1-\alpha$ is less than or equal to one by absolute value. This way the weights will decay in time because of the exponentiation in \@ref{eq:ETSADAMStateSpaceANNBackRecursion02}. This condition is satisfied, when $|1-\alpha|<1$, implying $\alpha \in(0, 1)$. As for the *forecastability* condition, in this case it implies that $\lim\limits_{t\rightarrow\infty}(1 -{\alpha})^t = \text{const}$. This is achievable, for example, when $\alpha=0$, but is violated, when $\alpha<0$ or $\alpha\geq2$. So, the bounds for the smoothing parameters in the ETS(A,N,N) model, guaranteeing the forecastability of the model (i.e. making it useful) are:
\begin{equation}
  \alpha \in [0, 1) .
  (\#eq:ETSADAMStateSpaceANNBounds)
\end{equation}

#### Comming back to the general case
In general, the logic is the same as with ETS(A,N,N), but it implies the usage of linear algebra. Due to our lagged formulation, the recursion becomes more complicated:
\begin{equation}
  \begin{aligned}
    \mathbf{v}_{t} = & \mathbf{D}_{m_1}^{\lceil\frac{t}{m_1}\rceil} \mathbf{v}_{0} + \sum_{j=0}^{\lceil\frac{t}{m_1}\rceil-1} \mathbf{D}_{m_1} \y_{t+m_1\lceil\frac{t}{m_1}\rceil-j} + \\
    & \mathbf{D}_{m_2}^{\lceil\frac{t}{m_2}\rceil} \mathbf{v}_{0} + \sum_{j=0}^{\lceil\frac{t}{m_2}\rceil-1} \mathbf{D}_{m_2} \y_{t+m_2\lceil\frac{t}{m_2}\rceil-j} + \\
    & \dots \\
    & \mathbf{D}_{m_d}^{\lceil\frac{t}{m_d}\rceil} \mathbf{v}_{0} + \sum_{j=0}^{\lceil\frac{t}{m_d}\rceil-1} \mathbf{D}_{m_d} \y_{t+m_d\lceil\frac{t}{m_d}\rceil-j}
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion04)
\end{equation}
where $\mathbf{D}_{m_i} = \mathbf{F}_{m_i} - \mathbf{g}_{m_i} \mathbf{w}_{m_i}'$ is the discount matrix for each lagged part of the model. The stability condition in this case is that the absolute values of all the non-zero eigenvalues of the discount matrices $\mathbf{D}_{m_i}$ are lower than one. This condition can be checked at the model construction stage, ensuring that the selected parameters guarantee the stability of the model. As for the forecastability, in our case this implies that $\lim\limits_{t\rightarrow\infty}\left(\mathbf{w}_{m_i}'\mathbf{D}_{m_i}\right)^{t} = \text{const}$.


### Distributional assumptions in pure additive ETS
While the conventional ETS assumes that the error term follows Normal distribution, ADAM ETS proposes some flexibility, implementing the following options for the error term [distribution](#distributions) in the additive error models:

1. Normal: $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$;
2. Laplace: $\epsilon_t \sim \mathcal{Laplace}(0, s)$;
3. S: $\epsilon_t \sim \mathcal{S}(0, s)$;
4. Generalised Normal: $\epsilon_t \sim \mathcal{GN}(0, s, \beta)$;
5. Logistic: $\epsilon_t \sim \mathcal{N}(0, s)$;
6. Student's T: $\epsilon_t \sim \mathcal{t}(\nu)$;
7. Asymmetric Laplace: $\epsilon_t \sim \mathcal{ALaplace}(0, s, \alpha)$

The conditional expectation and stability / forecastability conditions do not change for the model with these new assumptions. The main thing that changes is the scale and the width of prediction intervals. Given that scales of these distributions are linearly related with the variance, one can calculate the conditional variance as [discussed earlier](#pureAdditiveExpectationAndVariance) and then use the formulae from the [theory of distributions](#distributions) section in order to obtain the respective scales. Having the scales it becomes straightforward to calculate the needed quantiles for the prediction intervals.

In addition, the following more exotic options for the additive error models are available as well:

1. Log Normal: $\left(1+\frac{\epsilon_t}{\mu_t} \right) \sim \text{log}\mathcal{N}\left(-\frac{\sigma^2}, \sigma^2\right)$;
2. Log Laplace: $\left(1+\frac{\epsilon_t}{\mu_t} \right) \sim \text{log}\mathcal{Laplace}\left(-\frac{\sigma^2}{2}, s\right)$;
3. Log S: $\left(1+\frac{\epsilon_t}{\mu_t} \right) \sim \text{log}\mathcal{S}\left(-\frac{\sigma^2}{2}, s\right)$;
4. Log Generalised Normal: $\left(1+\frac{\epsilon_t}{\mu_t} \right) \sim \text{log}\mathcal{GN}\left(-\frac{\sigma^2}{2}, s, \beta\right)$;
5. Inverse Gaussian: $\left(1+\frac{\epsilon_t}{\mu_t} \right) \sim \text{log}\mathcal{IG}(1, s)$;
where $\mu_t = \mathbf{w}' \mathbf{v}_{t-\mathbf{l}}$ and the $-\frac{\sigma^2}{2}$ appears due to the restriction $\text{E}(\epsilon_t)=0$, where $\sigma^2$ is the variance of the error term in logarithms.

They arrise from a reformulation of the original pure additive model \@ref{eq:ETSADAMStateSpacePureAdditive} into:
\begin{equation}
  \begin{aligned}
    {y}_{t} = &\mathbf{w}' \mathbf{v}_{t-\mathbf{l}}\left(1 + \frac{\epsilon_t}{\mathbf{w}' \mathbf{v}_{t-\mathbf{l}}}\right) \\
    \mathbf{v}_{t} = &\mathbf{F} \mathbf{v}_{t-\mathbf{l}} + \mathbf{g} \epsilon_t
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveReformulated)
\end{equation}


## ADAM: Pure multiplicative ETS
TBA

### Local level model, ETS(M,N,N)
Another model that underlies the SES method is ETS(M,N,N), which is formulated as:
\begin{equation}
  \begin{split}
    y_{t} &= l_{t-1}(1 + \epsilon_t) \\
    l_t &= l_{t-1} (1 + \alpha \epsilon_t)
  \end{split} .
  (\#eq:ETSMNN)
\end{equation}
Note that in this model it is typically assumed that the mean of error term $\epsilon_t$ is equal to zero. The model has similar properties to \@ref(eq:ETSANN), but will have increasing variability with the increase of level due to the multiplication in the formula. The smoothing parameter
