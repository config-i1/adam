# A bit of statistics {#statistics}
Before we move to the discussion of ETS, ARIMA and ADAM, it makes sense to discuss some of the basics statistical terms and what they mean in the context of forecasting. Although, many of them originate from statistics and econometrics, we need to look at them from a different perspective: we are more interested in the forecasting process rather than in the estimation of parameters. Still, if you do not know statistics and econometrics well and want to have a good source on the topic, I would recommend reading an online book by @Hanck2020.

Before we move further, we need to agree what the term "estimator" means, which will be used several times in this chapter:

- **Estimate** of a parameter is an in sample result of application of a statistical procedure to the data for obtaining some coefficients of a model. The value calculated using the arithmetic mean would be an estimate of the population mean;
- **Estimator** is the rule for calculating estimates of parameters based on a sample of data. For example, arithmetic mean is an estimator of the population mean. Another example would be method of Ordinary Least Squares, which is a rule for producing estimates of parameters of a regression model and thus an estimator.


## Preliminary data analysis {#dataAnalysis}
One of the basic thing that is worth doing before starting any modelling is the preliminary data analysis. This can be done either using numerical or graphical analysis. The former is useful when you want to have a summary information about the data without trying to find detailed information about it. The latter is useful when you can spend more time, investigating relations and issues in the data. In many cases, they compliment each other.

### Numerical analysis
In this section we will use the classical `mtcars` dataset from `datasets` package for R. It contains 32 observations with 11 variables. While all the variables are numerical, some of them are in fact categorical variables encoded as binary ones. We can check the description of the dataset in R:

```{r eval=FALSE}
?mtcars
```

Judging by the explanation in the R documentation, the following variables are categorical:

1. vs - Engine (0 = V-shaped, 1 = straight),
2. am - Transmission (0 = automatic, 1 = manual).

In addition, the following variables are integer numeric ones:

1. cyl - Number of cylinders,
2. hp - Gross horsepower,
3. gear - Number of forward gears,
4. carb - Number of carburetors.

All the other variables are continuous numeric.

Takign this into account, we will create a data frame, encoding the categorical variables as factors for further analysis:

```{r}
mtcarsData <- data.frame(mtcars)
mtcarsData$vs <- factor(mtcarsData$vs,levels=c(0,1),labels=c("V-shaped","Straight"))
mtcarsData$am <- factor(mtcarsData$am,levels=c(0,1),labels=c("automatic","manual"))
```

Given that we only have two options in those variables, it is not compulsory to do this encoding, but it will help us in the further analysis.

We can start with the basic summary statistics. We remember from the scales of information (Section \@ref(scales)) that the nominal variables can be analysed only via frequencies, so this is what we can produce for them:

```{r}
table(mtcarsData$vs)
table(mtcarsData$am)
```

These tables are called **contingency tables**, they show the frequency of appearance of values of variables. Based on this, we can conclude that the cars with V-shaped engine are met more often in the dataset than the cars with the Straight one. In addition, the automatic transmission prevails in the data. The related statistics which is useful for analysis of categorical variables is called **mode**. It shows which of the values happens most often in the data. Judging by the frequencies above, we can conclude that the mode for the first variable is the value "V-shaped".

All of this is purely descriptive information, which does not provide us much. We could probably get more information if we analysed the contingency table based on these two variables:

```{r}
table(mtcarsData$vs,mtcarsData$am)
```

For now, we can only conclude that the cars with V-shaped engine and automatic transmission are met more often than the other cars in the dataset.

Next, we can look at the numerical variables. As we recall from Section \@ref(scales), this scale supports all operations, so we can use quantiles, mean, standard deviation etc. Here how we can analyse, for example, the variable mpg:

```{r}
setNames(mean(mtcarsData$mpg),"mean")
quantile(mtcarsData$mpg)
setNames(median(mtcarsData$mpg),"median")
```

The output above produces:

1. **Mean** - the average value of mpg in the dataset, $\bar{y}=\frac{1}{n}\sum_{j=1}^n y_j$.
2. **Quantiles** - the values that show, below which values the respective proportions of the dataset lie. For example, 25% of observations have mpg less than 15.425. The 25%, 50% and 75% quantiles are also called 1st, 2nd and 3rd **quartiles** respectively.
3. **Median**, which splits the sample in two halves. It corresponds to the 50% quantile.

If median is greater than mean, then this typically means that the distribution of the variable is skewed (it has some rare observations that have large values). This is the case in our case, we can investigate it further using skewness and kurtosis from `timeDate` package:

```{r}
timeDate::skewness(mtcarsData$mpg)
timeDate::kurtosis(mtcarsData$mpg)
```

**Skewness** shows the asymmetry of distribution. If it is greater than zero, then the distribution has the long right tail. If it is equal to zero, then it is symmetric.

**Kurtosis** shows the excess of distribution (fatness of tails) in comparison with the normal distribution. If it is equal to zero, then it is the same as for normal distribution.

<!-- Another potentially useful measure is the half moment (`hm` from `greybox`), which returns the complex number: -->
<!-- ```{r} -->
<!-- hm(mtcarsData$mpg) -->
<!-- ``` -->

<!-- It is calculated as: $\mathrm{hm}=\frac{1}{n}\sum_{j=1}^n \sqrt{y_j}$. The real part of this number corresponds to the positive values in comparison with the centre of distribution, while the imaginary shows the negative ones. Given that the imaginary part is greater than the real, we can conclude that the values are more density to the left of the mean of distribution rather than to the right. This corresponds to the positive skewness. The related coefficient based on that is coefficient of asymmetry (`asymmetry` from `greybox`, starting from v1.0.0): -->

<!-- ```{r} -->
<!-- asymmetry(mtcarsData$mpg) -->
<!-- ``` -->

<!-- It lies between -1 and 1 and is equal to zero only if the distribution is symmetric. It shows, where the higher density of values is (the negative number means that it is to the left from the centre, while the positive means that it is to the right). The main advantage of hm and complex asymmetry coefficients is that they are robust to outliers and are not as sensitive as the classical skewness coefficient is. -->

Based on all of this, we can conclude that the distribution of `mpg` is skewed and has the longer right tail. This is expected for such variable, because the cars that have higher mileage are not common in this dataset.

All the conventional statistics discussed above can be produced using the following summary for all variables in the dataset:
```{r}
summary(mtcarsData)
```


### Graphical analysis {#dataAnalysisGraphical}
Continuing our example with `mtcars` dataset, we now investigate what plots can be used for different types of data. As discussed earlier, we have two categorical variables: vs and am - and they need to be treated differently than the numerical ones. We can start by producing their barplots:

```{r barplotVS, fig.cap="Barplot for the engine type."}
barplotVS <- barplot(table(mtcarsData$vs), xlab="Type of engine")
text(barplotVS,table(mtcarsData$vs)/2,table(mtcarsData$vs),cex=1.25)
```

This is just a graphical presentation of the contingency table we have already discussed earlier. Note that histograms do not make sense in case of categorical variables, because they assume that variables are numerical and continuous. Barplots are useful when you deal with either categorical variables or integer numerical ones. Here is what we can produce in case of the integer variable `cyl`:

```{r barplotCYL, fig.cap="Barplot for the number of cylinders."}
barplotCYL <- barplot(table(mtcarsData$cyl), xlab="Number of cylinders")
text(barplotCYL,table(mtcarsData$cyl)/2,table(mtcarsData$cyl),cex=1.25)
```

Figure \@ref(fig:barplotCYL) shows that there are three types of cars in the data: with 4, 6 and 8 cylinders. The most frequently met is the car with 8 cylinders. Judging by the plot, half of cars have not more than 6 cylinders (median is equal to 6). All of this can be deducted from the barplot. And here how the histogram would look like for cylinders:

```{r histogramCYL, fig.cap="Histogram for the number of cylinders. Do not do this!"}
hist(mtcarsData$cyl)
```

Figure \@ref(fig:histogramCYL) is difficult to read, because on histogram, the bars show frequency at which continuous variable appears in pre-specified bins. In our case we would conclude that the most frequently cars in the dataset are those that have 7.5 - 8 cylinders, which is wrong and misleading. In addition, this basic plot does not have a readable label for x-axis and a meaningful title (in fact, we do not need one, given that we have caption). So, always label your axis and make sure that the text on plots is easy to understand for those people who do not work with the data.

Coming back to categorical variables, we can construct two-dimensional plots to investigate potential relations between variables. We will first try the same barplot as above, but with `vs` and `am` variables:

```{r barplotVSAM, fig.cap="Barplot for the type of engine and transmission."}
barplot(table(mtcarsData$vs,mtcarsData$am),
        xlab="Type of transmission", legend.text=levels(mtcarsData$vs))
```

Figure \@ref(fig:barplotVSAM) provides some information about the distribution of type of engine and transmission. For example, we can say that the most often met car in the dataset is the one with automatic transmission and V-shaped engine. However, it is not possible to say much about the relation between the two variables based on this plot. So, there is an alternative presentation, which uses the heat map (`tableplot()` from `greybox`):

```{r tableplotVSAM, fig.cap="Heat map for the type of engine and transmission."}
tableplot(mtcarsData$vs,mtcarsData$am,
          xlab="Type of engine", ylab="Type of transmission")
```

The idea of this plot is that the darkness of areas shows the frequency of occurrence of each specific value. The numbers inside the box show the proportions for each answer. So, we can conclude (again), that automatic transmission with V-shaped engine is met in 37.5% of cases. On the other hand, the least frequent type of car is the one with V-shaped engine and manual transmission. There might be some tendency in the dataset: the engine and transmission might be related (v-shaped with automatic vs Straigh with manual) - but it is not very well pronounced. The same table plot can be used for the analysis of relations between integer variables (and categorical). Here, for example, the plot between the number of cylinders and the type of engine:

```{r tableplotCYLVS, fig.cap="Heat map for the number of cylinders and the type of engine."}
tableplot(mtcarsData$cyl,mtcarsData$vs,
          xlab="Number of cylinders", ylab="Type of engine")
```

Figure \@ref(fig:tableplotCYLVS) allows making more solid conclusions about the relation between the two variables: we see that with the increase of the number of cylinders, the cars tend to switch from Straight to the V-shaped engines. This has an explanation: the engines with more cylinders need to have a different geometry to fit them all, and the V shape is more suitable for them. The table plot shows clearly this relation between the two variables.

Next, we can analyse the numerical variables. We can start with the basic histogram:

```{r histWeight, fig.cap="Distribution of the weights of cars."}
hist(mtcarsData$wt, xlab="Weight", main="", probability=TRUE)
lines(density(mtcarsData$wt), col="red")
```

The histogram \@ref(fig:histWeight) shows that there is a slight skewness in the data: the cars with weight from 3 to 4 thousands pounds are met more often than the cars with more than 5. The left tail of this distribution is slightly longer than the right one. Note that I have produced the probabilities on the y-axis of the plot in order to add the density curve, which smooths out the frequencies and shows how the distribution looks like.

An alternative presentation of the histogram is the boxplot, which graphically presents quantiles of distribution:

```{r boxWeight, fig.cap="Boxplot of the variable weight."}
boxplot(mtcarsData$wt, ylab="Weight")
points(mean(mtcarsData$wt),col="red", pch=16)
```

This plot has the box in the middle, the whiskers on the sides, points at the top and the red point at the centre. The box shows 1st, 2nd and 3rd quartiles of distribution, thus the black line in the middle is the median. The distance between the 1st and the 3rd quartiles is called "Interquartile range" (IQR) and is used for the calculation of the interval (1st / 3rd quartile $\pm 1.5 \times$IQR), which corresponds roughly to the 99.3% interval (read more about this in Section \@ref(confidenceIntervals)) from Normal distribution and is graphically drawn as the furthest observation in the interval. So, the lower whisker on our plot corresponds to the minimum value in the data, which is still in the interval, while the upper whisker corresponds to the bound of the interval. All the observations that lie beyond the interval are marked as potential outliers. Note that *this does not mean that the values are indeed outliers*, they just lie outside the 99.3% interval of Normal distribution. Finally, the red dot was added by me to show where the mean is. It is lower than median, this implies that there is a slight skewness in the distribution of weight.

There is also a way for producing the plots that would combine elements of histogram, density curve and boxplot. There is a plot called "violin plot". We will use `vioplot()` function from `vioplot` package in order to produce them:

```{r vioWeight, fig.cap="Violin plot together with boxplot of the variable weight."}
vioplot(mtcarsData$wt, ylab="Weight")
points(mean(mtcarsData$wt),col="red", pch=16)
```

Figure \@ref(fig:vioWeight) unites the boxplot and the density curve from the plots above, providing not only information about the quantiles, but also about the shape of the distribution.

Finally, if we want to compare the distribution of a variable with a known theoretical distribution, we can produce the QQ-plot. Here how it looks for Normal distribution:

```{r QQWeight, fig.cap="QQ plot of Normal distribution for variable weight."}
qqnorm(mtcarsData$wt)
qqline(mtcarsData$wt)
```

The idea of the plot on Figure \@ref(fig:QQWeight) is to compare theoretical quantiles with the empirical ones. If the variable would follow the specific distribution, then all the points would lie on the solid line. In our case, they do not: there are points in the right tail that are very far from the line - so we would conclude that the distribution of weight does not look Normal.

So far, we have discussed the univariate analysis of numerical variables, but we can also produce plots showing potential relations between them. We start with the classical scatterplot:

```{r scatterWeightMPG, fig.cap="Scatterplot diagram between weight and mileage."}
plot(mtcarsData$wt, mtcarsData$mpg, xlab="Weight", ylab="Mileage")
lines(lowess(mtcarsData$wt, mtcarsData$mpg), col="red")
```

The plot on Figure \@ref(fig:scatterWeightMPG) shows the observations that have specific weight and mileage. Based on this, we can see if there is a relation between variables or not and what sort of relation this is. In order to simplify analysis, I have added the lowess line to the plot. It smooths the relation between variables, drawing the smooth line through the points and helps in understanding the existing relations in the data. Judging by Figure \@ref(fig:scatterWeightMPG), there is a negative, slightly non-linear relation between the variables: the mileage decreases with reduced speed, when weight of a car increases. This relation makes sense, because heavier cars will consume more fuel and thus drive less on a gallon of petrol.

We could construct similar plots for all the other numerical variables, but not all plots would be helpful. For example, a plot of mileage versus number of forward gears would be very difficult to read (see Figure \@ref(fig:scatterGearMPG)).

```{r scatterGearMPG, fig.cap="Scatterplot diagram between weight and mileage."}
plot(mtcarsData$gear, mtcarsData$mpg, xlab="Number of gears", ylab="Mileage")
```

This is because one of the variables is integer and takes only a handful of values. In this case, a boxplot or a violin plot would be more useful:

```{r boxGearMPG, fig.cap="Boxplot of mileage vs number of gears."}
boxplot(mpg~gear, mtcarsData, xlab="Number of gears", ylab="Mileage")
points(tapply(mtcarsData$mpg, mtcarsData$gear, mean), col="red", pch=16)
```

The plot on Figure \@ref(fig:boxGearMPG) is more informative than the one on Figure \@ref(fig:scatterGearMPG): it shows how the distribution of mileage changes with the increase of the numeric variable number of gears. We can also see that the mean value first increases and then goes down slightly. I do not have any good explanation of this phenomenon, but it might be related with how efficient the cars become with the increase fo the number of gears, or this could happen due to some latent, unobserved factors. So, the data tells us that there is a non-linear relation between number of gears and mileage.

Similarly, we can produce violin plots for the same data using the following code:

```{r vioGearMPG, fig.cap="Violin plot of mileage vs number of gears.", eval=FALSE}
vioplot(mpg~gear, mtcarsData, xlab="Number of gears", ylab="Mileage")
points(tapply(mtcarsData$mpg, mtcarsData$gear, mean), col="red", pch=16)
```

Finally, using exactly the same idea with boxplots / violin plots, we can analyse relations between categorical and numerical variables. Figure \@ref(fig:vioAMMPG) shows the relation between transmission type and mileage. We can conclude that the cars with manual transmission tend to have a higher mileage than the ones with the automatic one in our dataset.

```{r vioAMMPG, fig.cap="Violin plot of mileage vs transmission type."}
vioplot(mpg~am, mtcarsData, xlab="Transmission type", ylab="Mileage")
points(tapply(mtcarsData$mpg, mtcarsData$am, mean), col="red", pch=16)
```

Finally, producing plots one by one might be a tedious and challenging task, so it is good to have some instruments for producing several of them together. The `plot()` method will produce scatterplot matrix for numerical variables, but does not deal well with integer and categorical variables:

```{r scatterMatrix, fig.cap="Scatterplot matrix for the mtcars dataset."}
plot(mtcars)
```

Figure \@ref(fig:scatterMatrix) is informative for the variables `mpg`, `cyl`, `disp`, `hp`, `drat`, `qsec` and `carb`, but is difficult to read for the others. In order to address this issue, we can use the `spread()` function from `greybox`, which will detect types of variables and produce the necessary plots automatically:

```{r spreadPlot, fig.cap="Spread plot for the mtcars dataset."}
spread(mtcarsData, lowess=TRUE)
```

The plot on Figure \@ref(fig:spreadPlot) is the collection of the plots discussed above, so I will not stop on explaining what it shows.

As a final word for this section, when analysing data, it is critically important not to just describe what we see, but also explain why a result or a relationship is meaningful, otherwise this becomes an exercise of stating the obvious which does not have any value. So, for example, concluding based on Figure \@ref(fig:spreadPlot) that the mileage has a negative relation with displacement is not enough. If you want to analyse the data properly, you need to explain that this relation is meaningful, because with the increase of the size of engine, the fuel consumption will increase as well, and as a result the mileage will go down. Furthermore, the relation is non-linear because the change in decrease will slow down with cars with bigger engines. Inevitably, the car with a gigantic engine will be able to travel a short distance on a gallon of fuel - the mileage will not become negative, so the non-linearity is not an artefact of the data, but an existing phenomenon.


## Law of Large Numbers and Central Limit Theorem {#LLNandCLT}
Consider a case, when you want to understand what is the average height of teenagers living in your town. It is very expensive and time consuming to go from one house to another and ask every single teenager (if you find one), what their hight is. If we could do that, we would get the true mean, true average height of teenagers living in the town. But in reality, it is more practical to ask a sample of teenagers and make conclusions about the "population" (all teenagers in the town) based on this sample. Indeed, you will spend much less time collecting the information about the height of 100 people rather than 100,000. However, when we take a sample of something, the statistics we work with will always differ from the truth: sample mean will never be equal to the true mean, but it can be shown mathematically that it will converge to the truth, when some specific conditions are met and when the sample size increases. If we set up the experiment correctly, then we can expect our statistics to follow some laws.

### Law of Large Numbers
The first law is called the **Law of Large Numbers** (LLN). It is the theorem saying that (under wide conditions) the average of a variable obtained over the large number of trials will be close to its expected value and will get closer to it with the increase of the sample size. This can be demonstrated with the following example:

```{r histY30Y1000, fig.cap="Histograms of samples of data from variable y.", }
obs <- 10000
# Generate data from normal distribution
y <- rnorm(obs,100,100)
# Create sub-samples of 50 and 100 observations
y30 <- sample(y, 30)
y1000 <- sample(y, 1000)
par(mfcol=c(1,2))
hist(y30, xlab="y")
abline(v=mean(y30), col="red")
hist(y1000, xlab="y")
abline(v=mean(y1000), col="red")
```

What we will typically see on the plots above is that the mean (red line) on the left plot will be further away from the true mean of 100 than in the case of the right plot. Given that this is randomly generated, the situation might differ, but the idea would be that with the increase of the sample size the estimated sample mean will converge to the true one. We can even produce a plot showing how this happens:

```{r eval=FALSE}
yMean <- vector("numeric",obs)
for(i in 1:obs){
    yMean[i] <- mean(sample(y,i))
}
plot(yMean, type="l", xlab="Sample size", ylab="Sample mean")
```

```{r statsSampleMean, fig.cap="Demonstration of Law of Large Numbers.", echo=FALSE}
knitr::include_graphics("images/02-statistics-LLN.png")
```

We can see from the plot above that with the increase of the sample size the sample mean reaches the true value of 100. This is a graphical demonstration of the Law of Large Numbers: it only tells us about what will happen when the sample size increases. But it is still useful, because it used for many statistical inferences and if it does not work, then the estimate of mean would be incorrect, meaning that we cannot make conclusions about the behaviour in population.

In order for LLN to work, the distribution of variable needs to have finite mean and variance. This is discussed in some detail in the next subsection. 

In summary, what LLN tells us is that if we average things out over a large number of observations, then that average starts looking very similar to the population value. However, this does not say anything about the performance of estimators on small samples.


### Central Limit Theorem
As we have already seen on Figure \@ref(fig:statsSampleMean), the sample mean is not exactly equal to the population mean even when the sample size is very large (thousands of observations). There is always some sort of variability around the population mean. In order to understand how this variability looks like, we could conduct a simple experiment. We could take a random sample of, for instance, 1000 observations several times and record each of the obtained means. We then can see how the variable will be distributed to see if there are any patterns in the behaviour of the estimator:

```{r histyMean, fig.cap="Histogram of the mean of the variable y."}
nIterations <- 1000
yMean <- vector("numeric",nIterations)
for(i in 1:nIterations){
    yMean[i] <- mean(sample(y,1000))
}
hist(yMean, xlab="Sample mean", main="")
```

There is a theorem that says that the distribution of mean in the experiment above will follow normal distribution under several conditions (discussed later in this section). It is called **Central Limit Theorem** (CLT) and very roughly it says that when independent random variables are added, their normalised sum will asymptotically follow normal distribution, even if the original variables do not follow it. Note that this is the theorem about *what happens with the estimate* (sum in this case), *not with individual observations*. This means that the error term might follow, for example, [Inverse Gaussian distribution](#IGDistribution), but the estimate of its mean (under some conditions) will follow normal distribution. There are different versions of this theorem, built with different assumptions with respect to the random variable and the estimation procedure, but we do not dicuss these details in this textbook.

In order for CLT to hold, the following important assumptions need to be satisfied:

1. **The true value of parameter is not near the bound**. e.g. if the variable follows uniform distribution on (0, $a$) and we want to estimate $a$, then its distribution will not be Normal (because in this case the true value is always approached from below). This assumption is important in our context, because ETS and ARIMA typically have restrictions on their parameters.
2. **The random variables are identically independent distributed** (i.i.d.). If they are not, then their average might not follow normal distribution (in some conditions it still might).
3. **The mean and variance of the distribution are finite**. This might seem as a weird issue, but some distributions do not have finite moments, so the CLT will not hold if a variable follows them, just because the sample mean will be all over the plane due to randomness and will not converge to the "true" value. Cauchy distribution is one of such examples.

If these assumptions hold, then CLT will work for the estimate of a parameter, no matter what the distribution of the random variable is. This becomes especially useful, when we want to test a hypothesis or construct a confidence interval for an estimate of a parameter.


## Properties of estimators {#estimatesProperties}
Before we move further, it is important to understand the terms **bias**, **efficiency** and **consistency** of estimates of parameters, which are directly related to [LLN and CLT](#LLNandCLT). Although there are strict statistical definitions of the aforementioned terms (you can easily find them in Wikipedia or anywhere else), I do not want to copy-paste them here, because there are only a couple of important points worth mentioning in our context.

Note that all the discussions in this chapter relate to **the estimates of parameters**, not to the distribution of a random variable itself. A common mistake that students make when studying statistics, is that they think that the properties apply to the variable $y_t$ instead of the estimate of its parameters (e.g. mean of $y_t$).


### Bias {#estimatesPropertiesBias}
**Bias** refers to the expected difference between the estimated value of parameter (on a specific sample) and the "[true](#intro)" one (in the true model). Having unbiased estimates of parameters is important because they should lead to more accurate forecasts (at least in theory). For example, if the estimated parameter is equal to zero, while in fact it should be 0.5, then the model would not take the provided information into account correctly and as a result will produce less accurate point forecasts and incorrect prediction intervals. In inventory context this may mean that we constantly order 100 units less than needed only because the parameter is lower than it should be.

The classical example of bias in statistics is the estimation of variance in sample. The following formula gives biased estimate of variance in sample:
\begin{equation}
    \mathrm{V}(y) = \frac{1}{T} \sum_{j=1}^T \left( y_t - \bar{y} \right)^2,
    (\#eq:varianceBiased)
\end{equation}
where $T$ is the sample size and $\bar{y} = \frac{1}{T} \sum_{j=1}^T y_t$ is the mean of the data. There is a lot of proofs in the literature of this issue (even @WikipediaVarianceBias2020 has one), we will not spend time on that. Instead, we will see this effect in the following simple simulation experiment:
```{r}
mu <- 100
sigma <- 10
nIterations <- 1000
# Generate data from normal distribution, 10,000 observations
y <- rnorm(10000,mu,sigma)
# This is the function, which will calculate the two variances
varFunction <- function(y){
   return(c(var(y), mean((y-mean(y))^2)))
}
# Calculate biased and unbiased variances for the sample of 30 observations,
# repeat nIterations times
varValues <- replicate(nIterations, varFunction(sample(y,30)))
```
This way we have generated 1000 samples with 30 observations and calculated variances using the formulae \@ref(eq:varianceBiased) and the corrected one for each step. Now we can plot it in order to see how it worked out:

```{r fig.cap="Histograms for biased and unbiased estimates of variance."}
par(mfcol=c(1,2))
# Histogram of the biased estimate
hist(varValues[2,], xlab="V(y)", ylab="y", main="Biased estimate of V(y)")
abline(v=mean(varValues[2,]), col="red")
legend("topright",legend=TeX(paste0("E$\\left(V(y)\\right)$=",round(mean(varValues[2,]),2))),lwd=1,col="red")

# Histogram of unbiased estimate
hist(varValues[1,], xlab="V(y)", ylab="y", main="Unbiased estimate of V(y)")
abline(v=mean(varValues[1,]), col="red")
legend("topright",legend=TeX(paste0("E$\\left(V(y)\\right)$=",round(mean(varValues[1,]),2))),lwd=1,col="red")

```

Every run of this experiment will produce different plots, but typically what we will see is that, the biased estimate of variance (the histogram on the right hand side of the plot) will have lower mean than the unbiased one. This is the graphical example of the effect of not taking the number of estimated parameters into account. The correct formula for the unbiased estimate of variance is:
\begin{equation}
    s^2 = \frac{1}{T-k} \sum_{j=1}^T \left( y_t - \bar{y} \right)^2,
    (\#eq:varianceUnBiased)
\end{equation}
where $k$ is the number of all independent estimated parameters. In this simple example $k=1$, because we only estimate mean (the variance is based on it). Analysing the formulae \@ref(eq:varianceBiased) and \@ref(eq:varianceUnBiased), we can say that with the increase of the sample size, the bias will disappear and the two formulae will give almost the same results: when the sample size $T$ becomes big enough, the difference between the two becomes negligible. This is the graphical presentation of the bias in the estimator.


### Efficiency {#estimatesPropertiesEfficiency}
**Efficiency** means, if the sample size increases, then the estimated parameters will not change substantially, they will vary in a narrow range (variance of estimates will be small). In the case with inefficient estimates the increase of sample size from 50 to 51 observations may lead to the change of a parameter from 0.1 to, let’s say, 10. This is bad because the values of parameters usually influence both point forecasts and prediction intervals. As a result the inventory decision may differ radically from day to day. For example, we may decide that we urgently need 1000 units of product on Monday, and order it just to realise on Tuesday that we only need 100. Obviously this is an exaggeration, but no one wants to deal with such an erratically behaving model, so we need to have efficient estimates of parameters.

Another classical example of not efficient estimator is the median, when used on the data that follows Normal distribution. Here is a simple experiment demonstrating the idea:
```{r eval=FALSE}
mu <- 100
sigma <- 10
nIterations <- 500
obs <- 100
varMeanValues <- vector("numeric",obs)
varMedianValues <- vector("numeric",obs)
y <- rnorm(100000,mu,sigma)
for(i in 1:obs){
    ySample <- replicate(nIterations,sample(y,i*100))
    varMeanValues[i] <- var(apply(ySample,2,mean))
    varMedianValues[i] <- var(apply(ySample,2,median))
}
```

In order to establish the efficiency of the estimators, we will take their variances and look at the ratio of mean over median. If both are equally efficient, then this ratio will be equal to one. If the mean is more efficient than the median, then the ratio will be less than one:

```{r eval=FALSE}
options(scipen=6)
plot(1:100*100,varMeanValues/varMedianValues, type="l", xlab="Sample size",ylab="Relative efficiency")
abline(h=1, col="red")
```

```{r statsEfficiecny, fig.cap="An example of a relatively inefficient estimator.", echo=FALSE}
knitr::include_graphics("images/02-statistics-efficiency.png")
```

What we should typically see on this graph, is that the black line should be below the red one, indicating that the variance of mean is lower than the variance of the median. This means that mean is more efficient estimator of the true location of the distribution $\mu$ than the median. In fact, it is easy to proove that asymptotically the mean will be 1.57 times more efficient than median [@WikipediaMedianEfficiency2020] (so, the line should converge approximately to the value of 0.64).


### Consistency {#estimatesPropertiesConsistency}
**Consistency** means that our estimates of parameters will get closer to the stable values (true value in the population) with the increase of the sample size. This follows directly from [LLN](#LLNandCLT) and is important because in the opposite case estimates of parameters will diverge and become less and less realistic. This once again influences both point forecasts and prediction intervals, which will be less meaningful than they should have been. In a way consistency means that with the increase of the sample size the parameters will become more efficient and less biased. This in turn means that the more observations we have, the better.

An example of inconsistent estimator is Chebyshev (or max norm) metric. It is formulated the following way:
\begin{equation}
    \mathrm{LMax} = \max \left(|y_1-\hat{y}|, |y_2-\hat{y}|, \dots, |y_T-\hat{y}| \right).
    (\#eq:chebyshevNorm)
\end{equation}
Minimising this norm, we can get an estimate $\hat{y}$ of the location parameter $\mu$. The simulation experiment becomes a bit more tricky in this situation, but here is the code to generate the estimates of the location parameter:
```{r eval=FALSE}
LMax <- function(y){
    estimator <- function(par){
        return(max(abs(y-par)));
    }
    
    return(optim(mean(y), fn=estimator, method="Brent", lower=min(y), upper=max(y)));
}

mu <- 100
sigma <- 10
nIterations <- 1000
y <- rnorm(10000, mu, sigma)
LMaxEstimates <- vector("numeric", nIterations)
for(i in 1:nIterations){
    LMaxEstimates[i] <- LMax(y[1:(i*10)])$par;
}
```

And here how the estimate looks with the increase of sample size:
```{r eval=FALSE}
plot(1:nIterations*10, LMaxEstimates, type="l", xlab="Sample size",ylab=TeX("Estimate of $\\mu$"))
abline(h=mu, col="red")
```

```{r statsConsistency, fig.cap="An example of inconsistent estimator.", echo=FALSE}
knitr::include_graphics("images/02-statistics-consistency.png")
```

While in the example with bias we could see that the lines converge to the red line (the true value) with the increase of the sample size, the Chebyshev metric example shows that the line does not approach the true one, even when the sample size is 10000 observations. The conclusion is that when Chebyshev metric is used, it produces inconsistent estimates of parameters.


::: remark
There is a prejudice in the world of practitioners that the situation in the market changes so fast that the old observations become useless very fast. As a result many companies just through away the old data. Although, in general the statement about the market changes is true, the forecasters tend to work with the models that take this into account (e.g. Exponential smoothing, ARIMA, discussed in this book). These models adapt to the potential changes. So, we may benefit from the old data because it allows us getting more consistent estimates of parameters. Just keep in mind, that you can always remove the annoying bits of data but you can never un-throw away the data.
:::


### Asymptotic normality {#asymptoticNormality}
Finally, **asymptotic normality** is not critical, but in many cases is a desired, useful property of estimates. What it tells us is that the distribution of the estimate of parameter will be well behaved with a specific mean (typically equal to $\mu$) and a fixed variance. This follows directly from [CLT](#LLNandCLT). Some of the statistical tests and mathematical derivations rely on this assumption. For example, when one conducts a significance test for parameters of model, this assumption is implied in the process. If the distribution is not Normal, then the confidence intervals constructed for the parameters will be wrong together with the respective t- and p- values.

Another important aspect to cover is what the term **asymptotic**, which we have already used, means in our context. Here and after in this book, when this word is used, we refer to an unrealistic hypothetical situation of having all the data in the multiverse, where the time index $t \rightarrow \infty$. While this is impossible in practice, the idea is useful, because asymptotic behaviour of estimators and models is helpful on large samples of data. Besides, even if we deal with small samples, it is good to know what to expect to happen if the sample size increases.



## Confidence and prediction intervals {#confidenceIntervals}
As mentioned in the previous section, we always work with samples and inevitably we deal with randomness just because of that even, when there are no other sources of uncertainty in the data. For example, if we want to estimate the mean of a variable based on the observed data, the value we get will differ from one sample to another. This should have become apparent from the examples we discussed [earlier](#estimatesProperties). And, if the [LLN and CLT](#LLNandCLT) hold, then we know that the estimate of our parameter will have its own distribution and will converge to the population value with the increase of the sample size. This is the basis for the confidence and prediction interval construction, discussed in this section. Depending on our needs, we can focus on the uncertainty of either the estimate of a parameter, or the random variable $y$ itself. When dealing with the former, we typically work with the **confidence interval** - the interval constructed for the estimate of a parameter, while in the latter case we are interested in the **prediction interval** - the interval constructed for the random variable $y$.

In order to simplify further discussion in this section, we will take the population mean and its in-sample estimate as an example. In this case we have:

1. A random variable $y$, which is assumed to follow some distribution with finite mean $\mu$ and variance $\sigma^2$;
2. A sample of size $T$ from the population of $y$;
3. Estimates of mean $\hat{\mu}=\bar{y}$ and variance $\hat{\sigma}^2 = s^2$, obtained based on the sample of size $T$.


### Confidence interval
What we want to get by doing this is an idea about the population mean $\mu$. The value $\bar{y}$ does not tell us much on its own due to randomness and if we do not capture its uncertainty, we will not know, where the true value $\mu$ can be. But using LLN and CLT, we know that the sample mean should converge to the true one and should follow normal distribution. So, the distribution of the sample mean would look like this (Figure \@ref(fig:normalCurveBasic)).

```{r normalCurveBasic, fig.cap="Distribution of the sample mean.", echo=FALSE}
hist(rnorm(10000), xlab=TeX("$\\bar{y}$"), ylab="Density", xlim=c(-5,5), axes=FALSE, main="")
par(new=TRUE)
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), xlab="", ylab="", type="l")
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=1, lty=1, col="darkblue")
lines(c(-5,5), c(0,0), col="black", lwd=1)
abline(v=0, col="darkblue", lwd=2)
legend("topright",legend=c(TeX("$\\mu$")),
       lwd=c(2), col=c("darkblue"))
```

On its own, this distribution just tells us that the variable is random around the true mean $\mu$ and that its density function has a bell-like shape. In order to make this more useful, we can construct the **confidence interval** for it, which would tell us where the true parameter is most likely to lie. We can cut the tails of this distribution to determine the width of the interval, expecting it to cover $(1-\alpha)\times 100$% of cases. In the ideal world, asymptotically, the confidence interval will be constructed based on the true value, like this:

```{r normalCurveIntervals, echo=FALSE, fig.cap="Distribution of the sample mean and the confidence interval based on the population data."}
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), xlab=TeX("$\\bar{y}$"), ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1)),rep(0,length(seq(-5,5,0.1)))), col="grey95", lty=0)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=1, lty=1, col="darkblue")
lines(c(-5,5), c(0,0), col="black", lwd=1)
polygon(c(seq(-5,qnorm(0.025),0.01),rev(seq(-5,qnorm(0.025),0.01))),
        c(dnorm(seq(-5,qnorm(0.025),0.01)), rep(0,length(seq(-5,qnorm(0.025),0.01)))), col="grey")
polygon(c(seq(qnorm(0.975),5,0.01),rev(seq(qnorm(0.975),5,0.01))),
        c(dnorm(seq(qnorm(0.975),5,0.01)), rep(0,length(seq(qnorm(0.975),5,0.01)))), col="grey")
abline(v=qnorm(c(0.025,0.975)), col="darkred", lwd=2)
abline(v=0, col="darkblue", lwd=2)
legend("topright",legend=c(TeX("$\\mu$"),"bounds",TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkblue","darkred","grey95","grey"))
```

Figure \@ref(fig:normalCurveIntervals) shows the classical normal distribution curve around the population mean $\mu$, confidence interval of the level $1-\alpha$ and the cut off tails, the overall surface of which corresponds to $\alpha$. The value $1-\alpha$ is called **confidence level**, while $\alpha$ is the **significance level**. By constructing the interval this way, we expect that in the $(1-\alpha)\times 100$% of cases the value will be inside the bounds, and in $\alpha\times 100$% it will not.

In reality we do not know the true mean $\mu$, so we do a slightly different thing: we construct a confidence interval based on the sample mean $\bar{y}$ and sample variance $s^2$, hoping that due to [LLN](#LLNandCLT) they will converge to the true values. We use Normal distribution, because we expect [CLT](#LLNandCLT) to work. This process looks something like in Figure \@ref(fig:normalCurveIntervalsShades), with the bell curve in the background representing the true distribution for the sample mean and the curve on the foreground representing the assumed distribution based on our sample:

```{r normalCurveIntervalsShades, echo=FALSE, fig.cap="Distribution of the sample mean and the confidence interval based on a sample."}
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1),-1), xlab=TeX("$\\bar{y}$"), ylab="Density", type="l", xlim=c(-5,5),
     lwd=1,lty=2, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1),-1),rep(0,length(seq(-5,5,0.1)))),col="grey95",lty=0)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1),-1), lwd=2, lty=1, col="darkblue")
abline(v=-1, col="darkblue", lwd=2, lty=2)
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1),0),rep(0,length(seq(-5,5,0.1)))),col="white",lty=0)
lines(c(-5,5), c(0,0), col="black", lwd=1)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1),-1), lwd=1, lty=2, col="darkblue")
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=2, lty=1, col="darkgreen")
lines(c(-1,-1), c(0,dnorm(-1)), col="darkblue", lwd=1, lty=2)
abline(v=qnorm(c(0.025,0.975)), col="darkred", lwd=2)
abline(v=0, col="darkgreen", lwd=2, lty=2)
legend("topright",legend=c(TeX("$\\mu$"),"Sample mean","Bounds","True PDF","Assumed PDF"),
       lwd=c(2,2,2,2,2), col=c("darkblue","darkgreen","darkred","darkblue","darkgreen"), lty=c(2,2,1,1,1))
```

So, what the confidence interval does in reality is tries to cover the unknown population mean, based on the sample values of $\bar{y}$ and $s^2$. If we construct the confidence interval of the width $1-\alpha$ (e.g. 0.95) for thousands of random samples (thousands of trials), then in $(1-\alpha)\times 100$% of cases (e.g. 95%) the true mean will be covered by the interval, while in $\alpha \times 100$% cases it will not be. The interval itself is random, and we rely on LLN and CLT, when constructing it, expecting for it to work asymptotically, with the increase of the number of trials.

Mathematically the red bounds in Figure \@ref(fig:normalCurveIntervalsShades) are represented using the following well-known formula for the confidence interval:
\begin{equation}
    \mu \in (\bar{y} + t_{\alpha/2}(df) s_{\bar{y}}, \bar{y} + t_{1-\alpha/2}(df) s_{\bar{y}}),
    (\#eq:confidenceInterval)
\end{equation}
where $t_{\alpha/2}(df)$ is Student's t-statistics for $df=T-k$ degrees of freedom ($T$ is the sample size and $k$ is the number of estimated parameters, e.g. $k=1$ in our case) and level $\alpha/2$, and $s_{\bar{y}}=\frac{1}{\sqrt{T}}s$ is the estimate of the standard deviation of the sample mean. If we knew for some reason the true variance $\sigma^2$, then we could use z-statistics instead of t, but we typically do not, so we need to take the uncertainty about the variance into account as well, thus the use of t-statistics.

Note, that in order to construct *confidence interval*, we do not care what distribution $y$ follows, as long as LLN and CLT hold.

### Prediction interval
If we are interested in capturing the uncertainty about the random variable $y$, then we should refer to prediction interval. In this case, we typically rely on [LLN](#LLNandCLT) and the [assumed distribution](#distributions) for the random variable $y$. For example, if we know that $y \sim \mathcal{N}(\mu, \sigma^2)$, then based on our sample we can construct a prediction interval of the width $1-\alpha$:
\begin{equation}
    y \in (\bar{y} + z_{\alpha/2} s, \bar{y} + z_{1-\alpha/2} s),
    (\#eq:predictionInterval)
\end{equation}
where $z_{\alpha/2}$ is the z-statistics (quantile of standard normal distribution) for the level $\alpha/2$ and $\bar{y}$ is the sample estimate of $\mu$ and $s$ is the sample estimate of $\sigma$. If we assume some other distributions for the random variable, the formula would change. In a way, the prediction interval just comes to getting the quantiles of the assumed distribution based on estimated parameters. In some cases, when some of the [assumptions](#assumptions) do not hold, we might switch to more advanced methods for prediction interval construction.


## Hypothesis testing {#hypothesisTesting}
While we will not need hypothesis testing for ADAM itself, we might face it in some parts of the textbook, so it is worth discussing it briefly.

Hypothesis testing arises naturally from the idea of confidence intervals: instead of constructing the interval and getting the idea about the uncertainty of the parameter, we could check, whether the sample agrees with our expectations or not. For example, we could test, whether the population mean is equal to zero based on our sample. We could either construct a confidence interval for the sample mean and see if zero is included in it (in which case it might indicate that zero is one of the possible values of the population mean), or we could reformulate the problem and compare some calculated value with the theoretical threshold. The latter approach is in the nutshell what hypothesis testing does.

Fundamentally, the hypothesis testing process relies on the ideas of induction and dichotomy: we have a null (H$_0$) and alternative (H$_1$) hypotheses about the process or a property in the population, and we want to find some evidence to reject the H$_0$. Rejecting a hypothesis is actually more useful than not rejecting it, because in the former case we know what not to expect from the data, while in the latter we just might not have enough evidence to make any solid conclusion. For example, we could formulate H$_0$ that all cats are white. Failing to reject this hypothesis based on the data that we have (e.g. a dataset of white cats) does not mean that they are all (in the universe) indeed white, it just means that we have not observed the non-white ones. If we collect enough evidence to reject H$_0$ (i.e. encountered a black cat), then we can conclude that not all cats are white. This is a more solid conclusion than the one in the previous case. So, if you are interested in a specific outcome, then it makes sense to put this in the alternative hypothesis and see if the data allows to reject the null. For example, if we want to see if the average salary of professors in the UK is higher than £100k per year we would formulate the hypotheses in the following way:
\begin{equation*}
    \mathrm{H}_0: \mu \leq 100, \mathrm{H}_1: \mu > 100.
\end{equation*}
Having formulated hypotheses, we can check them, but in order to do that, we need to follow a proper procedure, which can be summarised in the six steps:

1. Formulate null and alternative hypotheses (H$_0$ and H$_1$) based on your understanding of the problem;
2. Select the significance level $\alpha$ on which the hypothesis will be tested;
3. Select the test appropriate for the formulated hypotheses (1);
4. Conduct the test (3) and get the calculated value;
5. Compare the value in (4) with the threshold one;
6. Make a conclusion based on (5) on the selected level (2).

Note that the order of some elements might change depending on the circumstances, but (2) should always happen before (4), otherwise we might be dealing with so called "p-hacking", trying to make results look nicer than they really are.

Consider an example, where we want to check, whether the population mean $\mu$ is equal to zero, based on a sample of 36 observations, where $\bar{y}=-0.5$ and $s^2=1$. In this case, we formulate the null and alternative hypotheses:
\begin{equation*}
    \mathrm{H}_0: \mu=0, \mathrm{H}_1: \mu \neq 0.
\end{equation*}
We then select the significance level $\alpha=0.05$ (just as an example) and select the test. Based on the description of the task, this can be either a t-test, or a z-test, depending on whether the variance of the variable is known or not. Usually it is not, so we tend to use t-test. We then conduct the test using the formula:
\begin{equation}
    t = \frac{\bar{y} - \mu}{s_{\bar{y}}} = \frac{-0.5 - 0}{\frac{1}{\sqrt{36}}} = -3 .
    (\#eq:ttestFormula)
\end{equation}
After that we get the critical value of t with $df=36-1=35$ degrees of freedom and significance level $\alpha/2=0.025$, which is approximately equal to `r round(qt(0.025, 36-1),3)`. We compare this value with the \@ref(eq:ttestFormula) by absolute and reject H$_0$ if the calculated value is higher than the critical one. In our case it is, so it appears that we have enough evidence to say that the population mean is not equal to 0, on the 5% significance level.

Visually, the whole process of hypothesis testing explained above can be represented in the following way:

```{r hypothesisTesting01, echo=FALSE, fig.cap="The process of hypothesis testing with t value."}
plot(seq(-5,5,0.1), dt(seq(-5,5,0.1), 36-1), xlab="t", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dt(seq(-5,5,0.1), 36-1),rep(0,length(seq(-5,5,0.1)))), col="grey95", lty=0)
lines(seq(-5,5,0.1), dt(seq(-5,5,0.1), 36-1), lwd=1, lty=1, col="darkgreen")
lines(c(-5,5), c(0,0), col="black", lwd=1)
polygon(c(seq(-5,qt(0.025, 36-1),0.01),rev(seq(-5,qt(0.025, 36-1),0.01))),
        c(dt(seq(-5,qt(0.025, 36-1),0.01),36-1), rep(0,length(seq(-5,qt(0.025, 36-1),0.01)))), col="grey")
polygon(c(seq(qt(0.975, 36-1),5,0.01),rev(seq(qt(0.975, 36-1),5,0.01))),
        c(dt(seq(qt(0.975, 36-1),5,0.01),36-1), rep(0,length(seq(qt(0.975, 36-1),5,0.01)))), col="grey")
abline(v=qt(c(0.025,0.975),36-1), col="darkred", lwd=2)
abline(v=-3, col="darkblue", lwd=2)
legend("topright",legend=c("critical values","calculated t",TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkred","darkblue","grey95","grey"))
```

If the blue line on Figure \@ref(fig:hypothesisTesting01) would lie inside the red bounds (i.e. the calculated value is less than the critical value by absolute), then we would fail to reject H$_0$. But in our example it is outside the bounds, so we have enough evidence to conclude that the population mean is not equal to zero on 5% significance level. Notice, how similar the mechanisms of confidence interval construction and hypothesis testing are. This is because they are one and the same thing, presented differently. In fact, we could test the same hypothesis by constructing the 95% confidence interval using \@ref(eq:confidenceInterval) and checking, whether the interval covers the $\mu=0$:
\begin{equation*}
    \begin{aligned}
        & \mu \in \left(-0.50 -2.03 \frac{1}{\sqrt{36}}, -0.50 + 2.03 \frac{1}{\sqrt{36}} \right), \\
        & \mu \in (-0.84, -0.16).
    \end{aligned}
\end{equation*}
In our case it does not, so we conclude that we reject H$_0$ on 5% significance level. This can be roughly represented by the graph on  Figure \@ref(fig:hypothesisTesting02):

```{r hypothesisTesting02, echo=FALSE, fig.cap="The process of hypothesis testing with t value."}
plot(seq(-1.5,0.5,0.01), dt((seq(-1.5,0.5,0.01)+0.5)*6, 36-1), xlab="Sample mean", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-1.5,0.5,0.01),rev(seq(-1.5,0.5,0.01))),
        c(dt((seq(-1.5,0.5,0.01)+0.5)*6, 36-1),rep(0,length(seq(-1.5,0.5,0.01)))), col="grey95", lty=0)
lines(seq(-1.5,0.5,0.01), dt((seq(-1.5,0.5,0.01)+0.5)*6, 36-1), lwd=1, lty=1, col="darkgreen")
lines(c(-1.5,0.5), c(0,0), col="black", lwd=1)
polygon(c(seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01),rev(seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01))),
        c(dt((seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01)+0.5)*6,36-1), rep(0,length(seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01)))), col="grey")
polygon(c(seq(qt(0.975, 36-1)/6-0.5,0.5,0.01),rev(seq(qt(0.975, 36-1)/6-0.5,0.5,0.01))),
        c(dt((seq(qt(0.975, 36-1)/6-0.5,0.5,0.01)+0.5)*6,36-1), rep(0,length(seq(qt(0.975, 36-1)/6-0.5,0.5,0.01)))), col="grey")
abline(v=qt(c(0.025,0.975),36-1)/6-0.5, col="darkred", lwd=2)
abline(v=0, col="darkblue", lwd=2)
legend("topright",legend=c("bounds",TeX("$\\mu$=0"),TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkred","darkblue","grey95","grey"))
```

Note that the positioning of the blue line has changed in the case of confidence interval, which happens because of the transition from \@ref(eq:ttestFormula) to \@ref(eq:confidenceInterval). The idea and the message, however, stay the same: if the value is not inside the light grey area, then we reject H$_0$ on the selected significance level.

Also **note** that we never say that we accept H$_0$, because this is not what we do in hypothesis testing: if the value would lie inside the interval, then this would only mean that our sample shows that the tested value is covered by the region - the true value can be any of the numbers between the bounds.

Finally, there is a third way to test the hypothesis. We could calculate how much surface is left in the tails with the cut off of the assumed distribution by the blue line on Figure \@ref(fig:hypothesisTesting01) (calculated value). In R this can be done using the `pt()` function:
```{r}
pt(-3, 36-1)
```
Given that we had the inequality in the alternative hypothesis, we need to consider both tails, multiplying the value by 2 to get approximately `r round(pt(-3, 36-1)*2, 4)`. This is the significance level, for which the switch from "reject" to "do not reject" happens. We could compare this value with the pre-selected significance level directly, rejecting H$_0$ if it is lower than $\alpha$. This value is called "p-value" and simplifies the hypothesis testing, because we do not need to look at critical values or construct the confidence interval. There are different definitions of what it is, I personally find the following easier to comprehend: **p-value** is the smallest significance level at which a null hypothesis can be rejected.

Despite this simplification, we still need to follow the procedure and select $\alpha$ *before conducting the test*! We should not change the significance level after observing the p-values, otherwise we might end up bending reality for our needs. Also *note* that if in one case p-value is 0.2, while in the other it is 0.3, it does not mean that the the first case is more significant than the second! P-values are not comparable with each other and they do not tell you about the size of significance. *This is still a binary process*: we either reject, or fail to reject H$_0$, depending on whether p-value is smaller or greater than the selected significance level.

While p-value is a comfortable instrument, I personally prefer using confidence intervals, because they show the uncertainty clearer and are less confusing. Consider the following cases to see what I mean:

1. We reject H$_0$ because t-value is -3, which is smaller than the critical value of `r round(qt(0.025, 36-1),3)` (or equivalently the absolute of t-value is 3, while the critical is `r round(qt(0.975, 36-1),3)`);
2. We reject H$_0$ because p-value is `r round(pt(-3, 36-1)*2, 4)`, which is smaller than the significance level $\alpha=0.05$;
3. The confidence interval for the mean is $\mu \in (-0.84, -0.16)$. It does not include zero, so we reject $\mathrm{H}_0$.

In case of (3), we not only get the same message as in (1) and (2), but we also see how far the bound is from the tested value. In addition, in the situation, when we fail to reject H$_0$, the approach (3) gives more appropriate information. Consider the case, when we test, whether $\mu=-0.6$ in the example above. We then have the following three approaches to the problem:

1. We fail to reject H$_0$ because t-value is 0.245, which is smaller than the critical value of `r round(qt(0.975, 36-1),3)`;
2. We fail to reject H$_0$ because p-value is 0.808, which is greater than the significance level $\alpha=0.05$;
3. The confidence interval for the mean is $\mu \in (-0.84, -0.16)$. It includes -0.6, so we fail to reject H$_0$. *This does not mean that the true mean is indeed equal to -0.6*, but it means that the region will cover this number in 95% of cases if we do resampling many times.

In my opinion, the third approach is more informative and saves from making wrong conclusions about the tested hypothesis, making you work a bit more (you cannot change the confidence level on the fly, you would need to reconstruct the interval). Having said that, either of the three is fine, as long as you understand what they really imply.

Furthermore, if you do hypothesis testing and use p-values, it is worth mentioning the statement of American Statistical Association about p-values [@Wasserstein2016]. Among the different aspects discussed in this statement, there is a list of principles related to p-values, which I cite below:

1. P-values can indicate how incompatible the data are with a specified statistical model;
2. P-values do not measure:
- the probability that the studied hypothesis is true,
- or the probability that the data were produced by random chance alone;
3. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold;
4. Proper inference requires full reporting and transparency;
5. A p-value, or statistical significance, does not measure:
- the size of an effect
- or the importance of a result;
6. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.

The statement provides more details about that, but summarising, whatever hypothesis you test and however you test it, you should have apriori understanding of the problem. Diving in the data and trying to see what floats (i.e. which of the p-values is higher than $\alpha$) is not a good idea [@Wasserstein2016]. Follow the proper procedure if you want to test the hypothesis.

Finally, the hypothesis testing mechanism has been criticised by many scientists over the years. For example, @Cohen1994 discussed issues with the procedure, making several important points, some of which are outlined above. He also points out at the fundamental problem with hypothesis testing, which is typically neglected by proponents of the procedure: in practice, null hypothesis is always wrong. In reality, it is not possible for a value to be equal, for example, to zero. Even an unimportant effect of one variable on another would be close to zero, but not equal to it. This means that with the increase of the sample size, H$_0$ will inevitably be rejected. Furthermore, our mind operates with binary constructs: true / not true - while the hypothesis testing implies that there is a third one: "I don't know" - when there is not enough evidence to reject H$_0$. Summarising all of this, in my opinion, it makes sense to move away from hypothesis testing if possible and switch to other instruments for uncertainty measurement, such as confidence intervals.


### Common mistakes related to hypothesis testing {#hypothesisTestingMistakes}
Over the years of teaching statistics, I have seen many different mistakes, related to hypothesis testing. No wonder, this is a difficult topic to grasp. Here, I have decided to summarise several typical erroneous statements, providing explanations why they are wrong. They partially duplicate the 6 principles from ASA discussed above, but they are formulated slightly differently.

1. "Calculated value is lower than the critical one, so the null hypothesis is true".
- This is wrong on so many level, that I do not even know where to start. We can never know if the hypothesis is true or wrong. All the evidence might point towards the H$_0$ being correct, but it still can be wrong and at some point in future one observation might reject it. The classical example is the "Black swan in Australia". Up until the discover of Australia, the Europeans thought that there only exist white swans. This was supported by all the observations they had. Wise people would say that "We fail to reject H$_0$ that all swans are white". Uneducated people would be tempted to say that " H$_0$: All swans are white" is true. After discovering Australia in 1606, Europeans have collected evidence of existence of black swans, thus rejecting H$_0$ and showing that "not all swans are white", which implies that actually the alternative hypothesis is true. This is the essence of scientific method: we always try rejecting H$_0$, collecting some evidence. If we fail to reject it, it might just mean that we have not collected enough evidence or have not modelled it correctly.
2. "Calculated value is lower than the critical one, so we accept the null hypothesis".
- We **never** accept null hypothesis. Even if your house is on fire or there is a tsunami coming, you should not "accept H$_0$". This is a fundamental statistical principle. We collect evidence to reject the null hypothesis. If we do not have enough evidence, then we just fail to reject it, but we can never accept it, because failing to reject just means that we need to collect more data. As mentioned earlier, we focus on rejecting hypothesis, because this at least tells us, what the phenomenon is not (e.g. that not all swans are white).
3. "The parameter in the model is significant, so we can conclude that..."
- You cannot conclude if something is significant or not without specifying the significance level. Things are only significant if they pass specific test on a specified level $\alpha$. The correct sentence would be "The parameter in the model is significant on 3%, so we can conclude that...", where 3% is the selected significance level $\alpha$.
3. "The parameter in the model is significant because p-value<0.0000"
- Indeed, some statistical software will tell you that `p-value<0.0000`, but this just says that the value is very small and cannot be printed. Even if it is that small, you need to state your significance level and compare it with the p-value. You might wonder, "why bother if it is that low?". Well, if you change the sample size or change model specification, your p-value will change as well, and in some cases it might all of a sudden become higher than your significance level. So, you always need to keep it in mind and make conclusions based on the significance level, not just based on what software tells you.
5. "The parameter is not significant, so we remove the variable from the model".
- This is one of the worst motivations for removing variables that there is (statistical blasphemy!). There are thousands of reasons, why you might get p-value greater than your significance level ([assumptions](#assumptions) do not hold, sample is too small, the test is too weak, the true value is small etc) and only one of them is that the explanatory variable does not impact the response variable and thus you fail to reject H$_0$. Are you sure that you face exactly this one special case? If yes, then you already have some other (better) reasons to remove the variable. This means that you should not make decisions just based on the results of a statistical test. *You always need to have some fundamental reason to include or remove variables in the model*. Hypothesis testing just gives you additional information that can be helpful for decision making.
6. "The parameter in the new model is more significant than in the old one".
- There is no such thing as "more significant" or "less significant". **Significance is binary** and depends on the selected level. The only thing you can conclude is whether the parameter is significant on the chosen level $\alpha$ or not.
7. "The p-value of one variables is higher than the p-value of another, so...".
- p-values are not comparable between variables. They only show on what level the hypothesis is rejected and only work together with the chosen significance level. (6) is similar to this mistake.

Remember that the p-value itself is random and will change if you change the sample size or the model specification. Always keep this in mind, when conducting statistical tests. All these mistakes typically arise because of the misuse of p-values and hypothesis testing mechanism. This is one of the reasons, why I prefer [confidence intervals](#confidenceIntervals), when possible (as discussed above).

Finally, a related question to all of this, is *how to select the significance level*. Dave Worthington, a colleague of mine and a Statistics mentor at Lancaster University, has proposed an interesting motivation for that. If you do not have a level, driven by the problem (e.g. we need to satisfy 99% of demand, thus the significance level is 1%), then select the one for your life time. In how many cases in your life would you be ready to make a mistake? Would it be 5%? 3%? 1%? Select something and stick with it. Then over the years you will know that you have made the selected proportion of mistakes, when conducting different statistical test in variaous circumstances.


## Correlation and measures of association {#correlations}
Now that we have discussed confidence intervals and hypothesis testing, we can move towards the analysis of relations between variables, in a way continuing the preliminary data analysis that we finished in Section \@ref(dataAnalysis). We continue using the same dataset `mtcarsData` with the two categorical variables, `am` and `vs`.

### Nominal scale
As discussed in Section \@ref(scales), not all scales support the more advanced operations (such as taking mean in ordinal scale). This means that if we want to analyse relations between variables, we need to use appropriate instrument. The coefficients that show relations between variables are called "**measures of association**". We start their discussions with the simplest scale - nominal.

There are several measures of association for the variables in nominal scale. They are all based on calculating the number of specific values of variables, but use different formulae. The first one is called **contingency coefficient $\phi$** and can only be calculated between variables that have only two values. As the name says, this measure is based on the contingency table. Here is an example:

```{r}
table(mtcarsData$vs,mtcarsData$am)
```

The $\phi$ coefficient is calculated as:
\begin{equation}
    \phi = \frac{n_{1,1} n_{2,2} - n_{1,2} n_{2,1}}{\sqrt{n_{1,\cdot}\times n_{2,\cdot}\times n_{\cdot,1}\times n_{\cdot,2}}} ,
    (\#eq:measuresAssociationPhi)
\end{equation}
where $n_{i,j}$ is the element of the table on row $i$ and column $j$, $n_{i,\cdot}=\sum_{j}n_{i,j}$ - is the sum in row $i$ and $n_{\cdot,j}=\sum_{i} n_{i,j}$ - is the sum in column $j$. This coefficient lies between -1 and 1 and has a simple interpretation: if will be close to 1, when the elements on diagonal are greater than the off-diagonal ones, implying that there is a relation between variables. The value of -1 can only be obtained, when off-diagonal elements are non-zero, while the diagonal ones are zero. Finally, if the values in the contingency table are distributed evenly, the coefficient will be equal to zero. In our case the value of $\phi$ is:

```{r}
(12*7 - 6*7)/sqrt(19*13*14*18)
```
This is a very low value, so even if the two variables are related, the relation is not well pronounced. In order to see, whether this value is statistically significantly different from zero, we could test a statistical hypothesis (hypothesis testing was discussed in Section \@ref(hypothesisTesting)):

$H_0$: there is no relation between variables

$H_1$: there is some relation between variables

This can be done using **$\chi^2$ test**, the statistics for which is calculated via:
\begin{equation}
    \chi^2 = \sum_{i,j} \frac{n \times n_{i,j} - n_{i,\cdot} \times n_{\cdot,j}}{n \times n_{i,\cdot} \times n_{\cdot,j}} ,
    (\#eq:measuresAssociationChi)
\end{equation}
where $n$ is the sum of elements in the contingency table. The value calculated based on \@ref(eq:measuresAssociationChi) will follow $\chi^2$ distribution with $(r-1)(c-1)$ degrees of freedom, where $r$ is the number of rows and $c$ is the number of columns in contingency table. This is a proper statistical test, so it should be treated as one. We select my favourite significance level, 1% and can now conduct the test:

```{r}
chisq.test(table(mtcarsData$vs,mtcarsData$am))
```

Given that p-value is greater than 1%, we fail to reject the null hypothesis and can conclude that the relation does not seem to be different from zero - we do not find a relation between the variables in our data.

The main limitation of the coefficient $\phi$ is that it only works for the $2\times 2$ tables. In reality we can have variables in nominal scale that take several values and it might be useful to know relations between them. For example, we can have a variable `colour`, which takes values `red`, `green` and `blue` and we would want to know if it is related to the transmission type. We do not have this variable in the data, so just for this example, we will create one (using multinomial distribution):

```{r eval=FALSE}
colour <- c(1:3) %*% rmultinom(nrow(mtcars), 1,
                               c(0.4,0.5,0.6))
colour <- factor(colour, levels=c(1:3),
                 labels=c("red","green","blue"))
barplot(table(colour), xlab="Colour")
```

```{r include=FALSE}
load("data/measuresOfAssociation.Rdata")
```

In order to measure relation between the new variable and the `am`, we can use Cramer's V coefficient, which relies on the formula of $\chi^2$ test \@ref(eq:measuresAssociationChi):
\begin{equation}
    V = \sqrt{\frac{\chi^2}{n\times \min(r-1, c-1)}} .
    (\#eq:measuresAssociationCramer)
\end{equation}

Cramer's V always lies between 0 and 1, becoming close to one only if there is some relation between the two categorical variables. `greybox` package implements this coefficient in `cramer()` function:

```{r}
cramer(mtcarsData$am,colour)
```

The output above shows that the value of the coefficient is approximately 0.1, which is low, implying that the relation between the two variables is very weak. In addition, the p-value tells us that we fail to reject the null hypothesis on 1% level in the $\chi^2$ test \@ref(eq:measuresAssociationChi), and the relation does not look statistically significant. So we can conclude that according to our data, the two variables are not related (no wonder, we have generated one of them).

The main limitation of Cramer's V is that it is difficult to interpret beyond "there is a relation". Imagine a situation, where the colour would be related to the variable "class" of a car, that can take 5 values. What could we say more than to state the fact that the two are related? After all, in that case you end up with a contingency table of $3\times 5$, and it might not be possible to say how specifically one variable changes with the change of another one. Still, Cramer's V at least provides some information about the relation of two categorical variables.


### Ordinal scale
As discussed in Section \@ref(scales), ordinal scale has more flexibility than the nominal one - its values have natural ordering, which can be used, when we want to measure relations between several variables in ordinal scale. Yes, we can use Cramer's V and $\chi^2$ test, but this way we would not be using all advantages of the scale. So, what can we use in this case? There are three popular measures of association for variables in ordinal scale:

1. Goodman-Kruskal's $\gamma$,
2. Yule's Q,
3. Kendall's $\tau$.

Given that the ordinal scale does not have distances, the only thing we can do is to compare values of variables between different observations and say, whether one is greater than, less than or equal to another. What can be done with two variables in ordinal scale is the comparison of the values of those variables for a couple respondents. Based on that the pairs of the observations can be called:

1. **Concordant** if both $x_1 < x_2$ and $y_1 < y_2$ or $x_1 > x_2$ and $y_1 > y_2$ - implying that there is an agreement in order between the two variables (e.g. with a switch from a younger age group to the older one, the size of the T-shirt will switch from S to M);
2. **Discordant** if for $x_1 < x_2$ and $y_1 > y_2$ or for $x_1 > x_2$ and $y_1 < y_2$ - implying that there is a disagreement in the order of the two variables (e.g. with a switch from a younger age group to the older one, the satisfaction from drinking Coca-Cola will switch to the lower level);
3. **Ties** if both $x_1 = x_2$ and $y_1 = y_2$;
4. Neither otherwise (e.g. when $x_1 = x_2$ and $y_1 < y_2$).

All the measures of association for the variables in ordinal scale rely on the number of concordant, discordant variables and number of ties. All of these measures lie in the region of [-1, 1].

**Goodman-Kruskal's** $\gamma$ is calculated using the following formula:
\begin{equation}
    \gamma = \frac{n_c - n_d}{n_c + n_d},
    (\#eq:measuresAssociationGoodman)
\end{equation}
where $n_c$ is the number of concordant pairs, $n_d$ is the number of discordant pairs. This is a very simple measure of association, but it only works with scales of the same size (e.g. 5 options in one variable and 5 options in the other one) and ignores the ties.

In order to demonstrate this measure in action, we will create two artificial variables in ordinal scale:

1. Age of a person: young, adult and elderly;
2. Size of t-shirt they wear: S, M or L.

Here how we can do that in R:
```{r eval=FALSE}
age <- c(1:3) %*% rmultinom(nrow(mtcars), 1,
                               c(0.4,0.5,0.6))
age <- factor(age, levels=c(1:3),
                 labels=c("young","adult","elderly"))
size <- c(1:3) %*% rmultinom(nrow(mtcars), 1,
                               c(0.3,0.5,0.7))
size <- factor(size, levels=c(1:3),
                 labels=c("S","M","L"))
```

And here is how the relation between these two artificial variables looks:
```{r tableplotAgeSize, fig.cap="Heat map for age of a respondent and the size of their t-shirt."}
tableplot(age,size,xlab="Age",ylab="T-shirt size")
```

The graphical analysis based on Figure \@ref(fig:tableplotAgeSize) does not provide a clear information about the relation between the two variables. But this is where the Goodman-Kruskal's $\gamma$ becomes useful. We will use `GoodmanKruskalGamma()` function from `DescTools` package for R for this:

```{r}
DescTools::GoodmanKruskalGamma(age,size,conf.level=0.95)
```

This function returns three values: the $\gamma$, which is close to zero in our case, implying that there is no relation between the variables, lower and upper bounds of the 95% [confidence interval](#confidenceIntervals). Note that the interval shows us how big the uncertainty about the parameter is: the true value in the population can be anywhere between -0.51 and 0.44. But based on all these values we can conclude that we do not see any relation between the variables in our sample.

The next measure is called **Yule's Q** and is considered as a special case of Goodman-Kruskal's $\gamma$ for the variables that only have 2 options. It is calculated based on the resulting contingency $2\times 2$ table and has some similarities with the contingency coefficient $\phi$:
\begin{equation}
    \mathrm{Q} = \frac{n_{1,1} n_{2,2} - n_{1,2} n_{2,1}}{n_{1,1} n_{2,2} + n_{1,2} n_{2,1}} .
    (\#eq:measuresAssociationYule)
\end{equation}
The main difference from the contingency coefficient is that it assumes that the data has ordering, it implicitly relies on the number of concordant (on the diagonal) and discordant (on the off diagonal) pairs. In our case we could calculate it if we had two simplified variables based on age and size (in real life we would need to recode them to "young", "older" and "S", "Bigger than S" respectively):
```{r}
table(age,size)[1:2,1:2]

(2*2-4*2)/(2*2+4*2)
```

In our toy example, the measure shows that there is a weak negative relation between the trimmed age and size variables. We do not make any conclusions based on this, because this is not meaningful and is shown here just for purposes of demonstration.

Finally, there is **Kendall's** $\tau$. In fact, there are three different coefficients, which have the same name, so in the literature they are known as $\tau_a$, $\tau_b$ and $\tau_c$.

$\tau_a$ coefficient is calculated using the formula:
\begin{equation}
    \tau_a = \frac{n_c - n_d}{\frac{T (T-1)}{2}},
    (\#eq:measuresAssociationKendallTaua)
\end{equation}
where $T$ is the number of observations, and thus in the denominator, we have the number of all the pairs in the data. In theory this coefficient should lie between -1 and 1, but it does not solve the problem with ties, so typically it will not reach the boundary values and will say that the relation is weaker than it really is. Similar to Goodman-Kruskal's $\gamma$, it can only be applied to the variables that have the same number of levels (same sizes of scales). In order to resolve some of these issues, $\tau_b$ was developed:
\begin{equation}
    \tau_b = \frac{n_c - n_d}{\sqrt{\left(\frac{T (T-1)}{2} - n_x\right)\left(\frac{T (T-1)}{2} - n_y\right)}},
    (\#eq:measuresAssociationKendallTaub)
\end{equation}
where $n_x$ and $n_y$ are the number of ties calculated for both variables. This coefficient resolves the problem with ties and can now reach the boundary values in practice. However, this coefficient does not resolve the issue with different scale sizes. And in order to address this problem, we have $\tau_c$ (**Stuart-Kendall's** $\tau_c$):
\begin{equation}
    \tau_c = \frac{n_c - n_d}{\frac{n^2}{2}\frac{\min(r, c)-1}{\min(r, c)}},
    (\#eq:measuresAssociationKendallTaub)
\end{equation}
where $r$ is the number of rows and $c$ is the number of columns. This coefficient works for variables with different lengths of scales (e.g. age with 5 options and t-shirt size with 7 options). But now we are back to the problem with the ties...

In R, the `cor()` function implements Kendall's $\tau_a$ and $\tau_b$ (the function will select automatically based on the presence of ties). There are also functions `KendallTauA()`, `KendallTauB()` and `StuartTauC()` in `DescTools` package that implement the three respective measures of association. The main limitation of `cor()` function is that it only works with numerical variables, so we would need to transform variables before applying the function. The functions from `DescTools` package, on the other hand, work with factors. Here are the values of the three coefficients for our case:

```{r}
DescTools::KendallTauA(age,size,conf.level=0.95)
DescTools::KendallTauB(age,size,conf.level=0.95)
DescTools::StuartTauC(age,size,conf.level=0.95)
```

Given that both variables have the same scale sizes, we should use either $\tau_a$ or $\tau_b$ for the analysis. However, we do not know if there are any ties in the data, so the safer option would be to use $\tau_b$ coefficient. The value of the coefficient and its confidence interval tell us that there is no obvious association between the two variables in our sample. This is expected, because the two variables were generated independently of each other.


### Numerical scale {#correlationCoefficient}
Finally we come to the discussion of relations between variables measured in numerical scales. The most famous measure in this category is the **Pearson's correlation coefficient**, which population value is:
\begin{equation}
    \rho_{x,y} = \frac{\sigma_{x,y}}{\sigma_x \sigma_y},
    (\#eq:measuresAssociationPearsonPopulation)
\end{equation}
where $\sigma_{x,y}$ is the covariance between variables $x$ and $y$, while $\sigma_x$ and $\sigma_y$ are standard deviations of these variables. Typically, we do not know the population values, so this coefficient can be estimated in sample via:
\begin{equation}
    r_{x,y} = \frac{\cov(x,y)}{\sqrt{V(x)V(y)}},
    (\#eq:measuresAssociationPearson)
\end{equation}
where all the values from \@ref(eq:measuresAssociationPearsonPopulation) are substituted by their in-sample estimates. This coefficient measures the **strength of linear relation** between variables and lies between -1 and 1, where the boundary values correspond to perfect linear relation and 0 implies that there is no **linear** relation between the variables. In some textbooks the authors claim that this coefficient relies on Normal distribution of variables, but nothing in the formula assumes that. It was originally derived based on the simple linear regression (see Section \@ref(simpleLinearRelation)) and its rough idea is to get information about the angle of the straight line drawn on the scatterplot. It might be easier to explain this on an example:

```{r mtcarsScatterPlotDispMPG, fig.cap="Scatterplot for dispalcement vs mileage variables in mtcars dataset"}
plot(mtcarsData$disp,mtcarsData$mpg,
     xlab="Displacement",ylab="Mileage")
abline(lm(mpg~disp,mtcarsData),col="red")
```

Figure \@ref(fig:mtcarsScatterPlotDispMPG) shows the scatterplot between the two variables and also has the straight line, going through the cloud of points. The closer the points are to the line, the stronger the linear relation between the two variables is. The line corresponds to the formula $\hat{y}=a_0+a_1 x$, where $x$ is the displacement and $\hat{y}$ is the line value for the Mileage. The same relation can be presented if we swap the axes and draw the line $\hat{x}=b_0+b_1 y$:

```{r mtcarsScatterPlotMPGDisp, fig.cap="Scatterplot for mileage vs dispalcement"}
plot(mtcarsData$mpg,mtcarsData$disp,
     xlab="Displacement",ylab="Mileage")
abline(lm(disp~mpg,mtcarsData),col="red")
```

The slopes for the two lines will in general differ, and will only coincide if the two variables have functional relations (all the point lie on the line). Based on this property, the correlation coefficient was originally constructed, as a geometric mean of the two parameters of slopes: $r_{x,y}=\sqrt{a_1 b_1}$. We will come back to this specific formula later in Section \@ref(simpleLinearRelation). But this idea provides an explanation why the correlation coefficient measures the strength of linear relation. For the two variables of interest it will be:
```{r}
cor(mtcarsData$mpg,mtcarsData$disp)
```

Which shows strong negative linear relation between the displacement and mileage. This makes sense, because in general the cars with bigger engines will have bigger consumption and thus will make less miles per gallon of fuel. The more detailed information about the correlation is provided by the `cor.test()` function:
```{r}
cor.test(mtcarsData$mpg,mtcarsData$disp)
```

In addition to the value, we now have results of the hypothesis testing (where null hypothesis is $\rho_{x,y}=0$) and the confidence interval for the parameter. Given that the value of the parameter is close to its bound, we could conclude that the linear relation between the two variables is strong and statistically significant on 1% level.

Note however that the scatterplot in Figure \@ref(fig:mtcarsScatterPlotDispMPG) demonstrates some non-linearity in the relation between the two variables. So, it would make sense to have a different measure that could take it into account. This is where **Spearman's correlation coefficient** becomes useful. It is calculated using exactly the same formula \@ref(eq:measuresAssociationPearson), but applied to the data in ranks. By using ranks, we loose information about the natural zero and distances between values of the variable, but at the same time we linearise possible non-linear relations. So, Spearman's coefficient shows the strength of monotonic relation between the two variables:

```{r}
cor.test(mtcarsData$mpg,mtcarsData$disp,
         method="spearman")
```

We can notice that the value of the Spearman's coefficient in our case is higher than the value of the Pearson's correlation, which implies that there is indeed non-linear relation between variables. The two variables have a strong monotonic relation, which makes sense for the reasons discussed earlier. The non-linearity makes sense as well because the car with super powerful engines would still be able to do several miles on a gallon of fuel, no matter what. The relation will never be zero or even negative.

Note that while Spearman's correlation will tell you something about monotonic relations, it will fail to capture all other non-linear relations between variables. For example, in the following case the true relation is trigonometric:

```{r}
x <- c(1:100)
y <- sin(x)
plot(x,y,type="l")
```

But neither Pearson's nor Spearman's coefficients will be able to capture it:

```{r}
cor(x,y)
cor(x,y,method="spearman")
```

In order to correctly diagnose such non-linear relation, either one or both variables need to be transformed to linearise the relation. In our case this implies measuring the relation between $y$ and $\sin(x)$ instead of $y$ and $x$:

```{r}
cor(sin(x),y)
```





<!-- ### Mixed scales -->
<!-- Use the measure for the lowest scale. -->

<!-- Cramer can be used for ordinal vs nominal: -->

<!-- ```{r} -->
<!-- cramer(mtcarsData$am,mtcarsData$cyl) -->
<!-- ``` -->

<!-- Kendal's etc can be used on nominal scales, but their values will be misleading. -->

<!-- Multiple correlation -->




## Theory of distributions {#distributions}
There are several probability distributions that will be helpful in the further chapters of this textbook. Here, I want to briefly discuss those of them that will be used. While this might not seem important at this point, we might refer to this section of the textbook in the next chapters.

### Normal distribution {#distributionsNormal}
Every statistical textbook has Normal distribution. It is that one famous bell-curved distribution that every statistician likes because it is easy to work with and it is an asymptotic distribution for many other well-behaved distributions in some conditions (so called "Central Limit Theorem"). Here is the probability density function (PDF) of this distribution:
\begin{equation}
    f(y_t) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{\left(y_t - \mu_{y,t} \right)^2}{2 \sigma^2} \right) ,
    (\#eq:Normal)
\end{equation}
where $y_t$ is the value of the response variable, $\mu_{y,t}=\mu_{y,t|t-1}$ is the one step ahead conditional expectation on observation $t$, given the information on $t-1$ and $\sigma^2$ is the variance of the error term. The [maximum likelihood estimate](#likelihoodApproach) of $\sigma^2$ is:
\begin{equation}
    \hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^T \left(y_t - \hat{\mu}_{y,t} \right)^2 ,
    (\#eq:sigmaNormal)
\end{equation}
where $T$ is the sample size and $\hat{\mu}_{y,t}$ is the estimate of the conditional expecatation $\mu_{y,t}$. \@ref(eq:sigmaNormal) coincides with Mean Squared Error (MSE), discussed in the [section 1](#errorMeasures).

And here how this distribution looks (Figure \@ref(fig:dnormPlot)).

```{r dnormPlot, fig.cap="Probability Density Function of Normal distribution", echo=FALSE}
plot(seq(-3,3,0.1),dnorm(seq(-3,3,0.1)),type="l",ylab="Density",xlab="y",main="PDF of Normal distribution")
abline(v=0, col="red")
text(0.5,0.05,TeX("$\\mu =0$"))
```

What we typically assume in the basic time series models is that a variable is random and follows Normal distribution, meaning that there is a central tendency (in our case - the mean $mu$), around which the density of values is the highest and there are other potential cases, but their probability of appearance reduces proportionally to the distance from the centre.

The Normal distribution has skewness of zero and kurtosis of 3 (and excess kurtosis, being kurtosis minus three, of 0).

Additionally, if Normal distribution is used for the maximum likelihood estimation of a model, it gives the same parameters as the minimisation of MSE would give.

The log-likelihood based on the Normal distribution is derived by taking the sum of logarithms of the PDF of Normal distribution \@ref(eq:Normal):
\begin{equation}
    \ell(\mathbf{Y}| \theta, \sigma^2) = - \frac{T}{2} \log \left(2 \pi \sigma^2\right) - \sum_{t=1}^T \frac{\left(y_t - \mu_{y,t} \right)^2}{2 \sigma^2} ,
    (\#eq:NormalLogLik)
\end{equation}
where $\theta$ is the vector of all the estimated parameters in the model, and $\log$ is the natural logarithm. If one takes the derivative of \@ref(eq:NormalLogLik) with respect to $\sigma^2$, then the formula \@ref(eq:sigmaNormal) is obtained. Another useful thing to note is the concentrated log-likelihood, which is obtained by inserting the estimated variance \@ref(eq:sigmaNormal) in \@ref(eq:NormalLogLik):
\begin{equation}
    \ell(\mathbf{Y}| \theta, \hat{\sigma}^2) = - \frac{T}{2} \log \left( 2 \pi e \hat{\sigma}^2 \right) ,
    (\#eq:NormalLogLikConcentrated)
\end{equation}
where $e$ is the Euler's constant. The concentrated log-likelihood is handy, when estimating the model and calculating [information criteria](#modelSelection). Sometimes, statisticians drop the $2 \pi e$ part from the \@ref(eq:NormalLogLikConcentrated), because it does not affect any inferences, as long as one works only with Normal distribution (for example, this is what is done in `ets()` function from `forecast` package in R). However, it is not recommended to do [@Burnham2004], because this makes the comparison with other distributions impossible.

Normal distribution is available in `stats` package with `dnorm()`, `qnorm()`, `pnorm()` and `rnorm()` functions.


### Laplace distribution {#distributionsLaplace}
A more exotic distribution is Laplace, which has some similarities with Normal, but has higher excess. It has the following PDF:

\begin{equation}
    f(y_t) = \frac{1}{2 s} \exp \left( -\frac{\left| y_t - \mu_{y,t} \right|}{s} \right) ,
    (\#eq:Laplace)
\end{equation}
where $s$ is the scale parameter, which, when estimated using likelihood, is equal to the Mean Absolute Error ([MAE](#errorMeasures)):
\begin{equation}
    \hat{s} = \frac{1}{T} \sum_{t=1}^T \left| y_t - \hat{\mu}_{y,t} \right| .
    (\#eq:sLaplace)
\end{equation}

It has the shape shown on Figure \@ref(fig:dlaplacePlot).

```{r dlaplacePlot, fig.cap="Probability Density Function of Laplace distribution", echo=FALSE}
plot(seq(-3,3,0.01),dlaplace(seq(-3,3,0.01)),type="l",ylab="Density",xlab="y",main="PDF of Laplace distribution")
abline(v=0, col="red")
```

Similar to the Normal distribution, the skewness of Laplace is equal to zero. However, it has fatter tails - its kurtosis is equal to 6 instead of 3.

The variance of the random variable following Laplace distribution is equal to:
\begin{equation}
    \sigma^2 = 2 s^2.
    (\#eq:varianceLaplace)
\end{equation}

The `dlaplace`, `qlaplace`, `plaplace` and `rlaplace` functions from `greybox` package implement different sides of Laplace distribution in R.


### S distribution
This is something relatively new, but not ground braking. I have derived S distribution few years ago, but have never written a paper on that. It has the following density function (it is as a special case of [Generalised Normal distribution](distributionsGeneralisedNormal), when $\beta=0.5$):
\begin{equation}
    f(y_t) = \frac{1}{4 s^2} \exp \left( -\frac{\sqrt{|y_t - \mu_{y,t}|}}{s} \right) ,
    (\#eq:S)
\end{equation}
where $s$ is the scale parameter. If estimated via maximum likelihood, the scale parameter is equal to:
\begin{equation}
    \hat{s} = \frac{1}{2T} \sum_{t=1}^T \sqrt{\left| y_t - \hat{\mu}_{y,t} \right|} ,
    (\#eq:sS)
\end{equation}
which corresponds to the minimisation of a half of "Mean Root Absolute Error" or "Half Absolute Moment" (HAM). This is a more exotic type of scale, but the main benefit of this distribution is sever heavy tails - it has kurtosis of 25.2. It might be useful in cases of randomly occurring incidents and extreme values (Black Swans?).

```{r dsPlot, fig.cap="Probability Density Function of S distribution", echo=FALSE}
plot(seq(-3,3,0.01),ds(seq(-3,3,0.01)),type="l",ylab="Density",xlab="y",main="PDF of S distribution")
abline(v=0, col="red")
```

The variance of the random variable following S distribution is equal to:
\begin{equation}
    \sigma^2 = 120 s^4.
    (\#eq:varianceS)
\end{equation}

The `ds`, `qs`, `ps` and `rs` from `greybox` package implement the density, quantile, cumulative and random generation functions.


### Generalised Normal distribution {#distributionsGeneralisedNormal}
Generalised Normal ($\mathcal{GN}$) distribution (as the name says) is a generalisation for Normal distribution, which also includes Laplace and S as special cases [@Nadarajah2005]. There are two versions of this distribution: one with a shape and another with a skewness parameter. We are mainly interested in the first one, which has the following PDF:
\begin{equation}
    f(y_t) = \frac{\beta}{2 s \Gamma(\beta^{-1})} \exp \left( -\left(\frac{|y_t - \mu_{y,t}|}{s}\right)^{\beta} \right),
    (\#eq:GND)
\end{equation}
where $\beta$ is the shape parameter, and $s$ is the scale of the distribution, which, when estimated via MLE, is equal to:
\begin{equation}
    \hat{s} = \sqrt[^{\beta}]{\frac{\beta}{T} \sum_{t=1}^T\left| y_t - \hat{\mu}_{y,t} \right|^{\beta}},
    (\#eq:sGND)
\end{equation}
which has MSE, MAE and HAM as special cases, when $\beta$ is equal to 2, 1 and 0.5 respectively. The parameter $\beta$ influences the kurtosis directly, it can be calculated for each special case as $\frac{\Gamma(5/\beta)\Gamma(1/\beta)}{\Gamma(3/\beta)^2}$. The higher $\beta$ is, the lower the kurtosis is.

The advantage of $\mathcal{GN}$ distribution is its flexibility. In theory, it is possible to model extremely rare events with this distribution, if the shape parameter $\beta$ is fractional and close to zero. Alternatively, when $\beta \rightarrow \infty$, the distribution converges point-wise to the uniform distribution on $(\mu_{y,t} - s, \mu_{y,t} + s)$.

Note that the estimation of $\beta$ is a difficult task, especially, when it is less than 2 - the MLE of it looses properties of consistency and asymptotic normality.

Depending on the value of $\beta$, the distribution can have different shapes shown in Figure \@ref(fig:dgnormPlot)

```{r dgnormPlot, fig.cap="Probability Density Functions of Generalised Normal distribution", echo=FALSE}
plot(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,0.5),type="l",ylab="Density",xlab="y",main="PDF of Generalised Normal distribution",
     ylim=c(0,0.6))
lines(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,1),col="darkblue")
lines(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,2),col="darkred")
lines(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,1000),col="purple")
abline(v=0, col="red")
legend("topright",legend=c(TeX("$\\beta =0.5$"),TeX("$\\beta =1$"),TeX("$\\beta =2$"),TeX("$\\beta =1000$")),
       col=c("black","darkblue","darkred","purple"),lwd=1)
```

Typically, estimating $\beta$ consistently is a tricky thing to do, especially if it is less than one. Still, it is possible to do that by maximising the likelihood function \@ref(eq:GND).

The variance of the random variable following Generalised Normal distribution is equal to:
\begin{equation}
    \sigma^2 = s^2\frac{\Gamma(3/\beta)}{\Gamma(1/\beta)}.
    (\#eq:varianceGN)
\end{equation}

The working functions for the Generalised Normal are implemented in the `gnorm` package for R, but `smooth` and `greybox` packages use their own, faster, implementations.


### Asymmetric Laplace distribution {#distributionsALaplace}

Asymmetric Laplace distribution ($\mathcal{AL}$) can be considered as a two Laplace distributions with different parameters $s$ for left and right sides from the location $\mu_{y,t}$. There are several ways to summarise the probability density function, the neater one relies on the asymmetry parameter $\alpha$ [@Yu2005]:
\begin{equation}
    f(y_t) = \frac{\alpha (1- \alpha)}{s} \exp \left( -\frac{y_t - \mu_{y,t}}{s} (\alpha - I(y_t \leq \mu_{y,t})) \right) ,
    (\#eq:ALaplace)
\end{equation}
where $s$ is the scale parameter, $\alpha$ is the skewness parameter and $I(y_t \leq \mu_{y,t})$ is the indicator function, which is equal to one, when the condition is satisfied and to zero otherwise. The scale parameter $s$ estimated using likelihood is equal to the quantile loss:
\begin{equation}
    \hat{s} = \frac{1}{T} \sum_{t=1}^T \left(y_t - \hat{\mu}_{y,t} \right)(\alpha - I(y_t \leq \hat{\mu}_{y,t})) .
    (\#eq:sALaplace)
\end{equation}
Thus maximising the likelihood \@ref(eq:ALaplace) is equivalent to estimating the model via the minimisation of $\alpha$ quantile, making this equivalent to quantile regression approach. So quantile regression models assume indirectly that the error term in the model is $\epsilon_t \sim \mathcal{AL}(0, s, \alpha)$ [@Geraci2007].

Depending on the value of $\alpha$, the distribution can have different shapes, shown in Figure \@ref(fig:dALaplacePlot).

```{r dALaplacePlot, fig.cap="Probability Density Functions of Asymmetric Laplace distribution", echo=FALSE}
plot(seq(-3,3,0.01),dalaplace(seq(-3,3,0.01),0,1,0.5),type="l",ylab="Density",xlab="y",main="PDF of Asymmetric Laplace distribution",
     ylim=c(0,0.6))
lines(seq(-3,3,0.01),dalaplace(seq(-3,3,0.01),0,1,0.2),col="darkblue")
lines(seq(-3,3,0.01),dalaplace(seq(-3,3,0.01),0,1,0.8),col="darkred")
abline(v=0, col="red")
legend("topright",legend=c(TeX("$\\alpha =0.5$"),TeX("$\\alpha =0.2$"),TeX("$\\alpha =0.8$")),
       col=c("black","darkblue","darkred"),lwd=1)
```

Similarly to $\mathcal{GN}$ distribution, the parameter $\alpha$ can be estimated during the maximisation of the likelihood, although it makes more sense to set it to some specific values in order to obtain the desired quantile of distribution.

The variance of the random variable following Asymmetric Laplace distribution is equal to:
\begin{equation}
    \sigma^2 = s^2\frac{(1-\alpha)^2+\alpha^2}{\alpha^2(1-\alpha)^2}.
    (\#eq:varianceALaplace)
\end{equation}

Functions `dalaplace`, `qalaplace`, `palaplace` and `ralaplace` from `greybox` package implement the Asymmetric Laplace distribution.


### Log Normal, Log Laplace, Log S and Log GN distributions

In addition, it is possible to derive the log-versions of the Normal, $\mathcal{Laplace}$, $\mathcal{S}$, and $\mathcal{GN}$ distributions. The main differences between the original and the log-versions of density functions for these distributions can be summarised as follows:
\begin{equation}
    f_{log}(\log(y_t)) = \frac{1}{y_t} f(\log y_t).
    (\#eq:logDistribution)
\end{equation}
They are defined for positive values only and will have different right tail, depending on the location, scale and shape parameters. $\exp(\mu_{\log y,t})$ in this case represents the geometric mean (and median) of distribution rather than the arithmetic one. The conditional expectation in these distributions is typically higher than $\exp(\mu_{\log y,t})$ and depends on the value of the scale parameter. It is known for log$\mathcal{N}$ and is equal to:
\begin{equation}
    \mathrm{E}(y_t) = \mathrm{exp}\left(\mu_{\log y,t} + \frac{\sigma^2}{2} \right).
    (\#eq:logNMean)
\end{equation}
However, it does not have a simple form for the other distributions.


### Inverse Gaussian distribution {#IGDistribution}

An exotic distribution that will be useful for what comes in this textbook is the Inverse Gaussian ($\mathcal{IG}$), which is parameterised using mean value $\mu_{y,t}$ and either the dispersion parameter $s$ or the scale $\lambda$ and is defined for positive values only. This distribution is useful because it is scalable and has some similarities with the Normal one. In our case, the important property is the following:
\begin{equation}
    \text{if } (1+\epsilon_t) \sim \mathcal{IG}(1, s) \text{, then }
    y_t = \mu_{y,t} \times (1+\epsilon_t) \sim \mathcal{IG}\left(\mu_{y,t}, \frac{s}{\mu_{y,t}} \right),
    (\#eq:InverseGaussianModel)
\end{equation}
implying that the dispersion of the model changes together with the expectation. The PDF of the distribution of $1+\epsilon_t$ is:

\begin{equation}
    f(1+\epsilon_t) = \frac{1}{\sqrt{2 \pi s (1+\epsilon_t)^3}} \exp \left( -\frac{\epsilon_t^2}{2 s (1+\epsilon_t)} \right) ,
    (\#eq:InverseGaussian)
\end{equation}
where the dispersion parameter can be estimated via maximising the likelihood and is calculated using:
\begin{equation}
    \hat{s} = \frac{1}{T} \sum_{t=1}^T \frac{e_t^2}{1+e_t} ,
    (\#eq:InverseGaussianDispersion)
\end{equation}
where $e_t$ is the estimate of $\epsilon_t$. This distribution becomes very useful for multiplicative models, where it is expected that the data can only be positive.

Figure \@ref(fig:dIGPlot) shows how the PDF of $\mathcal{IG}(1,s)$ looks for different values of the dispersion $s$

```{r dIGPlot, fig.cap="Probability Density Functions of Inverse Gaussian distribution", echo=FALSE}
plot(seq(0,6,0.01),dinvgauss(seq(0,6,0.01),1,dispersion=10),type="l",ylab="Density",xlab="y",main="PDF of Inverse Gaussian distribution")
lines(seq(0,6,0.01),dinvgauss(seq(0,6,0.01),1,dispersion=1),col="darkblue")
lines(seq(0,6,0.01),dinvgauss(seq(0,6,0.01),1,dispersion=0.1),col="darkred")
abline(v=1, col="red")
legend("topright",legend=c(TeX("$s =10$"),TeX("$s =1$"),TeX("$s =0.1$")),
       col=c("black","darkblue","darkred"),lwd=1)
```

`statmod` package implements density, quantile, cumulative and random number generator functions for the $\mathcal{IG}$.


### Gamma distribution {#GammaDistribution}
Finally, another distribution that will be useful for ETS and ARIMA is Gamma ($\mathcal{\Gamma}$), which is parameterised using shape $\xi$ and scale $s$, and is defined for positive values only. This distribution is useful because it is scalable and is as flexible as ($\mathcal{IG}$) in terms of possible shapes. It also has an important scalability property (simila to $\mathcal{IG}$), but the shape needs to be restricted in order to make sense in ETS model:
\begin{equation}
    \text{if } (1+\epsilon_t) \sim \mathcal{\Gamma}(s^{-1}, s) \text{, then }
    y_t = \mu_{y,t} \times (1+\epsilon_t) \sim \mathcal{\Gamma}\left(s^{-1}, s \mu_{y,t} \right),
    (\#eq:GammaModel)
\end{equation}
implying that the scale of the model changes together with the expectation. The restriction on the shape parameters is needed in order to make the expectation of $(1+\epsilon_t)$ equal to one. The PDF of the distribution of $1+\epsilon_t$ is:

\begin{equation}
    f(1+\epsilon_t) = \frac{1}{\Gamma(s^{-1}) (s)^{s^{-1}}} (1+\epsilon_t)^{s^{-1}-1}\exp \left(-\frac{1+\epsilon_t}{s}\right) .
    (\#eq:Gamma)
\end{equation}
However, the scale $s$ cannot be estimated via the maximisation of likelihood analytically due to the restriction \@ref(eq:GammaModel). Luckliy, the method of moments can be used instead, where based on the expectation and variance we get:
\begin{equation}
    \hat{s} = \frac{1}{T} \sum_{t=1}^T e_t^2 ,
    (\#eq:GammaScale)
\end{equation}
where $e_t$ is the estimate of $\epsilon_t$. So, imposing the restrictions \@ref(eq:GammaModel) implies that the scale of $\mathcal{\Gamma}$ is equal to the variance of the error term.

Figure \@ref(fig:dGammaPlot) demonstrates how the PDF of $\mathcal{\Gamma}(s^{-1},s)$ looks for different values of $s$:

```{r dGammaPlot, fig.cap="Probability Density Functions of Gamma distribution", echo=FALSE}
plot(seq(0,6,0.01),dgamma(seq(0,6,0.01),0.1,scale=10),type="l",ylab="Density",xlab="y",main="PDF of Gamma distribution")
lines(seq(0,6,0.01),dgamma(seq(0,6,0.01),1,scale=1),col="darkblue")
lines(seq(0,6,0.01),dgamma(seq(0,6,0.01),10,scale=0.1),col="darkred")
abline(v=1, col="red")
legend("topright",legend=c(TeX("$s =10$"),TeX("$s =1$"),TeX("$s =0.1$")),
       col=c("black","darkblue","darkred"),lwd=1)
```

With the increase of the shape $\xi=s^{-1}$ (in our case this implies the decrease of variance $s$), $\mathcal{\Gamma}$ distribution converges to the normal one with $\mu=\xi s=1$ and variance $\sigma^2=s$. This demonstrates indirectly that the estimate of the scale \@ref(eq:GammaScale) maximises the likelihood of the function \@ref(eq:Gamma), although I do not have any proper proof of this.
