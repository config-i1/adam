# A bit of statistics {#statistics}
Before we move to the dicussion of ETS, ARIMA and ADAM, it makes sense to discuss some of the basics statistical terms and what they mean in the context of forecasting. Although, many of them originate from statistics and econometrics, we need to look at them from a different perspective: we are more interested in the forecasting process rather than in the estimation of parameters. Still, if you do not know statistics and econometrics well and want to have a good source on the topic, I would recommend reading an online book by @Hanck2020.

## Properties of estimators {#estimatesProperties}
Before moving forward and discussing distributions and models, it is also quite important to make sure that we understand what **bias**, **efficiency** and **consistency** of estimates of parameters mean. Although there are strict statistical definitions of the aforementioned terms (you can easily find them in Wikipedia or anywhere else), I do not want to copy-paste them here, because there are only a couple of important points worth mentioning in our context.

Note that all the discussions in this chapter relate to **the estimates of parameters**, not to the distribution of a random variable itself. A common mistake that students make when studying statistics, is that they think that the properties apply to the variable $y$ instead of the estimate of its parameters.


### Bias {#estimatesPropertiesBias}
**Bias** refers to the expected difference between the estimated value of parameter (on a specific sample) and the "[true](#intro)" one (in the true model). Having unbiased estimates of parameters is important because they should lead to more accurate forecasts (at least in theory). For example, if the estimated parameter is equal to zero, while in fact it should be 0.5, then the model would not take the provided information into account correctly and as a result will produce less accurate point forecasts and incorrect prediction intervals. In inventory context this may mean that we constantly order 100 units less than needed only because the parameter is lower than it should be.

The classical example of bias in statistics is the estimation of variance in sample. The following formula gives biased estimate of variance in sample:
\begin{equation}
    \mathrm{V}(y) = \frac{1}{T} \sum_{j=1}^T \left( y_t - \bar{y} \right)^2,
    (\#eq:varianceBiased)
\end{equation}
where $T$ is the sample size and $\bar{y} = \frac{1}{T} \sum_{j=1}^T y_t$ is the mean of the data. There is a lot of proofs in the literature of this issue (even @WikipediaVarianceBias2020 has one), we will not spend time on that. Instead, we will see this effect in the following simple simulation experiment:
```{r}
mu <- 100
sigma <- 10
nIterations <- 1000
# Generate data from normal distribution, 10,000 observations
y <- rnorm(10000,mu,sigma)
# This is the function, which will calculate the two variances
varFunction <- function(y){
   return(c(var(y), mean((y-mean(y))^2)))
}
# Calculate biased and unbiased variances for the sample of 30 observations,
# repeat nIterations times
varValues <- replicate(nIterations, varFunction(sample(y,30)))
```
This way we have generated 1000 samples with 30 observations and calculated variances using the formulae \@ref(eq:varianceBiased) and the corrected one for each step. Now we can plot it in order to see how it worked out:

```{r fig.cap="Histograms for biased and unbiased estimates of variance"}
par(mfcol=c(1,2))
# Histogram of unbiased estimate
hist(varValues[1,], xlab="V(y)", ylab="y", main="Unbiased estimate of V(y)")
abline(v=mean(varValues[1,]), col="red")
legend("topright",legend=TeX(paste0("E$\\left(V(y)\\right)$=",round(mean(varValues[1,]),2))),lwd=1,col="red")

# Histogram of the biased estimate
hist(varValues[2,], xlab="V(y)", ylab="y", main="Biased estimate of V(y)")
abline(v=mean(varValues[2,]), col="red")
legend("topright",legend=TeX(paste0("E$\\left(V(y)\\right)$=",round(mean(varValues[2,]),2))),lwd=1,col="red")
```

Every run of this experiment will produce different plots, but typically what we will see is that, the biased estimate of variance (the histogram on the right hand side of the plot) will have lower mean than the unbiased one. This is the graphical example of the effect of not taking the number of estimated parameters into account. The correct formula for the unbiased estimate of variance is:
\begin{equation}
    s^2 = \frac{1}{T-k} \sum_{j=1}^T \left( y_t - \bar{y} \right)^2,
    (\#eq:varianceUnBiased)
\end{equation}
where $k$ is the number of all independent estimated parameters. In this simple example $k=1$, because we only estimate mean (the variance is based on it). Analysing the formulae \@ref(eq:varianceBiased) and \@ref(eq:varianceUnBiased), we can say that with the increase of the sample size, the bias will disappear and the two formulae will give almost the same results: when the sample size $T$ becomes big enough, the difference between the two becomes negligible. This is the graphical presentation of the bias in the estimator.


### Efficiency {#estimatesPropertiesEfficiency}
**Efficiency** means, if the sample size increases, then the estimated parameters will not change substantially, they will vary in a narrow range (variance of estimates will be small). In the case with inefficient estimates the increase of sample size from 50 to 51 observations may lead to the change of a parameter from 0.1 to, letâ€™s say, 10. This is bad because the values of parameters usually influence both point forecasts and prediction intervals. As a result the inventory decision may differ radically from day to day. For example, we may decide that we urgently need 1000 units of product on Monday, and order it just to realise on Tuesday that we only need 100. Obviously this is an exaggeration, but no one wants to deal with such an erratically behaving model, so we need to have efficient estimates of parameters.

Another classical example of not efficient estimator is the median, when used on the data that follows Normal distribution. Here is a simple experiment demonstrating the idea:
```{r eval=FALSE}
mu <- 100
sigma <- 10
nIterations <- 500
obs <- 100
varMeanValues <- vector("numeric",obs)
varMedianValues <- vector("numeric",obs)
y <- rnorm(100000,mu,sigma)
for(i in 1:obs){
    ySample <- replicate(nIterations,sample(y,i*100))
    varMeanValues[i] <- var(apply(ySample,2,mean))
    varMedianValues[i] <- var(apply(ySample,2,median))
}
```

In order to establish the efficiency of the estimators, we will take their variances and look at the ratio of mean over median. If both are equally efficient, then this ratio will be equal to one. If the mean is more efficient than the median, then the ratio will be less than one:

```{r eval=FALSE}
options(scipen=6)
plot(1:100*100,varMeanValues/varMedianValues, type="l", xlab="Sample size",ylab="Relative efficiency")
abline(h=1, col="red")
```

```{r statsEfficiecny, fig.cap="An example of a relatively inefficient estimator", echo=FALSE}
knitr::include_graphics("images/02-statistics-efficiency.png")
```

What we should typically see on this graph, is that the black line should be below the red one, indicating that the variance of mean is lower than the variance of the median. This means that mean is more efficient estimator of the true location of the distribution $\mu$ than the median. In fact, it is easy to proove that asymptotically the mean will be 1.57 times more efficient than median [@WikipediaMedianEfficiency2020] (so, the line should converge approximately to the value of 0.64).


### Consistency {#estimatesPropertiesConsistency}
**Consistency** means that our estimates of parameters will get closer to the stable values (true value in the population) with the increase of the sample size. This is important because in the opposite case estimates of parameters will diverge and become less and less realistic. This once again influences both point forecasts and prediction intervals, which will be less meaningful than they should have been. In a way consistency means that with the increase of the sample size the parameters will become more efficient and less biased. This in turn means that the more observations we have, the better.

An example of inconsistent estimator is Chebyshev (or max norm) metric. It is formulated the following way:
\begin{equation}
    \mathrm{LMax} = \max \left(|y_1-\hat{y}|, |y_2-\hat{y}|, \dots, |y_T-\hat{y}| \right).
    (\#eq:chebyshevNorm)
\end{equation}
Minimising this norm, we can get an estimate $\hat{y}$ of the location parameter $\mu$. The simulation experiment becomes a bit more tricky in this situation, but here is the code to generate the estimates of the location parameter:
```{r eval=FALSE}
LMax <- function(y){
    estimator <- function(par){
        return(max(abs(y-par)));
    }
    
    return(optim(mean(y), fn=estimator, method="Brent", lower=min(y), upper=max(y)));
}

mu <- 100
sigma <- 10
nIterations <- 1000
y <- rnorm(10000, mu, sigma)
LMaxEstimates <- vector("numeric", nIterations)
for(i in 1:nIterations){
    LMaxEstimates[i] <- LMax(y[1:(i*10)])$par;
}
```

And here how the estimate looks with the increase of sample size:
```{r eval=FALSE}
plot(1:nIterations*10, LMaxEstimates, type="l", xlab="Sample size",ylab=TeX("Estimate of $\\mu$"))
abline(h=mu, col="red")
```

```{r statsConsistency, fig.cap="An example with inconsistent estimator", echo=FALSE}
knitr::include_graphics("images/02-statistics-consistency.png")
```

While in the example with bias we could see that the lines converge to the red line (the true value) with the increase of the sample size, the Chebyshev metric example shows that the line does not approach the true one, even when the sample size is 10000 observations. The conclusion is that when Chebyshev metric is used, it produces inconsistent estimates of parameters.


::: remark
There is a prejudice in the world of practitioners that the situation in the market changes so fast that the old observations become useless very fast. As a result many companies just through away the old data. Although, in general the statement about the market changes is true, the forecasters tend to work with the models that take this into account (e.g. Exponential smoothing, ARIMA, discussed in this book). These models adapt to the potential changes. So, we may benefit from the old data because it allows us getting more consistent estimates of parameters. Just keep in mind, that you can always remove the annoying bits of data but you can never un-throw away the data.
:::


### Asymptotic normality {#asymptoticNormality}
Finally, **asymptotic normality** is not critical, but in many cases is a desired, useful property of estimates. What it tells us is that the distribution of the estimate of parameter will be well behaved with a specific mean (typically equal to $\mu$) and a fixed variance. Some of the statistical tests and mathematical derivations rely on this assumption. For example, when one conducts a significance test for parameters of model, this assumption is implied in the process. If the distribution is not Normal, then the confidence intervals constructed for the parameters will be wrong together with the respective t- and p- values.

Another important aspect to cover is what the term **asymptotic**, which we have already used, means in our context. Here and after in this book, when this word is used, we refer to an unrealistic hypothetical situation of having all the data in the multiverse, where the time index $t \rightarrow \infty$. While this is impossible in practice, the idea is useful, because asymptotic behaviour of estimators and models is helpful on large samples of data. Besides, even if we deal with small samples, it is good to know what to expect to happen if the sample size increases.


### Law of Large Numbers and Central Limit Theorem {#LLNandCLT}
These are two important statistical notions about what happens asymptotically with an estimate of a parameter. They do not refer to what happens with the actual value or what happens with the error term of the model.

**Law of Large Numbers** (LLN) is a theorem saying that (under wide conditions) the average of a variable obtained over the large number of trials will be close to its expected value and will get closer to it with the increase of the sample size. In a way, it says that the average will be *unbiased* and *consistent* estimate of expected value.

**Central Limit Theorem** (CLT) says that when independent random variables are added, their normalised sum will asymptotically follow normal distribution, even if the original variables do not follow it. Note that this is the theorem about *what happens with the estimate* (sum in this case), *not with individual observations*. This means that the error term might follow, for example, [Inverse Gaussian distribution](#IGDistribution), but the estimate of its mean (under some conditions) will follow normal distribution. There are different versions of this theorem, built with different assumptions with respect to the random variable and the estimation procedure. This theorem directly relates to the [asymptotic normality](#asymptoticNormality) property discussed above.

Another thing to note about CLT is that it holds only, when:

1. **The true value of parameter is not near the bound**. e.g. if the variable follows uniform distribution on (0, $a$) and we want to estimate $a$, then its distribution will not be Normal (because in this case the true value is always approached from below). This assumption is important in our context, because ETS and ARIMA typically have restrictions on their parameters.
2. **The random variables are identically independent distributed** (i.i.d.). If they are not, then their average might not follow normal distribution (in some conditions it still might).
3. **The mean and variance of the distribution are finite**. This might seem as a weird issue, but some distributions do not have finite moments, so the CLT will not hold if a variable follows them, just because the sample mean will be all over the plane due to randomness and will not converget to the "true" value. Cauchy distribution is one of such examples.

If these assumptions hold, then CLT will work for the estimate of a parameter, no matter what the distribution of the random variable is. This becomes especially useful, when we want to test a hypothesis or construct a confidence interval for the estimate of parameter.


## Confidence and prediction intervals {#confidenceIntervals}
As mentioned in the previous section, we always work with samples and inevitably we deal with randomness just because of that even, when there are no other sources of uncertainty in the data. For example, if we want to estimate the mean of a variable based on the observed data, the value we get will differ from one sample to another. This should have become apparent from the examples we discussed [earlier](#estimatesProperties). And, if the [LLN and CLT](#LLNandCLT) hold, then we know that the estimate of our parameter will have its own distribution and will converge to the population value with the increase of the sample size. This is the basis for the confidence and prediction interval construction, discussed in this section. Depending on our needs, we can focus on the uncertainty of either the estimate of a parameter, or the random variable $y$ itself. When dealing with the former, we typically work with the **confidence interval** - the interval constructed for the estimate of a parameter, while in the latter case we are interested in the **prediction interval** - the interval constructed for the random variable $y$.

In order to simplify further discussion in this section, we will take the population mean and its in-sample estimate as an example. In this case we have:

1. A random variable $y$, which is assumed to follow some distribution with finite mean $\mu$ and variance $\sigma^2$;
2. A sample of size $T$ from the population of $y$;
3. Estimates of mean $\hat{\mu}=\bar{y}$ and variance $\hat{\sigma}^2 = s^2$, obtained based on the sample of size $T$.


### Confidence interval
What we want to get by doing this is an idea about the population mean $\mu$. The value $\bar{y}$ does not tell us much on its own due to randomness and if we do not capture its uncertainty, we will not know, where the true value $\mu$ can be. But using LLN and CLT, we know that the sample mean should converge to the true one and should follow normal distribution. So, the distribution of the sample mean would look like this:

```{r normalCurveBasic, echo=FALSE}
hist(rnorm(10000), xlab=TeX("$\\bar{y}$"), ylab="Density", xlim=c(-5,5), axes=FALSE, main="")
par(new=TRUE)
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), xlab="", ylab="", type="l")
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=1, lty=1, col="darkblue")
lines(c(-5,5), c(0,0), col="black", lwd=1)
abline(v=0, col="darkblue", lwd=2)
legend("topright",legend=c(TeX("$\\mu$")),
       lwd=c(2), col=c("darkblue"))
```

On its own, this distribution just tells us that the variable is random around the true mean $\mu$ and that its density function has a bell-like shape. In order to make this more useful, we can construct the **confidence interval** for it, which would tell us where the true parameter is most likely to lie. We can cut the tails of this distribution to determine the width of the interval, expecting it to cover $(1-\alpha)\times 100$% of cases. In the ideal world, asymptotically, the confidence interval will be constructed based on the true value, like this:

```{r normalCurveIntervals, echo=FALSE, fig.cap="Distribution of the sample mean and the confidence interval based on the population data."}
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), xlab=TeX("$\\bar{y}$"), ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1)),rep(0,length(seq(-5,5,0.1)))), col="grey95", lty=0)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=1, lty=1, col="darkblue")
lines(c(-5,5), c(0,0), col="black", lwd=1)
polygon(c(seq(-5,qnorm(0.025),0.01),rev(seq(-5,qnorm(0.025),0.01))),
        c(dnorm(seq(-5,qnorm(0.025),0.01)), rep(0,length(seq(-5,qnorm(0.025),0.01)))), col="grey")
polygon(c(seq(qnorm(0.975),5,0.01),rev(seq(qnorm(0.975),5,0.01))),
        c(dnorm(seq(qnorm(0.975),5,0.01)), rep(0,length(seq(qnorm(0.975),5,0.01)))), col="grey")
abline(v=qnorm(c(0.025,0.975)), col="darkred", lwd=2)
abline(v=0, col="darkblue", lwd=2)
legend("topright",legend=c(TeX("$\\mu$"),"bounds",TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkblue","darkred","grey95","grey"))
```

Figure \@ref(fig:normalCurveIntervals) shows the classical normal distribution curve around the population mean $\mu$, confidence interval of the level $1-\alpha$ and the cut off tails, the overall surface of which corresponds to $\alpha$. The value $1-\alpha$ is called **confidence level**, while $\alpha$ is the **significance level**. By constructing the interval this way, we expect that in the $(1-\alpha)\times 100$% of cases the value will be inside the bounds, and in $\alpha\times 100$% it will not.

In reality we do not know the true mean $\mu$, so we do a slightly different thing: we construct a confidence interval based on the sample mean $\bar{y}$ and sample variance $s^2$, hoping that due to [LLN](#LLNandCLT) they will converge to the true values. We use Normal distribution, because we expect [CLT](#LLNandCLT) to work. This process looks something like in Figure \@ref(fig:normalCurveIntervalsShades), with the bell curve in the background representing the true distribution for the sample mean and the curve on the foreground representing the assumed distribution based on our sample:

```{r normalCurveIntervalsShades, echo=FALSE, fig.cap="Distribution of the sample mean and the confidence interval based on a sample"}
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1),-1), xlab=TeX("$\\bar{y}$"), ylab="Density", type="l", xlim=c(-5,5),
     lwd=1,lty=2, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1),-1),rep(0,length(seq(-5,5,0.1)))),col="grey95",lty=0)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1),-1), lwd=2, lty=1, col="darkblue")
abline(v=-1, col="darkblue", lwd=2, lty=2)
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1),0),rep(0,length(seq(-5,5,0.1)))),col="white",lty=0)
lines(c(-5,5), c(0,0), col="black", lwd=1)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1),-1), lwd=1, lty=2, col="darkblue")
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=2, lty=1, col="darkgreen")
lines(c(-1,-1), c(0,dnorm(-1)), col="darkblue", lwd=1, lty=2)
abline(v=qnorm(c(0.025,0.975)), col="darkred", lwd=2)
abline(v=0, col="darkgreen", lwd=2, lty=2)
legend("topright",legend=c(TeX("$\\mu$"),"Sample mean","Bounds","True PDF","Assumed PDF"),
       lwd=c(2,2,2,2,2), col=c("darkblue","darkgreen","darkred","darkblue","darkgreen"), lty=c(2,2,1,1,1))
```

So, what the confidence interval does in reality is tries to cover the unknown population mean, based on the sample values of $\bar{y}$ and $s^2$. If we construct the confidence interval of the width $1-\alpha$ (e.g. 0.95) for thousands of random samples (thousands of trials), then in $(1-\alpha)\times 100$% of cases (e.g. 95%) the true mean will be covered by the interval, while in $\alpha \times 100$% cases it will not be. The interval itself is random, and we rely on LLN and CLT, when constructing it, expecting for it to work asymptotically, with the increase of the number of trials.

Mathematically the red bounds in Figure \@ref(fig:normalCurveIntervalsShades) are represented using the following well-known formula for the confidence interval:
\begin{equation}
    \mu \in (\bar{y} + t_{\alpha/2}(df) s_{\bar{y}}, \bar{y} + t_{1-\alpha/2}(df) s_{\bar{y}}),
    (\#eq:confidenceInterval)
\end{equation}
where $t_{\alpha/2}(df)$ is Student's t-statistics for $df=T-k$ degrees of freedom ($T$ is the sample size and $k$ is the number of estimated parameters, e.g. $k=1$ in our case) and level $\alpha/2$, and $s_{\bar{y}}=\frac{1}{\sqrt{T}}s$ is the estimate of the standard deviation of the sample mean. If we knew for some reason the true variance $\sigma^2$, then we could use z-statistics instead of t, but we typically do not, so we need to take the uncertainty about the variance into account as well, thus the use of t-statistics.

Note, that in order to construct *confidence interval*, we do not care what distribution $y$ follows, as long as LLN and CLT hold.

### Prediction interval
If we are interested in capturing the uncertainty about the random variable $y$, then we should refer to prediction interval. In this case, we typically rely on [LLN](#LLNandCLT) and the [assumed distribution](#distributions) for the random variable $y$. For example, if we know that $y \sim \mathcal{N}(\mu, \sigma^2)$, then based on our sample we can construct a prediction interval of the width $1-\alpha$:
\begin{equation}
    y \in (\bar{y} + z_{\alpha/2} s, \bar{y} + z_{1-\alpha/2} s),
    (\#eq:predictionInterval)
\end{equation}
where $z_{\alpha/2}$ is the z-statistics (quantile of standard normal distribution) for the level $\alpha/2$ and $\bar{y}$ is the sample estimate of $\mu$ and $s$ is the sample estimate of $\sigma$. If we assume some other distributions for the random variable, the formula would change. In a way, the prediction interval just comes to getting the quantiles of the assumed distribution based on estimated parameters. In some cases, when some of the [assumptions](#assumptions) do not hold, we might switch to more advanced methods for prediction interval construction.


## Hypothesis testing {#hypothesisTesting}
While we will not need hypothesis testing for ADAM itself, we might face it in some parts of the textbook, so it is worth discussing it briefly.

Hypothesis testing arises naturally from the idea of confidence intervals: instead of constructing the interval and getting the idea about the uncertainty of the parameter, we could check, whether the sample agrees with our expectations or not. For example, we could test, whether the population mean is equal to zero based on our sample. We could either construct a confidence interval for the sample mean and see if zero is included in it (in which case it might indicate that zero is one of the possible values of the population mean), or we could reformulate the problem and compare some calculated value with the theoretical threshold. The latter approach is in the nutshell what hypothesis testing does.

Fundamentally, the hypothesis testing process relies on the ideas of induction and dichotomy: we have a null (H$_0$) and alternative (H$_1$) hypotheses about the process or a property in the population, and we want to find some evidence to reject the H$_0$. Rejecting a hypothesis is actually more useful than not rejecting it, because in the former case we know what not to expect from the data, while in the latter we just might not have enough evidence to make any solid conclusion. For example, we could formulate H$_0$ that all cats are white. Failing to reject this hypothesis based on the data that we have (e.g. a dataset of white cats) does not mean that they are all (in the universe) indeed white, it just means that we have not observed the non-white ones. If we collect enough evidence to reject H$_0$ (i.e. encountered a black cat), then we can conclude that not all cats are white. This is a more solid conclusion than the one in the previous case. So, if you are interested in a specific outcome, then it makes sense to put this in the alternative hypothesis and see if the data allows to reject the null. For example, if we want to see if the average salary of professors in the UK is higher than Â£100k per year we would formulate the hypotheses in the following way:
\begin{equation*}
    \mathrm{H}_0: \mu \leq 100, \mathrm{H}_1: \mu > 100.
\end{equation*}
Having formulated hypotheses, we can check them, but in order to do that, we need to follow a proper procedure, which can be summarised in the six steps:

1. Formulate null and alternative hypotheses (H$_0$ and H$_1$) based on your understanding of the problem;
2. Select the significance level $\alpha$ on which the hypothesis will be tested;
3. Select the test appropriate for the formulated hypotheses (1);
4. Conduct the test (3) and get the calculated value;
5. Compare the value in (4) with the threshold one;
6. Make a conclusion based on (5) on the selected level (2).

Note that the order of some elements might change depending on the circumstances, but (2) should always happen before (4), otherwise we might be dealing with so called "p-hacking", trying to make results look nicer than they really are.

Consider an example, where we want to check, whether the population mean $\mu$ is equal to zero, based on a sample of 36 observations, where $\bar{y}=-0.5$ and $s^2=1$. In this case, we formulate the null and alternative hypotheses:
\begin{equation*}
    \mathrm{H}_0: \mu=0, \mathrm{H}_1: \mu \neq 0.
\end{equation*}
We then select the significance level $\alpha=0.05$ (just as an example) and select the test. Based on the description of the task, this can be either a t-test, or a z-test, depending on whether the variance of the variable is known or not. Usually it is not, so we tend to use t-test. We then conduct the test using the formula:
\begin{equation}
    t = \frac{\bar{y} - \mu}{s_{\bar{y}}} = \frac{-0.5 - 0}{\frac{1}{\sqrt{36}}} = -3 .
    (\#eq:ttestFormula)
\end{equation}
After that we get the critical value of t with $df=36-1=35$ degrees of freedom and significance level $\alpha/2=0.025$, which is approximately equal to `r round(qt(0.025, 36-1),3)`. We compare this value with the \@ref(eq:ttestFormula) by absolute and reject H$_0$ if the calculated value is higher than the critical one. In our case it is, so it appears that we have enough evidence to say that the population mean is not equal to 0, on the 5% significance level.

Visually, the whole process of hypothesis testing explained above can be represented in the following way:

```{r hypothesisTesting01, echo=FALSE, fig.cap="The process of hypothesis testing with t value"}
plot(seq(-5,5,0.1), dt(seq(-5,5,0.1), 36-1), xlab="t", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dt(seq(-5,5,0.1), 36-1),rep(0,length(seq(-5,5,0.1)))), col="grey95", lty=0)
lines(seq(-5,5,0.1), dt(seq(-5,5,0.1), 36-1), lwd=1, lty=1, col="darkgreen")
lines(c(-5,5), c(0,0), col="black", lwd=1)
polygon(c(seq(-5,qt(0.025, 36-1),0.01),rev(seq(-5,qt(0.025, 36-1),0.01))),
        c(dt(seq(-5,qt(0.025, 36-1),0.01),36-1), rep(0,length(seq(-5,qt(0.025, 36-1),0.01)))), col="grey")
polygon(c(seq(qt(0.975, 36-1),5,0.01),rev(seq(qt(0.975, 36-1),5,0.01))),
        c(dt(seq(qt(0.975, 36-1),5,0.01),36-1), rep(0,length(seq(qt(0.975, 36-1),5,0.01)))), col="grey")
abline(v=qt(c(0.025,0.975),36-1), col="darkred", lwd=2)
abline(v=-3, col="darkblue", lwd=2)
legend("topright",legend=c("critical values","calculated t",TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkred","darkblue","grey95","grey"))
```

If the blue line on Figure \@ref(fig:hypothesisTesting01) would lie inside the red bounds (i.e. the calculated value is less than the critical value by absolute), then we would fail to reject H$_0$. But in our example it is outside the bounds, so we have enough evidence to conclude that the population mean is not equal to zero on 5% significance level. Notice, how similar the mechanisms of confidence interval construction and hypothesis testing are. This is because they are one and the same thing, presented differently. In fact, we could test the same hypothesis by constructing the 95% confidence interval using \@ref(eq:confidenceInterval) and checking, whether the interval covers the $\mu=0$:
\begin{equation*}
    \begin{aligned}
        & \mu \in \left(-0.50 -2.03 \frac{1}{\sqrt{36}}, -0.50 + 2.03 \frac{1}{\sqrt{36}} \right), \\
        & \mu \in (-0.84, -0.16).
    \end{aligned}
\end{equation*}
In our case it does not, so we conclude that we reject H$_0$ on 5% significance level. This can be roughly represented by the graph on  Figure \@ref(fig:hypothesisTesting02):

```{r hypothesisTesting02, echo=FALSE, fig.cap="The process of hypothesis testing with t value"}
plot(seq(-1.5,0.5,0.01), dt((seq(-1.5,0.5,0.01)+0.5)*6, 36-1), xlab="Sample mean", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-1.5,0.5,0.01),rev(seq(-1.5,0.5,0.01))),
        c(dt((seq(-1.5,0.5,0.01)+0.5)*6, 36-1),rep(0,length(seq(-1.5,0.5,0.01)))), col="grey95", lty=0)
lines(seq(-1.5,0.5,0.01), dt((seq(-1.5,0.5,0.01)+0.5)*6, 36-1), lwd=1, lty=1, col="darkgreen")
lines(c(-1.5,0.5), c(0,0), col="black", lwd=1)
polygon(c(seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01),rev(seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01))),
        c(dt((seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01)+0.5)*6,36-1), rep(0,length(seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01)))), col="grey")
polygon(c(seq(qt(0.975, 36-1)/6-0.5,0.5,0.01),rev(seq(qt(0.975, 36-1)/6-0.5,0.5,0.01))),
        c(dt((seq(qt(0.975, 36-1)/6-0.5,0.5,0.01)+0.5)*6,36-1), rep(0,length(seq(qt(0.975, 36-1)/6-0.5,0.5,0.01)))), col="grey")
abline(v=qt(c(0.025,0.975),36-1)/6-0.5, col="darkred", lwd=2)
abline(v=0, col="darkblue", lwd=2)
legend("topright",legend=c("bounds",TeX("$\\mu$=0"),TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkred","darkblue","grey95","grey"))
```

Note that the positioning of the blue line has changed in the case of confidence interval, which happens because of the transition from \@ref(eq:ttestFormula) to \@ref(eq:confidenceInterval). The idea and the message, however, stay the same: if the value is not inside the light grey area, then we reject H$_0$ on the selected significance level.

Also **note** that we never say that we accept H$_0$, because this is not what we do in hypothesis testing: if the value would lie inside the interval, then this would only mean that our sample shows that the tested value is covered by the region - the true value can be any of the numbers between the bounds.

Finally, there is a third way to test the hypothesis. We could calculate how much surface is left in the tails with the cut off of the assumed distribution by the blue line on Figure \@ref(fig:hypothesisTesting01) (calculated value). In R this can be done using the `pt()` function:
```{r}
pt(-3, 36-1)
```
Given that we had the inequality in the alternative hypothesis, we need to consider both tails, multiplying the value by 2 to get approximately `r round(pt(-3, 36-1)*2, 4)`. This is the significance level, for which the switch from "reject" to "do not reject" happens. We could compare this value with the pre-selected significance level directly, rejecting H$_0$ if it is lower than $\alpha$. This value is called "p-value" and simplifies the hypothesis testing, because we do not need to look at critical values or construct the confidence interval. There are different definitions of what it is, I personally find the following easier to comprehend: **p-value** is the smallest significance level at which a null hypothesis can be rejected.

Despite this simplification, we still need to follow the procedure and select $\alpha$ *before conducting the test*! We should not change the significance level after observing the p-values, otherwise we might end up bending reality for our needs. Also *note* that if in one case p-value is 0.2, while in the other it is 0.3, it does not mean that the the first case is more significant than the second! P-values are not comparable with each other and they do not tell you about the size of significance. *This is still a binary process*: we either reject, or fail to reject H$_0$, depending on whether p-value is smaller or greater than the selected significance level.

While p-value is a comfortable instrument, I personally prefer using confidence intervals, because they show the uncertainty clearer and are less confusing. Consider the following cases to see what I mean:

1. We reject H$_0$ because t-value is -3, which is smaller than the critical value of `r round(qt(0.025, 36-1),3)` (or equivalently the absolute of t-value is 3, while the critical is `r round(qt(0.975, 36-1),3)`);
2. We reject H$_0$ because p-value is `r round(pt(-3, 36-1)*2, 4)`, which is smaller than the significance level $\alpha=0.05$;
3. The confidence interval for the mean is $\mu \in (-0.84, -0.16)$. It does not include zero, so we reject $\mathrm{H}_0$.

In case of (3), we not only get the same message as in (1) and (2), but we also see how far the bound is from the tested value. In addition, in the situation, when we fail to reject H$_0$, the approach (3) gives more appropriate information. Consider the case, when we test, whether $\mu=-0.6$ in the example above. We then have the following three approaches to the problem:

1. We fail to reject H$_0$ because t-value is 0.245, which is smaller than the critical value of `r round(qt(0.975, 36-1),3)`;
2. We fail to reject H$_0$ because p-value is 0.808, which is greater than the significance level $\alpha=0.05$;
3. The confidence interval for the mean is $\mu \in (-0.84, -0.16)$. It includes -0.6, so we fail to reject H$_0$. *This does not mean that the true mean is indeed equal to -0.6*, but it means that the region will cover this number in 95% of cases if we do resampling many times.

In my opinion, the third approach is more informative and saves from making wrong conclusions about the tested hypothesis, making you work a bit more (you cannot change the confidence level on the fly, you would need to reconstruct the interval). Having said that, either of the three is fine, as long as you understand what they really imply.

Finally, if you do hypothesis testing and use p-values, it is worth mentioning the statement of American Statistical Association about p-values [@Wasserstein2016]. Among the different aspects discussed in this statement, there is a list of principles related to p-values, which I cite below:

1. P-values can indicate how incompatible the data are with a specified statistical model;
2. P-values do not measure:
- the probability that the studied hypothesis is true,
- or the probability that the data were produced by random chance alone;
3. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold;
4. Proper inference requires full reporting and transparency;
5. A p-value, or statistical significance, does not measure:
- the size of an effect
- or the importance of a result;
6. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.

The statement provides more details about that, but summarising, whatever hypothesis you test and however you test it, you should have apriori understanding of the problem. Diving in the data and trying to see what floats (i.e. which of the p-values is higher than $\alpha$) is not a good idea [@Wasserstein2016]. Follow the proper procedure if you want to test the hypothesis.


### Common mistakes related to hypothesis testing {#hypothesisTestingMistakes}
Over the years of teaching statistics, I have seen many different mistakes, related to hypothesis testing. No wonder, this is a difficult topic to grasp. Here, I have decided to summarise several typical erroneous statements, providing explanations why they are wrong. They partially duplicate the 6 principles from ASA discussed above, but they are formulated slightly differently.

1. "Calculated value is lower than the critical one, so the null hypothesis is true".
- This is wrong on so many level, that I do not even know where to start. We can never know if the hypothesis is true or wrong. All the evidence might point towards the H$_0$ being correct, but it still can be wrong and at some point in future one observation might reject it. The classical example is the "Black swan in Australia". Up until the discover of Australia, the Europeans thought that there only exist white swans. This was supported by all the observations they had. Wise people would say that "We fail to reject H$_0$ that all swans are white". Uneducated people would be tempted to say that " H$_0$: All swans are white" is true. After discovering Australia in 1606, Europeans have collected evidence of existence of black swans, thus rejecting H$_0$ and showing that "not all swans are white", which implies that actually the alternative hypothesis is true. This is the essence of scientific method: we always try rejecting H$_0$, collecting some evidence. If we fail to reject it, it might just mean that we have not collected enough evidence or have not modelled it correctly.
2. "Calculated value is lower than the critical one, so we accept the null hypothesis".
- We **never** accept null hypothesis. Even if your house is on fire or there is a tsunami coming, you should not "accept H$_0$". This is a fundamental statistical principle. We collect evidence to reject the null hypothesis. If we do not have enough evidence, then we just fail to reject it, but we can never accept it, because failing to reject just means that we need to collect more data. As mentioned earlier, we focus on rejecting hypothesis, because this at least tells us, what the phenomenon is not (e.g. that not all swans are white).
3. "The parameter in the model is significant, so we can conclude that..."
- You cannot conclude if something is significant or not without specifying the significance level. Things are only significant if they pass specific test on a specified level $\alpha$. The correct sentence would be "The parameter in the model is significant on 3%, so we can conclude that...", where 3% is the selected significance level $\alpha$.
3. "The parameter in the model is significant because p-value<0.0000"
- Indeed, some statistical software will tell you that `p-value<0.0000`, but this just says that the value is very small and cannot be printed. Even if it is that small, you need to state your significance level and compare it with the p-value. You might wonder, "why bother if it is that low?". Well, if you change the sample size or change model specification, your p-value will change as well, and in some cases it might all of a sudden become higher than your significance level. So, you always need to keep it in mind and make conclusions based on the significance level, not just based on what software tells you.
5. "The parameter is not significant, so we remove the variable from the model".
- This is one of the worst motivations for removing variables that there is (statistical blasphemy!). There are thousands of reasons, why you might get p-value greater than your significance level ([assumptions](#assumptions) do not hold, sample is too small, the test is too weak, the true value is small etc) and only one of them is that the explanatory variable does not impact the response variable and thus you fail to reject H$_0$. Are you sure that you face exactly this one special case? If yes, then you already have some other (better) reasons to remove the variable. This means that you should not make decisions just based on the results of a statistical test. *You always need to have some fundamental reason to include or remove variables in the model*. Hypothesis testing just gives you additional information that can be helpful for decision making.
6. "The parameter in the new model is more significant than in the old one".
- There is no such thing as "more significant" or "less significant". **Significance is binary** and depends on the selected level. The only thing you can conclude is whether the parameter is significant on the chosen level $\alpha$ or not.
7. "The p-value of one variables is higher than the p-value of another, so...".
- p-values are not comparable between variables. They only show on what level the hypothesis is rejected and only work together with the chosen significance level. (6) is similar to this mistake.

Remember that the p-value itself is random and will change if you change the sample size or the model specification. Always keep this in mind, when conducting statistical tests. All these mistakes typically arise because of the misuse of p-values and hypothesis testing mechanism. This is one of the reasons, why I prefer [confidence intervals](#confidenceIntervals), when possible (as discussed above).

Finally, a related question to all of this, is *how to select the significance level*. Dave Worthington, a colleague of mine and a Statistics mentor at Lancaster University, has proposed an interesting motivation for that. If you do not have a level, driven by the problem (e.g. we need to satisfy 99% of demand, thus the significance level is 1%), then select the one for your life time. In how many cases in your life would you be ready to make a mistake? Would it be 5%? 3%? 1%? Select something and stick with it. Then over the years you will know that you have made the selected proportion of mistakes, when conducting different statistical test in variaous circumstances.


## Typical assumptions of statistical models {#assumptions}
In order for a statistical model to work adequately and not to fail on data, several assumptions about it, when it is *applied* to the data, should hold. If they do not, then the model might lead to biased or inefficient estimates and forecasts. Here we briefly discuss the main of them. Here they are:

1. Model is correctly specified;
2. The expectation of residuals is zero, no matter what;
3. Residuals are independent and identicaly distributed (i.i.d.);
4. The explanatory variables are not correlated with anything but the response variable;
5. The residuals follow the specified distribution.


### Model is correctly specified
This implies that:

1. We have not omitted important variables in the model (underfitting the data);
2. We do not have redundant variables in the model (overfitting the data);
3. The necessary transformation of the variables are applied;
4. We do not have outliers in the model.

(1): if there are some important variables that we did not include in the model, then the estimates of the parameters might be *biased* and in some cases quite seriously (e.g. positive sign instead of the negative one). This also means that the point forecasts from the model might be *biased* as well (systematic under or over forecasting).

(2): if there are redundant variables that are not needed in the model, then the estimates of parameters and point forecasts might be *unbiased*, but *inefficient.* This implies that the variance of parameters can be lower than needed and the prediction intervals can be narrower than needed.

(3): this means that, for example, instead of using a multiplicative model, we apply an additive one. The estimates of parameters and the point forecasts might be *biased* in this case as well: the model will produce linear trajectory of the forecast, when a non-linear one is needed.

(4): in a way, this is similar to (1), the presence of outliers might mean that we have missed some important information, meaning that the estimates of parameters and forecasts would be *biased* as well. There can be other reasons for outliers as well. For example, we might be using a wrong distributional assumptions. If so, this would imply that the prediction intervals from the model are narrower than needed.


### The expectation of residuals is zero, no matter what
While in sample, this holds automatically in many cases (e.g. when using Least Squares method), this assumption might be violated in the holdout sample. In this case the point forecasts would be *biased*, because they typically do not take the non-zero mean of forecast error into account, and the prediction interval might be off as well, because of the wrong estimation of the scale of distribution (e.g. variance is higher than needed).

This assumption also implies that the expectation of residuals is zero even conditional on the explanatory variables in the model. If it is not, then this might mean that there is still some important information omitted in the applied model.


### Residuals are i.i.d.
There are two assumptions in this group:

1. There is no autocorrelation in the residuals;
2. The residuals are homoscedastic.

(1): we expect that the model captures all the important aspects, so if the residuals are autocorrelated, then something is neglected by the applied model. Typically, this leads to *inefficient* estimates of parameters and in some cases they can also become *biased*. As a result, the point forecasts can be less accurate than expected and the prediction intervals might be wrong (wider or narrower than needed).

(2): if this is violated, then we say that there is a **heteroscedasticity** in the model. This means that with a change of variable, the variance of the residuals changes as well. If the model neglects this, then typically the estimates of parameters become *inefficient* and prediction intervals are wrong: they are wider than needed in some cases and narrower than needed in the other ones.


### The explanatory variables are not correlated with anything but the response variable
There are two cases here as well:

1. No multicollinearity;
2. No endogeneity;

(1): the effect of **multicollinearity** implies that the variables included in the model are linearly dependent from each other. In this case, it becomes difficult to distinguish the effect of one variables from the other one. As a result, the estimates of parameters become *inefficient* and might become *biased* in some sever cases. In case of forecasting, the effect is not as straight forward, and in some cases might not damage the point forecasts, but can lead to prediction intervals of an incorrect width.

(2): **endogeneity** applies to the situation, when the dependent variable $y_t$ influences the explanatory variable $x_t$ in the model on the same observation. The relation in this case becomes bi-directional, meaning that the basic model is not appropriate in this situation anymore. The parameters and forecasts will typically be *biased*, and a different estimation method is needed or maybe a different model would need to be constructed in order to fix this.


### The variable follows the specified distribution
Finally, in some cases we are interested in using methods that imply specific distributional assumptions about the model and its residuals. For example, it is assumed in the classical linear model that the error term follows Normal distribution. Estimating this model using MLE with the probability density function of Normal distribution or via minimisation of [Mean Squared Error](#errorMeasures) (MSE) would give *efficient* and *consistent* estimates of parameters. If the assumption of normality does not hold, then the estimates might be *inefficient* and in some cases *inconsistent*. When it comes to forecasting, the main issue in the wrong distributional assumption appears, when prediction intervals are needed: they might rely on a wrong distribution and be narrower or wider than needed. Finally, if we deal with the wrong distribution, then the model selection mechanism might be flawed and would lead to the selection of an inappropriate model.

In many cases, in our discussions in this textbook, we assume that all of these assumptions hold. In some of the cases, we will say explicitly, which are violated and what needs to be done in those situations.

Now that we have a basic understanding of these statistical terms, we can move to the next topic, distributions.


## Theory of distributions {#distributions}
There are several probability distributions that will be helpful in the further chapters of this textbook. Here, I want to briefly discuss those of them that will be used. While this might not seem important at this point, we might refer to this section of the textbook in the next chapters.

### Normal distribution {#distributionsNormal}
Every statistical textbook has Normal distribution. It is that one famous bell-curved distribution that every statistician likes because it is easy to work with and it is an asymptotic distribution for many other well-behaved distributions in some conditions (so called "Central Limit Theorem"). Here is the probability density function (PDF) of this distribution:
\begin{equation}
    f(y_t) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{\left(y_t - \mu_{y,t} \right)^2}{2 \sigma^2} \right) ,
    (\#eq:Normal)
\end{equation}
where $y_t$ is the value of the response variable, $\mu_{y,t}=\mu_{y,t|t-1}$ is the one step ahead conditional expectation on observation $t$, given the information on $t-1$ and $\sigma^2$ is the variance of the error term. The [maximum likelihood estimate](#likelihoodApproach) of $\sigma^2$ is:
\begin{equation}
    \hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^T \left(y_t - \hat{\mu}_{y,t} \right)^2 ,
    (\#eq:sigmaNormal)
\end{equation}
where $T$ is the sample size and $\hat{\mu}_{y,t}$ is the estimate of the conditional expecatation $\mu_{y,t}$. \@ref(eq:sigmaNormal) coincides with Mean Squared Error (MSE), discussed in the [section 1](#errorMeasures).

And here how this distribution looks:

```{r dnormPlot, echo=FALSE}
plot(seq(-3,3,0.1),dnorm(seq(-3,3,0.1)),type="l",ylab="Density",xlab="y",main="PDF of Normal distribution")
abline(v=0, col="red")
text(0.5,0.05,TeX("$\\mu =0$"))
```

What we typically assume in the basic time series models is that a variable is random and follows Normal distribution, meaning that there is a central tendency (in our case - the mean $mu$), around which the concentration of values is the highest and there are other potential cases, but their probability of appearance reduces proportionally to the distance from the centre.

The Normal distribution has skewness of zero and kurtosis of 3 (and excess kurtosis, being kurtosis minus three, of 0).

Additionally, if Normal distribution is used for the maximum likelihood estimation of a model, it gives the same parameters as the minimisation of MSE would give.

The log-likelihood based on the Normal distribution is derived by taking the sum of logarithms of the PDF of Normal distribution \@ref(eq:Normal):
\begin{equation}
    \ell(\mathbf{Y}| \theta, \sigma^2) = - \frac{T}{2} \log \left(2 \pi \sigma^2\right) - \sum_{t=1}^T \frac{\left(y_t - \mu_{y,t} \right)^2}{2 \sigma^2} ,
    (\#eq:NormalLogLik)
\end{equation}
where $\theta$ is the vector of all the estimated parameters in the model, and $\log$ is the natural logarithm. If one takes the derivative of \@ref(eq:NormalLogLik) with respect to $\sigma^2$, then the formula \@ref(eq:sigmaNormal) is obtained. Another useful thing to note is the concentrated log-likelihood, which is obtained by inserting the estimated variance \@ref(eq:sigmaNormal) in \@ref(eq:NormalLogLik):
\begin{equation}
    \ell(\mathbf{Y}| \theta, \hat{\sigma}^2) = - \frac{T}{2} \log \left( 2 \pi e \hat{\sigma}^2 \right) ,
    (\#eq:NormalLogLikConcentrated)
\end{equation}
where $e$ is the Euler's constant. The concentrated log-likelihood is handy, when estimating the model and calculating [information criteria](#modelSelection). Sometimes, statisticians drop the $2 \pi e$ part from the \@ref(eq:NormalLogLikConcentrated), because it does not affect any inferences, as long as one works only with Normal distribution (for example, this is what is done in `ets()` function from `forecast` package in R). However, it is not recommended to do [@Burnham2004], because this makes the comparison with other distributions impossible.

Normal distribution is available in `stats` package with `dnorm()`, `qnorm()`, `pnorm()` and `rnorm()` functions.


### Laplace distribution {#distributionsLaplace}
A more exotic distribution is Laplace, which has some similarities with Normal, but has higher excess. It has the following PDF:

\begin{equation}
    f(y_t) = \frac{1}{2 s} \exp \left( -\frac{\left| y_t - \mu_{y,t} \right|}{s} \right) ,
    (\#eq:Laplace)
\end{equation}
where $s$ is the scale parameter, which, when estimated using likelihood, is equal to the Mean Absolute Error ([MAE](#errorMeasures)):
\begin{equation}
    \hat{s} = \frac{1}{T} \sum_{t=1}^T \left| y_t - \hat{\mu}_{y,t} \right| .
    (\#eq:sLaplace)
\end{equation}

It has the following shape:

```{r dlaplacePlot, echo=FALSE}
plot(seq(-3,3,0.01),dlaplace(seq(-3,3,0.01)),type="l",ylab="Density",xlab="y",main="PDF of Laplace distribution")
abline(v=0, col="red")
```

Similar to the Normal distribution, the skewness of Laplace is equal to zero. However, it has fatter tails - its kurtosis is equal to 6 instead of 3.

The variance of the random variable following Laplace distribution is equal to:
\begin{equation}
    \sigma^2 = 2 s^2.
    (\#eq:varianceLaplace)
\end{equation}

The `dlaplace`, `qlaplace`, `plaplace` and `rlaplace` functions from `greybox` package implement different sides of Laplace distribution in R.


### S distribution
This is something relatively new, but not ground braking. I have derived S distribution few years ago, but have never written a paper on that. It has the following density function (it is as a special case of [Generalised Normal distribution](distributionsGeneralisedNormal), when $\beta=0.5$):
\begin{equation}
    f(y_t) = \frac{1}{4 s^2} \exp \left( -\frac{\sqrt{|y_t - \mu_{y,t}|}}{s} \right) ,
    (\#eq:S)
\end{equation}
where $s$ is the scale parameter. If estimated via maximum likelihood, the scale parameter is equal to:
\begin{equation}
    \hat{s} = \frac{1}{2T} \sum_{t=1}^T \sqrt{\left| y_t - \hat{\mu}_{y,t} \right|} ,
    (\#eq:sS)
\end{equation}
which corresponds to the minimisation of a half of "Mean Root Absolute Error" or "Half Absolute Moment" (HAM). This is a more exotic type of scale, but the main benefit of this distribution is sever heavy tails - it has kurtosis of 25.2. It might be useful in cases of randomly occurring incidents and extreme values (Black Swans?).

```{r dsPlot, echo=FALSE}
plot(seq(-3,3,0.01),ds(seq(-3,3,0.01)),type="l",ylab="Density",xlab="y",main="PDF of S distribution")
abline(v=0, col="red")
```

The variance of the random variable following S distribution is equal to:
\begin{equation}
    \sigma^2 = 120 s^4.
    (\#eq:varianceS)
\end{equation}

The `ds`, `qs`, `ps` and `rs` from `greybox` package implement the density, quantile, cumulative and random generation functions.


### Generalised Normal distribution {#distributionsGeneralisedNormal}
Generalised Normal ($\mathcal{GN}$) distribution (as the name says) is a generalisation for Normal distribution, which also includes Laplace and S as special cases [@Nadarajah2005]. There are two versions of this distribution: one with a shape and another with a skewness parameter. We are mainly interested in the first one, which has the following PDF:
\begin{equation}
    f(y_t) = \frac{\beta}{2 s \Gamma(\beta^{-1})} \exp \left( -\left(\frac{|y_t - \mu_{y,t}|}{s}\right)^{\beta} \right),
    (\#eq:GND)
\end{equation}
where $\beta$ is the shape parameter, and $s$ is the scale of the distribution, which, when estimated via MLE, is equal to:
\begin{equation}
    \hat{s} = \sqrt[^{\beta}]{\frac{\beta}{T} \sum_{t=1}^T\left| y_t - \hat{\mu}_{y,t} \right|^{\beta}},
    (\#eq:sGND)
\end{equation}
which has MSE, MAE and HAM as special cases, when $\beta$ is equal to 2, 1 and 0.5 respectively. The parameter $\beta$ influences the kurtosis directly, it can be calculated for each special case as $\frac{\Gamma(5/\beta)\Gamma(1/\beta)}{\Gamma(3/\beta)^2}$. The higher $\beta$ is, the lower the kurtosis is.

The advantage of $\mathcal{GN}$ distribution is its flexibility. In theory, it is possible to model extremely rare events with this distribution, if the shape parameter $\beta$ is fractional and close to zero. Alternatively, when $\beta \rightarrow \infty$, the distribution converges point-wise to the uniform distribution on $(\mu_{y,t} - s, \mu_{y,t} + s)$.

Note that the estimation of $\beta$ is a difficult task, especially, when it is less than 2 - the MLE of it looses properties of consistency and asymptotic normality.

Depending on the value of $\beta$, the distribution can have different shapes:

```{r dgnormPlot, echo=FALSE}
plot(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,0.5),type="l",ylab="Density",xlab="y",main="PDF of Generalised Normal distribution",
     ylim=c(0,0.6))
lines(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,1),col="darkblue")
lines(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,2),col="darkred")
lines(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,1000),col="purple")
abline(v=0, col="red")
legend("topright",legend=c(TeX("$\\beta =0.5$"),TeX("$\\beta =1$"),TeX("$\\beta =2$"),TeX("$\\beta =1000$")),
       col=c("black","darkblue","darkred","purple"),lwd=1)
```

Typically, estimating $\beta$ consistently is a tricky thing to do, especially if it is less than one. Still, it is possible to do that by maximising the likelihood function \@ref(eq:GND).

The variance of the random variable following Generalised Normal distribution is equal to:
\begin{equation}
    \sigma^2 = s^2\frac{\Gamma(3/\beta)}{\Gamma(1/\beta)}.
    (\#eq:varianceGN)
\end{equation}

The working functions for the Generalised Normal are implemented in the `gnorm` package for R, but `smooth` and `greybox` packages use their own, faster, implementations.


### Asymmetric Laplace distribution {#distributionsALaplace}

Asymmetric Laplace distribution ($\mathcal{AL}$) can be considered as a two Laplace distributions with different parameters $s$ for left and right sides from the location $\mu_{y,t}$. There are several ways to summarise the probability density function, the neater one relies on the asymmetry parameter $\alpha$ [@Yu2005]:
\begin{equation}
    f(y_t) = \frac{\alpha (1- \alpha)}{s} \exp \left( -\frac{y_t - \mu_{y,t}}{s} (\alpha - I(y_t \leq \mu_{y,t})) \right) ,
    (\#eq:ALaplace)
\end{equation}
where $s$ is the scale parameter, $\alpha$ is the skewness parameter and $I(y_t \leq \mu_{y,t})$ is the indicator function, which is equal to one, when the condition is satisfied and to zero otherwise. The scale parameter $s$ estimated using likelihood is equal to the quantile loss:
\begin{equation}
    \hat{s} = \frac{1}{T} \sum_{t=1}^T \left(y_t - \hat{\mu}_{y,t} \right)(\alpha - I(y_t \leq \hat{\mu}_{y,t})) .
    (\#eq:sALaplace)
\end{equation}
Thus maximising the likelihood \@ref(eq:ALaplace) is equivalent to estimating the model via the minimisation of $\alpha$ quantile, making this equivalent to quantile regression approach. So quantile regression models assume indirectly that the error term in the model is $\epsilon_t \sim \mathcal{AL}(0, s, \alpha)$ [@Geraci2007].

Depending on the value of $\alpha$, the distribution can have different shapes:

```{r dALaplacePlot, echo=FALSE}
plot(seq(-3,3,0.01),dalaplace(seq(-3,3,0.01),0,1,0.5),type="l",ylab="Density",xlab="y",main="PDF of Asymmetric Laplace distribution",
     ylim=c(0,0.6))
lines(seq(-3,3,0.01),dalaplace(seq(-3,3,0.01),0,1,0.2),col="darkblue")
lines(seq(-3,3,0.01),dalaplace(seq(-3,3,0.01),0,1,0.8),col="darkred")
abline(v=0, col="red")
legend("topright",legend=c(TeX("$\\alpha =0.5$"),TeX("$\\alpha =0.2$"),TeX("$\\alpha =0.8$")),
       col=c("black","darkblue","darkred"),lwd=1)
```

Similarly to $\mathcal{GN}$ distribution, the parameter $\alpha$ can be estimated during the maximisation of the likelihood, although it makes more sense to set it to some specific values in order to obtain the desired quantile of distribution.

The variance of the random variable following Asymmetric Laplace distribution is equal to:
\begin{equation}
    \sigma^2 = s^2\frac{(1-\alpha)^2+\alpha^2}{\alpha^2(1-\alpha)^2}.
    (\#eq:varianceALaplace)
\end{equation}

Functions `dalaplace`, `qalaplace`, `palaplace` and `ralaplace` from `greybox` package implement the Asymmetric Laplace distribution.


### Log Normal, Log Laplace, Log S and Log GN distributions

In addition, it is possible to derive the log-versions of the Normal, $\mathcal{Laplace}$, $\mathcal{S}$, and $\mathcal{GN}$ distributions. The main differences between the original and the log-versions of density functions for these distributions can be summarised as follows:
\begin{equation}
    f_{log}(\log(y_t)) = \frac{1}{y_t} f(\log y_t).
    (\#eq:logDistribution)
\end{equation}
They are defined for positive values only and will have different right tail, depending on the location, scale and shape parameters. $\exp(\mu_{\log y,t})$ in this case represents the geometric mean (and median) of distribution rather than the arithmetic one. The conditional expectation in these distributions is typically higher than $\exp(\mu_{\log y,t})$ and depends on the value of the scale parameter. It is known for log$\mathcal{N}$ and is equal to:
\begin{equation}
    \mathrm{E}(y_t) = \mathrm{exp}\left(\mu_{\log y,t} + \frac{\sigma^2}{2} \right).
    (\#eq:logNMean)
\end{equation}
However, it does not have a simple form for the other distributions.


### Inverse Gaussian distribution {#IGDistribution}

An exotic distribution that will be useful for what comes in this textbook is the Inverse Gaussian ($\mathcal{IG}$), which is parameterised using mean value $\mu_{y,t}$ and either the dispersion parameter $s$ or the scale $\lambda$ and is defined for positive values only. This distribution is useful because it is scalable and has some similarities with the Normal one. In our case, the important property is the following:
\begin{equation}
    \text{if } (1+\epsilon_t) \sim \mathcal{IG}(1, s) \text{, then }
    y_t = \mu_{y,t} \times (1+\epsilon_t) \sim \mathcal{IG}\left(\mu_{y,t}, \frac{s}{\mu_{y,t}} \right),
    (\#eq:InverseGaussianModel)
\end{equation}
implying that the dispersion of the model changes together with the expectation. The PDF of the distribution of $1+\epsilon_t$ is:

\begin{equation}
    f(1+\epsilon_t) = \frac{1}{\sqrt{2 \pi s (1+\epsilon_t)^3}} \exp \left( -\frac{\epsilon_t^2}{2 s (1+\epsilon_t)} \right) ,
    (\#eq:InverseGaussian)
\end{equation}
where the dispersion parameter can be estimated via maximising the likelihood and is calculated using:
\begin{equation}
    \hat{s} = \frac{1}{T} \sum_{t=1}^T \frac{e_t^2}{1+e_t} ,
    (\#eq:InverseGaussianDispersion)
\end{equation}
where $e_t$ is the estimate of $\epsilon_t$. This distribution becomes very useful for multiplicative models, where it is expected that the data can only be positive.

Here is how the PDF of $\mathcal{IG}(1,s)$ looks for different values of the dispersion $s$:

```{r dIGPlot, echo=FALSE}
plot(seq(0,6,0.01),dinvgauss(seq(0,6,0.01),1,dispersion=10),type="l",ylab="Density",xlab="y",main="PDF of Inverse Gaussian distribution")
lines(seq(0,6,0.01),dinvgauss(seq(0,6,0.01),1,dispersion=1),col="darkblue")
lines(seq(0,6,0.01),dinvgauss(seq(0,6,0.01),1,dispersion=0.1),col="darkred")
abline(v=1, col="red")
legend("topright",legend=c(TeX("$s =10$"),TeX("$s =1$"),TeX("$s =0.1$")),
       col=c("black","darkblue","darkred"),lwd=1)
```

`statmod` package implements density, quantile, cumulative and random number generator functions for the $\mathcal{IG}$.


### Gamma distribution {#GammaDistribution}
Finally, another distribution that will be useful for ETS and ARIMA is Gamma ($\mathcal{\Gamma}$), which is parameterised using shape $\xi$ and scale $s$, and is defined for positive values only. This distribution is useful because it is scalable and is as flexible as ($\mathcal{IG}$) in terms of possible shapes. It also has an important scalability property (simila to $\mathcal{IG}$), but the shape needs to be restricted in order to make sense in ETS model:
\begin{equation}
    \text{if } (1+\epsilon_t) \sim \mathcal{\Gamma}(s^{-1}, s) \text{, then }
    y_t = \mu_{y,t} \times (1+\epsilon_t) \sim \mathcal{\Gamma}\left(s^{-1}, s \mu_{y,t} \right),
    (\#eq:GammaModel)
\end{equation}
implying that the scale of the model changes together with the expectation. The restriction on the shape parameters is needed in order to make the expectation of $(1+\epsilon_t)$ equal to one. The PDF of the distribution of $1+\epsilon_t$ is:

\begin{equation}
    f(1+\epsilon_t) = \frac{1}{\Gamma(s^{-1}) (s)^{s^{-1}}} (1+\epsilon_t)^{s^{-1}-1}\exp \left(-\frac{1+\epsilon_t}{s}\right) .
    (\#eq:Gamma)
\end{equation}
However, the scale $s$ cannot be estimated via the maximisation of likelihood analytically due to the restriction \@ref(eq:GammaModel). Luckliy, the method of moments can be used instead, where based on the expectation and variance we get:
\begin{equation}
    \hat{s} = \frac{1}{T} \sum_{t=1}^T e_t^2 ,
    (\#eq:GammaScale)
\end{equation}
where $e_t$ is the estimate of $\epsilon_t$. So, imposing the restrictions \@ref(eq:GammaModel) implies that the scale of $\mathcal{\Gamma}$ is equal to the variance of the error term.

Here how the PDF of $\mathcal{\Gamma}(s^{-1},s)$ looks for different values of $s$:

```{r dGammaPlot, echo=FALSE}
plot(seq(0,6,0.01),dgamma(seq(0,6,0.01),0.1,scale=10),type="l",ylab="Density",xlab="y",main="PDF of Gamma distribution")
lines(seq(0,6,0.01),dgamma(seq(0,6,0.01),1,scale=1),col="darkblue")
lines(seq(0,6,0.01),dgamma(seq(0,6,0.01),10,scale=0.1),col="darkred")
abline(v=1, col="red")
legend("topright",legend=c(TeX("$s =10$"),TeX("$s =1$"),TeX("$s =0.1$")),
       col=c("black","darkblue","darkred"),lwd=1)
```

With the increase of the shape $\xi=s^{-1}$ (in our case this implies the decrease of variance $s$), $\mathcal{\Gamma}$ distribution converges to the normal one with $\mu=\xi s=1$ and variance $\sigma^2=s$. This demonstrates indirectly that the estimate of the scale \@ref(eq:GammaScale) maximises the likelihood of the function \@ref(eq:Gamma), although I do not have any proper proof of this.


## Likelihood Approach {#likelihoodApproach}
We will use different estimation techniques throughout this book, one of the main of which is **Maximum Likelihood Estimate** (MLE). The very rough idea of the approach is to maximise the chance that each observation in the sample follows a pre-selected distribution with specific set of parameters. In a nutshell, what we try to do when using likelihood for estimation, is fit the distribution function to the data. In order to demonstrate this idea, we start in a non-conventional way, with an example in R. We will then move to the mathematical side of the problem.

### An example in R
We consider a simple example, when we want to estimate the model $y_t = \mu_y + \epsilon_t$ (global average), assuming that the error term follows normal distribution: $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$, which means that $y_t \sim \mathcal{N}(\mu_{y}, \sigma^2)$. In this case we want to estimate two parameters using likelihood: $\hat{\mu}_y$ and $\hat{\sigma}^2$. First, we generate the random variable in R and plot its distribution:
```{r}
y <- rnorm(1000, 100, 10)
hist(y, xlim=c(50,150), main="", probability=TRUE)
```

As expected, the distribution of this variable (1000 observations) has the bell shape of Normal distribution. In order to estimate the parameters, for the distribution, we will try them one by one and see how the likelihood and the shape of the fitted curve to this histogram change. We start with $\hat{\mu}_y=80$ and $\hat{\sigma}=10$ just to see how the probability density function of normal distribution fits the data:

```{r MLENormalExample01, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=80$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),80,10),col="red",lwd=2)
abline(v=80,col="red",lwd=2)
```

and we get the following log-likelihood value (we will discuss how this formula can be obtained later):
```{r}
sum(dnorm(y,80,10,log=T))
```
In order for the normal distribution on \@ref(fig:MLENormalExample01) to fit the data well, we need to shift the estimate of $\mu_y$ to the right, thus increasing the value to, let's say, $\hat{\mu}_y=90$:

```{r MLENormalExample02, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=90$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),90,10),col="orange",lwd=2)
abline(v=90,col="orange",lwd=2)
```

Now, in Figure \@ref(fig:MLENormalExample02), the normal curve is much closer to the data, but it is still a bit off. The log-likelihood value in this case is `r round(sum(dnorm(y,90,10,log=T)),3)`, which is higher than the previous one, indicating that we are moving towards the maximum of the likelihood function. Moving it further, setting $\hat{\mu}_y=100$, we get:

```{r MLENormalExample03, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=100$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),100,10),col="green3",lwd=2)
abline(v=100,col="green3",lwd=2)
```

Figure \@ref(fig:MLENormalExample02) demonstrates a much better fit than in the previous cases with the log-likelihood of `r round(sum(dnorm(y,100,10,log=T)),3)`, which is even higher than in the previous case. We are almost there. In fact, in order to maximise this likelihood, we just need to calculate the sample mean of the variable (this is the MLE of the location parameter in normal distribution) and insert it in the function to obtain:

```{r MLENormalExample04, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=\\bar{y}$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),mean(y),10),col="darkgreen",lwd=2)
abline(v=mean(y),col="darkgreen",lwd=2)
```

So the value of $\hat{\mu}_y=\bar{y}=$ `r round(mean(y),3)` (where $\bar{y}$ is the sample mean) maximises the likelihood function, resultin in log-likelihood of `r round(sum(dnorm(y,mean(y),10,log=T)),3)`.

In a similar fashion we can get the MLE of the scale parameter $\sigma^2$ of the model. In this case, we will be changing the height of the distribution. Here is an example with $\hat{\mu}_y=$ `r round(mean(y),3)` and $\hat{\sigma}=15$:

```{r MLENormalExample05, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=\\bar{y}$ and $\\hat{\\sigma}=15$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),mean(y),15),col="royalblue",lwd=2)
abline(v=mean(y),col="royalblue",lwd=2)
```
Figure \@ref(fig:MLENormalExample05) demonstrates that the curve is located lower than needed, which implies that the scale parameter $\hat{\sigma}$ is too high. The log-likelihood value in this case is `r round(sum(dnorm(y,mean(y),15,log=T)),3)`. In order to get a better fit of the curve to the data, we need to reduce the $\hat{\sigma}$. Here how the situation would look for the case of $\hat{\sigma}=10$:

```{r MLENormalExample06, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=\\bar{y}$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),mean(y),10),col="darkblue",lwd=2)
abline(v=mean(y),col="darkblue",lwd=2)
```

The fit on Figure \@ref(fig:MLENormalExample06) is better than on Figure \@ref(fig:MLENormalExample05), which is also reflected in the log-likelihood value being equal to `r round(sum(dnorm(y,mean(y),10,log=T)),3)` instead of `r round(sum(dnorm(y,mean(y),15,log=T)),3)`. The best fit and the maximum of the likelihood is obtained, when the scale parameter is estimated using the formula $\hat{\sigma}^2 = \frac{1}{T}\sum_{t=1}^T\left(y_t - \bar{y}\right)^2$, resulting in log-likelihood of `r round(sum(dnorm(y,mean(y),sqrt(mean((y-mean(y))^2)),log=T)),3)`. Note that if we use the unbiased estimate of the variance $\hat{s}^2 = \frac{1}{T-1}\sum_{t=1}^T\left(y_t - \bar{y}\right)^2$, the log-likelihood will not reach the maximum and will be equal to `r round(sum(dnorm(y,mean(y),sd(y),log=T)),3)`. In our special case the difference between the two is infinitesimal, because of the large sample (1000 observations), but it will be more substantial on small samples. Still, the two likelihood values are diffrent, which can be checked in R via the following commands:
```{r, eval=FALSE}
# The maximum log-likelihood with the biased variance
logLik01 <- sum(dnorm(y,mean(y),sqrt(mean((y-mean(y))^2)),log=TRUE))
# The log-likelihood value with the unbiased variance
logLik02 <- sum(dnorm(y,mean(y),sd(y),log=TRUE))
# The difference between the two
logLik01 - logLik02
```

All of this is great, but so far we have discussed a very special case, when the data follows normal distribution and we fit the respective model. But what if the model is wrong (no kidding!)? In that case the idea stays the same: we need to find the parameters of the normal distribution, that would guarantee the best possible fit to the non-normal data. Here is an example with MLE of parameters of Normal distribution for the data following Log Normal one:

```{r MLENormalExample07, fig.cap="ML example with Normal curve on Log Normal data"}
y <- rlnorm(1000, log(80), 0.4)
hist(y, main="", probability=T, xlim=c(0,300))
lines(c(0:300),dnorm(c(0:300),mean(y),sd(y)),col="blue",lwd=2)
```

Figure \@ref(fig:MLENormalExample07) shows that the Normal model does not fit the Log Normal data properly, but this is the best we can get, given our assumptions. The log-likelihood in this case is `r round(sum(dnorm(y,mean(y),sd(y),log=TRUE)),3)`. The much better model would be the Log Normal one:

```{r MLENormalExample08, fig.cap="ML example with Log Normal curve on Log Normal data"}
hist(y, main="", probability=T, xlim=c(0,300))
lines(c(0:300),dlnorm(c(0:300),mean(log(y)),sd(log(y))),col="red",lwd=2)
```

The model in Figure \@ref(fig:MLENormalExample08) has the log likelihood of `r round(sum(dlnorm(y,mean(log(y)),sd(log(y)),log=TRUE)),3)`. This indicates that the Log Normal model is more appropriate for the data and gives us an idea that it is possible to compare different distributions via the likelihood, finding the better fit to the data. This idea is explored further in the [next section](#modelSelection).

As a final word, when it comes to more complicated models with more parameters and dynamic structure, the specific curves and data become more complicated, but the logic of the likelihood approach stays the same.

### Mathematical explanation {#likelihoodApproachMaths}
Now we can discuss the same idea from the mathematical point of view. We use an example of [normal distribution](#distributionsNormal) and a simple model as before:
\begin{equation}
    y_t = \mu_{y} + \epsilon_t,
    (\#eq:MLESimpleRegression)
\end{equation}
where $\mu_{y}$ is the population location parameter (the true parameter, the global mean). The typical assumption in regression context is that $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$, which means that $y_t \sim \mathcal{N}(\mu_{y}, \sigma^2)$. We can use this assumption in order to calculate the point likelihood value for each observation based on the [PDF of Normal distribution](#distributionsNormal):
\begin{equation}
    \mathcal{L} (\mu_{y}, \sigma^2 | y_t) = f(y_t | \mu_{y}, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{\left(y_t - \mu_{y,t} \right)^2}{2 \sigma^2} \right).
    (\#eq:MLEPointLik)
\end{equation}
Very roughly, what the value \@ref(eq:MLEPointLik) shows is the chance that the specific observation comes from the assumed model with specified parameters (we know that in real world the data does not come from any model, but this interprertation is easier to work with). Note that the likelihood is not the same as probability, because for any continuous random variables the probability for it to be equal to any specific number is equal to zero. However, the idea of likelihood has some similarities with the probability, so we prefer to refer to it as a "chance". The point likelihood \@ref(eq:MLEPointLik) is not very helpful on its own, but we can get $T$ values like that, based on our sample of data. We can then summarise it in one number, that would characterise the whole sample, given the assumed distribution, applied model and selected values of parameters:
\begin{equation}
    \mathcal{L} (\boldsymbol{\theta} | \mathbf{y}) = \mathcal{L} (\mu_{y}, \sigma^2 | \mathbf{y}) = \prod_{t=1}^T f(y_t | \mu_{y}, \sigma^2),
    (\#eq:MLEFullLik)
\end{equation}
where $\boldsymbol{\theta}$ is the vector of all parameters in the model (in our example, it is just the two of them). We take the product of likelihoods in \@ref(eq:MLEFullLik) because we need to get the joint likelihood for all observations and because we can typically assume that the point likelihoods are independent of each other (for example, the value on observation $t$ will not be influenced by the value on $t-1$). The value \@ref(eq:MLEFullLik) shows the summary chance that the data comes from the assumed model with specified parameters. Having this value, we can change the values of parameters of the model, getting different value of \@ref(eq:MLEFullLik) (as we did in the example above). Using an iterative procedure, we can get such estimates of parameters that would maximise the likelihood \@ref(eq:MLEFullLik), which are called Maximum Likelihood Estimates (MLE) of parameters. However, working with the products in that formula is difficult, so typically we linearise it using natural logarithm, obtaining log-likelihood:
\begin{equation}
    \ell (\boldsymbol{\theta} | \mathbf{y}) = \log \mathcal{L} (\boldsymbol{\theta} | \mathbf{y}) = -\frac{T}{2} \log(2 \pi \sigma^2) -\sum_{t=1}^T \frac{\left(y_t - \mu_{y,t} \right)^2}{2 \sigma^2} .
    (\#eq:MLEFullLogLik)
\end{equation}
Based on that, we can find some of parameters of the model analytically. For example, we can take derivative of \@ref(eq:MLEFullLogLik) with respect to the scale $\hat{\sigma}^2$ (which is an estimate of the true parameter $\sigma^2$) and equate it to zero in order to find the value that maximises the log-likelihood function in our sample:
\begin{equation}
    \frac{d \ell (\boldsymbol{\theta} | \mathbf{y})}{d \hat{\sigma}^2} = -\frac{T}{2} \frac{1}{\hat{\sigma}^2} + \frac{1}{2 \hat{\sigma}^4}\sum_{t=1}^T \left(y_t - \mu_{y,t} \right)^2 =0 , 
    (\#eq:MLEFullLogLikScale01)
\end{equation}
which after multiplication of both sides by $2 \hat{\sigma}^4$ leads to:
\begin{equation}
    T \hat{\sigma}^2 = \sum_{t=1}^T \left(y_t - \mu_{y,t} \right)^2 , 
    (\#eq:MLEFullLogLikScale02)
\end{equation}
or
\begin{equation}
    \hat{\sigma}^2 = \frac{1}{T}\sum_{t=1}^T \left(y_t - \mu_{y,t} \right)^2 .
    (\#eq:MLEFullLogLikScale)
\end{equation}
The value \@ref(eq:MLEFullLogLikScale) is in fact a [Mean Squared Error](#errorMeasures) (MSE) of the model. If we calculate the value of $\hat{\sigma}^2$ using the formula \@ref(eq:MLEFullLogLikScale), we will maximise the likelihood with respect to the scale parameter. In fact, we can insert \@ref(eq:MLEFullLogLikScale) in \@ref(eq:MLEFullLogLik) in order to obtain the so called concentrated (or profile) log-likelihood for the normal distribution:
\begin{equation}
    \ell^* (\boldsymbol{\theta}, \hat{\sigma}^2 | \mathbf{y}) = -\frac{T}{2}\left( \log(2 \pi e) + \log \hat{\sigma}^2 \right) .
    (\#eq:MLEFullLogLikConcentrated)
\end{equation}
This function is useful because it simplifies some calculations and also demonstrates the condition, for which the likelihood is maximised: the first part on the right hand side of the formula does not depend on the parameters of the model, it is only the $\log \hat{\sigma}^2$ that does. So, the maximum of the concentrated log-likelihood \@ref(eq:MLEFullLogLikConcentrated) is obtained, when $\hat{\sigma}^2$ is minimised, implying the minimisation of MSE, which is the mechanism behind the "Ordinary Least Squares" (OLS)
) estimation method. By doing this, we have just demonstrated that if we assume normality in the model, then the estimates of its parameters obtained via the maximisation of the likelihood coincide with the values obtained from OLS. So, why bother with MLE, when we have OLS?

First, the finding above holds for normal distribution only. If we assume a different [distribution](#distributions), we would get different estimates of parameters. In some cases, it might not be possible or reasonable to use OLS, but MLE would be a plausible option (for example, logistic, Poisson and any other non-standard model).

Second, the MLE of parameters have good statistical properties: they are [consistent](#estimatesPropertiesConsistency) and [efficient](#estimatesPropertiesEfficiency). These properties hold almost universally for many likelihoods under very mild conditions. Note that the MLE of parameters are not necessarily [unbiased](#estimatesPropertiesBias), but after estimating the model, one can de-bias some of them (for example, calculate the standard deviation of the error via devision of the sum of squared errors by the number of degrees of freedom $T-k$ instead of $T$).

Third, likelihood can be used for the model assessment, even when the standard statistics, such as $R^2$ or F-test are not available. We do not discuss these aspects in this textbook.

Finally, it permits the [model selection](#modelSelection) via information criteria. In general, this is not possible to do unless you assume a distribution and maximise the respective likelihood. In some statistical literature, you can notice that information criteria are calculated for the models estimated via OLS, but what the authors of such resources do not tell you is that there is still an assumption of normality behind this (see the link between OLS and MLE of Normal distribution above).

Note that the likelihood approach assumes that all parameters of the model are estimated, including location, scale, shape, shift etc of distribution. So typically it has more parameters to estimate than, for example, the OLS does. This is discussed in some detail later in the [next section](#statisticsNumberOfParameters).


## Model selection mechanism {#modelSelection}
There are different ways how to select the most appropriate model for the data. One can use judgment, statistical tests, cross-validation or meta learning. The state of the art one in the field of exponential smoothing relies on the calculation of information criteria and on selection of the model with the lowest value. This approach is discussed in detail in @Burnham2004. Here we briefly explain how this approach works and what are its advantages and disadvantages.

### Information criteria idea {#informationCriteria}
Before we move to the mathematics and well-known formulae, it makes sense to understand what we are trying to do, when we use information criteria. The idea is that we have a pool of model under consideration, and that there is a true model somewhere out there (not necessarily in our pool). This can be presented graphically in the following way:

```{r AICModelsPlot, echo=FALSE, fig.width=6, fig.height=5, fig.cap="An example of a model space"}
par(mar=c(1,1,1,1))
plot(c(0,-3,-6,3,6),c(0,2,-6,5,-1),xlim=c(-8,8),ylim=c(-8,8),pch=16,axes=F,xlab="",ylab="")
lines(c(0,-3),c(0,2), col="grey", lty=2)
lines(c(0,-6),c(0,-6), col="grey", lty=2)
lines(c(0,3),c(0,5), col="grey", lty=2)
lines(c(0,6),c(0,-1), col="grey", lty=2)
points(0,0,col="red",pch=16)
text(c(0,-3,-6,3,6),c(0,2,-6,5,-1),c("True model","Model 1","Model 2","Model 3","Model 4"),pos=3)
box()
```

This plot \@ref(fig:AICModelsPlot) represents a space of models. There is a [true one](#intro) in the middle, and there are four models under consideration: Model 1, Model 2, Model 3 and Model 4. They might differ in terms of functional form (additive vs. multiplicative), or in terms of included/omitted variables. All models are at some some distance (the grey dashed lines) from the true model in this hypothetic model space: Model 1 is closest while Model 2 is farthest. Models 3 and 4 have similar distances to the truth.

In the model selection exercise what we typically want to do is to select the model closest to the true one (Model 1 in our case). This is easy to do when you know the true model: just measure the distances and select the closest one. This can be written very roughly as:
\begin{equation}
    \begin{split}
        d_1 = \ell^* - \ell_1 \\
        d_2 = \ell^* - \ell_2 \\
        d_3 = \ell^* - \ell_3 \\
        d_4 = \ell^* - \ell_4
    \end{split} ,
    (\#eq:AICdistances)
\end{equation}
where $\ell_j$ is the position of the $j^{th}$ model and $\ell^*$ is the position of the true one. One of ways of getting the position of the model is by calculating the [log-likelihood](#likelihoodApproach) (logarithms of likelihood) values for each model, based on the assumed [distributions](#distributions). The likelihood of the true model will always be fixed, so if it is known it just comes to calculating the values for the models 1 - 4, inserting them in the equations in \@ref(eq:AICdistances), and selecting the model that has the lowest distance $d_j$.

In reality, however, we _never_ know the true model. We therefore need to find some other way of measuring the distances. The neat thing about the maximum likelihood approach is that the true model has the highest possible likelihood by definition! This means that it is not important to know $\ell^*$ -- it will be the same for all the models. So, we can drop the $\ell^*$ in the formulae \@ref(eq:AICdistances) and compare the models via their likelihoods $\ell_1, \ell_2, \ell_3 \text{ and } \ell_4$ alone:
\begin{equation}
    \begin{split}
        d_1 = - \ell_1 \\
        d_2 = - \ell_2 \\
        d_3 = - \ell_3 \\
        d_4 = - \ell_4
    \end{split} ,
    (\#eq:AICdistancesfixed)
\end{equation}
This is a very simple method that allows us to get to the model closest to the true one in the pool. However, we should not forget that we usually work with samples of data instead of the entire population and correspondingly will have only _estimates_ of likelihoods and not the true ones. Inevitably, they will be biased and will need to be corrected. @Akaike1974 showed that the bias can be corrected if the number of parameters in each model is added to the distances \@ref(eq:AICdistancesfixed) resulting in the bias corrected formula:
\begin{equation}
    d_j = k_j - \ell_j
    (\#eq:AICNormal),
\end{equation}
where $k_j$ is the number of estimated parameters in model $j$ (this typically includes scale parameters when dealing with Maximum Likelihood Estimates). @Akaike1974 suggests "An Information Criterion" which multiplies both parts of the right-hand side of \@ref(eq:AICNormal) by 2 so that there is a correspondence between the criterion and the well-known likelihood ratio test [@WikipediaLikelihoodRatioTest2020]:
\begin{equation}
    \mathrm{AIC}_j = 2 k_j - 2 \ell_j
    (\#eq:AIC).
\end{equation}

This criterion now more commonly goes by the "Akaike Information Criterion".

Various alternative criteria motivated by similar ideas have been proposed. The following are worth mentioning:

- AICc [@Sugiura1978], which is a sample corrected version of the AIC for normal and related distributions, which takes the number of observations into account:
\begin{equation}
    \mathrm{AICc}_j = 2 \frac{T}{T-k_j-1} k_j - 2 \ell_j
    (\#eq:AICc),
\end{equation}
where $T$ is the sample size.

- BIC [@Schwarz1978] (aka "Schwarz criterion"), which is derived from Bayesian statistics:
\begin{equation}
    \mathrm{BIC}_j = \log(T) k_j - 2 \ell_j
    (\#eq:BIC).
\end{equation}

- BICc [@McQuarrie1999] - the sample-corrected version of BIC, relying on the assumption of normality:
\begin{equation}
    \mathrm{BICc}_j = \frac{T \log (T)}{T-k_j-1} k_j - 2 \ell_j
    (\#eq:BICc).
\end{equation}

In general, the use of the sample-corrected versions of the criteria (AICc, BICc) is recommended unless sample size is very large (thousands of observations), in which case the effect of the number of observations on the criteria becomes negligible. The main issue is that corrected versions of information criteria for non-normal distributions need to be derived separately and will differ from \@ref(eq:AICc) and \@ref(eq:BICc). Still, @Burnham2004 recommend using formulae \@ref(eq:AICc) and \@ref(eq:BICc) in small samples even if the distribution of variables is not  normal and the correct formulae are not known. The motivation for this is that the corrected versions still take sample size into account, correcting the sample bias in criteria to some extent.

A thing to note is that the approach relies on asymptotic properties of estimators and assumes that the estimation method used in the process guarantees that the likelihood functions of the models are maximised. In fact, it relies on [asymptotic](#likelihoodApproach) behaviour of parameters, so it is not very important whether the maximum of the likelihood in sample is reached or not or whether the final solution is near the maximum. If the sample size changes, the parameters guaranteeing the maximum will change as well so we cannot get the point correctly in sample anyway. However, it is much more important to use an estimation method that will guarantee consistent maximisation of the likelihood. This implies that we might select wrong models in some cases in sample, but that is okay, because if we use the adequate approach for estimation and selection, with the increase of the sample size, we will select the correct model more often than an incorrect one. While the "increase of sample size" might seem as an unrealistic idea in some real life cases, keep in mind that this might mean not just the increase of $T$, but also the increase of the number of series under consideration. So, for example, the approach should select the correct model on average, when you test it on a sample of 10,000 SKUs.

Summarising, the idea of model selection via information criteria is to:

1. form a pool of competing models,
2. construct and estimate them,
3. calculate their likelihoods,
4. calculate the information criteria,
5. and finally, select the model that has the lowest value under the information criterion.

This approach is relatively fast (in comparison with cross-validation, judgmental selection or meta learning) and has good theory behind it. It can also be shown that for normal distributions selecting time series models on the basis of AIC is asymptotically equivalent to the selection based on [leave-one-out cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation) with MSE. This becomes relatively straightforward, if we recall that typically time series models rely on one step ahead errors $(e_t = y_t - \mu_{t|t-1})$ and that the maximum of the likelihood of Normal distribution gives the same estimates as the minimum of MSE.

As for the disadvantages of the approach, as mentioned above, it relies on the in-sample value of the likelihood, based on one step ahead error, and does not guarantee that the selected model will perform well for the holdout for multiple steps ahead. Using the cross-validation or [rolling origin](#rollingOrigin) for the full horizon could give better results if you suspect that information criteria do not work. Furthermore, any criterion is random on its own, and will change with the sample This means that there is model selection uncertainty and that which model is best might change with new observations. In order to address this issue, combinations of models can be used, which allows mitigating this uncertainty.


### Common confusions related to information criteria {#informationCriteriaMistakes}
Similar [to the discussion of hypothesis testing](#hypothesisTestingMistakes), I have decided to collect common mistakes and confusions related to information criteria. Here they are:

1. "AIC relies on Normal distribution".
- This is not correct. AIC relies on the value of maximised likelihood function. It will use whatever you provide it, so it all comes to the assumptions you make. Having said that, if you use the sample corrected versions of information criteria, such as AICc or BICc, then the formulae \@ref(eq:AICc) and \@ref(eq:BICc) are derived for Normal distribution. If you use a different one (not related to Normal, so not Log Normal, Box-Cox Normal, Logit Normal etc), then you would need to derive AICc and BICc for it. Still @Burnham2004 argue that even if you do not have the correct formula for your distribution, using \@ref(eq:AICc) and \@ref(eq:BICc) is better than using the non-corrected versions, because there is at least some correction of the bias caused by sample size.
2. "We have removed outlier from the model, AIC has decreased".
- AIC will always decrease if you decrease the sample size and fit the model with the same specification. This is because likelihood function relies on the joint PDF of all observations in sample. If the sample decreases, the likelihood increases. This effect is observed not only in cases, when outliers are removed, but also in case of taking differences of the data. So, when comparing models, make sure that they are constructed on exactly the same data.
3. "We have estimated model with logarithm of response variable, and AIC has decreased" (in comparison with the linear one).
- AIC is comparable only between models with the same response variable. If you transform the response variable, you inevitably assume a different distribution. For example, taking logarithm and assuming that error term follows normal distribution is equivalent to assuming that the original data follows log-normal distribution. If you want to make information criteria comparable in this case, either estimate the original model with a different distribution or [transform AIC for the multiplicative model](https://forecasting.svetunkov.ru/en/2018/03/22/comparing-additive-and-multiplicative-regressions-using-aic-in-r/).
4. "We have used quantile regression, assuming normality and AIC is..."
- Information criteria only work, when the likelihood with the assumed distribution is maximised, because only then it can be guaranteed that the estimates of parameters will be consistent and efficient. If you assume normality, then you either need to maximise the respective likelihood or minimise MSE - they will give the same solution. If you use quantile regression, then you should use likelihood of [Asymmetric Laplace](#distributionsALaplace). If you estimate parameters via minimisation of MAE, then [Laplace distribution](#distributionsLaplace) of residuals is a suitable assumption for your model. In the cases when distribution and loss are not connected, the selection mechanism might break and not work as intended.


### Calculating number of parameters in models {#statisticsNumberOfParameters}
When performing model selection and calculating different statistics, it is important to know how many parameters were estimated in the model. While this might seems trivial there are a number of edge cases and wrinkles that are seldom discussed in detail.

When it comes to the calculation of information criteria the general idea is to calculate the number of **all the independent estimated parameters** $k$. This typically includes all initial components and all coefficients of the model together with the scale, shape and shift parameters of the assumed distribution (e.g. variance in the Normal distribution).

```{example}
In a simple regression model: $y_t = \beta_0 + \beta_1 x_t + \epsilon_t$ - assuming Normal distribution for $\epsilon_t$, using the MLE will result in the estimation of $k=3$: the two parameters of the model ($\beta_0$ and $\beta_1$) and the variance of the error term $\sigma^2$.
```

If likelihood is not used, then the number of parameters might be different. For example, if we estimate the model via the minimisation of MSE (similar to OLS), then the number of all estimated parameters does not include the variance anymore - it is obtained as a by product of the estimation. This is because the likelihood needs to have all the parameters of distribution in order to be maximised, but with MSE, we just minimise the mean of squared errors, and the variance of the distribution is obtained automatically. While the values of parameters might be the same, the logic is slightly different.

```{example}
This means that for the same simple linear regression, estimated using OLS, the number of parameters is equal to 2: estimates of $\beta_0$ and $\beta_1$.
```

In addition, all the restrictions on the parameters can reduce the number of estimated parameters, when they get to the boundary values.

```{example}
If we know that the parameter $\beta_1$ lies between 0 and 1, and in the estimation process it gets to the value of 1 (due to how the optimiser works), it can be considered as a restriction $\beta_1=1$. So, when estimated via the minimum of MSE with this restriction, this would imply that $k=1$.
```

In general, if a parameter is provided in the model, then it does not count towards the number of all estimated parameters. So, setting $b_1=1$ acts in the same fashion.

Finally, if a parameter is just a function of another one, then it does not count towards the $k$ as well.

```{example}
If we know that in the same simple linear regression $\beta_1 = \frac{\beta_0}{\sigma^2}$, then the number of all the estimated parameter via the maximum likelihood is 2: $\beta_0$ and $\sigma^2$.
```

We will come back to the number of parameters later in this textbook, when we discuss specific models.

A final note: typically, the standard maximum likelihood estimators for the scale, shape and shift parameters are biased in small samples and do not coincide with the OLS estimators. For example, in case of Normal distribuiton, OLS estimate of variance has $T-k$ in the denominator, while the likelihood one has just $T$. This needs to be taken into account, when the variance is used in forecasting.
