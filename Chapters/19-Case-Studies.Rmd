# Case studies {#CaseStudies}
In this chapter, we consider case studies, demonstrating how ADAM can be used for specific tasks. At the moment, we only have one case study: number of visitors of the [openforecast.org](https://www.openforecast.org) blog.

## Openforecast.org blog visits
For this example, we consider statistics of the number of visitors of the [openforecast.org](https://www.openforecast.org) blog. This statistics does not include the visits to the online monographs published on the website and only focuses on the Wordpress blog, which has existed since 2015. It started as a blog with materials written in Russian but then slowly but surely has started focusing on the international audience. The [statistics was collected](http://top.mail.ru/jump?from=2629777) by a Russian service "Mail.RU" via a counter at the bottom of the website. Although the audience has changed over the years, the recent (10th October 2024) data tells that the largest group of visitors (18.38%) still comes from Russia, which is mainly because I have several pages on basics of forecasting in Russian language. I downloaded the data in csv files, aggregated it to daily level, and as a result got 3502 observations of my website visitors.

A thing to note is that whenever I publish posts, I usually advertise them on LinkedIn and they are also reposted automatically on [R-bloggers](https://www.r-bloggers.com/). This boosts the number of visitors, but typically does not change the core audience.

As I usually say, forecasting and analytics need to be done to inform decisions. If they are done just for the sake of doing them, this might be a waste of time. So, what decision can I make based on this statistics? I decided that the main decision I want to make is to decide, when to publish a new post to get a larger audience. I also want to understand the impact of LinkedIn/R-bloggers reposts on the number of visitors. So, I formulated the following questions:

1. When should I publish my new post to get a higher coverage (larger audience of readers).
2. What is the average effect of reposts on LinkedIn on the number of website visitors?
3. What is the average effect of reposts on R-bloggers on the number of website visitors?

To answer these questions, I decided to use ADAM, compare it with several simple models to decide what works better and what can give me a better answer.

### Preliminary analysis

```{r echo=FALSE}
load("data/openForecastVisits.Rdata")
```

The data is available [on github of the monograph in the Rdata format](https://github.com/config-i1/adam/blob/master/data/openForecastVisits.Rdata):

```{r eval=FALSE}
load(url("https://github.com/config-i1/adam/raw/refs/heads/master/data/openForecastVisits.Rdata"))
```

It is a `zoo` object of daily data with specific dates. It has `r length(openForecastVisits)` observations and exhibits seasonality. The plot of all the observations shows how the number of visitors of the blog evolved over time (Figure \@ref(fig:openForecastVisitsPlot)):

```{r openForecastVisitsPlot, fig.cap="Number of visitors of the Openforecast.org blog"}
plot(openForecastVisits, ylab="Visitors")
```

The thing that is easily noticeable is the number of outliers in the data. These were caused by reposts on R-bloggers and LinkedIn. On the other hand, the seasonality is hard to spot in Figure \@ref(fig:openForecastVisitsPlot) because of the sheer number of observations. To see it clearer, we can plot a specific part of the data:

```{r openForecastVisits56, fig.cap="8 weeks of data from the whole time series}
plot(openForecastVisits[3000+1:56], ylab="Visitors")
```

Figure \@ref(fig:openForecastVisits56) shows that there is a pattern that repeats itself every 7 days: on weekends people tend not to visit the website, while on work week they do that more often. The seasonality is multiplicative because we can see the increase of the fluctuations in Figure \@ref(fig:openForecastVisits) with the increase of the level of data.

But that's not all, because in addition to that, there is also seasonal pattern and calendar effects in the data, which make sense:

1. In Summer, people are on holidays and have other things to do than reading about forecasting;
2. when there are Christmas and New Year holidays, people also tend not to read about forecasting and statistics.

One way to spot this in our data is to aggregate it to weekly level and produce a seasonal plot (I use here `seasplot()` function from the `tsutils` package by @R-tsutils):

```{r openForecastVisitsWeekly, fig.cap="Seasonal plot for the weekly viitors of the blog."}
# Load xts, which allows easily aggregating data
library(xts)
# Aggregate to weekly
openForecastVisitsWeekly <- apply.weekly(openForecastVisits, sum)
# Produce a seasonal plot, set trend=TRUE, because we had changes in data
tsutils::seasplot(openForecastVisitsWeekly, m=52, trend=TRUE)
```

We can spot that there is a decline between the weeks 18 - 30 in Figure \@ref(fig:openForecastVisitsWeekly) and then another one around week 45. The first one is due to Summer holidays, the second one is because of Christmas.

Finally, while there was an increase in the number of visitors over the first few years, it stabilised after 2021 and started slowing down. The increase itself is not very rapid, so we can get away with a level model (no trend). This becomes more apparent if we add a smooth line to the linear graph from Figure \@ref(fig:openForecastVisits) (see Figure \@ref(fig:openForecastVisitsSmooth)):

```{r openForecastVisitsSmooth, fig.cap="Original data and its smoothed values via Friedman's super smoother."}
plot(openForecastVisits, ylab="Visitors")
# Use Friedman's Super Smoother
lines(time(openForecastVisits),
      exp(supsmu(1:length(openForecastVisits),log(openForecastVisits))$y),
      col="red2", lwd=2)
```

Summarising this basic visual analysis, there are several characteristics that we should take into account in our model:

1. Multiplicative seasonality:
    - Day of week pattern,
    - Day of year or week of year pattern;
2. Multiplicative error (variance increases together with the level of data);
3. No trend;
4. Calendar events (Christmas, New Year, Easter?);
5. Special events (R-bloggers and LinkedIn posts).

Based on this we can construct ETS(M,N,M), ETSX(M,N,M), logARIMA(0,1,1)(0,1,1), logARIMAX(0,1,1)(0,1,1), and multiplicative regression. Comparing these models will allow us to choose the most suitable for the data.


### Getting explanatory variables
Before we move to the forecasting and the initialisation, we need to make sure that we have all the necessary features (calendar effects and promotions) to work with. We start with calendar effects. To better investigate what calendar effects might have effect on the number of visitors, I do decomposition using the `msdecompose()` function for one year of data (2022), specifying the frequency of 7 only:

```{r}
# Add 7 to have exactly 365 fitted values. Otherwise, we would loose them
# because of the simple moving average
openForecast2022Decompose <- msdecompose(openForecastVisits[2475+1:365+7],
                                         lags=c(7), type="m")
```

After that, we produce the deseasonalised series:

```{r}
# Record the dates
openForecast2022Time <- time(openForecastVisits)[2475+1:365+7]
# Get multiply level by error to get the deseasonalised series
(openForecast2022Decompose$states[,1] *
    exp(resid(openForecast2022Decompose))) |> 
    zoo(order.by=openForecast2022Time) -> 
    openForecast2022Deseas
```

and plot it to be able to see the possible effects clearer (Figure \@ref(fig:openForecast2022Deseas)):

```{r openForecast2022Deseas, fig.cap="Deseasonalised series of visits in 2022."}
# Plot the series
plot(openForecast2022Deseas, type="l", ylab="Visits")
# Mark potential important dates
# Catholic Christmas
abline(v=as.Date(c("2021-12-23")):as.Date(c("2021-12-25")),
       col=rgb(0.5,0.8,0.5,0.5), lwd=2)
# Russian New Year holidays
abline(v=as.Date(c("2021-12-29")):as.Date(c("2022-01-07")),
       col=rgb(1,0.5,0.5,0.5), lwd=2)
# May holidays
abline(v=as.Date(c("2022-05-01")):as.Date(c("2022-05-10")),
       col=rgb(0.5,0.5,0.8,0.5), lwd=2)
# Misc public holidays
abline(v=as.Date(c("2022-02-23","2022-03-08",
                   "2022-06-13","2022-11-04")),
       col=rgb(0.8,0.5,0.8,0.5), lwd=2)
```

In Figure \@ref(fig:openForecast2022Deseas), we are not interested in peaks (they happen because of promotions), but instead we want to see troughs caused by potential holidays. The main apparent effect in Figure \@ref(fig:openForecast2022Deseas) is the dip in January 2022. This is because the website has lots of visitors from ex-soviet countries (Russia, Belarus, Ukraine and others), and there, the first week in January is typically the week of holidays. The May holidays show also a potential decline in visitors number and are worth investigating. Some of the other holidays have effect as well, but this will need to be investigated more thoroughly.

I created a table with categorical variable for the public holidays for the whole period and saved it in the same file in github under the name `openForecastHolidays`.




### The evaluation
Given the amount of data, we can do rolling origin evaluation (from Section \@ref(rollingOrigin)) to select the most robust model. I will do this for the last year of data and will produce forecasts for each week ahead from each Sunday (rolling origin step of 7). To see how models perform, I will use RMSE and ME. I will produce the forecasts for horizon of 7 days because I need to select the most suitable day for the post over the next week.

For this example, I will use the `ro()` function from the `greybox` package in R [@R-greybox] v2.0.3, which has all the necessary features I need. I will have a fixed out-of-sample and an increasing in-sample, meaning that I will set `co=TRUE`, `ci=FALSE`. The following setting will give us almost a year of data for the holdout:

```{r}
origins <- 52
step <- 7
h <- 7

# Check the size of the holdout sample
origins*step + (h-step)
```


