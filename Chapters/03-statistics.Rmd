# A short introduction to main statistical ideas {#statistics}

Before we move to the dicussion of ETS, ARIMA and ADAM, it makes sense to discuss some of the basics statistical terms and what they mean in the context of forecasting. Although, many of them originate from statistics and econometrics, we need to look at them from a different perspective: we are more interested in the forecasting process rather than in the estimation of parameters. Still, if you do not know statistics and econometrics well and want to have a good source on the topic, I would recommend reading an online book by @Hanck2020.

## Properties of estimators {#estimatesProperties}
Before moving forward and discussing distributions and models, it is also quite important to make sure that we understand what **bias**, **efficiency** and **consistency** of estimates of parameters mean. Although there are strict statistical definitions of the aforementioned terms (you can easily find them in Wikipedia or anywhere else), I do not want to copy-paste them here, because there are only a couple of important points worth mentioning in our context.

### Bias
**Bias** refers to the expected difference between the estimated value of parameter (on a specific sample) and the "true" one (in the true model). Having unbiased estimates of parameters is important because they should lead to more accurate forecasts (at least in theory). For example, if the estimated parameter is equal to zero, while in fact it should be 0.5, then the model would not take the provided information into account correctly and as a result will produce less accurate point forecasts and incorrect prediction intervals. In inventory context this may mean that we constantly order 100 units less than needed only because the parameter is lower than it should be.

The classical example of bias in statistics is the estimation of variance in sample. The following formula gives biased estimate of variance in sample:
\begin{equation}
    s^2 = \frac{1}{T} \sum_{j=1}^T \left( y_t - \bar{y} \right)^2,
    (\#eq:varianceBiased)
\end{equation}
where $T$ is the sample size and $\bar{y} = \frac{1}{T} \sum_{j=1}^T y_t$ is the mean of the data. There is a lot of proofs in the literature of this issue (even @WikipediaVarianceBias2020 has one), we will not spend time on that. Instead, we will see this effect in the following simple simulation experiment:
```{r}
mu <- 100
sigma <- 10
nIterations <- 1000
varianceValues <- vector("numeric",nIterations)
varianceValuesBiased <- vector("numeric",nIterations)
x <- rnorm(1000,mu,sigma)
for(i in 1:nIterations){
    varianceValuesBiased[i] <- mean((x[1:i]-mean(x[1:i]))^2)
    varianceValues[i] <- var(x[1:i])
}
```
This way we have generated 1000 samples, increasing the number of observations 10 times on each iteration. We have calculated standard deviation (square root of variance) using the formula \@ref(eq:varianceBiased) for each step. Now we can plot it in order to see how it worked out:
```{r eval=FALSE}
plot(10+1:nIterations,varianceValuesBiased, type="l", xlab="Sample size",ylab="Variance values")
lines(10+1:nIterations,varianceValues, col="blue")
abline(h=sigma^2, col="red")
legend("bottomright", legend=c("Biased","Unbiased","Truth"), col=c("black","blue","red"), lwd=1)
```

```{r statsBias, fig.cap="An example with biased estimator", echo=FALSE}
knitr::include_graphics("images/03-statistics-bias.png")
```


Every run of this experiment will produce different plots, but typically what we will see is that, the biased estimate of variance (the black line) will be slightly below the unbiased one (the blue line) on small samples, and will converge to it assymptotically (with the increase of the sample size). In the example above, the black line is below the blue one all the time until the sample size becomes big enough that the difference between the two becomes negligible. This is the graphical presentation of the bias in the estimator.


### Efficiency
**Efficiency** means, if the sample size increases, then the estimated parameters will not change substantially, they will vary in a narrow range (variance of estimates will be small). In the case with inefficient estimates the increase of sample size from 50 to 51 observations may lead to the change of a parameter from 0.1 to, letâ€™s say, 10. This is bad because the values of parameters usually influence both point forecasts and prediction intervals. As a result the inventory decision may differ radically from day to day. For example, we may decide that we urgently need 1000 units of product on Monday, and order it just to realise on Tuesday that we only need 100. Obviously this is an exaggeration, but no one wants to deal with such an erratically behaving model, so we need to have efficient estimates of parameters.

Another classical example of not efficient estimator is the median, when used on the data that follows normal distribution. Here is a simple experiment demonstrating the idea:
```{r}
mu <- 100
sigma <- 10
nIterations <- 1000
meanValues <- vector("numeric",nIterations)
medianValues <- vector("numeric",nIterations)
x <- rnorm(100000,mu,sigma)
for(i in 1:nIterations){
    meanValues[i] <- mean(x[1:(i*100)])
    medianValues[i] <- median(x[1:(i*100)])
}
```

In order to establish the efficiency of the estimators, we will take their variances and look at the ratio of mean over median. If both are equally efficient, then this ratio will be equal to one. If the mean is more efficient than the median, then the ratio will be less than one:

```{r eval=FALSE}
variancesRatios <- vector("numeric",nIterations-1)
for(i in 2:nIterations){
    variancesRatios[i-1] <- var(meanValues[1:i]) / var(medianValues[1:i])
}
plot(10+2:nIterations*100,variancesRatios, type="l", xlab="Sample size",ylab="Relative efficiency", ylim=range(c(1,variancesRatios)))
abline(h=1, col="red")
```

```{r statsEfficiecny, fig.cap="An example with inefficient estimator", echo=FALSE}
knitr::include_graphics("images/03-statistics-efficiency.png")
```

What we should typically see on this graph, is that the back like should be below the red one, indicating that the variance of mean is lower than the variance of the median. This means that mean is more efficient estimator of the true location of the distribution $\mu$ than the median. In fact, it is easy to proove that asymptotically the mean will be 1.57 times more efficient than median [@WikipediaMedianEfficiency2020] (so, the line should converge to approximately 0.64).


### Consistency
**Consistency** means that our estimates of parameters will get closer to the stable values (true value in the population) with the increase of the sample size. This is important because in the opposite case estimates of parameters will diverge and become less and less realistic. This once again influences both point forecasts and prediction intervals, which will be less meaningful than they should have been. In a way consistency means that with the increase of the sample size the parameters will become more efficient and less biased. This in turn means that the more observations we have, the better.

An example of inconsistent estimator is Chebyshev (or max norm) metric. It is formulated the following way:
\begin{equation}
    \text{LMax} = \max \left(|y_1-\hat{y}|, |y_2-\hat{y}|, \dots, |y_T-\hat{y}| \right).
    (\#eq:chebyshevNorm)
\end{equation}
Minimising this norm, we can get an estimate $\hat{y}$ of the location parameter $\mu$. The simulation experiment becomes a bit more tricky in this situation, but here is the code to generate the estimates of the location parameter:
```{r eval=FALSE}
LMax <- function(x){
    estimator <- function(par){
        return(max(abs(x-par)));
    }
    
    return(optim(mean(x), fn=estimator, method="Brent", lower=min(x), upper=max(x)));
}

mu <- 100
sigma <- 10
nIterations <- 1000
x <- rnorm(10000, mu, sigma)
LMaxEstimates <- vector("numeric", nIterations)
for(i in 1:nIterations){
    LMaxEstimates[i] <- LMax(x[1:(i*10)])$par;
}
```

And here how the estimate looks with the increase of sample size:
```{r eval=FALSE}
plot(1:nIterations*10, LMaxEstimates, type="l", xlab="Sample size",ylab="Estimator of mu")
abline(h=mu, col="red")
```

```{r statsConsistency, fig.cap="An example with inconsistent estimator", echo=FALSE}
knitr::include_graphics("images/03-statistics-consistency.png")
```

While in the example with bias we could see that the lines converge to the red line (the true value) with the increase of the sample size, the Chebyshev metric example shows that the line does not approach the true one, even when the sample size is 10000 observations. The conclusion is that when Chebyshev metric is used, it produces inconsistent estimates of parameters.


```{block, type="remark"}
There is a prejudice in the world of practitioners that the situation in the market changes so fast that the old observations become useless very fast. As a result many companies just through away the old data. Although, in general the statement about the market changes is true, the forecasters tend to work with the models that take this into account (e.g. Exponential smoothing, ARIMA, discussed in this book). These models adapt to the potential changes. So, we may benefit from the old data because it allows us getting more consistent estimates of parameters. Just keep in mind, that you can always remove the annoying bits of data but you can never un-throw away the data.
```


### Asymptotic normality
Finally, **asymptotic normality** is not critical, but in many cases is a desired, useful property of estimates. What it tells us is that the distribution of the estimate of parameter will be well behaved with a specific mean (typically equal to $\mu$) and a fixed variance. Some of the statistical tests and mathematical derivations rely on this assumption. For example, when one conducts a significance test for parameters of model, this assumption is implied in the process. If the distribution is not normal, then the confidence intervals constructed for the parameters will be wrong together with the respective t- and p- values.


### Asymptotics and Likelihood {#asymptoticsAndLikelihood}
Another important aspect to cover is what the term **asymptotic**, which we have already used, means in our context. Here and after in this book, when this word is used, we refer to an unrealistic hypothetical situation of having all the data in the multiverse, where the time index $t \rightarrow \infty$. While this is impossible in practice, the idea is useful, because asymptotic behaviour of estimators and models is helpful on large samples of data. Besides, even if we deal with small samples, it is good to know what to expect to happen if the sample size increases.

Finally, we will use different estimation techniques throughout this book, one of the main of which is **Maximum Likelihood Estimate** (MLE). We will not go into explanation of what specifically this is at this stage, but a rough understanding should suffice. In case of MLE, we assume that a variable follows some distribution and that the parameters of the model that we use can be optimised in order to maximise the respective probability density function. The main advantages of MLE is that it gives *consistent*, asymptotically *efficient* and normal estimates of parameters and allows doing model selection via information criteria.


## Typical assumptions of statistical models {#assumptions}
In order for a statistical model to work adequately and not to fail on data, several assumptions about it, when it is *applied* to the data, should hold. If they do not, then the model might lead to biased or inefficient estimates and forecasts. Here we briefly discuss the main of them. Here they are:

1. Model is correctly specified;
2. The expectation of residuals is zero, no matter what;
3. Residuals are independent and identicaly distributed (i.i.d.);
4. The explanatory variables are not correlated with anything but the response variable;
5. The residuals follow the specified distribution.


### Model is correctly specified
This implies that:

1. We have not omitted important variables in the model (underfitting the data);
2. We do not have redundant variables in the model (overfitting the data);
3. The necessary transformation of the variables are applied;
4. We do not have outliers in the model.

(1): if there are some important variables that we did not include in the model, then the estimates of the parameters might be *biased* and in some cases quite seriously (e.g. positive sign instead of the negative one). This also means that the point forecasts from the model might be *biased* as well (systematic under or over forecasting).

(2): if there are redundant variables that are not needed in the model, then the estimates of parameters and point forecasts might be *unbiased*, but *inefficient.* This implies that the variance of parameters can be lower than needed and the prediction intervals can be narrower than needed.

(3): this means that, for example, instead of using a multiplicative model, we apply an additive one. The estimates of parameters and the point forecasts might be *biased* in this case as well: the model will produce linear trajectory of the forecast, when a non-linear one is needed.

(4): in a way, this is similar to (1), the presence of outliers might mean that we have missed some important information, meaning that the estimates of parameters and forecasts would be *biased* as well. There can be other reasons for outliers as well. For example, we might be using a wrong distributional assumptions. If so, this would imply that the prediction intervals from the model are narrower than needed.


### The expectation of residuals is zero, no matter what
While in sample, this holds automatically in many cases (e.g. when using Least Squares method), this assumption might be violated in the holdout sample. In this case the point forecasts would be *biased*, because they typically do not take the non-zero mean of forecast error into account, and the prediction interval might be off as well, because of the wrong estimation of the scale of distribution (e.g. variance is higher than needed).

This assumption also implies that the expectation of residuals is zero even conditional on the explanatory variables in the model. If it is not, then this might mean that there is still some important information omitted in the applied model.


### Residuals are i.i.d.
There are two assumptions in this group:

1. There is no autocorrelation in the residuals;
2. The residuals are homoscedastic.

(1): we expect that the model captures all the important aspects, so if the residuals are autocorrelated, then something is neglected by the applied model. Typically, this leads to *inefficient* estimates of parameters and in some cases they can also become *biased*. As a result, the point forecasts can be less accurate than expected and the prediction intervals might be wrong (wider or narrower than needed).

(2): if this is violated, then we say that there is a **heteroscedasticity** in the model. This means that with a change of variable, the variance of the residuals changes as well. If the model neglects this, then typically the estimates of parameters become *inefficient* and prediction intervals are wrong: they are wider than needed in some cases and narrower than needed in the other ones.


### The explanatory variables are not correlated with anything but the response variable
There are two cases here as well:

1. No multicollinearity;
2. No endogeneity;

(1): the effect of **multicollinearity** implies that the variables included in the model are linearly dependent from each other. In this case, it becomes difficult to distinguish the effect of one variables from the other one. As a result, the estimates of parameters become *inefficient* and might become *biased* in some sever cases. In case of forecasting, the effect is not as straight forward, and in some cases might not damage the point forecasts, but can lead to prediction intervals of an incorrect width.

(2): **endogeneity** applies to the situation, when the dependent variable $y_t$ influences the explanatory variable $x_t$ in the model on the same observation. The relation in this case becomes bi-directional, meaning that the basic model is not appropriate in this situation anymore. The parameters and forecasts will typically be *biased*, and a different estimation method is needed or maybe a different model would need to be constructed in order to fix this.


### The variable follows the specified distribution
Finally, in some cases we are interested in using methods that imply specific distributional assumptions about the model and its residuals. For example, it is assumed in the classical linear model that the error term follows Normal distribution. Estimating this model using MLE with the probability density function of Normal distribution or via minimisation of [MSE](#errorMeasures) would give *efficient* and *consistent* estimates of parameters. If the assumption of normality does not hold, then the estimates might be *inefficient* and in some cases *inconsistent*. When it comes to forecasting, the main issue in the wrong distributional assumption appears, when prediction intervals are needed: they might rely a wrong distribution and be narrower or wider than needed. Finally, if we deal with the wrong distribution, then the model selection mechanism might be flawed and would lead to the selection of an inappropriate model.

In many cases, in our discussions in this textbook, we assume that all of these assumptions hold. In some of the cases, we will say explicitly, which are violated and what needs to be done in those situations.

Now that we have a basic understanding of these statistical terms, we can move to the next topic, distributions.


## Theory of distributions {#distributions}
There are several probability distributions that will be helpful in the further chapters of this textbook. Here, I want to briefly discuss those of them that will be used. While this might not seem important at this point, we might refer to this section of the textbook in the next chapters.

### Normal distribution
Every statistical textbook has normal distribution. It is that one famous bell-curved distribution that every statistician likes because it is easy to work with and it is an asymptotic distribution for many other well-behaved distributions in some conditions (so called "Central Limit Theorem"). Here is the probability density function (PDF) of this distribution:
\begin{equation}
    f(y_t) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{\left(y_t - \mu_t \right)^2}{2 \sigma^2} \right) ,
    (\#eq:Normal)
\end{equation}
where $y_t$ is the value of the response variable, $\mu_t$ is the mean on observation $t$ and $\sigma^2$ is the variance of the error term. The maximum likelihood estimate of $\sigma^2$ is:
\begin{equation}
    \hat{\sigma}^2 = \frac{1}{T} \sum_{t=1}^T \left(y_t - \mu_t \right)^2 ,
    (\#eq:sigmaNormal)
\end{equation}
which coincides with Mean Squared Error (MSE), discussed in the [section 1](#errorMeasures).

And here how this distribution looks:

```{r dnormPlot, echo=FALSE}
plot(seq(-3,3,0.1),dnorm(seq(-3,3,0.1)),type="l",ylab="Density",xlab="x",main="PDF of Normal distribution")
abline(v=0, col="red")
text(0.5,0.05,TeX("$\\mu =0$"))
```

What we typically assume in the basic time series models is that a variable is random and follows normal distribution, meaning that there is a central tendency (in our case - the mean $mu$), around which the concentration of values is the highest and there are other potential cases, but their probability of appearance reduces proportionally to the distance from the centre.

The normal distribution has skewness of zero and kurtosis of 3 (and excess kurtosis, being kurtosis minus three, of 0).

Additionally, if normal distribution is used for the maximum likelihood estimation of a model, it gives the same parameters as the minimisation of MSE would give.


### Laplace distribution
A more exotic distribution is Laplace, which has some similarities with Normal, but has higher excess. It has the following PDF:

\begin{equation}
    f(y_t) = \frac{1}{2 s} \exp \left( -\frac{\left| y_t - \mu_t \right|}{s} \right) ,
    (\#eq:Laplace)
\end{equation}
where $s$ is the scale parameter, which, when estimated using likelihood, is equal to the Mean Absolute Error (MAE, from [section 1](#errorMeasures)):
\begin{equation}
    \hat{s} = \frac{1}{T} \sum_{t=1}^T \left| y_t - \mu_t \right| .
    (\#eq:sLaplace)
\end{equation}

It has the following shape:

```{r dlaplacePlot, echo=FALSE}
plot(seq(-3,3,0.01),dlaplace(seq(-3,3,0.01)),type="l",ylab="Density",xlab="x",main="PDF of Laplace distribution")
abline(v=0, col="red")
```

Similar to the normal distribution, the skewness of Laplace is equal to zero. However, it has fatter tails - its kurtosis is equal to 6 instead of 3.

The `dlaplace`, `qlaplace`, `plaplace` and `rlaplace` functions from `greybox` package implement different sides of Laplace distribution in R.


### S distribution
This is something relatively new, but not ground braking. I have derived S distribution few years ago, but have never written a paper on that. It has the following density function:
\begin{equation}
    f(y_t) = \frac{1}{4 s^2} \exp \left( -\frac{\sqrt{|y_t - \mu_t|}}{s} \right) ,
    (\#eq:S)
\end{equation}
where $s$ is the scale parameter. If estimated via maximum likelihood, the scale parameter is equal to:
\begin{equation}
    \hat{s} = \frac{1}{2T} \sum_{t=1}^T \sqrt{\left| y_t - \mu_t \right|} ,
    (\#eq:sS)
\end{equation}
which corresponds to the minimisation of a half of "Mean Root Absolute Error" or "Half Absolute Moment" (HAM). This is a more exotic type of scale, but the main benefit of this distribution is sever heavy tails - it has kurtosis of 25.2. It might be useful in cases of randomly occurring incidents and extreme values (Black Swans?).

```{r dsPlot, echo=FALSE}
plot(seq(-3,3,0.01),ds(seq(-3,3,0.01)),type="l",ylab="Density",xlab="x",main="PDF of S distribution")
abline(v=0, col="red")
```

The `ds`, `qs`, `ps` and `rs` from `greybox` package implement the density, quantile, cumulative and random generation functions.


### Generalised Normal distribution
Generalised Normal ($\mathcal{GN}$) distribution (as the name says) is a generalisation for normal distribution, which also includes Laplace and S as special cases [@Nadarajah2005]. There are two versions of this distribution: one with a shape and another with a skewness parameter. We are mainly interested in the first one, which has the following PDF:
\begin{equation}
    f(y_t) = \frac{\lambda}{2 s \Gamma(\lambda^{-1})} \exp \left( -\left(\frac{|y_t - \mu_t|}{s}\right)^{\lambda} \right),
    (\#eq:GND)
\end{equation}
where $\lambda$ is the shape parameter, and $s$ is the scale of the distribution, which, when estimated via MLE, is equal to:
\begin{equation}
    \hat{s} = \sqrt[^{\lambda}]{\frac{\lambda}{T} \sum_{t=1}^T\left| y_t - \mu_t \right|^{\lambda}},
    (\#eq:sGND)
\end{equation}
which has MSE, MAE and HAM as special cases, when $\lambda$ is equal to 2, 1 and 0.5 respectively. The parameter $\lambda$ influences the kurtosis directly, it can be calculated for each special case as $\frac{\Gamma(5/\lambda)\Gamma(1/\lambda)}{\Gamma(3/\lambda)^2}$. The higher $\lambda$ is, the lower the kurtosis is.

The advantage of $\mathcal{GN}$ distribution is it's flexibility. In theory, it is possible to model extremely rare events with this distribution, if the shape parameter $\lambda$ is fractional and close to zero. Alternatively, when $\lambda \rightarrow \infty$, the distribution converges point-wise to the uniform distribution on $(\mu_t - s, \mu_t + s)$.

Note that the estimation of $\lambda$ is a difficult task, especially, when it is less than 2 - the MLE of it looses properties of consistency and asymptotic normality.

Depending on the value of $\lambda$, the distribution can have different shapes:

```{r dgnormPlot, echo=FALSE}
plot(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,0.5),type="l",ylab="Density",xlab="x",main="PDF of Generalised Normal distribution",
     ylim=c(0,0.6))
lines(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,1),col="darkblue")
lines(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,2),col="darkred")
lines(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,1000),col="purple")
abline(v=0, col="red")
legend("topright",legend=c(TeX("$\\lambda =0.5$"),TeX("$\\lambda =1$"),TeX("$\\lambda =2$"),TeX("$\\lambda =1000$")),
       col=c("black","darkblue","darkred","purple"),lwd=1)
```

Typically, estimating $\lambda$ consistently is a tricky thing to do, especially if it is less than one. Still, it is possible to do that by maximising the likelihood function \@ref(eq:GND).

The working functions for the Generalised Normal are implemented in the `gnorm` package for R.


### Asymmetric Laplace distribution

Asymmetric Laplace distribution ($\mathcal{AL}$) can be considered as a two Laplace distributions with different parameters $s$ for left and right sides from the location $\mu_t$. There are several ways to summarise the probability density function, the neater one relies on the asymmetry parameter $\lambda$ [@Yu2005]:
\begin{equation}
    f(y_t) = \frac{\lambda (1- \lambda)}{s} \exp \left( -\frac{y_t - \mu_t}{s} (\lambda - I(y_t \leq \mu_t)) \right) ,
    (\#eq:ALaplace)
\end{equation}
where $s$ is the scale parameter, $\lambda$ is the skewness parameter and $I(y_t \leq \mu_t)$ is the indicator function, which is equal to one, when the condition is satisfied and to zero otherwise. The scale parameter $s$ estimated using likelihood is equal to the quantile loss:
\begin{equation}
    \hat{s} = \frac{1}{T} \sum_{t=1}^T \left(y_t - \mu_t \right)(\lambda - I(y_t \leq \mu_t)) .
    (\#eq:sALaplace)
\end{equation}
Thus maximising the likelihood \@ref(eq:ALaplace) is equivalent to estimating the model via the minimisation of $\lambda$ quantile, making this equivalent to quantile regression approach. So quantile regression models assume indirectly that the error term in the model is $\epsilon_t \sim \mathcal{AL}(0, s, \lambda)$ [@Geraci2007].

Depending on the value of $\lambda$, the distribution can have different shapes:

```{r dALaplacePlot, echo=FALSE}
plot(seq(-3,3,0.01),dalaplace(seq(-3,3,0.01),0,1,0.5),type="l",ylab="Density",xlab="x",main="PDF of Asymmetric Laplace distribution",
     ylim=c(0,0.6))
lines(seq(-3,3,0.01),dalaplace(seq(-3,3,0.01),0,1,0.2),col="darkblue")
lines(seq(-3,3,0.01),dalaplace(seq(-3,3,0.01),0,1,0.8),col="darkred")
abline(v=0, col="red")
legend("topright",legend=c(TeX("$\\lambda =0.5$"),TeX("$\\lambda =0.2$"),TeX("$\\lambda =0.8$")),
       col=c("black","darkblue","darkred"),lwd=1)
```

Similarly to $\mathcal{GN}$ distribution, the parameter $\lambda$ can be estimated during the maximisation of the likelihood, although it makes more sense to set it to some specific values in order to obtain the desired quantile of distribution.

Functions `dalaplace`, `qalaplace`, `palaplace` and `ralaplace` from `greybox` package implement the Asymmetric Laplace distribution.


### Log Normal, Log Laplace, Log S and Log GN distributions

In addition, it is possible to derive the log-versions of the Normal, \mathcal{Laplace}, \mathcal{S}, and \mathcal{GN} distributions. The main differences between the original and the log-versions of density functions for these distributions can be summarised as follows:
\begin{equation}
    f_{log}(\log(y_t)) = \frac{1}{y_t} f(\log y_t).
    (\#eq:logDistribution)
\end{equation}
They are defined for positive values only and will have different right tail, depending on the location, scale and shape parameters. $\exp(\mu_t)$ in this case represents the geometric mean (and median) of distribution rather than the arithmetic one. The conditional expectation in these distributions is typically higher than $\exp(\mu_t)$ and depends on the value of the scale parameter.


### Inverse Gaussian distribution

An exotic distribution that will be useful for what comes in this textbook is the Inverse Gaussian ($\mathcal{IG}$), which is parameterised using mean value $\mu_t$ and either the dispersion parameter $s$ or the scale $\lambda$ and is defined for positive values only. This distribution is useful because it is scalable and has some similarities with the Normal one. In our case, the important property is the following:
\begin{equation}
    \text{if } \epsilon_t \sim \mathcal{IG}(1, s) \text{, then }
    y_t = \mu_t \times \epsilon_t \sim \mathcal{IG}\left(\mu_t, \frac{s}{\mu_t} \right),
    (\#eq:InverseGaussianModel)
\end{equation}
implying that the dispersion of the model changes together with the expectation. The PDF of the distribution of $\epsilon_t$ is:

\begin{equation}
    f(\epsilon_t) = \frac{1}{\sqrt{2 \pi s \epsilon_t^3}} \exp \left( -\frac{\left(\epsilon_t - 1 \right)^2}{2 s \epsilon_t} \right) ,
    (\#eq:InverseGaussian)
\end{equation}
where the dispersion parameter can be estimated via maximising the likelihood and is calculated using:
\begin{equation}
    \hat{s} = \frac{1}{T} \sum_{t=1}^T \frac{\left(\epsilon_t - 1 \right)^2}{\epsilon_t} .
    (\#eq:InverseGaussianDispersion)
\end{equation}
This distribution becomes very useful for multiplicative models, where it is expected that the data can only be positive.

Here is how the PDF of $\mathcal{IG}(1,s)$ looks for different values of the dispersion $s$:
```{r dIGPlot, echo=FALSE}
plot(seq(0,6,0.01),dinvgauss(seq(0,6,0.01),1,dispersion=10),type="l",ylab="Density",xlab="x",main="PDF of Inverse Gaussian distribution")
lines(seq(0,6,0.01),dinvgauss(seq(0,6,0.01),1,dispersion=1),col="darkblue")
lines(seq(0,6,0.01),dinvgauss(seq(0,6,0.01),1,dispersion=0.1),col="darkred")
abline(v=1, col="red")
legend("topright",legend=c(TeX("$s =10$"),TeX("$s =1$"),TeX("$s =0.1$")),
       col=c("black","darkblue","darkred"),lwd=1)
```

`statmod` package implements density, quantile, cumulative and random number generator functions for the $\mathcal{IG}$.


## Model selection mechanism {#modelSelection}
There are different ways how to select the most appropriate model for the data. One can use judgment, statistical tests, cross-validation or meta learning. The state of the art one in the field of exponential smoothing relies on the calculation of information criteria and on selection of the model with the lowest value. This approach is discussed in detail in @Burnham2004. Here we briefly explain how this approach works and what are its advantages and disadvantages.

Before we move to the mathematics and well-known formulae, it makes sense to understand what we are trying to do, when we use information criteria. The idea is that we have a pool of model under consideration, and that there is a true model somewhere out there (not necessarily in our pool). This can be presented graphically in the following way:

```{r AICModelsPlot, echo=FALSE, fig.width=6, fig.height=5, fig.cap="An example of a model space"}
par(mar=c(1,1,1,1))
plot(c(0,-3,-6,3,6),c(0,2,-6,5,-1),xlim=c(-8,8),ylim=c(-8,8),pch=16,axes=F,xlab="",ylab="")
lines(c(0,-3),c(0,2), col="grey", lty=2)
lines(c(0,-6),c(0,-6), col="grey", lty=2)
lines(c(0,3),c(0,5), col="grey", lty=2)
lines(c(0,6),c(0,-1), col="grey", lty=2)
points(0,0,col="red",pch=16)
text(c(0,-3,-6,3,6),c(0,2,-6,5,-1),c("True model","Model 1","Model 2","Model 3","Model 4"),pos=3)
box()
```

This plot \@ref(fig:AICModelsPlot) represents a space of models. There is a [true one](#intro) in the middle, and there are three models under consideration: Model 1, Model 2, Model 3 and Model 4. They might differ from each other via the form (additive or multiplicative), included or omitted variables. They have some distances (they grey dashed lines) from the true model on this hypothetic model space: Model 1 is the closest to the true one than the others, while Model 2 is the farthest. Models 3 and 4 have a similar distance to the truth.

In the model selection exercise, what we typically want to do is to select the model, closest to the true one (Model 1 in our case). It is relatively easy to do, when you know the true model: just measure the distances and select the closest one. This can be written very roughly as:
\begin{equation}
    \begin{split}
        d_1 = \ell^* - \ell_1 \\
        d_2 = \ell^* - \ell_2 \\
        d_3 = \ell^* - \ell_3 \\
        d_4 = \ell^* - \ell_4
    \end{split} ,
    (\#eq:AICdistances)
\end{equation}
where $\ell_j$ is the position of the $j^{th}$ model and $\ell^*$ is the position of the true one. One of ways of getting the position of the model is by calculating the [log-likelihood](#asymptoticsAndLikelihood) (logarithms of likelihood) values for each model, based on the assumed [distributions](#distributions). The likelihood of the true model will always be fixed, so if it is known, it just comes to calculating the values for the models 1 - 4 and inserting all the known values in the equation \@ref(eq:AICdistances), and selecting the model that has the lowest distance $d_j$.

However, in real life we never know the true model, so we need to find some other way of measuring this distance. The good thing about this approach is that the true model will always have the highest possible likelihood. This means that it is not important to know $\ell^*$ - it will be the same for all the models. So, we can drop the $\ell^*$ in the formulae \@ref(eq:AICdistances) and compare the models via their likelihoods $\ell_1, \ell_2, \ell_3 \text{ and } \ell_4$:
\begin{equation}
    \begin{split}
        d_1 = - \ell_1 \\
        d_2 = - \ell_2 \\
        d_3 = - \ell_3 \\
        d_4 = - \ell_4
    \end{split} ,
    (\#eq:AICdistancesfixed)
\end{equation}
This is a very simple method that allows us to get to the model, closest to the true one in the pool. However, we should not forget that we usually work with samples of data, not the population. So, we will inevitably have estimates of likelihoods, not the true ones. They will be biased and will need to be corrected. @Akaike1974 showed that the bias can be corrected if the number of parameters in each model is added to the distances \@ref(eq:AICdistancesfixed), resultin in the bias corrected formula:
\begin{equation}
    d_j = k_j - \ell_j
    (\#eq:AICNormal),
\end{equation}
where $k_j$ is the number of all estimated parameters in the model $j$ (this typically also includes scale parameters, when dealing with Maximum Likelihood Estimates). When studying the properties of \@ref(eq:AICNormal), @Akaike1974 suggested to multiply both parts of the right hand side by 2, so that there is a connection between the proposed criterion and the well known likelihood-ratio test [@WikipediaLikelihoodRatioTest2020], and he proposed the following "An Information Criterion":
\begin{equation}
    \text{AIC}_j = 2 k_j - 2 \ell_j
    (\#eq:AIC).
\end{equation}

After that, there have been proposed different other criteria, motivated by similar ideas, among which it is worth mentioning:

- AICc [@Sugiura1978], which is a sample corrected version of AIC (taking number of observations into account) for normal and related distributions:
\begin{equation}
    \text{AICc}_j = 2 \frac{T}{T-k_j-1} k_j - 2 \ell_j
    (\#eq:AICc),
\end{equation}
where $T$ is the sample size.

- BIC [@Schwarz1978] (aka "Schwarz criterion"), which criterion, developed on Bayesian statistics:
\begin{equation}
    \text{BIC}_j = \log(T) k_j - 2 \ell_j
    (\#eq:BIC).
\end{equation}

- BICc [@McQuarrie1999] - the sample-corrected version of BIC, relying on the assumption of normality:
\begin{equation}
    \text{BICc}_j = \frac{\log T}{T-k_j-1} k_j - 2 \ell_j
    (\#eq:BICc).
\end{equation}

In general, it is recommended to use the sample-corrected versions of criteria (AICc, BICc) and use the others in cases of large samples (thousands of observations), where the effect of number of observations on criteria becomes negligible. The main issue is that the corrected versions of information criteria for the non-normal distributions need to be derived separately and will differ from \@ref(eq:AICc) and \@ref(eq:BICc). Still, @Burnham2004 recommend using formulae \@ref(eq:AICc) and \@ref(eq:BICc) in cases of small samples, even if the distribution of variable does not follow the normal one and the correct formulae are not known. The motivation for this is that the corrected versions still take sample size into account, correcting the sample bias in criteria to some extent.

A thing to note is that the approach relies on asymptotic properties of estimators and in fact assumes that the parameters estimation method used in the process, guarantees that the likelihood functions of the models are maximised. In fact, it relies on [asymptotic](#asymptoticsAndLikelihood) behaviour of parameters, so it is not very important to get to the maximum of the likelihood in sample. However, it is more important to use an estimation method that will guarantee consistent maximisation of the likelihood.

Summarising, the idea of model selection via information criteria is to form a pool of competing models, construct them, calculate likelihood function, based on them - an information criterion, and finally, select the model that has the lowest value. This approach is relatively fast (in contrast comparison with cross-validation, judgmental selection or meta learning) and has good theory behind it. It can also be shown that in case of normal distribution, the selection for time series models based on AIC is asymptotically equivalent to the selection based on [leave-one-out cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation) with MSE. This becomes relatively straightforward, if we recall that typically time series models rely on one step ahead errors $(e_t = y_t - \mu_{t|t-1})$ and that the maximum of the likelihood of normal distribution gives the same estimates as the minimum of MSE.

As for the disadvantages of the approach, as mentioned above, it relies on the in-sample value of the likelihood, based on one step ahead error, and does not guarantee that the selected model will perform well for the holdout for multiple steps ahead. Using the cross-validation or [rolling origin](#rollingOrigin) for the full horizon could be a better option if you suspect that information criteria do not work. Furthermore, any criterion is random on its own, and will change with the change of the sample size. This means that there is a model selection uncertainty and that the best model might change with the new observations. In order to address this issue, combination of models can be used instead, which allows mitigating this uncertainty.
