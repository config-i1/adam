# Forecasting functions of the smooth package
While ADAM is the main focus of this monograph, there are several functions in the `smooth` package that implement some special cases of it or some related models. In fact, all the models implemented in the `smooth` package are formulated in one and the same SSOE framework, discussed in this monograph. In this chapter, I briefly explain how the specific functions of the package are related to ADAM and how they are formulated. I conclude the chapter with an `adam()` cheat sheet, listing the main commands that can be used to apply the model to a data in variety of scenarios.

Some of the functions discussed in this chapter are also explained in @svetunkov2023smooth.

## Exponential Smoothing
### ETS, es()
The `es()` function from the `smooth` package implements the conventional ETS model [from @Hyndman2008b] with some modifications. The fundamental difference of the function from the `ets()` from the `forecast` package is that it is implemented in the SSOE framework discussed in Section \@ref(ADAMETSGeneral), i.e. with lagged values of the state vector and a smaller transition matrix. In fact, starting from the `smooth` v3.2.0, it is just a wrapper of the `adam()` function, restricting it to:

- The single seasonal ETS (multiple seasonal cycles are supported by `adam()`, as discussed in Chapter \@ref(ADAMMultipleFrequencies));
- The Normal distribution for the $\epsilon_t$ (Sections \@ref(ADAMETSAdditiveDistributions) and \@ref(ADAMETSMultiplicativeDistributions));
- Continuous data only (no support for the occurrence model from Chapter \@ref(ADAMIntermittent));
- ETS and regression (no ARIMA elements as in Chapter \@ref(ADAMARIMA));
- Location model only (no scale model from Chapter \@ref(ADAMscaleModel)).

Still, the function supports the main features of ETS and introduces those that are not available in either `ets()` from the `forecast` package or `ETS()` from the `fable`:

1. Explanatory variables in the ETSX model from Chapter \@ref(ADAMX);
2. A variety of loss functions, including the custom one, as discussed in Chapter \@ref(ADAMETSEstimation);
3. Different initialisation methods (see Section \@ref(ADAMInitialisation));
4. Diagnostic plots from Chapter \@ref(diagnostics);
5. Components and explanatory variables selection and combinations via approaches discussed in Chapter \@ref(ADAMSelection);
6. Covariance matrix of parameters and related methods for reapplication of an ETS model (Chapter \@ref(ADAMUncertainty));
7. Data simulation as in Section \@ref(ADAMUncertaintySimulation);
8. All types of prediction intervals and other related aspects discussed in Chapter \@ref(ADAMForecasting).

Being a wrapper of the `adam()`, `es()` is agnostic of the input data, working with any object, including `ts`, `zoo`, `tibble`, etc as long as the provided object has only one column. The seasonal lag can be regulated in the function using the `lags` variable, the same way it is done in `adam()`. The explanatory variables in `es()` are provided in a form of a matrix in the variable `xreg` (similarly to how it is done in `arima()` from the `stats` package).


### Complex Exponential Smoothing, ces()
Complex Exponential Smoothing (CES) is a model lying outside of the conventional ETS taxonomy. It does not have explicit level/trend components, but instead it has a non-linear trend, which changes its shape depending on the value of the complex smoothing parameter. I developed this model in my PhD, and it is explained in detail in the @Svetunkov2015 paper. It is formulated as a set of the following equations:
\begin{equation}
	\begin{aligned}
		& y_t = l_{t-1} + \epsilon_t \\
		& l_t = l_{t-1} - (1 - \alpha_1) c_{t-1} + (\alpha_0 - \alpha_1) \epsilon_t \\
		& c_t = l_{t-1} + (1 - \alpha_0) c_{t-1} + (\alpha_0 + \alpha_1) \epsilon_t
	\end{aligned} ,
  (\#eq:cesstatespace)
\end{equation}
where $\epsilon_t$ is the white noise error term, $l_t$ is the level component, $c_t$ is the non-linear trend component, and $\alpha_0$ and $\alpha_1$ are the smoothing parameters. The model can also be formulated in the same SSOE framework \@ref(eq:ETSADAMStateSpacePureAdditive) discussed in Section \@ref(ADAMETSPureAdditive), which makes it easy to implement using the same code and to compare with ADAM ETS and/or ARIMA via information criteria. As for the term "complex", it arises because of how the model was originally formulated in its linear form, using complex variables:
\begin{equation}
	\hat y_{t} + i \hat e_{t} = (\alpha_0 + i\alpha_1)(y_{t-1} + i e_{t-1}) + (1 - \alpha_0 + i - i\alpha_1)(\hat y_{t-1} + i \hat e_{t-1}) ,
  (\#eq:cesalgebraic)
\end{equation}
where $i^2=-1$ is the imaginary unit. The model itself allows distributing the weights over the actual observations in a harmonic or in an exponential way, capturing long-term non-linear patterns in the data. It works well in a median combination of models, together with ETS, ARIMA, and Theta, as it was shown in @Petropoulos2020.

In addition to the non-seasonal CES, there is a modification that allows capturing either additive or multiplicative seasonality via lagged components, similarly to \@ref(eq:cesstatespace), which is formulated as:
\begin{equation}
	\begin{aligned}
		& y_t = l_{0,t-1} + l_{1,t-m} + \epsilon_t \\
		& l_{0,t} = l_{0,t-1} - (1 - \alpha_1) c_{0,t-1} + (\alpha_0 - \alpha_1) \epsilon_t \\ 
		& c_{0,t} = l_{0,t-1} + (1 - \alpha_0) c_{0,t-1} + (\alpha_0	+ \alpha_1) \epsilon_t \\
		& l_{1,t} = l_{1,t-m} - (1 - \beta_1) c_{1,t-m} + (\beta_0 - \beta_1) \epsilon_t \\
		& c_{1,t} = l_{1,t-m} + (1 - \beta_0) c_{1,t-m} + (\beta_0 + \beta_1) \epsilon_t
	\end{aligned} .
	(\#eq:cesseasonalfull)
\end{equation}
Similarly to the non-seasonal CES, \@ref(eq:cesseasonalfull) can be formulated in the SSOE framework discussed in Section \@ref(ADAMETSPureAdditive). Based on that, it is possible to select between the seasonal and the non-seasonal CES using information criteria. The model is also extendible to include other components or explanatory variables. Finally, while it is possible to formulate the model with a variety of distributions, the implementation so far only supports the Normal distribution.

The function `ces()` in the `smooth` package supports many features discussed in this monograph:

1. Explanatory variables, similar to the ETSX model from Chapter \@ref(ADAMX), done via `xreg` variable;
2. A variety of loss functions, as discussed in Chapter \@ref(ADAMETSEstimation), via `loss` variable;
3. Different initialisation methods (see Section \@ref(ADAMInitialisation));
4. Diagnostic plots (Chapter \@ref(diagnostics));
5. Explanatory variables selection (Section \@ref(ETSXSelection));
7. Data simulation (similar to the ADAM discussed in Section \@ref(ADAMUncertaintySimulation), done using the `sim.ces()` function or the `simulate()` method applied to an estimated `ces()` model);
8. All types of prediction intervals and other related aspects discussed in Chapter \@ref(ADAMForecasting).

Finally, the `auto.ces()` function does selection between the seasonal and non-seasonal CES models using information criteria. By default, in addition to the models discussed above, it will also consider a "simple" seasonal CES with a classical additive seasonal component, similar to the one in the conventional ETS, and will select the model with the lowest AICc.


## ARIMA
The `smooth` package also includes two distinct ARIMA functions, which are implemented in the SSOE framework, and have slightly different purposes.

### Multiple Seasonal ARIMA, msarima()
The `msarima()` function implements the multiple seasonal ARIMA model as discussed in Chapter \@ref(ADAMARIMA). In fact, starting from smooth v3.2.0, it is just a wrapper of `adam()`, similarly to how `es()` is its wrapper for the ETS counterpart. Being a wrapper, the function does not support all the features of ADAM, but still has the following functionality (similar to the `es()`):

1. Explanatory variables in the ARIMAX model (Chapter \@ref(ADAMX));
2. Different loss functions, including the custom one (Chapter \@ref(ADAMETSEstimation));
3. Different initialisation methods (Section \@ref(ADAMInitialisation));
4. Diagnostic plots (Chapter \@ref(diagnostics));
5. Orders and explanatory variables selection (Chapter \@ref(ADAMSelection));
6. Covariance matrix of parameters and related methods for reapplication of the ARIMA model (Chapter \@ref(ADAMUncertainty));
7. Data generation from an estimated model (Section \@ref(ADAMUncertaintySimulation));
8. All types of prediction intervals and other related aspects (Chapter \@ref(ADAMForecasting)).

Being a part of ADAM, the function is tailored for multiple seasonality and works efficiently in this case, especially when compared with other ARIMA implementations in R. It only supports the Normal distribution and focuses on the location model for continuous data (non-intermittent). Furthermore, similar to the `es()` function, it is agnostic of the provided data and will work with any type of univariate object (vector, ts, zoo, etc). Finally, the explanatory variables can be provided via the `xreg` parameter, similar to how it is done in the `arima()` function from the `stats` package.

The selection of the ARIMA orders is implemented via the `auto.msarima()` function, which has a slightly different algorithm than the one explained in Section \@ref(ARIMASelection). It does selection based on an information criteria (IC), checking sequentially the orders of I, then MA and the AR for all the orders up until the ones provided by the user. So, for example, the following code:

```{r eval=FALSE}
auto.msarima(y, orders=list(ar=c(3,3),
                            i=c(2,1),
                            ma=c(3,3)),
             lags=c(1,12))
```

will test different orders of SARIMA up to SARIMA(3,2,3)(3,1,3)$_{12}$. If the parameter `fast` is `TRUE` then the function will skip orders that do not lead to the reduction of IC (e.g. if MA(3) does not improve the IC, it will not test MA(2) and MA(1)). As a final step, the function will compare the model with constant with the model without it and select the most appropriate one. Finally, the `auto.msarima()` also supports combination of forecasts from ARIMA models from the same pool as in the case of order selection described above.


### State space ARIMA, ssarima()
The `ssarima()` function implements ARIMA in a different form, the one explained in Chapter 11 of @Hyndman2008b and then discussed in @Svetunkov2019. It can be considered a conventional SSOE state space ARIMA. The main difference with MSARIMA is in its architecture, i.e. the measurement vector $\mathbf{w}$ and the transition matrix $\mathbf{F}$ differ between the two implementations. The measurement vector is always of the style:
\begin{equation}
	\mathbf{w} = \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix} ,
  (\#eq:SSARIMAMeasurement)
\end{equation}
while the size of the transition matrix is $K^2$, where $K$ is the maximum order of the ARI or MA polynomial. This contrasts with the ADAM ARIMA (and consequently `msarima()`), where the size of the matrix equals to the number of elements in the polynomial equation. So, for example, SARIMA(1,1,2)(0,1,0)$_12$ can be expanded into the following equation (based on a similar model discussed in Section \@ref(StateSpaceARIMA)):
\begin{equation*}
    {y}_{t} = (1+\phi_1) {y}_{t-1} -\phi_1 {y}_{t-2} + {y}_{t-12} -(1+\phi_1) {y}_{t-13} + \phi_1 {y}_{t-14} + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \epsilon_t .
\end{equation*}
Based on this, in the case of SSARIMA, the size of the transition matrix will be $14 \times 14$, while in the case of MSARIMA, the matrix will be $5 \times 5$. This difference in complexity pays of for MSARIMA when a multiple frequency model is needed, because the size of the transition matrix will impact the computational speed. Summarising all of this, `ssarima()` is not suitable for multiple frequencies, because it relies on the conventional state space architecture. Still, it has an advantage: it is in general easier to initialise than the ADAM ARIMA, because the vector of initials equals to $K$, while in case of the latter it is up to $\frac{K(K+1)}{2}$.

Finally, the `auto.ssarima()` function implements the order selection for state space ARIMA based on the algorithm explained in @Svetunkov2019.


## Cheat sheet for adam() function {#cheatSheet}
This section summarises the main ways of using `adam()` in R, with references to the chapters and sections where specific elements were discussed.

Estimate the best ETS model for the provided data (Section \@ref(ETSSelection)):
```{r eval=FALSE}
ourModel <- adam(y)
```

The best ETS, taking the last 10 observations of time series as a holdout to test the performance of model:
```{r eval=FALSE}
ourModel <- adam(y, h=10, holdout=TRUE)
```

Build a pure multiplicative seasonal ETS with an arbitrary seasonal lag of 7 (can be applied to an object `y` of any class, including `ts`, `vector`, `matrix`, `zoo`, `tibble`, etc):
```{r eval=FALSE}
ourModel <- adam(y, model="YYM", lags=7)
```

Estimate the best ARIMA model for the provided data (assuming seasonal lag of 7, Section \@ref(ARIMASelection)):
```{r eval=FALSE}
ourModel <- adam(y, model="NNN", lags=7,
                 orders=list(ar=c(3,2),
                             i=c(2,1),
                             ma=c(3,2),
                             select=TRUE))
```

Build ARIMA(0,1,1) with drift (Chapter \@ref(ARIMA) for the general ARIMA and Section \@ref(ARMAConstant) for the one with constant):
```{r eval=FALSE}
ourModel <- adam(y, model="NNN",
                 orders=c(0,1,1), constant=TRUE)
```

Estimate ETS(A,N,N)+ARIMA(1,0,0) model (Sections \@ref(ARIMAandETS) and \@ref(ETSAndARIMA)):
```{r eval=FALSE}
ourModel <- adam(y, model="ANN", orders=c(1,0,0))
```

Use Generalised Normal distribution for the residuals of ADAM ETS(A,A,N) (Sections \@ref(ADAMETSAdditiveDistributions), \@ref(ADAMETSMultiplicativeDistributions), and \@ref(ADAMETSEstimationLikelihood)):
```{r eval=FALSE}
ourModel <- adam(y, model="AAN", distribution="dgnorm")
```

Select the best distribution for the specific ADAM ETS(A,A,N) (Chapter \@ref(ADAMSelection)):
```{r eval=FALSE}
ourModel <- auto.adam(y, model="AAN")
```

Select the most appropriate ETSX model for the provided `data` (which can be any two-dimensional object, such as `matrix`, `data.frame`, or `tibble`, see Chapter \@ref(ADAMX)):
```{r eval=FALSE}
ourModel <- adam(data)
```

Specify, which explanatory variables to include and in what form (Section \@ref(ADAMXFormulation)):
```{r eval=FALSE}
ourModel <- adam(data, formula=y~x1+x2+I(x2^2))
```

Select the set of explanatory variables for ETSX(M,N,N) based on AIC (Section \@ref(ETSXSelection)):
```{r eval=FALSE}
ourModel <- adam(data, model="MNN",
                 regressors="select", ic="AIC")
```

Estimate ETS(A,Ad,N) model using a multistep loss function, GTMSE (Section \@ref(multistepLosses)):
```{r eval=FALSE}
ourModel <- adam(y, model="AAdN",
                 h=10, loss="GTMSE")
```

Estimate ARIMA(1,1,2) using a multistep loss function (Section \@ref(multistepLosses)) with backcasting of initials (Section \@ref(ADAMInitialisation)):
```{r eval=FALSE}
ourModel <- adam(y, model="NNN", orders=c(1,1,2),
                 h=10, loss="GTMSE", initial="backcasting")
```

Select and estimate the most appropriate ETS model on the data with multiple frequencies (Chapter \@ref(ADAMMultipleFrequencies)):
```{r eval=FALSE}
ourModel <- adam(y, model="ZXZ", lags=c(24,24*7,24*365))
```

Select and estimate the triple seasonal ARIMA on the data with multiple frequencies (Chapter \@ref(ADAMMultipleFrequencies)):
```{r eval=FALSE}
ourModel <- adam(y, model="NNN", lags=c(1,24,24*7,24*365),
                 orders=list(ar=c(3,2,2,2),
                             i=c(2,1,1,1),
                             ma=c(3,2,2,2),
                             select=TRUE),
                 initial="backcasting")
```

Apply an automatically selected occurrence part of the model to intermittent data (Section \@ref(ADAMOccurrence)):
```{r eval=FALSE}
oesModel <- oes(y, model="YYY", occurrence="auto")
```

Use the estimated occurrence model in `adam()` to model intermittent data (Section \@ref(ADAMDemandSizes)):
```{r eval=FALSE}
ourModel <- adam(y, model="YYY", occurrence=oesModel)
```

Or alternatively just use the same model for occurrence and demand sizes part (Chapter \@ref(ADAMIntermittent)):
```{r eval=FALSE}
ourModel <- adam(y, model="YYY", occurrence="auto")
```

Estimate the scale model for previously estimated ADAM (Chapter \@ref(ADAMscaleModel)):
```{r eval=FALSE}
scaleModel <- sm(ourModel, model="YYY")
```

Implant the scale model into the ADAM for future use (e.g. for forecasting):
```{r eval=FALSE}
mergedModel <- implant(ourModel, scaleModel)
```

Produce diagnostic plots to see if the ADAM can be improved any further (Chapter \@ref(diagnostics)):
```{r eval=FALSE}
par(mfcol=c(2,2))
plot(ourModel, which=c(1,2,4,6))
```

Extract conventional, standardised, and studentised residuals (Chapter \@ref(diagnostics)):
```{r eval=FALSE}
residuals(ourModel)
rstandard(ourModel)
rstudent(ourModel)
```

Plot time series decomposition according to ADAM ETS (Section \@ref(ETSTaxonomy)):
```{r eval=FALSE}
plot(ourModel, which=12)
```

Produce point forecast and prediction interval from ADAM for 10 steps ahead (Chapter \@ref(ADAMForecasting)):
```{r eval=FALSE}
forecast(ourModel, h=10, interval="prediction")
```

Produce point forecast and prediction interval for ADAM, cumulative over the lead time of 10 (Subsection \@ref(forecastingADAMOtherCumulative)):
```{r eval=FALSE}
forecast(ourModel, h=10, interval="prediction",
         cumulative=TRUE)
```

Produce point forecast and empirical prediction interval for upper bound (upper quantile of distribution, Sections \@ref(ADAMForecastingPIEmpirical) and \@ref(forecastingADAMOtherOneSided)):
```{r eval=FALSE}
forecast(ourModel, h=10, interval="empirical",
         side="upper")
```

Produce summary of ADAM (Chapter \@ref(ADAMUncertainty)):
```{r eval=FALSE}
summary(ourModel)
```

Reapply ADAM with randomly selected initials and parameters (to capture the uncertainty of parameters) and produce forecasts from each of these models (Section \@ref(adamRefitted)):
```{r eval=FALSE}
reapply(ourModel)
reforecast(ourModel, h=10, interval="prediction")
```

Extract multistep forecast errors from ADAM (Subsection \@ref(diagnosticsResidualsIIDExpectationMultiple)):
```{r eval=FALSE}
rmultistep(ourModel, h=10)
```

Extract covariance matrix of multistep forecast errors from ADAM (Section \@ref(multistepLosses)):
```{r eval=FALSE}
multicov(ourModel, h=10)
```

Extract actual and fitted values from ADAM:
```{r eval=FALSE}
actuals(ourModel)
fitted(ourModel)
```
