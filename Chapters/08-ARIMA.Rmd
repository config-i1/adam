# Introduction to ARIMA {#ARIMA}
Another important dynamic element in ADAM is the ARIMA model [developed originally by @Box1976]. ARIMA stands for "AutoRegressive Integrated Moving Average", although the name does not tell much on its own and needs additional explanation, which will be provided in this chapter.

The main idea of the model is that the data might have dynamic relations over time, where the new values depend on the values on the previous observations. This becomes more obvious in the case of engineering systems and modelling physical processes. For example, @Box1976 use an example of a series of CO$_2$ output of a furnace when the input gas rate changes. In this case, the elements of the ARIMA process are natural, as the CO$_2$ cannot just drop to zero when the gas is switched off -- it will leave the furnace in reducing quantity over time (i.e. leaving $\phi_1\times100\%$ of CO$_2$ in the next minute, where $\phi_1$ is a parameter in the model).

Another example where AR processes are natural is modelling the temperature in the room, measured with 5 minutes intervals. In this case, the temperature at 5:30 pm will depend on the one at 5:25 pm: if the temperature outside the room is lower, then the one in the room will go down slightly due to the loss of heat. Every 5 minutes it will go down on average by some quantity $\phi_1$.

Both these examples describe the AR(1) process, and in both of them the ARIMA model can be considered a "true model" (see discussion in Section \@ref(modelsMethods)). Unfortunately, when it comes to time series in the social or business domains, it becomes very difficult to motivate ARIMA usage from the modelling point of view. For example, the demand for a product does not reproduce itself and in real life does not depend on the demand on previous observations. So, if we construct ARIMA for such a process, we turn a blind eye to the fact that the observed time series relations in the data are most probably spurious. At best, in this case, ARIMA can be considered a very crude approximation of a complex process (demand is typically influenced by price changes, consumer behaviour, and promotional activities, etc). Thus, whenever we work with ARIMA models in social or business domains, we should keep in mind that they are wrong even from the philosophical point of view. Nevertheless, they still can be useful (as was pointed out by one of the original authors, George Box), which is why we discuss them in this chapter. We focus our discussion on forecasting with ARIMA. A reader interested in time series analysis is directed to @Box1976 or to more modern editions of that textbook. ARIMA is also well explained in Chapter 6 of @Ord2017.

This chapter will discuss the main theoretical properties of ARIMA processes (i.e. what would happen if the data indeed followed the specified model), moving to more practical aspects in the next chapter. We start the discussion with the non-seasonal ARIMA models, explaining how the forecasts from those models would look like, then move to the seasonal and multi-seasonal ARIMA, then discuss the classical Box-Jenkins approach for ARIMA order selection and its limitations. Finally, we explain the connection between ARIMA and ETS models.


## Introduction to ARIMA {#ARIMAIntro}
ARIMA contains several elements:

1. AR(p) -- the AutoRegressive part, showing how the variable is impacted by its values on the previous observations. It contains $p$ lags. For example, the quantity of the liquid in a vessel with an opened tap on some observation will depend on the quantity on the previous steps;
2. I(d) -- the number of differences $d$ taken in the model (I stands for "Integrated"). Working with differences rather than with the original data means that we deal with changes and rates of changes, rather than the original values. Technically, differences are needed to make data stationary (i.e. with fixed expectation and variance, although there are different definitions of the term *stationarity*, see below);
3. MA(q) -- the Moving Average component, explaining how the previous white noise impacts the variable. It contains $q$ lags. Once again, in technical systems, the idea that random error can affect the value has a relatively simple explanation. For example, when the liquid drips out of a vessel, we might not be able to predict the air fluctuations, which would impact the flow and could be perceived as elements of random noise. This randomness might, in turn, influence the quantity of liquid in a vessel on the following observation, thus introducing the MA elements in the model.

I intentionally do not provide ARIMA examples from the demand forecasting area, as these are much more difficult to motivate and explain than the examples from the more technical areas.

Before continuing our discussion, we should define the term **stationarity**. There are two definitions in the literature: one refers to "strict stationarity", while the other refers to the "weak stationarity":

- Time series is said to be **weak stationary** when its unconditional expectation and variance are constant, and the variance is finite on all observations;
- Time series is said to be **strong stationary** when its unconditional joint probability distribution does not change over time. This automatically implies that all its moments are constant (i.e. the process is also weak stationary).

The stationarity is essential in the ARIMA context and plays an important role in regression analysis. If the series is not stationary, it might be challenging to estimate its moments correctly using conventional methods. In some cases, it might be impossible to get the correct parameters (e.g. there is an infinite combination of parameters that would produce the minimum of the selected loss function). To avoid this issue, the series is transformed to ensure that the moments are finite and constant. Taking differences or detrending time series (e.g. via seasonal decomposition discussed in Section \@ref(ClassicalDecomposition)) allows making the first moment (mean) constant, while taking logarithms or doing the Box-Cox transform of the original series typically stabilises the variance, making it constant as well. After that, the model becomes easier to identify.

In contrast with ARIMA, the ETS models are almost always non-stationary and do not require the series to be stationary. We will see the connection between the two approaches in Section \@ref(ARIMAandETS).

Finally, conventional ARIMA is always a pure additive model, and as a result its point forecasts coincide with the conditional expectation, and it has closed forms for the conditional moments and quantiles.


### AR(p) {#AR}
We start with a simple AR(1) model, which is written as:
\begin{equation}
  {y}_{t} = \phi_1 y_{t-1} + \epsilon_t ,
  (\#eq:ARIMA100Example)
\end{equation}
where $\phi_1$ is the parameter of the model. This formula tells us that the value on the previous observation is carried over to the new one in the proportion of $\phi_1$. Typically, the parameter $\phi_1$ is restricted with the region (-1, 1) to make the model stationary, but very often in real life, $\phi_1$ lies in the (0, 1) region. If the parameter is equal to 1, the model becomes equivalent to Random Walk (Section \@ref(Naive)).

The forecast trajectory (conditional expectation several steps ahead) of this model would typically correspond to the exponentially declining curve. This is because for the $y_{t+h}$ value AR(1) is:
\begin{equation}
  {y}_{t+h} = \phi_1 y_{t+h-1} + \epsilon_{t+h} = \phi_1^2 y_{t+h-2} + \phi_1 \epsilon_{t+h-1} + \epsilon_{t+h} = \dots  \phi_1^h y_{t} + \sum_{j=0}^{h-1} \phi_1^{j} \epsilon_{t+h-j}.
  (\#eq:ARIMA100ExamplehSteps)
\end{equation}
If we then take expectation conditional on the information available up until observation $t$, all error terms will disappear (because we assume that $\mathrm{E}(\epsilon_{t+j})=0$ for all $j$) leading to:
\begin{equation}
  \mathrm{E}({y}_{t+h} |t) = \phi_1^h y_{t} .
  (\#eq:ARIMA100ExamplehStepsExpectation)
\end{equation}
If $\phi_1 \in (0, 1)$, the forecast trajectory will decline exponentially.

Here is a simple example in R of a very basic forecast from AR(1) with $\phi_1=0.9$ (see Figure \@ref(fig:AR1Trajectory)):
```{r AR1Trajectory, fig.cap="Forecast trajectory for AR(1) with $\\phi_1=0.9$."}
y <- vector("numeric", 20)
y[1] <- 100
phi <- 0.9
for(i in 2:length(y)){
    y[i] <- phi * y[i-1]
}
plot(y, type="l", xlab="horizon", ylab="Forecast")
```

If, for some reason, we get $\phi_1>1$, then the trajectory will exhibit an exponential increase, becoming explosive, implying non-stationary behaviour. The model, in this case, becomes very difficult to work with, even if the parameter is close to one. So it is typically advised to restrict the parameter with the stationarity region (we will discuss this in more detail later in this chapter).

In general, it is possible to imagine the situation, when the value at the moment of time $t$ would depend on several previous values, so the model AR(p) would be needed:
\begin{equation}
  {y}_{t} = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t ,
  (\#eq:ARIMAp00Example)
\end{equation}
where $\phi_i$ is the parameter for the $i$-th lag of the model. So, the model assumes that the data on the recent observation is influenced by the $p$ previous observations. The more lags we introduce in the model, the more complicated the forecasting trajectory becomes, potentially demonstrating harmonic behaviour. Here is an example of a point forecast from AR(3) model ${y}_{t} = 0.9 y_{t-1} -0.7 y_{t-2} + 0.6 y_{t-3} + \epsilon_t$ (Figure \@ref(fig:AR3Trajectory)):

```{r AR3Trajectory, fig.cap="Forecast trajectory for an AR(3) model."}
y <- vector("numeric", 30)
y[1:3] <- c(100, 75, 30)
phi <- c(0.9,-0.7,0.6)
for(i in 4:30){
    y[i] <- phi[1] * y[i-1] + phi[2] * y[i-2] + phi[3] * y[i-3]
}
plot(y, type="l", xlab="horizon", ylab="Forecast")
```

No matter what the forecast trajectory of the AR model is, it will asymptotically converge to zero as long as the model is stationary.

An example of AR(3) time series generated using `sim.ssarima()` function from the `smooth` package and forecast for it via `msarima()` is shown in Figure \@ref(fig:ARExample).

```{r ARExample, fig.cap="An example of AR(3) series and forecast for it.", fig.width=5, fig.height=4, echo=FALSE}
sim.ssarima(orders=list(ar=3), lags=1, obs=120, AR=c(0.9, -0.7, 0.6), initial=c(10,15,20), mean=0, sd=10) |>
    msarima(orders=list(ar=3), constant=FALSE,
            h=10, holdout=TRUE) |>
    forecast(interval="pred") |>
    plot(main="")
```

As can be seen from Figure \@ref(fig:ARExample), the series does not exhibit any distinct characteristics so that it would be possible to identify the order of the model just by looking at it. The only thing that we can probably say is that there is some structure in it: it has some periodic fluctuations and some parts of series are consistently above zero, while the others are consistently below. The latter is an indicator of autocorrelation in data. We do not discuss how to identify order of AR in this Section, we will come back to it in Section \@ref(BJApproach).


### MA(q) {#MA}
Before discussing the "Moving Averages" model, we should acknowledge that the name is quite misleading and that the model has *nothing to do* with Centred Moving Averages used in time series decomposition (Section \@ref(ClassicalDecomposition)) or Simple Moving Averages (discussed in Section \@ref(SMA)) used in forecasting. The idea of the simplest MA(1) model can be summarised in the following mathematical way:
\begin{equation}
  {y}_{t} = \theta_1 \epsilon_{t-1} + \epsilon_t ,
  (\#eq:ARIMA001Example)
\end{equation}
where $\theta_1$ is the parameter of the model, typically lying between (-1, 1), showing what part of the error is carried out to the next observation. Because of the conventional assumption that the error term has a zero mean ($\mathrm{E}(\epsilon_{t})=0$), the forecast trajectory of this model is just a straight line coinciding with zero starting from the $h=2$. But for the one step ahead point forecast we have:
\begin{equation}
  \mathrm{E}({y}_{t+1}|t) = \theta_1 \mathrm{E}(\epsilon_{t}|t) + \mathrm{E}(\epsilon_{t+1}|t) = \theta_1 \epsilon_{t}.
  (\#eq:ARIMA001ExampleForecast)
\end{equation}
Starting from $h=2$ there are no observable error terms $\epsilon_t$, so all the values past that are equal to zero:
\begin{equation}
  \mathrm{E}({y}_{t+2}|t) = \theta_1 \mathrm{E}(\epsilon_{t+1}|t) + \mathrm{E}(\epsilon_{t+2}|t) = 0.
  (\#eq:ARIMA001ExampleForecastInSample)
\end{equation}
So, the forecast trajectory for MA(1) model drops to zero, when $h>1$.

More generally, MA(q) model is written as:
\begin{equation}
  {y}_{t} = \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} + \epsilon_t ,
  (\#eq:ARIMA00qExample)
\end{equation}
where $\theta_i$ is the parameters for the $i$-th lag of the error term, typically restricted with the so-called invertibility region (discussed in the next section). In this case, the model implies that the recent observation is influenced by several errors on previous observations (your mistakes in the past will haunt you in the future). The more lags we introduce, the more complicated the model becomes. As for the forecast trajectory, it will reach zero when $h>q$.

An example of MA(3) time series and a forecast for it is shown in Figure \@ref(fig:MAExample).

```{r MAExample, fig.cap="An example of MA(3) series and forecast for it.", fig.width=5, fig.height=4, echo=FALSE}
sim.ssarima(orders=list(ma=3), lags=1, obs=120, MA=c(0.9, -0.3, -0.6), initial=c(10,10,5), mean=0, sd=10) |>
    msarima(orders=list(ma=3), constant=FALSE,
            h=10, holdout=TRUE) |>
    forecast(interval="pred") |>
    plot(main="")
```

Similarly to how it was with AR(3), Figure \@ref(fig:MAExample) does not show anything specific that could tell us that this is MA(3) process. The proper identification of MA orders will be discussed in Section \@ref(BJApproach).


### ARMA(p,q) {#ARMA}
Connecting the models \@ref(eq:ARIMAp00Example) and \@ref(eq:ARIMA00qExample), we get the more complicated model, ARMA(p,q):
\begin{equation}
  {y}_{t} = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} + \epsilon_t ,
  (\#eq:ARIMAp0q)
\end{equation}
which has the properties of the two models discussed above. The forecast trajectory from this model will have a combination of trajectories for AR and MA for $h \leq q$ and then will correspond to AR(p) for $h>q$.

To simplify the work with ARMA models, the equation \@ref(eq:ARIMAp0q) is typically rewritten by moving all terms with $y_t$ to the left-hand side:
\begin{equation}
  {y}_{t} -\phi_1 y_{t-1} -\phi_2 y_{t-2} -\dots -\phi_p y_{t-p} = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} .
  (\#eq:ARIMAp0qLeft)
\end{equation}
Furthermore, in order to make this even more compact, the backshift operator B can be introduced. It just shows by how much the subscript of the variable is shifted back in time:
\begin{equation}
  {y}_{t} B^i = {y}_{t-i}.
  (\#eq:backshiftOperator)
\end{equation}
Using \@ref(eq:backshiftOperator), the ARMA model can be written as:
\begin{equation}
  {y}_{t} (1 -\phi_1 B -\phi_2 B^2 -\dots -\phi_p B^p) = \epsilon_t (1 + \theta_1 B + \theta_2 B^2 + \dots + \theta_q B^q) .
  (\#eq:ARIMAp0qCompacter)
\end{equation}
Finally, we can also introduce the AR and MA polynomial functions (corresponding to the elements in the brackets of \@ref(eq:ARIMAp0qCompacter)) to make the model even more compact:
\begin{equation}
\begin{aligned}
  & \varphi^p(B) = 1 -\phi_1 B -\phi_2 B^2 -\dots -\phi_p B^p \\ 
  & \vartheta^q(B) = 1 + \theta_1 B + \theta_2 B^2 + \dots + \theta_q B^q .
\end{aligned}
  (\#eq:ARMAPolynomials)
\end{equation}
Inserting the functions \@ref(eq:ARMAPolynomials) in \@ref(eq:ARIMAp0qCompacter) leads to the compact presentation of ARMA model:
\begin{equation}
  {y}_{t} \varphi^p(B) = \epsilon_t \vartheta^q(B) .
  (\#eq:ARIMAp0qCompact)
\end{equation}
The model \@ref(eq:ARIMAp0qCompact) can be considered a compact form of \@ref(eq:ARIMAp0q). It is more difficult to understand and interpret but easier to work with from a mathematical point of view. In addition, this form permits introducing additional elements, which will be discussed later in this chapter.

Figure \@ref(fig:ARMAExample) shows an example of ARMA(3,3) series with a forecast from it.

```{r ARMAExample, fig.cap="An example of ARMA(3,3) series and forecast for it.", fig.width=5, fig.height=4, echo=FALSE}
sim.ssarima(orders=list(ar=3, ma=3), lags=1, obs=120, AR=c(0.9, -0.7, 0.6),
            MA=c(0.9, -0.3, -0.6), initial=c(10,10,5), mean=0, sd=10) |>
    msarima(orders=list(ar=3, ma=3), constant=FALSE,
            h=10, holdout=TRUE) |>
    forecast(interval="pred") |>
    plot(main="")
```

Similarly to AR(3) and MA(3) examples above, the process is not visually distinguishable from other ARMA process.

Coming back to the ARMA model \@ref(eq:ARIMAp0q), as discussed earlier, it assumes convergence of forecast trajectory to zero, the speed of which is regulated by its parameters. This implies that the data has the mean of zero, and ARMA should be applied to somehow pre-processed data so that it is stationary and varies around zero. This means that if you work with non-stationary and/or with non-zero mean data, the pure AR/MA or ARMA will be inappropriate -- some prior transformations will be required.


### ARMA with constant {#ARMAConstant}
One of the simpler ways to deal with the issue with zero forecasts is to introduce the constant (or intercept) in ARMA:
\begin{equation}
  {y}_{t} = a_0 + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_p \epsilon_{t-p} + \epsilon_t 
  (\#eq:ARIMAp0qExample)
\end{equation}
or
\begin{equation}
  {y}_{t} \varphi^p(B) = a_0 + \epsilon_t \vartheta^q(B) ,
  (\#eq:ARIMAp0qCompactConstant)
\end{equation}
where $a_0$ is the constant parameter, which in this case also works as the unconditional mean of the series. The forecast trajectory in this case would converge to a non-zero number, but with some minor differences from the trajectory of ARMA without constant. For example, in case of ARMA(1,1) with constant we will have:
\begin{equation}
  {y}_{t} = a_0 + \phi_1 y_{t-1} + \theta_1 \epsilon_{t-1} + \epsilon_t .
  (\#eq:ARIMA101ConstExample01)
\end{equation}
The conditional expectation of $y_{t+h}$ for $h=1$ and $h=2$ can be written as (based on the discussions in previous sections):
\begin{equation}
\begin{aligned}
  & \mathrm{E}({y}_{t+1}|t) = a_0 + \phi_1 y_{t} + \theta_1 \epsilon_{t} \\
  & \mathrm{E}({y}_{t+2}|t) = a_0 + \phi_1 \mathrm{E}(y_{t+1}|t) = a_0 + \phi_1 a_0 + \phi_1^2 y_{t} + \phi_1 \theta_1 \epsilon_t
\end{aligned} ,
  (\#eq:ARIMA101ConstExampleForecasth1)
\end{equation}
or in general for the horizon $h$:
\begin{equation}
  \mathrm{E}({y}_{t+h}|t) = \sum_{j=1}^h a_0\phi_1^{j-1} + \phi_1^h y_{t} + \phi_1^{h-1} \theta_1 \epsilon_{t} .
  (\#eq:ARIMA101ConstExampleForecast)
\end{equation}
So, the forecast trajectory from this model dampens out, similar to the ETS(A,Ad,N) model (Subsection \@ref(ETSAAdN)), and the rate of dampening is regulated by the value of $\phi_1$. The following simple example demonstrates this point (see Figure \@ref(fig:ARMAConstantTrajectory); I drop the MA(1) part because it does not change the shape of the curve, but only shifts it):

```{r ARMAConstantTrajectory, fig.cap="Forecast trajectory for an ARMA(1,1) model with constant."}
y <- vector("numeric", 20)
y[1] <- 100
phi <- 0.9
for(i in 2:length(y)){
    y[i] <- 100 + phi * y[i-1]
}
plot(y, type="l", xlab="horizon", ylab="Forecast")
```

The more complicated ARMA(p,q) models with p>1 will have more complex trajectories with potential harmonics, but the idea of dampening in AR(p) part of the model stays.

An example of time series generated from ARMA(3,3) with constant is provided in Figure \@ref(fig:ARMAConstExample).

```{r ARMAConstExample, fig.cap="An example of ARMA(3,3) with constant series and forecast for it.", fig.width=5, fig.height=4, echo=FALSE}
x <- sim.ssarima(orders=list(ar=3, ma=3), lags=1, obs=120, AR=c(0.9, -0.8, 0.3),
                 MA=c(0.7, -0.3, 0.6), initial=c(300,-150,80), constant=200, mean=0, sd=10)
msarima(x$data,orders=list(ar=3, ma=3), constant=TRUE,
        h=10, holdout=TRUE) |>
    forecast(interval="parametric") |>
    plot(main="",legend=FALSE)
```

Finally, as alternative to adding $a_0$, each actual value of $y_t$ can be centred via $y^\prime_t = y_t -\bar{y}$, where $\bar{y}$ is the in-sample mean, making sure that the mean of $y^\prime_t$ is zero and ARMA can be applied to the $y^\prime_t$ data instead of $y_t$. However, this approach introduces additional steps, but the result on stationary data is typically the same as adding the constant.


### I(d) {#Differences}
Based on the previous discussion, we can conclude that ARMA cannot be efficiently applied to non-stationary data. So, if we deal with one, we need to make it stationary. The conventional way of doing that is by taking differences in the data. The logic behind this is straightforward: if the data is not stationary, the mean somehow changes over time. This can be, for example, due to a trend in the data. In this case, we should be talking about the change of variable $y_t$ rather than the variable itself. So we should work on the following data instead:
\begin{equation}
  \Delta y_t = y_t -y_{t-1} = y_t (1 -B),
  (\#eq:ARIMADifferencesFirst)
\end{equation}
if the first differences have constant mean. The simplest model with differences is I(1), which is also known as the **Random walk** (see Section \@ref(Naive)):
\begin{equation}
  \Delta y_t = \epsilon_t.
  (\#eq:ARIMARandomWalk)
\end{equation}
It can also be reformulated in a simpler, more interpretable form by inserting \@ref(eq:ARIMADifferencesFirst) in \@ref(eq:ARIMARandomWalk) and regrouping elements:
\begin{equation}
  y_t = y_{t-1} + \epsilon_t.
  (\#eq:ARIMARandomWalk02)
\end{equation}
The model \@ref(eq:ARIMARandomWalk02) can also be perceived as AR(1) with $\phi_1=1$. This is a non-stationary model (meaning that the unconditional mean of $y_t$ is not constant) and the point forecast from it corresponds to the one from the Naïve method (see Section \@ref(Naive)) with a straight line equal to the last observed actual value (again, assuming that $\mathrm{E}(\epsilon_{t})=0$ and that the other basic assumptions from Section \@ref(assumptions) hold):
\begin{equation}
  \mathrm{E}(y_{t+h}|t) = \mathrm{E}(y_{t+h-1}|t) + \mathrm{E}(\epsilon_{t+h}|t) = y_{t} .
  (\#eq:ARIMARandomWalkForecast)
\end{equation}
Visually, the Random Walk data and the forecast from it are shown in Figure \@ref(fig:naiveExample2).

```{r naiveExample2, fig.cap="A Random Walk data example.", fig.width=5, fig.height=4, echo=FALSE}
sim.sma(1, 120) |>
    sma(order=1,
        h=10, holdout=TRUE) |>
    forecast(interval="pred") |>
    plot(main="")
```

Another simple model that relies on differences of the data is called **Random Walk with drift** (which was discussed in Section \@ref(RWWithDrift)) and is formulated by adding constant $a_0$ to the right hand side of equation \@ref(eq:ARIMARandomWalk):
\begin{equation}
  \Delta y_t = a_0 + \epsilon_t.
  (\#eq:ARIMARandomWalkWithDrift)
\end{equation}
This model has some similarities with the global level model, which is formulated via the actual value rather than differences (see Section \@ref(GlobalMean)):
\begin{equation*}
  {y}_{t} = a_0 + \epsilon_t.
\end{equation*}
Using a similar regrouping as with the Random Walk, we can obtain a simpler form of \@ref(eq:ARIMARandomWalkWithDrift):
\begin{equation}
  y_t = a_0 + y_{t-1} + \epsilon_t.
  (\#eq:ARIMARandomWalkWithDrift02)
\end{equation}
which is, again, equivalent to AR(1) model with $\phi_1=1$, but this time with a constant. The term "drift" appears because $a_0$ acts as an additional element, showing the tendency in the data: if it is positive, the model will exhibit a positive trend; if it is negative, the trend will be negative. This can be seen for the conditional mean, for example, for the case of $h=2$:
\begin{equation}
  \mathrm{E}(y_{t+2}|t) = \mathrm{E}(a_0) + \mathrm{E}(y_{t+1}|t) + \mathrm{E}(\epsilon_{t+2}|t) = a_0 + \mathrm{E}(a_0 + y_t + \epsilon_t|t) = 2 a_0 + y_t ,
  (\#eq:ARIMARandomWalkWithDriftForecasth2)
\end{equation}
or in general for the horizon $h$:
\begin{equation}
  \mathrm{E}(y_{t+h}|t) = h a_0 + y_t .
  (\#eq:ARIMARandomWalkWithDriftForecasth)
\end{equation}
Visually, the data and the forecast from it are shown in Figure \@ref(fig:RWDriftExample2).

```{r RWDriftExample2, fig.cap="A Random Walk with drift with $a_0=2$.", fig.width=5, fig.height=4, echo=FALSE}
sim.ssarima(orders=list(i=1), lags=1, obs=120,
            constant=2) |>
    msarima(orders=list(i=1), constant=TRUE,
            h=10, holdout=TRUE) |>
    forecast(interval="pred") |>
    plot(main="")
```

In a manner similar to \@ref(eq:ARIMADifferencesFirst), we can also introduce second differences of the data (differences of differences) if we suspect that the change of variable over time is not stationary, which would be written as:
\begin{equation}
  \Delta^2 y_t = \Delta y_t -\Delta y_{t-1} = y_t -y_{t-1} -y_{t-1} + y_{t-2},
  (\#eq:ARIMADifferencesSecond)
\end{equation}
or using the backshift operator:
\begin{equation}
  \Delta^2 y_t = y_t(1 -2B + B^2) = y_t (1 -B)^2.
  (\#eq:ARIMADifferencesSecondBackshift)
\end{equation}
In fact, we can introduce higher level differences if we want to (but typically we should not) based on the idea of \@ref(eq:ARIMADifferencesSecondBackshift):
\begin{equation}
  \Delta^d = (1-B)^d.
  (\#eq:ARIMADifferences)
\end{equation}
Using \@ref(eq:ARIMADifferences), the I(d) model is formulated as:
\begin{equation}
  \Delta^d y_t = \epsilon_t.
  (\#eq:ARIMA0d0)
\end{equation}


### ARIMA(p,d,q)
Finally, having made the data stationary via the differences, we can introduce ARMA elements \@ref(eq:ARIMAp0qCompact) to it which would be done on the differenced data, instead of the original $y_t$:
\begin{equation}
  y_t \Delta^d(B) \varphi^p(B) = \epsilon_t \vartheta^q(B) ,
  (\#eq:ARIMApdqCompact)
\end{equation}
or in a more general form \@ref(eq:ARIMAp0qCompacter) with \@ref(eq:ARIMADifferences):
\begin{equation}
  y_t (1-B)^d (1 -\phi_1 B -\dots -\phi_p B^p) = \epsilon_t (1 + \theta_1 B + \dots + \theta_q B^q),
  (\#eq:ARIMApdq)
\end{equation}
which is ARIMA(p,d,q) model. This model allows producing trends with some values of differences and also inherits the trajectories from both AR(p) and MA(q). This implies that the point forecasts from the model can exhibit complicated trajectories, depending on the values of p, d, q and the model's parameters.

The model \@ref(eq:ARIMApdq) is difficult to interpret in a general form, but opening the brackets and moving all elements but $y_t$ to the right-hand side helps understanding each specific model.

Figure \@ref(fig:ARIMA112Example) demonstrates how data would look if it was generated from ARIMA(1,1,2) and how the forecasts would look if the same model ARIMA(1,1,2) was applied to the data. Because of the AR(1) term in the model, its forecast trajectory is dampening, similar to how it was done in ETS(A,Ad,N) model.

```{r ARIMA112Example, fig.cap="An example of ARIMA(1,1,2) data and forecasts for it.", fig.width=5, fig.height=4, echo=FALSE}
sim.ssarima(orders=list(ar=1,i=1,ma=2), lags=1, obs=120, AR=0.7, initial=c(500, -350),
            mean=0, sd=10) |>
    msarima(orders=list(ar=1,i=1,ma=2), constant=FALSE,
            h=10, holdout=TRUE) |>
    forecast(interval="pred") |>
    plot(main="")
```


### Parameters bounds {#ARIMABounds}
ARMA models have two conditions that need to be satisfied for them to be useful and to work appropriately:

1. Stationarity,
2. Invertibility.

Condition (1) has already been discussed in Section \@ref(ARIMAIntro). It is imposed on the model's AR parameters, ensuring that the forecast trajectories do not exhibit explosive behaviour (in terms of both mean and variance). (2) is equivalent to the stability condition in ETS (Section \@ref(stabilityConditionAdditiveError)) and refers to the MA parameters: it guarantees that the old observations do not have an increasing impact on the recent ones. The term "invertibility" comes from the idea that any MA process can be represented as an infinite AR process via the inversion of the parameters as long as the parameters lie in some specific bounds. For example, MA(1) model, which is written as:
\begin{equation}
  y_t = \epsilon_t + \theta_1 \epsilon_{t-1} = \epsilon_t (1 + \theta_1 B) ,
  (\#eq:ARIMA100Example01)
\end{equation}
can be rewritten as:
\begin{equation}
  y_t (1 + \theta_1 B)^{-1} = \epsilon_t,
  (\#eq:ARIMA100Example02)
\end{equation}
or in a slightly easier to digest form (based on \@ref(eq:ARIMA100Example01) and the idea that $\epsilon_{t} = y_{t} -\theta_1 \epsilon_{t-1}$, implying that $\epsilon_{t-1} = y_{t-1} -\theta_1 \epsilon_{t-2}$):
\begin{equation}
  y_t = \theta_1 y_{t-1} -\theta_1^2 \epsilon_{t-2} + \epsilon_t = \theta_1 y_{t-1} -\theta_1^2 y_{t-2} + \theta_1^3 \epsilon_{t-2} + \epsilon_t = \sum_{j=1}^\infty -1^{j-1} \theta_1^j y_{t-j} + \epsilon_t.
  (\#eq:ARIMA100Example03)
\end{equation}
The recursion in \@ref(eq:ARIMA100Example03) shows that the recent actual value $y_t$ depends on the previous infinite number of values of $y_{t-j}$ for $j=\{1,\dots,\infty\}$. The parameter $\theta_1$, in this case, is exponentiated to the power $j$ and leads to the exponential distribution of weights in this infinite series (reminds SES from Section \@ref(SES), doesn't it?). The *invertibility* condition makes sure that those weights decline over time with the increase of $j$ so that the older observations do not have an increasing impact on the most recent $y_t$.

There are different ways how to check both conditions, the conventional one is by calculating the roots of the polynomial equations:
\begin{equation}
\begin{aligned}
  & \varphi^p(B) = 0 \text{ for AR} \\
  & \vartheta^q(B) = 0 \text{ for MA}
\end{aligned} ,
  (\#eq:ARIMApdqConditionsCompact)
\end{equation}
or expanding the functions in \@ref(eq:ARIMApdqConditionsCompact) and substituting $B$ with a variable $x$ (for convenience):
\begin{equation}
\begin{aligned}
  & 1 -\phi_1 x -\phi_2 x^2 -\dots -\phi_p x^p = 0 \text{ for AR} \\
  & 1 + \theta_1 x + \theta_2 x^2 + \dots + \theta_q x^q = 0 \text{ for MA}
\end{aligned} .
  (\#eq:ARIMApdqConditions)
\end{equation}
Solving the first equation for $x$ in \@ref(eq:ARIMApdqConditions), we get $p$ roots (some of them might be complex numbers). For the model to be stationary, all the roots must be greater than one by absolute value. Similarly, if all the roots of the second equation in \@ref(eq:ARIMApdqConditions) are greater than one by absolute value, then the model is invertible (aka stable).

Calculating roots of polynomials is a difficult task, so there are simpler special cases for both conditions that guarantee that the more complicated ones are satisfied:
\begin{equation}
\begin{aligned}
  & 0 < \sum_{j=1}^p \phi_j < 1 \\
  & 0 < \sum_{j=1}^q \theta_j < 1
\end{aligned} .
  (\#eq:ARIMApdqConditionsSpecial)
\end{equation}
But note that the condition \@ref(eq:ARIMApdqConditionsSpecial) is rather restrictive and is not generally applicable for all ARIMA models. It can be used to skip the check of the more complicated condition \@ref(eq:ARIMApdqConditions) if it is satisfied by a set of estimated parameters.

Finally, in a special case with AR(p) model with $0 < \phi_j < 1$ for all $j$ and $\sum_{j=1}^p \phi_j = 1$, we end up with the moving weighted average, which is a non-stationary model. This becomes apparent from the connection between Simple Moving Average and AR processes [@Svetunkov2017].


## Seasonal ARIMA
### Single seasonal ARIMA
When it comes to the actual data, we typically have relations between consecutive observations and between observations happening with some fixed seasonal lags. In the ETS framework, the latter relations are taken care of by seasonal indices, repeating every $m$ observations. In the ARIMA framework, this is done by introducing lags in the model elements. For example, seasonal AR(P)$_m$ with lag $m$ can be written similar to AR(p), but with some minor modifications:
\begin{equation}
  {y}_{t} = \phi_{m,1} y_{t-m} + \dots + \phi_{m,P} y_{t-Pm} + \varepsilon_t ,
  (\#eq:SARIMAP00Example)
\end{equation}
where $\phi_{m,i}$ is the parameter for the lagged actual value in the model, and $\varepsilon_t$ is the error term of the seasonal AR model. We use the underscore "$m$" to show that the parameters here refer to the seasonal part of the model. The idea of the model \@ref(eq:SARIMAP00Example) on the example of monthly data is that the current observation is influenced by a similar value, the same month a year ago, then the same month two years ago etc. This is hard to justify from the theoretical point of view (demand two years ago impacts demand this year?), but this model allows capturing complex relations in the data.

Similarly to seasonal AR(P), we can have seasonal MA(Q)$_m$:
\begin{equation}
    {y}_{t} = \theta_{m,1} \varepsilon_{t-m} + \dots + \theta_{m,Q} \varepsilon_{t-Qm} + \varepsilon_t ,
  (\#eq:SARIMA00QExample)
\end{equation}
where $\theta_{m,i}$ is the parameter for the lagged error term in the model. This model is even more difficult to justify than the MA(q) because it is difficult to explain how the white noise the same month last year can impact the actual value this year. Still, this is a useful instrument for forecasting purposes.

Finally, we have the seasonal differences, I(D)$_m$, which are easier to present using the backshift operator:
\begin{equation}
  y_t (1-B^m)^D = \varepsilon_t.
  (\#eq:SARIMA0D0Example)
\end{equation}
The seasonal differences allow dealing with the seasonality that changes its amplitude from year to year, i.e. model the multiplicative seasonality via ARIMA by making the seasonality itself stationary.

A special case of I(D) model is I(1)$_m$, which is a seasonal **Random Walk**, underlying Seasonal Naïve method from Section \@ref(NaiveSeasonal):
\begin{equation}
  y_t (1-B^m) = \varepsilon_t,
  (\#eq:SARIMA010Example1)
\end{equation}
or equivalently:
\begin{equation}
  y_t = y_{t-m} + \varepsilon_t.
  (\#eq:SARIMA010Example2)
\end{equation}

Combining \@ref(eq:SARIMAP00Example), \@ref(eq:SARIMA00QExample) and \@ref(eq:SARIMA0D0Example) we get pure seasonal ARIMA(P,D,Q)$_m$ model, similar to the ARIMA(p,d,q):
\begin{equation}
  y_t (1-B^m)^D (1 -\phi_{m,1} B^m -\dots -\phi_{m,P} B^{Pm}) = \varepsilon_t (1 + \theta_{m,1} B^m + \dots + \theta_{m,Q} B^{Qm}),
  (\#eq:SARIMAPDQ)
\end{equation}
or if we introduce the polynomial functions for seasonal AR and MA and use notations similar to \@ref(eq:ARIMADifferencesSecondBackshift):
\begin{equation}
  y_t \Delta^D(B^m) \varphi^P(B^m) = \varepsilon_t \vartheta^Q(B^m),
  (\#eq:SARIMAPDQCompact)
\end{equation}
where
\begin{equation}
\begin{aligned}
  & \Delta^D(B^m) = (1-B^m)^D \\
  & \varphi^P(B^m) = 1 -\phi_{m,1} B^m -\dots -\phi_{m,P} B^{Pm} \\ 
  & \vartheta^Q(B^m) = 1 + \theta_{m,1} B^m + \dots + \theta_{m,Q} B^{Qm} .
\end{aligned}
  (\#eq:SARIMAPolynomials)
\end{equation}
Now that we have taken care of the seasonal part of the model, we should not forget that there is a non-seasonal one. If it exists in the data, then $\varepsilon_t$ would not be just a white noise, but could be modelled using a non-seasonal ARIMA(p,d,q):
\begin{equation}
  \varepsilon_t \Delta^d(B) \varphi^p(B) = \epsilon_t \vartheta^q(B),
  (\#eq:ARIMApdqForError)
\end{equation}
implying that:
\begin{equation}
  \varepsilon_t = \epsilon_t \frac{\vartheta^q(B)}{\Delta^d(B) \varphi^p(B)}.
  (\#eq:ARIMApdqForErrorRewritten)
\end{equation}
Inserting \@ref(eq:ARIMApdqForErrorRewritten) into \@ref(eq:SARIMAPDQCompact), we get the final SARIMA(p,d,q)(P,D,Q)$_m$ model in the compact form after regrouping the polynomials:
\begin{equation}
  y_t \Delta^D(B^m) \varphi^P(B^m) \Delta^d(B) \varphi^p(B) = \epsilon_t \vartheta^Q(B^m) \vartheta^q(B) .
  (\#eq:SARIMApdqPDQCompact)
\end{equation}
The equation \@ref(eq:SARIMApdqPDQCompact) does not tell us much about what specifically happens in the model. It just shows how different elements interact with each other in it. To understand, what SARIMA means, we need to consider a specific order of model and see what impacts the current actual value. For example, here is what we will have in the case of SARIMA(1,0,1)(1,0,1)$_4$ (i.e. applied to stationary quarterly data):
\begin{equation}
  y_t \Delta^0(B^4) \varphi^1(B^4) \Delta^0(B) \varphi^1(B) = \epsilon_t \vartheta^1(B^4) \vartheta^1(B) .
  (\#eq:SARIMA101101Example01)
\end{equation}
Inserting the values of polynomials \@ref(eq:SARIMAPolynomials), \@ref(eq:ARIMADifferences) and \@ref(eq:ARMAPolynomials) in \@ref(eq:SARIMA101101Example01), we get:
\begin{equation}
  y_t (1 -\phi_{4,1} B^4)(1 -\phi_{1} B) = \epsilon_t (1 + \theta_{4,1} B^4) (1 + \theta_{1} B),
  (\#eq:SARIMA101101Example02)
\end{equation}
which is slightly easier to understand but still does not explain how past values impact the present one. So, we open the brackets and move all the elements except for $y_t$ to the right-hand side of the equation to get:
\begin{equation}
  y_t = \phi_{1} y_{t-1} + \phi_{4,1} y_{t-4} -\phi_{1} \phi_{4,1} y_{t-5} + \theta_1 \epsilon_{t-1} + \theta_{4,1} \epsilon_{t-4} + \theta_{1} \theta_{4,1} \epsilon_{t-5} + \epsilon_t .
  (\#eq:SARIMA101101Example03)
\end{equation}
So, now we see that SARIMA(1,0,1)(1,0,1)$_4$ implies that the present value is impacted by the value in the previous quarter ($\phi_{1} y_{t-1} + \theta_1 \epsilon_{t-1}$), the value last year ($\phi_{4,1} y_{t-4} + \theta_{4,1} \epsilon_{t-4}$) on the same quarter and the value from last year on the previous quarter ($-\phi_{1} \phi_{4,1} y_{t-5} + \theta_{1} \theta_{4,1} \epsilon_{t-5}$), which introduces a much more complicated interaction than any ETS model can. However, this complexity is obtained with a minimum number of parameters: we have three lagged actual values and three lagged error terms, but we only have four parameters to estimate, not six. Thus the SARIMA model mentioned above can be considered as parsimonious for modelling this specific situation.

The more complicated SARIMA models would have even more complicated interactions, making it more challenging to interpret, but all of that comes with a benefit of having a parsimonious model with just $p+q+P+Q$ parameters to estimate.

When it comes to forecasting from such model as SARIMA(1,0,1)(1,0,1)$_4$, the trajectories would have elements of the classical ARMA model, discussed in Section \@ref(ARMA), converging to zero as long as there is no constant and the model is stationary. The main difference would be in having the seasonal element. Here is an R example of a prediction for such a model for $h>m+1$ (see Figure \@ref(fig:SARIMATrajectory); MA part is dropped because the expectation of the error term is assumed to be equal to zero):

```{r SARIMATrajectory, fig.cap="Forecast trajectory for a SARIMA(1,0,1)(1,0,1)$_4$."}
y <- vector("numeric", 100)
y[1:5] <- c(97,87,85,94,95)
phi <- c(0.6,0.8)
for(i in 6:length(y)){
    y[i] <- phi[1] * y[i-1] + phi[2] * y[i-4] -
      phi[1] * phi[2] * y[i-5]
}
plot(y, type="l", xlab="horizon", ylab="Forecast")
```

As we see from Figure \@ref(fig:SARIMATrajectory), the values converge to zero due to $0<\phi_1<1$ and the seasonality disappears because $0<\phi_{4,1}<1$ as well. So, this is the forecast implied by the SARIMA without differences. If the differences are introduced, then the model would produce non-stationary and seasonally non-stationary trajectories.

### SARIMA with constant
In addition, it is possible to add the constant term to the SARIMA model, and it will have a more complex effect on the forecast trajectory, depending on the order of the model. In case of zero differences, the effect will be similar to ARMA with constant (Section \@ref(ARMAConstant)), introducing the dampening trajectory. Here is an example (see Figure \@ref(fig:SARIMAConstantTrajectory)):

```{r SARIMAConstantTrajectory, fig.cap="Forecast trajectory for a SARIMA model with constant."}
y <- vector("numeric", 100)
y[1:5] <- c(97,87,85,94,95)
phi <- c(0.6,0.8)
for(i in 6:length(y)){
    y[i] <- phi[1] * y[i-1] + phi[2] * y[i-4] -
      phi[1] * phi[2] * y[i-5] + 8
}
plot(y, type="l", xlab="horizon", ylab="Forecast")
```

In case of the model with the differences, the constant would have a two-fold effect: working as a drift for the non-seasonal part and increasing the amplitude of seasonality for the seasonal one. Here is an example from SARIMA(1,0,0)(1,1,0)$_4$ with constant:
\begin{equation}
  y_t (1 -\phi_{4,1} B^4)(1 -\phi_{1} B) (1 -B^4) = \epsilon_t + a_0 ,
  (\#eq:SARIMA101110Example01)
\end{equation}
which can be reformulated as (after opening brackets and moving elements to the right-hand side):
\begin{equation}
  y_t = \phi_{1} y_{t-1} + (1+\phi_{4,1}) y_{t-4} -(1+\phi_{4,1}) \phi_{1} y_{t-5} -\phi_{4,1} y_{t-8} + \phi_1 \phi_{4,1} y_{t-9} + a_0 + \epsilon_t .
  (\#eq:SARIMA101110Example02)
\end{equation}
This formula can then be used to see, how the trajectory from such model will look:

```{r SARIMADiffConstantTrajectory, fig.cap="Forecast trajectory for a SARIMA model with differences and a constant."}
y <- vector("numeric", 100)
y[1:9] <- c(96,87,85,94,97,88,86,95,98)
phi <- c(0.6,0.8)
for(i in 10:length(y)){
    y[i] <- phi[1] * y[i-1] + (1+phi[2]) * y[i-4] -
      (1+ phi[2]) *phi[1] * y[i-5] - phi[2] * y[i-8] +
      phi[1] * phi[2] * y[i-9] + 0.1
}
plot(y, type="l", xlab="horizon", ylab="Forecast")
```

As we see from Figure \@ref(fig:SARIMADiffConstantTrajectory), the trajectory exhibits a drift, coming from the non-seasonal part of the model and a stable seasonality (the amplitude of which does not converge to zero anymore). More complex behaviours for the future trajectories can be obtained with higher orders of seasonal and non-seasonal parts of the SARIMA model.

### Multiple seasonal ARIMA {#MSARIMA}
Using the same approach as the conventional SARIMA, we can introduce more terms [similar to how @Taylor2003a did it] with several seasonal frequencies. For example, we can have an hour of the day, a day of the week and a week of year frequencies in the data. Given that we work with the hourly data in this situation, we should introduce three seasonal ARIMA elements with seasonalities $m_1=24$, $m_2=24 \times 7=168$ and $m_3=24 \times 7 \times 365= 61,320$. In this example, we would have AR, I and MA polynomials for each seasonal part of the model, introducing a triple seasonal ARIMA, which is not even easy to formulate in the compact form. This type of model with multiple seasonal components can be called "Multiple Seasonal ARIMA", MSARIMA, which in general can be written as:
\begin{equation}
  y_t \Delta^{D_n}(B^{m_n}) \varphi^{P_n}(B^{m_n}) \dots \Delta^{D_0}(B^{m_0}) \varphi^{P_0}(B^{m_0}) = \epsilon_t \vartheta^{Q_n}(B^{m_n}) \dots \vartheta^{Q_0}(B^{m_0}) ,
  (\#eq:MSARIMACompact)
\end{equation}
where $n$ is the number of seasonal cycles, and $D_0=d$, $P_0=p$, $Q_0=q$ and $m_0=1$ for convenience. The slightly more compact and even less comprehensible form of \@ref(eq:MSARIMACompact) is:
\begin{equation}
  y_t \prod_{j=0}^n \Delta^{D_j} (B^{m_j}) \varphi^{P_j}(B^{m_j}) = \epsilon_t \prod_{j=0}^n \vartheta^{Q_j}(B^{m_j}) ,
  (\#eq:MSARIMACompactFinal)
\end{equation}
Conceptually, the model \@ref(eq:MSARIMACompactFinal) is neat, as it captures all the complex relations in the data, but it is not easy to understand and work with, not to mention the potential estimation and order selection problems. To understand what the forecast from such a model can be, we would need to take a special case, multiply the polynomials and move all the past elements on the right-hand side, leaving only $y_t$ on the left-hand side, as we did with SARIMA example above. It is worth noting that the `msarima()` function from the `smooth` package implements the model \@ref(eq:MSARIMACompactFinal), although not in this form, but in the state space form, discussed in Chapter \@ref(ADAMARIMA).

### Parameters bounds for MSARIMA {#MSARIMABounds}
When it comes to parameters bounds of SARIMA, the logic stays similar to the process discussed for the case of non-seasonal model in Section \@ref(ARIMABounds), with the only difference being that instead of analysing the polynomials of a specific part of a model, we need to consider the product of all polynomials. So, the *stationarity* condition for the MSARIMA is for all the roots of the following polynomial to be greater than one by absolute value (lie outside the unit circle):
\begin{equation}
  \prod_{j=0}^n \varphi^{P_j}(B^{m_j}) = 0,
  (\#eq:MSARIMABoundsStationarity)
\end{equation}
while the invertibility condition is for all the roots of the following polynomial to lie outside the unit circle:
\begin{equation}
  \prod_{j=0}^n \vartheta^{Q_j}(B^{m_j}) = 0.
  (\#eq:MSARIMABoundsInvertibility)
\end{equation}
Both of these conditions are difficult to check, especially for high frequencies $m_j$: the polynomial equation of order $n$ has $n$ complex roots, so if you fit a multiple seasonal ARIMA on hourly data, where the maximum frequency is $24\times 7\times 365 = 61,320$, then the equation will have at least 61,320 roots (this number will increase if there are lower frequencies or non-seasonal orders of the model). Finding all of them is not a trivial task even for modern computers (for example, the `polyroot()` function from the `base` package cannot handle this). So, when considering ARIMA on high-frequency data with high seasonal frequency values, it might make sense to find other ways of checking the stationarity and stability conditions. The `msarima()` and the `adam()` functions in the `smooth` package use the state space form of ARIMA (discussed in Chapter \@ref(StateSpaceARIMA)) and rely on slightly different principles of checking the same conditions. They do that more efficiently than in the case of the conventional approach of finding the roots of polynomials \@ref(eq:MSARIMABoundsStationarity) and \@ref(eq:MSARIMABoundsInvertibility).


## Box-Jenkins approach {#BJApproach}
Now that we are more or less familiar with the idea of ARIMA models, we can move to practicalities. As it might become apparent from the previous sections, one of the issues with the model is the identification of orders p, d, q, P$_j$, D$_j$, Q$_j$ etc. Back in the 20th century, when computers were slow, this was a challenging task, so George Box and Gwilym Jenkins [@Box1976] developed a methodology for identifying and estimating ARIMA models. While there are more efficient ways of order selection for ARIMA nowadays, some of their principles are still used in time series analysis and in forecasting. We briefly outline the idea in this section, not purporting to give a detailed explanation of the approach.

### Identifying stationarity
Before doing any time series analysis, we need to make the data stationary, which is done via the differences in the context of ARIMA (Section \@ref(Differences)). But before doing anything, we need to understand whether the data is stationary or not in the first place: over-differencing typically is harmful to the model and would lead to misspecification issues. At the same time, in the case of under-differencing, it might not be possible to identify the model correctly.

There are different ways of understanding whether the data is stationary or not. The simplest of them is just looking at the data: in some cases, it becomes apparent that the mean of the data changes or that there is a trend in the data, so the conclusion would be relatively straight forward. If it is not stationary, then taking differences and analysing the differenced data again would be the next step to ensure that the second differences are not needed.

The more formal approach would be to conduct statistical tests, such as ADF (the `adf.test()` from the `tseries` package) or KPSS (the `kpss.test()` from the `tseries` package). Note that they test different hypotheses:

1. In the case of ADF, it is:
* H$_0$: the data is **not** stationary;
* H$_1$: the data is stationary;

2. In the case of KPSS:
* H$_0$: the data is stationary;
* H$_1$: the data is **not** stationary;

I do not intent to discuss how the tests are conducted and what they imply in detail, the interested reader is referred to original papers of @Dickey1979 and @Kwiatkowski1992. It should suffice to say that ADF is based on estimating parameters of the AR model and then testing the hypothesis for those parameters, while KPSS includes the component of Random Walk in a model (with potential trend) and checks whether the variance of that component is zero or not. Both tests have their advantages and disadvantages and sometimes might contradict each other. No matter what test you choose, do not forget what testing a statistical hypothesis means [see, for example, Section 8.1 of @SvetunkovSBA]: if you fail to reject H$_0$, it does not mean that it is true.

::: remark
Even if you select the test-based approach, the procedure should still be iterative: test the hypothesis, take differences if needed, test the hypothesis again etc. This way, we can determine the order of differences I(d).
:::

When you work with seasonal data, the situation becomes more complicated. Yes, you can probably spot seasonality by visualising the data, but it is not easy to conclude whether the seasonal differences are needed. In this case, the Canova-Hansen test (the `ch.test()` in the `uroot` package) can be used to formally test the hypothesis similar to the one in the KPSS test, but applicable to the seasonal differences.

Only after making sure that the data is stationary we can move to the identification of AR and MA orders.

### Autocorrelation function (ACF) {#ACF}
In the core of the Box-Jenkins approach, lies the idea of autocorrelation and partial autocorrelation functions. **Autocorrelation** is the correlation [see Section 9.3 of @SvetunkovSBA] of a variable with itself from a different period of time. Here is an example of autocorrelation coefficient for lag 1:
\begin{equation}
  \rho(1) = \frac{\sigma_{y_t,y_{t-1}}}{\sigma_{y_t}\sigma_{y_{t-1}}} = \frac{\sigma_{y_t,y_{t-1}}}{\sigma_{y_t}^2},
  (\#eq:autoCorrelation)
\end{equation}
where $\rho(1)$ is the "true" autocorrelation coefficient, $\sigma_{y_t,y_{t-1}}$ is the covariance between $y_t$ and $y_{t-1}$, while $\sigma_{y_t}$ and $\sigma_{y_{t-1}}$ are the "true" standard deviations of $y_t$ and $y_{t-1}$. Note that $\sigma_{y_t}=\sigma_{y_{t-1}}$, because we are talking about one and the same variable. This is why we can simplify the formula to get the one on the right-hand side of \@ref(eq:autoCorrelation). The formula \@ref(eq:autoCorrelation) corresponds to the classical correlation coefficient, so this interpretation is the same as for the classical one: the value of $\rho(1)$ shows the closeness of the lagged relation to linear. If it is close to one, then this means that variable has a strong linear relation with itself on the previous observation. It obviously does not tell you anything about the causality, just shows a technical relation between variables, even if it is spurious in real life.

Using the formula \@ref(eq:autoCorrelation), we can calculate the autocorrelation coefficients for other lags as well, just substituting $y_{t-1}$ with $y_{t-2}$, $y_{t-3}$, $\dots$, $y_{t-\tau}$ etc. In a way, $\rho(\tau)$ can be considered as a function of a lag $\tau$, which is called the "Autocorrelation function" (ACF). If we know the order of the ARIMA process we deal with, we can plot the values of ACF on the y-axis by changing the $\tau$ on the x-axis. @Box1976 discuss different theoretical functions for several special cases of ARIMA, which we do not plan to repeat here fully. But, for example, they show that if you deal with AR(1) process, then the $\rho(1)=\phi_1$, $\rho(2)=\phi_1^2$ etc. This can be seen on the example of $\rho(1)$ by calculating the covariance for AR(1):
\begin{equation}
  \sigma_{y_t,y_{t-1}} = \mathrm{cov}(y_t,y_{t-1}) = \mathrm{cov}(\phi_1 y_{t-1} + \epsilon_t, y_{t-1}) = \mathrm{cov}(\phi_1 y_{t-1}, y_{t-1}) = \phi_1 \sigma_{y_t}^2 ,
  (\#eq:autoCovarianceAR1)
\end{equation}
which when inserted in \@ref(eq:autoCorrelation) leads to $\rho(1)=\phi_1$. The ACF for AR(1) with a positive $\phi_1$ will have the shape shown in Figure \@ref(fig:ACFExampleAR1) (on the example of $\phi_1=0.9$).

```{r ACFExampleAR1, fig.cap="ACF for AR(1) model.", echo=FALSE}
y <- vector("numeric",21)
y[] <- 0.9^c(0:(length(y)-1))
plot(0:(length(y)-1), y, type="l", xlab="Lag", ylab="ACF")
```

Note that $\rho(0)=1$ just because the value is correlated with itself, so lag 0 is typically dropped as not being useful. The declining shape of the ACF tells us that if $y_t$ is correlated with $y_{t-1}$, then the correlation between $y_{t-1}$ and $y_{t-2}$ will be exactly the same, also implying that $y_{t}$ is somehow correlated with $y_{t-2}$, even if there is no true relation between them. It is difficult to say anything for the AR process based on ACF exactly because of this temporal relation of the variable with itself.

On the other hand, ACF can be used to judge the order of MA(q) process. For example, if we consider MA(1) (Section \@ref(MA)), then the $\rho(1)$ will depend on the following covariance:
\begin{equation}
  \sigma_{y_t,y_{t-1}} = \mathrm{cov}(y_t,y_{t-1}) = \mathrm{cov}(\theta_1 \epsilon_{t-1} + \epsilon_t, \theta_1 \epsilon_{t-2} + \epsilon_{t-1}) = \mathrm{cov}(\theta_1 \epsilon_{t-1}, \epsilon_{t-1}) = \theta_1 \sigma^2 ,
  (\#eq:autoCovarianceMA1)
\end{equation}
where $\sigma^2$ is the variance of the error term, which in case of MA(1) is equal to $\sigma^2_{y_t}$, because E$(y_t)=0$. However, the covariance between the higher lags will be equal to zero for the pure MA(1) (given that the usual assumptions from Section \@ref(assumptions) hold). @Box1976 showed that for the moving averages, ACF tells more about the order of the model than for the autoregressive one: **ACF will drop rapidly right after the specific lag q for the MA(q) process**.

When it comes to seasonal models, in the case of seasonal AR(P), ACF will decrease exponentially from season to season (e.g. you would see a decrease on lags 4, 8, 12 etc. for SAR(1) and $m=4$), while in case of seasonal MA(Q), ACF would drop abruptly, starting from the lag $(Q+1)m$ (so, the subsequent seasonal lag from the one that the process has, e.g. on lag 8, if we deal with SMA(1) with $m=4$).


### Partial autocorrelation function (PACF) {#PACF}
The other instrument useful for time series analysis with respect to ARIMA is called "partial ACF". The idea of this follows from ACF directly. As we have spotted, if the process we deal with follows AR(1), then $\rho(2)=\phi_1^2$ just because of the temporal relation. In order to get rid of this temporal effect, when calculating the correlation between $y_t$ and $y_{t-2}$ we could remove the correlation $\rho(1)$ in order to get the clean effect of $y_{t-2}$ on $y_t$. This type of correlation is called "partial", denoting it as $\varrho(\tau)$. There are different ways how to do that. One of the simplest is to estimate the following regression model:
\begin{equation}
  y_t = a_1 y_{t-1} + a_2 y_{t-2} + \dots + a_\tau y_{t-\tau} + \epsilon_t,
  (\#eq:PACFRegression)
\end{equation}
where $a_i$ is the parameter for the $i$-th lag of the model. In this regression, all the relations between $y_t$ and $y_{t-\tau}$ are captured separately, so the last parameter $a_\tau$ is clean of all the temporal effects discussed above. We then can use the value $\varrho(\tau) = a_\tau$ as the coefficient, showing this relation. In order to obtain the PACF, we would need to construct and estimate regressions \@ref(eq:PACFRegression) for each lag $\tau=\{1, 2, \dots, p\}$ and get the respective parameters $a_1$, $a_2$, ..., $a_p$, which would correspond to $\varrho(1)$, $\varrho(2)$, ..., $\varrho(p)$.

Just to show what this implies, we consider calculating PACF for AR(1) process. In this case, the true model is:
\begin{equation*}
  y_t = \phi_1 y_{t-1} + \epsilon_t.
\end{equation*}
For the first lag we estimate exactly the same model, so that $\varrho(1)=\phi_1$. For the second lag we estimate the model:
\begin{equation*}
  y_t = a_1 y_{t-1} + a_2 y_{t-2} + \epsilon_t.
\end{equation*}
But we know that for AR(1), $a_2=0$, so when estimated in population, this would result in $\varrho(2)=0$ (in case of a sample, this would be a value very close to zero). If we continue with other lags, we will come to the same conclusion: for all lags $\tau>1$ for the AR(1), we will have $\varrho(\tau)=0$. This is one of the properties of PACF: **if we deal with AR(p) process, then PACF drops rapidly to zero right after the lag $p$**.

When it comes to MA(q) process, PACF behaves differently. In order to understand how it would behave, we take an example of MA(1) model, which is formulated as:
\begin{equation*}
  y_t = \theta_1 \epsilon_{t-1} + \epsilon_t.
\end{equation*}
As it was discussed in Section \@ref(ARIMABounds), MA process can be also represented as an infinite AR (see \@ref(eq:ARIMA100Example03) for derivation):
\begin{equation*}
  y_t = \sum_{j=1}^\infty -1^{j-1} \theta_1^j y_{t-j} + \epsilon_t.
\end{equation*}
If we construct and estimate the regression \@ref(eq:PACFRegression) for any lag $\tau$ for such process we will get $\varrho(\tau)=-1^{\tau-1} \theta_1^\tau$. This would correspond to the exponentially decreasing curve (if the parameter $\theta_1$ is positive, this will be an alternating series of values), similar to the one we have seen for the AR(1) and ACF. More generally, PACF will decline exponentially for MA(q) process, starting from the $\varrho(q)=\theta_q$.

When it comes to seasonal ARIMA models, the behaviour of PACF would resemble one of the non-seasonal ones, but with lags, multiple to the seasonality $m$. e.g., for the SAR(1) process with $m=4$, the $\varrho(4)=\phi_{4,1}$, while $\varrho(8)=0$.


### Summary {#BJApproachSummary}
Summarising the discussions in this section, we can conclude that:

1. For AR(p) process, ACF will decrease either exponentially or alternating (depending on the parameters' values), starting from the lag $p$;
2. For AR(p) process, PACF will drop abruptly right after the lag $p$;
3. For MA(q) process, ACF will drop abruptly right after the lag $q$;
4. For MA(q) process, PACF will decline either exponentially or alternating (based on the specific values of parameters), starting from the lag $q$.

These rules are tempting to use when determining the appropriate order of the ARMA model. This is what @Box1976 propose to do, but in an iterative way, analysing the ACF/PACF of residuals of models obtained on previous steps. The authors also recommend identifying the seasonal orders before moving to the non-seasonal ones.

However, these rules are not necessarily bi-directional and might not work in practice. e.g. *if we deal with MA(q), ACF drops abruptly right after the lag q, but if ACF drops abruptly after the lag q, then this does not necessarily mean that we deal with MA(q)*. The former follows directly from the assumed "true" model, while the latter refers to the identification of the model on the data, and there can be different reasons for the ACF to behave in a way it does. The logic here is similar to the following:

```{example}
All birds have wings. Sarah has wings. Thus Sarah is a bird.

Here is Sarah:
```

```{r SarahButterfly, out.width="75%", echo=FALSE, fig.cap="Sarah by Yegor Kamelev"}
# All defaults
knitr::include_graphics("./images/09-Sarah.jpeg")
```

This slight discrepancy led to issues in ARIMA identification over the years. We now understand that we should not rely entirely on the Box-Jenkins approach when identifying ARIMA orders. There are more appropriate methods for order selection, which can be used in the context of ARIMA [see, for example, @Hyndman2008Forecast], and we will discuss some of them in the Chapter \@ref(ADAMSelection). Still, ACF and PACF could be helpful in diagnostics. They might show you whether anything important is missing in the model. But they should not be used on their own. They are helpful together with other additional instruments (see discussion in Section \@ref(diagnosticsResidualsIIDAuto)).


### Examples of ACF/PACF for several ARMA processes {#BJApproachExample}
Building upon the behaviours of ACF/PACF for AR and MA processes, we consider several examples of data generated from AR/MA. This might provide some guidelines of how the orders of ARMA can be identified in practice and show how challenging this is in some situations.

Figure \@ref(fig:acfpacfExampleAR1) shows the plot of the data and ACF/PACF for AR(1) process with $\phi_1=0.9$. As discussed earlier, the ACF in this case declines exponentially, starting from lag 1, while the PACF drops right after the lag 1. The plot also shows how the fitted values and the forecast would look like if the order of the model was correctly selected and the parameters were known.

```{r acfpacfExampleAR1, fig.cap="AR(1) process with $\\phi_1=0.9$", echo=FALSE}
load("data/armaExamples.Rdata")
# xAR1 <- sim.ssarima(list(ar=1,i=0,ma=0),obs=120,AR=0.9,initial=10)$data
layout(matrix(c(1,2,1,3),2,2))
ssarima(xAR1, orders=c(1,0,0), AR=0.9, h=20, holdout=TRUE) |>
    forecast(interval="parametric") |>
    plot(legend=FALSE, parReset=FALSE, ylab="Data", xlab="Time")
forecast::Acf(xAR1, main="")
pacf(xAR1, main="")
```

::: remark
You might notice that there are some values of PACF lying outside the standard 95% confidence bounds in the plots above. This does not mean that we need to include the respective AR elements (AR(6) and AR(18)), because, by the definition of confidence interval, we can expect 5% of coefficients of correlation to lie outside the bounds, so we can ignore them.
:::

Similar idea is shown in Figure \@ref(fig:acfpacfExampleAR2) for AR(2) process with $\phi_1=0.9$ and $\phi_2=-0.4$. In the Figure, the ACF oscillates slightly around the origin, while the PACF drops to zero right after the lag 2. These are the characteristics that we have discussed earlier in this Section.

```{r acfpacfExampleAR2, fig.cap="AR(2) process with $\\phi_1=0.9$ and $\\phi_2=-0.4$", echo=FALSE}
# xAR2 <- sim.ssarima(list(ar=2,i=0,ma=0),obs=120,AR=c(0.9,-0.4),initial=c(10,-5))$data
layout(matrix(c(1,2,1,3),2,2))
ssarima(xAR2, orders=c(2,0,0), AR=c(0.9,-0.4), h=20, holdout=TRUE) |>
    forecast(interval="parametric") |>
    plot(legend=FALSE, parReset=FALSE, ylab="Data", xlab="Time")
forecast::Acf(xAR2, main="")
pacf(xAR2, main="")
```

Similar plots but with different behaviours of ACF and PACF can be shown for MA(1) and MA(2) processes. Figure \@ref(fig:acfpacfExampleMA2) shows how the MA(2) process looks like with $\theta_1=-0.9$ and $\theta_2=0.8$. As we can see, the correct diagnostics of the MA order in this case is already challenging: while the ACF drops to zero after lag 2, the PACF seems to contain some values outside of the interval on lags 3 and 4. So, using the @Box1976 approach, we would be choosing between MA(2) and AR(1) / AR(4) models.

```{r acfpacfExampleMA2, fig.cap="MA(2) process with $\\theta_1=-0.9$ and $\\theta_2=0.8$", echo=FALSE}
# xMA2 <- sim.ssarima(list(ar=0,i=0,ma=2),obs=120,MA=maValues,initial=c(10,-5))$data
layout(matrix(c(1,2,1,3),2,2))
ssarima(xMA2, orders=c(0,0,2), MA=c(-0.9,0.8), h=20, holdout=TRUE) |>
    forecast(interval="parametric") |>
    plot(legend=FALSE, parReset=FALSE, ylab="Data", xlab="Time")
forecast::Acf(xMA2, main="")
pacf(xMA2, main="")
```

To make things even more complicated, we present similar plots for ARMA(2,2) model in Figure \@ref(fig:acfpacfExampleARMA22).

```{r acfpacfExampleARMA22, fig.cap="ARMA(2,2) process with $\\phi_1=0.9, \\phi_2=-0.4$, $\\theta_1=-0.9$ and $\\theta_2=0.8$", echo=FALSE}
# xARMA22 <- sim.ssarima(list(ar=2,i=0,ma=2),obs=120,MA=maValues,AR=c(0.9,-0.4),initial=c(10,-5))$data
layout(matrix(c(1,2,1,3),2,2))
ssarima(xARMA22, orders=c(2,0,2), AR=c(0.9,-0.4), MA=c(-0.9,0.8), h=20, holdout=TRUE) |>
    forecast(interval="parametric") |>
    plot(legend=FALSE, parReset=FALSE, ylab="Data", xlab="Time")
forecast::Acf(xARMA22, main="")
pacf(xARMA22, main="")
```

Based on ACF/PACF in Figure \@ref(fig:acfpacfExampleARMA22), we can conclude that the process is not fundamentally distinguishable from AR(2) and/or MA(2). In order to correctly identify the order in this situation, @Box1976 recommend doing the identification sequentially, by fitting an AR model, then analysing the residuals and selecting an appropriate MA order. Still, even this iterative process is challenging and does not guarantee that the correct order will be selected. This is one of the reasons why modern ARIMA order selection methods do not fully rely on the Box-Jenkins approach and involve selection based on information criteria [@Hyndman2008Forecast, @Svetunkov2019].


## ARIMA and ETS {#ARIMAandETS}
@Box1976 showed in their textbook that several exponential smoothing methods could be considered special cases of the ARIMA model. Because of that, statisticians have thought for many years that ARIMA is a superior model and paid no attention to exponential smoothing. It took many years, many papers and a lot of effort [@Makridakis1982; @Fildes1998; @Makridakis2000] to show that this is not correct and that if you are interested in forecasting, then exponential smoothing, being a simpler model, typically does a better job than ARIMA. It was only after @Ord1997 that statisticians have started considering ETS as a separate model with its own properties. Furthermore, it seems that some of the conclusions from the previous competitions mainly apply to the [Box-Jenkins approach](#BJApproach) [for example, see @Makridakis1997], pointing out that selecting the correct order of ARIMA models is a much more challenging task than the statisticians have thought before.

Still, there is a connection between ARIMA and ETS models, which can benefit both models, so it is worth discussing this in a separate section of the monograph.

### ARIMA(0,1,1) and ETS(A,N,N) {#ARIMAETS011}
@Muth1960 was one of the first authors who showed that Simple Exponential Smoothing (Section \@ref(SES)) has an underlying ARIMA(0,1,1) model. This becomes apparent, when we study the error correction form of SES:
\begin{equation*}
  \hat{y}_{t} = \hat{y}_{t-1} + \hat{\alpha} e_{t-1}.
\end{equation*}
Recalling that $e_t=y_t-\hat{y}_t$, this equation can be rewritten as:
\begin{equation*}
  y_{t} = y_{t-1} -e_{t-1} + \hat{\alpha} e_{t-1} + e_t,
\end{equation*}
or after regrouping elements:
\begin{equation*}
  y_{t} -y_{t-1} = e_t + (\hat{\alpha} -1) e_{t-1}.
\end{equation*}
Finally, using the backshift operator for ARIMA, substituting the estimated values by their "true" ones, we get the ARIMA(0,1,1) model:
\begin{equation*}
  y_{t}(1 -B) = \epsilon_t(1 + \theta_1 B),
\end{equation*}
where $\theta_1 = \alpha-1$. This relation was one of the first hints that $\alpha$ in SES should lie in a wider interval: based on the fact that $\theta_1 \in (-1, 1)$, the smoothing parameter $\alpha \in (0, 2)$. This is the same region we get when we deal with the admissible bounds of the ETS(A,N,N) model (Section \@ref(SESandETS)). This connection between the parameters of ARIMA(0,1,1) and ETS(A,N,N) is useful on its own because we can transfer the properties of ETS to ARIMA. For example, we know that the level in ETS(A,N,N) will change slowly when $\alpha$ is close to zero. Similar behaviour would be observed in ARIMA(0,1,1) with $\theta_1$ close to -1. In addition, we know that ETS(A,N,N) reverts to Random Walk, when $\alpha=1$, which corresponds to $\theta_1=0$. So, the closer $\theta_1$ to zero, the more abrupt behaviour the ARIMA model exhibits. In cases of $\theta_1>0$, the model's behaviour becomes even more uncertain. In a way, this relation gives us the idea of what to expect from more complicated ARIMA(p,d,q) models when the parameters for moving average are negative -- the model should typically behave smoother. However, this might differ from one model to another, depending on the MA order.

The main conceptual difference between ARIMA(0,1,1) and ETS(A,N,N) is that the latter still makes sense, when $\alpha=0$, while in case of ARIMA(0,1,1) the condition $\theta_1=-1$ is unacceptable. The global level model with $\theta_1=-1$ corresponds to just a different model, ARIMA(0,0,0) with constant.

Finally, the connection between the two models tells us that if we have the ARIMA(0,1,q) model, this model would be suitable for the data called "level" in the ETS framework. The length of $q$ would define the distribution of the weights in the model. The specific impact of each MA parameter on the actual values would differ, depending on the order $q$ and values of parameters. The forecast from the ARIMA(0,1,q) would be a straight line parallel to the x-axis for $h\geq q$.

In order to demonstrate the connection between the two models we consider the following example in R using functions `sim.es()`, `es()` and `msarima()` from `smooth` package:
```{r}
# Generate data from ETS(A,N,N) with alpha=0.2
y <- sim.es("ANN", obs=120, persistence=0.2)
# Estimate ETS(A,N,N)
esModel <- es(y$data, "ANN")
# Estimate ARIMA(0,1,1)
msarimaModel <- msarima(y$data, c(0,1,1), initial="optimal")
```

Given the the two models in smooth have the same initialisation mechanism, they should be equivalent. The result might differ slightly only because of the optimisation routine in the two functions. The values of their losses and information criteria should be similar:
```{r}
# Loss values
setNames(c(esModel$lossValue, msarimaModel$lossValue),
         c("ETS(A,N,N)","ARIMA(0,1,1)"))
# AIC
setNames(c(AIC(esModel), AIC(msarimaModel)),
         c("ETS(A,N,N)","ARIMA(0,1,1)"))
```
In addition, their parameters should be related based on the formula discussed above. The following two lines should produce similar values:
```{r}
# Smoothing parameter and theta_1
setNames(c(esModel$persistence, msarimaModel$arma$ma+1),
         c("ETS(A,N,N)","ARIMA(0,1,1)"))
```
Finally, the fit and the forecasts from the two models should be exactly the same if the parameters are linearly related (Figure \@ref(fig:ETSARIMAConnection)):

```{r ETSARIMAConnection, fig.cap="ETS(A,N,N) and ARIMA(0,1,1) models producing the same fit and forecast trajectories."}
par(mfcol=c(2,1), mar=c(2,2,2,1))
plot(esModel,7)
plot(msarimaModel,7)
```

We expect the ETS(A,N,N) and ARIMA(0,1,1) models to be similar in this example because they are estimated using the respective functions `es()` and `msarima()`, which are implemented in the same way, using the same framework. If the framework, initialisation, construction or estimation would be different, then the relation between the applied models might be not exact but approximate.


### ARIMA(0,2,2) and ETS(A,A,N) {#ARIMAETS022}
@Nerlove1964 showed that there is an underlying ARIMA(0,2,2) for the Holt's method (Subsection \@ref(ETSAAN)), although they do not say that explicitly in their paper. Skipping the derivations, the relation between Holt's method and the ARIMA model is expressed in the following two equations about their parameters (in the form of ARIMA discussed in this monograph):
\begin{equation*}
  \begin{aligned}
    &\theta_1 = \alpha + \beta -2 \\
    &\theta_2 = 1 -\alpha
  \end{aligned}
\end{equation*}
We also know from the section \@ref(ETSExamples) that Holt's method has an underlying ETS(A,A,N) model. Thus there is a connection between this model and ARIMA(0,2,2). This means that ARIMA(0,2,2) will produce linear trajectories for the data and that the MA parameters of the model regulate the speed of the update of values. Because of the second difference, ARIMA(0,2,q) will produce a straight line as a forecasting trajectory for any $h\geq q$.

Similarly to the ARIMA(0,1,1) vs ETS(A,N,N), one of the important differences between the models is that the boundary values for parameters are not possible for ARIMA(0,2,2): $\alpha=0$ and $\beta=0$ are possible in ETS, but the respective $\theta_1=2$ and $\theta_2=-1$ in ARIMA are not.

Furthermore, the model that corresponds to the situation, when $\alpha=1$ and $\beta=0$ is formulated as ARIMA(0,1,0) with drift (discussed in Section \@ref(Differences)), while the global trend ARIMA could hypothetically appear in the boundary case with $\theta_1=-2$ and $\theta_2=1$, implying the following model:
\begin{equation*}
    y_t (1 -B)^2 = \epsilon_t -2\epsilon_{t-1} + \epsilon_{t-2} = \epsilon_t (1 -B)^2 ,
\end{equation*}
which tells us that in ARIMA framework, the global trend model is only available as a global mean on second differences of the data.

Finally, the ETA(A,A,N) and ARIMA(0,2,2) will fit the data similarly and produce the exact forecasts as long as they are constructed, initialised and estimated in the same way.


### ARIMA(1,1,2) and ETS(A,Ad,N) {#ARIMAETS112}
@Roberts1982 proposed damped trend exponential smoothing method (Section \@ref(ETSAAdN)), showing that it is related to ARIMA(1,1,2) model, with the following connection between the parameters of the two:
\begin{equation*}
  \begin{aligned}
    &\theta_1 = \alpha -1 + \phi (\beta -1) \\
    &\theta_2 = \phi(1 -\alpha) \\
    &\phi_1 = \phi
  \end{aligned} .
\end{equation*}
At the same time, the damped trend method has underlying ETS(A,Ad,N), thus the two models are connected. Recalling that ETS(A,Ad,N) reverts to ETS(A,A,N), when $\phi=1$, we can see a similar property in ARIMA: when $\phi_1=1$, the model should be reformulated as ARIMA(0,2,2) instead of ARIMA(1,1,2). Given the direct connection between the dampening parameters and the AR(1) parameter of the two models, we can conclude that AR(1) defines the forecasting trajectory's dampening effect. We have already noticed this in Section \@ref(ARMAConstant). However, we should acknowledge that the dampening only happens when $\phi_1 \in (0,1)$. The case of $\phi_1>1$ is unacceptable in the ARIMA framework and is not very useful in the case of ETS, producing explosive exponential trajectories. The case of $\phi_1 \in (-1, 0)$ is possible but is less useful in practice, as the trajectory will oscillate.

The lesson to learn from the connection between the two models is that AR(p) part of ARIMA can act as a dampening element for the forecasting trajectories, although the specific shape would depend on the value of $p$ and the values of parameters.


### ARIMA and other ETS models {#ARIMAETSOther}
The pure additive seasonal ETS models (Chapter \@ref(ADAMETSPureAdditive)) also have a connection with ARIMA, but the resulting models are not parsimonious. For example, ETS(A,A,A) is related to SARIMA(0,1,m+1)(0,1,0)$_m$ [@Mckenzie1976; @Chatfield1977] with some restrictions on parameters. If we were to work with SARIMA and wanted to model the seasonal time series, we would probably apply SARIMA(0,1,1)(0,1,1)$_m$ instead of this larger model.

When it comes to pure multiplicative (Chapter \@ref(ADAMETSPureMultiplicativeChapter)) and mixed (Section \@ref(ADAMETSMixedModels)) ETS models, there are no appropriate ARIMA analogues for them. For example, @Chatfield1977 showed that there are no ARIMA models for the exponential smoothing with the multiplicative seasonal component. This makes ETS distinct from ARIMA. The closest one can get to a pure multiplicative ETS model is the ARIMA applied to logarithmically transformed data when the smoothing parameters of ETS are close to zero, coming from the limit \@ref(eq:limitOf1x).


### ETS + ARIMA
Finally, based on the discussion above, it is possible to have a combination of ETS and ARIMA, but not all combinations would be meaningful and helpful. For example, fitting a combination of ETS(A,N,N)+ARIMA(0,1,1) is not a good idea due to the connection of the two models (Subsection \@ref(ARIMAETS011)). However, doing ETS(A,N,N) and adding ARIMA(1,0,0) component would make sense -- the resulting model would exhibit the dampening trend as discussed in Section \@ref(ARIMAETS112) but would have fewer parameters to estimate than ETS(A,Ad,N). @Gardner1985 pointed out that using AR(1) with exponential smoothing methods improves forecasting accuracy, so this combination of the two models is potentially beneficial for ETS. In the next chapter, we will discuss how specifically the two models can be united in one framework.


## Examples of application {#ARIMAExampleInR}
### Non-seasonal data
Using the time series from the Box-Jenkins textbook [@Box1976], we fit the ARIMA model to the data but based on our judgment rather than their approach. Just a reminder, here is how the data looks (series `BJsales`, Figure \@ref(fig:BJSalesPlot)):

```{r BJSalesPlot, fig.cap="Box-Jenkins sales data.", echo=FALSE}
plot(BJsales)
```

It seems to exhibit the trend in the data, so we can consider ARIMA(1,1,2), ARIMA(0,2,2) and ARIMA(1,1,1) models. We do not consider models with drift in this example, because they would imply the same slope over time for the whole series, which does not seem to be the case here. We use `msarima()` function from the `smooth` package, which is a wrapper for the `adam()` function:
```{r}
adamARIMABJ <- vector("list",3)
# ARIMA(1,1,2)
adamARIMABJ[[1]] <- msarima(BJsales, orders=c(1,1,2),
                            h=10, holdout=TRUE)
# ARIMA(0,2,2)
adamARIMABJ[[2]] <- msarima(BJsales, orders=c(0,2,2),
                            h=10, holdout=TRUE)
# ARIMA(1,1,1)
adamARIMABJ[[3]] <- msarima(BJsales, orders=c(1,1,1),
                            h=10, holdout=TRUE)
names(adamARIMABJ) <- c("ARIMA(1,1,2)", "ARIMA(0,2,2)",
                        "ARIMA(1,1,1)")
```
Comparing information criteria (we will use AICc) of the three models, we can select the most appropriate one:
```{r}
sapply(adamARIMABJ, AICc)
```

::: remark
Note that this comparison is possible in `adam()`, `ssarima()` and `msarima()` because the implemented ARIMA is formulated in state space form, sidestepping the issue of the conventional ARIMA (where taking differences reduces the sample size).
:::

Based on this comparison, it looks like the ARIMA(0,2,2) is the most appropriate model (among the three) for the data. Here how the fit and the forecast from the model looks (Figure \@ref(fig:adamARIMAPlotBJSales)):

```{r adamARIMAPlotBJSales, fig.cap="BJSales series and ARIMA(0,2,2)"}
plot(adamARIMABJ[[2]], which=7)
```

Comparing this model with the ETS(A,A,N), we will see a slight difference because the two models are initialised and estimated differently:
```{r}
adam(BJsales, "AAN", h=10, holdout=TRUE)
```

If we are interested in a more classical Box-Jenkins approach, we can always analyse the residuals of the constructed model and try improving it further. Here is an example of ACF and PACF of the residuals of the ARIMA(0,2,2):

```{r adamARIMAPlotBJSalesACFPACF, fig.cap="ACF and PACF of ARIMA(0,2,2) on BJSales data"}
par(mfcol=c(1,2))
plot(adamARIMABJ[[2]], which=c(10,11), main="")
```

As we see from the plot in Figure \@ref(fig:adamARIMAPlotBJSalesACFPACF), all autocorrelation coefficients lie inside the confidence interval, implying that there are no significant AR / MA lags to include in the model.

### Seasonal data {#ARIMAExampleInRSeasonal}
Similarly to the previous cases, we use Box-Jenkins AirPassengers data, which exhibits a multiplicative seasonality and a trend. We will model this using SARIMA(0,2,2)(0,1,1)$_{12}$, SARIMA(0,2,2)(1,1,1)$_{12}$ and SARIMA(0,2,2)(1,1,0)$_{12}$ models, which are selected to see what type of seasonal ARIMA is more appropriate to the data:
```{r}
adamSARIMAAir <- vector("list",3)
# SARIMA(0,2,2)(0,1,1)[12]
adamSARIMAAir[[1]] <- msarima(AirPassengers, lags=c(1,12),
                              orders=list(ar=c(0,0), i=c(2,1),
                                          ma=c(2,1)),
                              h=12, holdout=TRUE)
# SARIMA(0,2,2)(1,1,1)[12]
adamSARIMAAir[[2]] <- msarima(AirPassengers, lags=c(1,12),
                              orders=list(ar=c(0,1), i=c(2,1),
                                          ma=c(2,1)),
                              h=12, holdout=TRUE)
# SARIMA(0,2,2)(1,1,0)[12]
adamSARIMAAir[[3]] <- msarima(AirPassengers, lags=c(1,12),
                              orders=list(ar=c(0,1), i=c(2,1),
                                          ma=c(2,0)),
                              h=12, holdout=TRUE)
names(adamSARIMAAir) <- c("SARIMA(0,2,2)(0,1,1)[12]",
                          "SARIMA(0,2,2)(1,1,1)[12]",
                          "SARIMA(0,2,2)(1,1,0)[12]")
```
Note that now that we have a seasonal component, we need to provide the SARIMA lags: 1 and $m=12$ and specify `orders` differently -- as a list with values for AR, I and MA orders separately. This is done because the SARIMA implemented in `msarima()` and `adam()` supports multiple seasonalities (e.g. you can have `lags=c(1,24,24*7)` if you want). The resulting information criteria of models are:
```{r}
sapply(adamSARIMAAir, AICc)
```
It looks like the second model is slightly better than the other two, so we will use it in order to produce forecasts (see Figure \@ref(fig:adamSARIMAPlotAir)):

```{r adamSARIMAPlotAir, fig.cap="Forecast of AirPassengers data from SARIMA(0,2,2)(1,1,1)$_{12}$ model."}
forecast(adamSARIMAAir[[2]], h=12,
         interval="prediction") |>
     plot(main="")
```

This model is directly comparable with ETS models, so here is, for example, AICc of ETS(M,A,M) on the same data:
```{r}
AICc(adamETSAir <- adam(AirPassengers, "MAM",
                        h=12, holdout=TRUE))
```

It is lower than for the SARIMA model, which means that ETS(M,A,M) is more appropriate for the data in terms of information criteria than SARIMA(0,2,2)(1,1,1)$_{12}$. We can also investigate if there is a way to improve ETS(M,A,M) by adding some ARMA components (Figure \@ref(fig:adamETSPlotAirACFPACF)):

```{r adamETSPlotAirACFPACF, fig.cap="ACF and PACF of ETS(M,A,M) on AirPassengers data"}
par(mfcol=c(1,2))
plot(adamETSAir, which=c(10,11), main="")
```

Judging by the plots in Figure \@ref(fig:adamETSPlotAirACFPACF), significant correlation coefficients exist for some lags. Still, it is not clear whether they appear due to randomness or not. Just to check, we will see if adding SARIMA(0,0,0)(0,0,1)$_{12}$ helps (reduces AICc) in this case:
```{r}
adam(AirPassengers, "MAM", h=12, holdout=TRUE,
     order=list(ma=c(0,1)), lags=c(1,12)) |>
    AICc()
```
As we see, the increased complexity does not decrease the AICc (probably because now we need to estimate 13 parameters more than in just ETS(M,A,M)), so we should not add the SARIMA component. We could try adding other SARIMA elements to see if they improve the model, but we do not aim to find the best model here. The interested reader is encouraged to do that as an additional exercise.
