# Forecasts evaluation {#forecastsEvaluation}
Forecasts ought to serve a specific purpose. They should not be made "just because" but be useful in the making of a decision. The decision then dictates the kind of forecast that should be made -- its form and its time horizon(s). It also dictates how the forecast should be evaluated -- a forecast only being as good as the quality of the decisions it enables.

```{example}
Retailers typically need to order some amount of milk that they will sell over the next week. They do not know how much they will sell so they usually order, hoping to satisfy, let us say, 95% of demand. This situation tells us that the forecasts need to be made a week ahead, they should be cumulative (considering the overal demand during a week before the next order) and that they should focus on an upper bound of a 95% prediction interval. Producing only point forecasts would not be useful in this situation.
```

When you understand how your system works and what sort of forecasts you should produce you can start an evaluation process; measuring the performance of different forecasting models / methods and selecting the most appropriate for your data. There are different ways to measure and compare the performance of models / methods In this chapter, we discuss the most common approaches, first focusing on the evaluation of point forecasts, then moving towards prediction intervals and quantile forecasts. After that we discuss how to choose the appropriate error measure and to make sure that the model performs consistently on the available data via rolling origin and statistical tests.


## Measuring accuracy of point forecasts {#errorMeasures}
We start with a setting in which we are interested in point forecasts only. In this case we typically start by splitting the available data into train and test sets, apply the models under consideration to the former and produce forecasts on the latter, not showing that part to the models. This is called the "fixed origin" approach: we fix the point in time from which to produce forecasts, we produce them, calculate some sort of error measures and compare the models.

There are different error measures that can be used in this case. Which measure ought to be used depends on the specific need. Here we briefly discuss the most important measures and refer to [@Davydenko2013; @SvetunkovAccuracy2019; @SvetunkovAPEs2017] for the gory details.

The most important error measures are Root Mean Squared Error (RMSE):
\begin{equation}
    \mathrm{RMSE} = \sqrt{\frac{1}{h} \sum_{j=1}^h \left( y_{t+j} - \hat{y}_{t+j} \right)^2 },
    (\#eq:RMSE)
\end{equation}
and Mean Absolute Error (MAE):
\begin{equation}
    \mathrm{MAE} = \frac{1}{h} \sum_{j=1}^h \left| y_{t+j} - \hat{y}_{t+j} \right| ,
    (\#eq:MAE)
\end{equation}
where $y_{t+j}$ is the actual value $j$ steps ahead from the holdout, $\hat{y}_{t+j}$ is the $j$ steps ahead point forecast (conditional expectation of the model) and $h$ is the forecast horizon. As you see, these error measures aggregate the performance of competing forecasting methods across the forecasting horizon, averaging out the specific performances on each $j$. If this information needs to be retained, then the summation can be dropped to obtain just "SE" and "AE".

It is well-known [see, for example, @Kolassa2016] that **RMSE is minimised by the mean value** of a distribution, and **MAE is minimised by the median**. So, when selecting between the two, you should consider this property. This means, for example, that MAE-based error measures should not be used for the evaluation of models on intermittent demand, because zero forecast will minimise MAE, when the sample contains more than 50% of zeroes.

Another error measure that has been used in some cases is Root Mean Squared Logarithmic Error (RMSLE):
\begin{equation}
    \mathrm{RMSLE} = \frac{1}{h} \sum_{j=1}^h \left( \log y_{t+j} - \log \hat{y}_{t+j} \right)^2 .
    (\#eq:RMSLE)
\end{equation}
It assumes that the actual values and the forecasts are positive and is minimised by geometric mean.

The main advantage of these error measures is that they are very simple and have a clear interpretations: they are the "average" distances between the point forecasts and the observed values. They are perfect if you work with only one time series. However, they are not suitable, when you have several time series and want to see the performance of methods across them. This is mainly because they are scale dependent and contain specific units: if you measures sales of apples in pounds, then MAE and RMSE will show the error in pounds. And, as we know, you should not add up pounds of apples with pounds of oranges - the result might not make sense.

In order to tackle this issue, different error scaling techniques have been proposed, resulting in a zoo of error measures:

1. MAPE - Mean Absolute Percentage Error:
\begin{equation}
    \mathrm{MAPE} = \frac{1}{h} \sum_{j=1}^h \frac{y_{t+j} - \hat{y}_{t+j}}{y_{t+j}},
    (\#eq:MAPE)
\end{equation}
2. MASE - Mean Absolute Scaled Error [@Hyndman2006]:
\begin{equation}
    \mathrm{MASE} = \frac{1}{h} \sum_{j=1}^h \frac{|y_{t+j} - \hat{y}_{t+j}|}{\bar{\Delta}_y},
    (\#eq:MASE)
\end{equation}
where $\bar{\Delta}_y = \frac{1}{t-1}\sum_{j=2}^t |\Delta y_{j}|$ is the mean absolute value of the first differences $\Delta y_{j}=y_j-y_{j-1}$ of the in-sample data;
3. rMAE - Relative Mean Absolute Error [@Davydenko2013]:
\begin{equation}
    \mathrm{rMAE} = \frac{\mathrm{MAE}_a}{\mathrm{MAE}_b},
    (\#eq:rMAE)
\end{equation}
where $\mathrm{MAE}_a$ is the mean absolute error of the model under consideration and $\mathrm{MAE}_b$ is the MAE of the benchmark model;
4. sMAE - scaled Mean Absolute Error [@Petropoulos2015]:
\begin{equation}
    \mathrm{sMAE} = \frac{\mathrm{MAE}}{\bar{y}},
    (\#eq:sMAE)
\end{equation}
where $\bar{y}$ is the mean of the in-sample data.
5. and others.

There is no "best" error measure. All have advantages and disadvantages. For example:

1. MAPE is scale sensitive (if the actual values are measured in thousands of units, the resulting error will be much lower than in the case of hundreds of units) and cannot be estimated on data with zeroes. However, it has a simple interpretation as it shows the percentage error (as the name suggests);
2. MASE avoids the disadvantages of MAPE, but does so at the cost of a simple interpretation due to the division by the first differences of the data (some interpret this as an in-sample one-step-ahead Na√Øve forecast);
3. rMAE avoids the disadvantages of MAPE, has a simple interpretation (it shows by how much one model is better than the other), but fails, when either $\mathrm{MAE}_a$ or $\mathrm{MAE}_b$ for a specific time series is equal to zero;
4. sMAE avoids the disadvantages of MAPE has an interpretation close to it, but breaks down when the data has a trend.

When comparing different forecasting methods it can make sense to calculate several of the error measures for comparison. The choice of metric might depend on the specific needs of the forecaster. Here's a few rules of thumb, however: If you want a robust measure that works consistently, but you do not care about the interpretation, then go with MASE. If you want an interpretation, then either go with rMAE, or sMAE (just keep in mind that if you decide to use rMAE or any other relative measure, you might get attacked by its creator, Andrey Davydenko, who might blame you for stealing his creation, even if you put a reference to his work). You should typically avoid MAPE and other percentage error measures because they are highly influenced by the actual values you have in the holdout. Furthermore, similar to the measures above, there have been proposed RMSE-based scaled and relative error metrics, which would measure the performance of methods in terms of means rather than medians. Here is a brief list of some of them:

1. RMSSE - Root Mean Squared Scaled Error:
\begin{equation}
    \mathrm{RMSSE} = \sqrt{\frac{1}{h} \sum_{j=1}^h \frac{(y_{t+j} - \hat{y}_{t+j})^2}{\bar{\Delta}_y^2}} ;
    (\#eq:RMSSE)
\end{equation}
3. rRMSE - Relative Root Mean Squared Error:
\begin{equation}
    \mathrm{rRMSE} = \frac{\mathrm{RMSE}_a}{\mathrm{RMSE}_b} ;
    (\#eq:rRMSE)
\end{equation}
4. sRMSE - scaled Root Mean Squared Error:
\begin{equation}
    \mathrm{sRMSE} = \frac{\mathrm{RMSE}}{\bar{y}} .
    (\#eq:sRMSE)
\end{equation}

Similarly, RMSSLE, rRMSLE and sRMSLE can be proposed, using the same principles as in \@ref(eq:RMSSE), \@ref(eq:rRMSE) and \@ref(eq:sRMSE).

Finally, when aggregating the performance of forecasting methods across several time series, sometimes it makes sense to look at the distribution of errors - this way you will know which of the methods fails seriously and which does a consistently good job.

### Demonstration in R
In R, there is a variety of functions that calculate the error measures discussed above, including the `accuracy()` function from `forecast` package and `measures()` from `greybox`. `greybox` also has individual measures, such as `MAE()`, `MSE()`, `MASE()` etc. Here is an example of how the measures can be calculated based on a couple of forecasting functions from `smooth` package for R:
```{r}
y <- rnorm(100,100,10)
model1 <- es(y,h=10,holdout=TRUE)
model2 <- ces(y,h=10,holdout=TRUE)

rbind(measures(model1$holdout, model1$forecast,
               actuals(model1)),
      measures(model1$holdout, model2$forecast,
               actuals(model1)))
```
The default benchmark methods for relative measures above is Na\"ive.


## Measuring uncertainty {#uncertainty}
While point forecasts are useful in order to understand what to expect on average, prediction intervals are important in many applications to be able to quantify what to expect in $1-\alpha$ share of cases. Intervals quantify the uncertainty around the point forecasts and thus the riskiness of the decision. In a way, if you do not have prediction intervals, then you cannot adequately assess the uncertainty about future outcomes. If you cannot say that sales next week will be between 1,000 and 1,200 units with a confidence level of 95% then you cannot say anything useful about future sales because as per the [previous discussion](#intro) point forecasts represent only mean values and typically will not be equal to the actual observations from the holdout sample. Hopefully, all of this explains why the prediction intervals are needed in forecasting.

As with point forecasts multiple measures can be used to evaluate prediction intervals. Here are the most popular ones:

1. **Coverage**, showing the percentage of observations lying inside the interval:
\begin{equation}
    \mathrm{coverage} = \frac{1}{h} \sum_{j=1}^h \left( \mathbb{1}(y_{t+j} < l_{t+j}) \times \mathbb{1}(y_{t+j} > u_{t+j}) \right),
    (\#eq:coverage)
\end{equation}
where $l_{t+j}$ is the lower bound and $u_{t+j}$ is the upper bound of the interval and $\mathbb{1}(\cdot)$ is the indicator function, returning one, when the condition is true and zero otherwise. Ideally, the coverage should be equal to the confidence level of the interval, but in reality, this can only be observed [asymptotically](#estimatesProperties), as the sample size increases due to the inheritted randomness of any sample estimates of parameters;
2. **Range**, showing the width of the prediction interval:
\begin{equation}
    \mathrm{range} = \frac{1}{h} \sum_{j=1}^h (u_{t+j} -l_{t+j});
    (\#eq:range)
\end{equation}
3. **Mean Interval Score** [@Gneiting2007], which shows a combination of the previous two:
\begin{equation}
    \begin{aligned}
    \mathrm{MIS} = & \frac{1}{h} \sum_{j=1}^h \left( (u_{t+j} -l_{t+j}) + \frac{2}{\alpha} (l_{t+j} -y_{t+j}) \mathbb{1}(y_{t+j} < l_{t+j}) +\right. \\
    & \left. \frac{2}{\alpha} (y_{t+j} -u_{t+j}) \mathbb{1}(y_{t+j} > u_{t+j}) \right) ,
    \end{aligned}
    (\#eq:MIS)
\end{equation}
where $\alpha$ is the significance level. If the actual values lie outside of the interval, they get penalised with a ratio of $\frac{2}{\alpha}$, proportional to the distance from the interval bound. At the same time the width of the interval positively influences the value of the measure: the wider the interval, the higher the score. The ideal model with $\mathrm{MIS}=0$ should have all the actual values in the holdout lying on the bounds of the interval and $u_{t+j}=l_{t+j}$, implying that the bounds coincide with each other and that there is no uncertainty about the future (which is not possible in the real life).
4. **Pinball Score** [@Koenker1978], which measures the accuracy of models in terms of specific quantiles (this is usually applied to different quantiles produced from the model, not just to the lower and upper bounds of 95% interval):
\begin{equation}
    \mathrm{PS} = (1 -\alpha) \sum_{y_{t+j} < q_{t+j}, j=1,\dots,h } |y_{t+j} -q_{t+j}| + \alpha \sum_{y_{t+j} \geq q_{t+j} , j=1,\dots,h } |y_{t+j} -q_{t+j}|,
    (\#eq:pinball)
\end{equation}
where $q_{t+j}$ is the value of the specific quantile of the distribution. What PS shows, is how well we capture the specific quantile in the data. The lower the value of pinball is, the closer the bound is to the specific quantile of the holdout distribution. If the PS is equal to zero, then we have done the perfect job in hitting that specific quantile. The main issue with PS is that it is very difficult to assess the quantiles correctly on small samples. For example, in order to get a better idea of how the 0.975 quantile performs, we would need to have at least 40 observations, so that 39 of them would be expected to lie below this bound $\left(\frac{39}{40} = 0.975\right)$. In fact, the quantiles are not always uniquely defined [see, for example, @Taylor2020], which makes the measurement difficult.

Similar to the pinball function, it is possible to propose the expectile-based score, but while it has nice statistical properties [@Taylor2020], it is more difficult to interpret.

Range, MIS and PS are unit-dependent. In order to be able to aggregate them over several time series they need to be scaled (as we did with MAE and RMSE in [previous section](#errorMeasures)) either via division by the in-sample mean or in-sample mean absolute differences in order to obtain the scaled counterparts of the measures or via division by the values from the benchmark model in order to obtain the relative one.

If you are interested in the overall performance of the model, then MIS provides this information. However, it does not show what specifically happens inside and is difficult to interpret. Coverage and range are easier to interpret but only give information about the specific prediction interval and typically must be traded off against each other (i.e. one can either cover more or have a narrower interval). Academics prefer the pinball for the purposes of uncertainty assessment, as it shows more detailed information about the predictive distribution from each model, but, while it is easier to interpret than MIS, it is still not as straightforward as coverage and range. So, the selection of the measure, again, depends on your specific situation and on the understanding of statistics by decision makers.

### Example in R
Continuing the example from the previous section, we could produce prediction intervals from the two models and compare them using MIS and pinball:

```{r}
model1Forecast <- forecast(model1,h=10,interval="p",level=0.95)
model2Forecast <- forecast(model2,h=10,interval="p",level=0.95)

# Mean Interval Score
setNames(c(MIS(model1$holdout, model1Forecast$lower,
               model1Forecast$upper, 0.95),
           MIS(model2$holdout, model2Forecast$lower,
               model2Forecast$upper, 0.95)),
         c("Model 1", "Model 2"))

# Pinball for the upper bound
setNames(c(pinball(model1$holdout, model1Forecast$upper, 0.975),
           pinball(model2$holdout, model2Forecast$upper, 0.975)),
         c("Model 1", "Model 2"))


# Pinball for the lower bound
setNames(c(pinball(model1$holdout, model1Forecast$lower, 0.025),
           pinball(model2$holdout, model2Forecast$lower, 0.025)),
         c("Model 1", "Model 2"))

# Coverage
setNames(c(mean(model1$holdout > model1Forecast$lower &
                model1$holdout < model1Forecast$upper),
           mean(model2$holdout > model2Forecast$lower &
                model2$holdout < model2Forecast$upper)),
         c("Model 1", "Model 2"))
```
These measures do not tell much in terms of performance of models, when only applied to one time series. In order to see a proper difference, we need to apply models to a set of time series, produce forecasts, calculate measures and then look at their aggregate performance, e.g. via mean / median or quantiles.


## How to choose appropriate error measure {#errorMeasuresSelection}
While in general the selection of error measure should be dictated by the specific problem at hand, there are some guidelines that might be helpful in the process. I have summarised them in the flowchart in Figure \@ref(fig:errorMeasuresFlowChart).

```{r errorMeasuresFlowChart, out.width="100%", fig.cap="Error measures selection flowchart.", echo=FALSE}
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\includegraphics{./images/errorMeasuresFlowChart.pdf}')
} else {
    knitr::include_graphics("./images/errorMeasuresFlowChart.gif")
}
```

The flowchart does not provide the excessive options, and is a simplification of the possible process. It does not discuss the quantile and interval measures in detail, as there are many more of them, and the idea is to list the most important ones. The aim of the flowchart is to provide a basic idea that the selection can be done based on:

1. Number of time series under consideration. If there is several of them and you need to aggregate the error measure, then you need to use either scaled or relative ones. In case of just one time series, you do not need to scale the error measure;
2. What specifically you want to measure: point forecasts, quantiles, prediction interval or something else;
3. Whether the interpretability of the error measure is important or not. If not, then scaled measures similar to @Hyndman2006 can be used. If yes, then the choice is between relative and scaled using mean measures;
4. Whether the data is stationary or not. If it is then it is safe to use scaled measures similar to @Petropoulos2015, because the division by in-sample mean would be meaningful. Otherwise you should either use @Hyndman2006 scaling or relative measures;
5. Whether the data is intermittent or not. If it is and you are interested in point forecasts, then you should use RMSE based measures - other measures might recommend zero forecast as the best one;
6. Symmetry of distribution. If it is symmetric (which does not happen very often), then median will coincide with mean and geometric mean, and it would not be important, whether to use RMSE-, MAE- or RMSLE- based measure. In that case, just use MAE-based one;
7. What you need (denoted as "What do you like?" in the flowchart). If you are interested in mean performance then use RMSE based measures. MAE is minimised by median, and RMSLE is minimised by geometric mean.

The point forecast related error measures have been discussed in Section \@ref(errorMeasures), while the interval and quantile ones - in Section \@ref(uncertainty).

You can also download this flowchart in pdf format via [this link](./images/errorMeasuresFlowChart-v2.pdf).


## Rolling origin {#rollingOrigin}
::: remark
The text in this section is based on the vignette for the [greybox package](https://cran.r-project.org/package=greybox), written by the author of this textbook.
:::

When there is a need to select the most appropriate forecasting model or method for the data, the forecaster usually splits the sample into two parts: in-sample (aka "training set") and holdout sample (aka out-sample or "test set"). The model is estimated on the in-sample and its forecasting performance evaluated [using some error measure](#errorMeasures) on the holdout sample.

Using this procedure only once is known as "fixed origin" evaluation. Evaluating a forecast thus, however, might give a misleading impression of its accuracy. If, for example, the time series contains outliers or level shifts a poor model might perform better in fixed origin evaluation than a more appropriate one. An alternative procedure known as "rolling origin" evaluation is much more robust to such issues.

In rolling origin evaluation the forecasting origin is repeatedly moved forward and forecasts are produced from each origin [@Tashman2000]. This technique allows obtaining several forecast errors for time series, which gives a better understanding of how the models perform. This can be considered as a time series analogue to cross-validation techniques [@WikipediaCrossValidation2020]. Here is a simple graphical representation, courtesy of [Nikos Kourentzes](https://kourentzes.com/forecasting/).

```{r ROProcessAnimation, out.width="75%", fig.cap="Rolling origin illustrated, by Nikos Kourentzes", echo=FALSE}
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\includegraphics{./images/03-ROAnimation.jpg}')
} else {
    knitr::include_graphics("./images/03-ROAnimation.gif")
}
```

There are different options of how this can be done.

### Principles of Rolling origin

Figure \@ref(fig:ROProcessCO) [@Svetunkov2017] illustrates the basic idea behind rolling origin. White cells correspond to the in-sample data while the light grey cells correspond to the three-steps-ahead forecasts. The time series in the figure has 25 observations and forecasts are produced from 8 origins starting from origin 15. The model is estimated on the first in-sample set and forecasts are produced. Next, another observation is added to the end of the in-sample set, the test set is advanced and the procedure is repeated. The process stops when there is no more data left. This is a rolling origin with a **constant holdout** sample size. As a result of this procedure 8 one to three steps ahead forecasts are produced. Based on them we can calculate the preferred error measures and choose the best performing model.

```{r ROProcessCO, out.width="75%", echo=FALSE, fig.cap="Rolling origin with constant holdout size"}
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\includegraphics{./images/03-ROProcessCO.jpg}')
} else {
    knitr::include_graphics("./images/03-ROProcessCO.gif")
}
```

Another option for producing forecasts from 8 origins would be to start from origin 17 instead of 15, as shown in Figure \@ref(fig:ROProcessNoCO). In this case the procedure continues until origin 22 when the last full set of three-steps-ahead forecasts can be produced produced but then continues with a decreasing forecasting horizon. So the two-steps-ahead forecast is produced from origin 23 and only a one-step-ahead forecast is produced from origin 24. As a result we obtain 8 one-step-ahead forecasts, 7 two-steps-ahead forecasts and 6 three-steps-ahead forecasts. This is a rolling origin with a **non-constant holdout** sample size, which can be useful with small samples when we don't have any observations to spare.

```{r ROProcessNoCO, out.width="75%", echo=FALSE, fig.cap="Rolling origin with non-constant holdout size"}
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\includegraphics{./images/03-ROProcessNoCO.jpg}')
} else {
    knitr::include_graphics("./images/03-ROProcessNoCO.gif")
}
```

Finally, in both of the cases above we had the **increasing in-sample** size. However for some research purposes we might need a **constant in-sample**. Figure \@ref(fig:ROProcessCOCI) demonstrates such a setup. In this case, in each iteration we add an observation to the end of the in-sample series and remove one from the beginning (dark grey cells).

```{r ROProcessCOCI, out.width="75%", echo=FALSE, fig.cap="Rolling origin with constant in-sample size"}
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\includegraphics{./images/03-ROProcessCOCI.jpg}')
} else {
    knitr::include_graphics("./images/03-ROProcessCOCI.gif")
}
```

### Rolling origin in R
The function `ro()` from `greybox` package (written by Yves Sagaert and Ivan Svetunkov in 2016 on the way to the International Symposium on Forecasting) implements the rolling origin evaluation for any function you like with a predefined `call` and returns the desired `value`. It heavily relies on the two variables: `call` and `value` - so it is quite important to understand how to formulate them in order to get the desired results. `ro()` is a very flexible function but as a result it is not very simple. In this subsection we will see how it work on a couple of examples.

We start with a simple example, generating a series from normal distribution:
```{r}
x <- rnorm(100,100,10)
```

We use an ARIMA(0,1,1) model implemented in the `stats` package:
```{r}
ourCall <- "predict(arima(x=data,order=c(0,1,1)),n.ahead=h)"
```

The call that we specify includes two important elements: `data` and `h`. `data` specifies where the in-sample values are located in the function that we want to use, and **it needs to be called "data"** in the call. `h` will tell our function, where the forecasting horizon is specified in the selected function. Note that in this example we use `arima(x=data,order=c(0,1,1))`, which produces a desired ARIMA(0,1,1) model and then we use `predict(..., n.ahead=h)`, which produces an h steps ahead forecast from that model.

Having the call, we need also to specify what the function should return. This can be the conditional mean (point forecasts), prediction intervals, the parameters of a model, or, in fact, anything that the model returns (e.g. name of the fitted model and its likelihood). However, there are some differences in what `ro()` returns depending on what the function returns. If it is a vector, then `ro()` will produce a matrix (with values for each origin in columns). If it is a matrix then an array is returned. Finally, if it is a list, then a list of lists is returned.

In order not to overcomplicate things, we will collect the conditional mean from the `predict()` function:
```{r}
ourValue <- c("pred")
```

**NOTE**: If you do not specify the value to return, the function will try to return everything, but it might fail, especially if a lot of values are returned. So, in order to be on the safe side, **always provide the `value`, when possible**.

Now that we have specified `ourCall` and `ourValue`, we can produce forecasts from the model using rolling origin. Let's say that we want three-steps-ahead forecasts and 8 origins with the default values of all the other parameters:
```{r}
returnedValues1 <- ro(x, h=3, origins=8,
                      call=ourCall, value=ourValue)
```

The function returns a list with all the values that we asked for plus the actual values from the holdout sample. We can calculate some basic error measure based on those values, for example, scaled Mean Absolute Error [@Petropoulos2015]:
```{r}
apply(abs(returnedValues1$holdout - returnedValues1$pred),
      1, mean, na.rm=TRUE) /
  mean(returnedValues1$actuals)
```

In this example we use `apply()` function in order to distinguish between the different forecasting horizons and have an idea of how the model performs for each of them. These numbers do not tell us much on their own, but if we compared the performance of this model with another one, then we could infer if one model is more appropriate for the data than the other one. For example, applying ARIMA(1,1,2) to the same data, we will get:
```{r}
ourCall <- "predict(arima(x=data,order=c(1,1,2)),n.ahead=h)"
returnedValues2 <- ro(x, h=3, origins=8,
                      call=ourCall, value=ourValue)
apply(abs(returnedValues2$holdout - returnedValues2$pred),
      1, mean, na.rm=TRUE) /
  mean(returnedValues2$actuals)
```
Comparing these errors with the ones from the previous model, we can conclude, which of the approaches is more adequate for the data.

We can also plot the forecasts from the rolling origin, which shows how the selected model behaves:
```{r fig.width=8, fig.height=6}
par(mfcol=c(2,1))
plot(returnedValues1)
plot(returnedValues2)
```

In this example the forecasts from different origins are close to each other. This is because the data is stationary and the model is quite stable.

The rolling origin function from the `greybox` package also allows working with explanatory variables and returning prediction intervals if needed. Some further examples are discussed in the vignette of the package: `vignette("ro","greybox")`.

Practically speaking, if we have a set of forecasts from different models we can analyse the distribution of error measures and come to conclusions about performance of models. Here is an example with analysis of performance for $h=1$ based on absolute errors:

```{r}
aeValuesh1 <- cbind(abs(returnedValues1$holdout -
                          returnedValues1$pred)[1,],
                    abs(returnedValues1$holdout -
                          returnedValues2$pred)[1,])
colnames(aeValuesh1) <- c("ARIMA(0,1,1)","ARIMA(1,1,2)")
boxplot(aeValuesh1)
points(apply(aeValuesh1,2,mean),pch=16,col="red")
```

The boxplots above can be interpreted as any other boxplots applied to random variables (see for example, discussion in Section 2.2 of @SvetunkovSBA).


## Statistical comparison of forecasts {#statisticalTests}
After applying several competing models to the data and obtaining a distribution of error terms, we might find that some of them performed very similar. In this case, there might be a question, whether the difference is significant and which of the forecasting model we should select. Consider the following artificial example, where we had 4 competing models and measured their performance in terms of RMSSE:
```{r eval=FALSE}
smallCompetition <- matrix(NA, 100, 4,
                           dimnames=list(NULL,
                                         paste0("Method",c(1:4))))
smallCompetition[,1] <- rnorm(100,1,0.35)
smallCompetition[,2] <- rnorm(100,1.2,0.2)
smallCompetition[,3] <- runif(100,0.5,1.5)
smallCompetition[,4] <- rlnorm(100,0,0.3)
```

```{r echo=FALSE}
load("data/smallCompetition.Rdata")
```
We can check the mean and median error measures in this example in order to see, how the methods perform overall:
```{r}
overalResults <- matrix(c(colMeans(smallCompetition), 
                          apply(smallCompetition, 2, median)),
                        4, 2, dimnames=list(colnames(smallCompetition),
                                            c("Mean","Median")))
round(overalResults,5)
```
In this artificial example, it looks like the most accurate method in terms of mean and median RMSSE is Method 4, and the least accurate one is Method 2. However, the difference in terms of accuracy between methods 1, 3 and 4 does not look big. So, should we conclude that the Method 4 is the best? Let's first look at the distribution of errors using `vioplot()` function from `vioplot` package (Figure \@ref(fig:smallCompetitionBoxplot)).

```{r smallCompetitionBoxplot, fig.cap="Boxplot of RMSE for the artificial example"}
vioplot::vioplot(smallCompetition)
points(colMeans(smallCompetition), col="red", pch=16)
```

What the violin plots in Figure \@ref(fig:smallCompetitionBoxplot) show is that the distribution of errors for the Method 2 is shifted higher than the distributions of other methods, but it also looks like Method 2 is working more consistently, meaning that the variability of the errors is lower (the size of the box on the graph). It's difficult to tell whether Method 1 is better than Methods 3 and 4 or not - their boxes intersect and roughly look similar, with Method 4 having slightly shorter box and Method 3 having the box slightly lower positioned.

This is all the basics of descriptive statistics, which allows to conclude that in general Methods 1, 3 and 4 do slightly better job than the Method 2. This is also reflected in the mean and median error measures, discussed above. So, what should we conclude?

We should not make hasty decisions and forget that we are dealing with a sample of data (100 time series), so inevitably the performance of methods will change if we try them on different data sets. If we had a population of all the time series in the world, then we could run our methods and make a more solid conclusion about their performance. But here we deal with a sample. So it might make sense to see, whether the difference in performance of methods is significant. How should we do that?

First, we can **compare means** of distributions of errors using a parametric statistical test. We can try F-test [@WikipediaFTest], which will tell us whether the mean performance of methods is similar or not. Unfortunately, this will not tell us, how the methods compare. But t-test [@WikipediaTTest] could be used to do that instead for pairwise comparison. One could also use a regression with dummy variables for methods, which will then give us parameters and their confidence intervals (based on t-statistics), telling us, how the means of methods compare. However F-test, t-test and t-statistics from regression rely on strong assumptions related to the distribution of the means of error measures (normality). If we had large sample (e.g. a thousand of series) and well behaved distribution (but distribution of error measures is typically skewed), then we could try it, hoping that central limit theorem would work, and might get something relatively meaningful. However, on 100 observations this still could be an issue, especially given that the distribution of error measures is typically asymmetric (this means that the estimate of mean might be biased, which leads to a lot of issues).

Second, we could **compare medians** of distributions of errors. They are robust to outliers, so their estimates should not be too biased in case of skewed distributions on smaller samples. In order to have a general understanding of performance (is everything the same or is there at least one method that performs differently), we could try Friedman test [@WikipediaFriedmanTest], which could be considered as a non-parametric alternative of F-test. This should work in our case, but won't tell us how specifically the methods compare. We could try Wilcoxon signed-ranks test [@WikipediaWilcoxonTest], which could be considered as a non-parametric counterpart of t-test, but it is only applicable for the comparison of two variables, while we want to compare four.

Luckily, there is Nemenyi test [@Demsar2006], which is equivalent to MCB test [@Koning2005]. What the test does, is it ranks performance of methods for each time series and then takes mean of those ranks and produces confidence bounds for those means. The means of ranks correspond to medians, so this means that by using this test, we compare medians of errors of different methods. If the confidence bounds for different methods intersect, then we can conclude that the medians are not different from statistical point of view. Otherwise, we can see which of the methods has higher rank, and which has the lower one. There are different ways how to present the results of the test and there are several R functions that implement it, including `nemenyi()` from `tsutils` package. However, we will use a function `rmcb()` from `greybox` which has more flexible plotting capabilities, supporting all the default parameters for `plot()` method.

```{r eval=FALSE}
smallCompetitionTest <- rmcb(smallCompetition, plottype="none")
smallCompetitionTest
plot(smallCompetitionTest, "mcb", main="")
```

```{r mcbForCompetition, fig.cap="MCB test results for small competition.", echo=FALSE}
smallCompetitionTest
plot(smallCompetitionTest, "mcb", main="")
```

Figure \@ref(fig:mcbForCompetition) shows that Methods 1, 3 and 4 are not statistically different - their intervals intersect, so we cannot really tell the difference between them, even though the mean rank of Method 4 is lower than for the other methods. Method 2, on the other hand, is significantly worse than the other methods: it has the highest mean rank of all and its interval does not intersect intervals of other methods.

Note that while this is a good way of presenting the results, all the MCB test does is comparison of mean ranks. It does not tell much about the distribution of errors and neglects the distances between values (i.e. 0.1 is lower than 0.11, so the first method has lower rank, which is exactly the same result as with comparing 0.1 and 100). This happens because by doing the test, we move from numerical scale to the ordinal one (see Section 1.2 of @SvetunkovSBA). Finally, as any other statistical test, it will get its power, when the sample increases - we know that the null hypothesis "variables are equal to each other" in reality is always wrong (see Section 5.3 of @SvetunkovSBA), so the increase of sample size will lead at some point to the right conclusion: methods are statistically different. Here is a demonstration of this thesis:

```{r eval=FALSE}
largeCompetition <- 
  matrix(NA, 100000, 4,
         dimnames=list(NULL, paste0("Method",c(1:4))))
# Generate data
largeCompetition[,1] <- rnorm(100000,1,0.35)
largeCompetition[,2] <- rnorm(100000,1.2,0.2)
largeCompetition[,3] <- runif(100000,0.5,1.5)
largeCompetition[,4] <- rlnorm(100000,0,0.3)
# Run the test
largeCompetitionTest <- rmcb(largeCompetition, plottype="none")
plot(largeCompetitionTest, "mcb", main="")
```

```{r mcbForCompetitionLarge, fig.cap="MCB test results for large competition.", echo=FALSE}
plot(largeCompetitionTest, "mcb", main="")
```

In the plot in Figure \@ref(fig:mcbForCompetitionLarge), Method 4 has become significantly worse than Methods 1 and 3 in terms of mean ranks (note that it was winning in the small competition). The difference between Methods 1 and 3 is still not significant, but it would become if we continue increasing the sample size. This example tells us that we need to be careful, when selecting the best method, as this might change under different circumstances. At least we knew from the start that Method 2 is not good.
