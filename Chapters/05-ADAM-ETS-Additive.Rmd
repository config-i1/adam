# Pure additive ADAM ETS {#ADAMETSIntroduction}
Now that we are familiar with the conventional ETS, we can move to the discussion of ADAM implementation of ETS, which has several important differences from the classical one. This chapter focuses on technical details of the pure additive model, discussing general formulation in algebraic form, then moving to recursive relations, which are needed to understand how to produce forecasts from the model and how to estimate it correctly (i.e. impose restrictions on the parameters). Finally, we discuss the distributional assumptions for ADAM ETS, introducing not only the Normal distribution but also showing how to use Laplace, S, Generalised Normal, Log-Normal, Gamma, and Inverse Gaussian distributions in the context.


## Model formulation {#ADAMETSPureAdditive}
The pure additive case is interesting, because this is the group of models that have closed forms for conditional moments (mean and variance) and support parametric predictive distribution for several steps ahead values. In order to understand how we can get to the general model form, we consider an example of an ETS(A,A,A) model, which, as discussed in Section \@ref(ETSTaxonomyMaths), is formulated as:
\begin{equation}
	\begin{aligned}
		& y_t = l_{t-1} + b_{t-1} + s_{t-m} + \epsilon_{t} \\
		& l_{t} = l_{t-1} + b_{t-1} + \alpha \epsilon_{t} \\ 
		& b_{t} = b_{t-1} + \beta \epsilon_{t} \\
		& s_{t} = s_{t-m} + \gamma \epsilon_{t}
	\end{aligned}.
  (\#eq:ETSADAMAAA)
\end{equation}
This model can be formatted in the following way:
\begin{equation}
  \begin{aligned}
    y_{t} = & l_{t-1} & + & b_{t-1} & + & s_{t-m} & + & \epsilon_t \\
    l_t = & l_{t-1} & + & b_{t-1} & + & 0 & + & \alpha \epsilon_t \\
    b_t = & 0 & + & b_{t-1} & + & 0 & + & \beta \epsilon_t \\
    s_t = & 0 & + & 0 & + & s_{t-m} & + & \gamma \epsilon_t 
  \end{aligned} .
  (\#eq:ETSADAMAAASimplified)
\end{equation}
To see how its elements can then be represented in the matrix form based on \@ref(eq:ETSADAMAAASimplified):
\begin{equation}
  \begin{aligned}
    y_t & = \begin{pmatrix} 1 & 1 & 1 \end{pmatrix}
    \begin{pmatrix}
      l_{t-1} \\ b_{t-1} \\ s_{t-m}
    \end{pmatrix} +
    \epsilon_t \\
    \begin{pmatrix}
      l_t \\ b_t \\ s_t
    \end{pmatrix} & =
    \begin{pmatrix}
      1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
      l_{t-1} \\ b_{t-1} \\ s_{t-m}
    \end{pmatrix} +
    \begin{pmatrix}
      \alpha \\ \beta \\ \gamma
    \end{pmatrix}
    \epsilon_t 
  \end{aligned}.
  (\#eq:ETSADAMAAAMatrixForm)
\end{equation}
I use tabulation in \@ref(eq:ETSADAMAAASimplified) to show how the matrix form is related to the general one. The positions of $l_{t-1}$, $b_{t-1}$ and $s_{t-m}$ correspond to the non-zero values in the transition matrix in \@ref(eq:ETSADAMAAAMatrixForm). Now we can define each matrix and vector, for example:
\begin{equation}
  \begin{aligned}
    \mathbf{w} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}, & \mathbf{F} = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}, & \mathbf{g} = \begin{pmatrix} \alpha \\ \beta \\ \gamma \end{pmatrix}, \\
    \mathbf{v}_{t} = \begin{pmatrix} l_t \\ b_t \\ s_t \end{pmatrix}, & \mathbf{v}_{t-\boldsymbol{l}} = \begin{pmatrix} l_{t-1} \\ b_{t-1} \\ s_{t-m} \end{pmatrix}, & \boldsymbol{l} = \begin{pmatrix} 1 \\ 1 \\ m \end{pmatrix}
  \end{aligned}.
  (\#eq:ETSADAMAAAMatrices)
\end{equation}
Substituting \@ref(eq:ETSADAMAAAMatrices) into \@ref(eq:ETSADAMAAAMatrixForm), we get the general pure additive ADAM ETS model:
\begin{equation}
  \begin{aligned}
    {y}_{t} = &\mathbf{w}^\prime \mathbf{v}_{t-\boldsymbol{l}} + \epsilon_t \\
    \mathbf{v}_{t} = &\mathbf{F} \mathbf{v}_{t-\boldsymbol{l}} + \mathbf{g} \epsilon_t
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditive)
\end{equation}
where $\mathbf{w}$ is the measurement vector, $\mathbf{F}$ is the transition matrix, $\mathbf{g}$ is the persistence vector, $\mathbf{v}_{t-\boldsymbol{l}}$ is the vector of lagged components, and $\boldsymbol{l}$ is the vector of lags. The important thing to note is that the ADAM is based on the model discussed in Section \@ref(ETSConventionalModelAdditive), but it is formulated using lags of components rather than their transition over time. This comes to the elements of the vector $\boldsymbol{l}$. Just for the comparison, the conventional ETS(A,A,A), formulated according to \@ref(eq:ETSConventionalStateSpace) would have the following transition matrix (instead of \@ref(eq:ETSADAMAAAMatrices)):
\begin{equation}
  \mathbf{F} = \begin{pmatrix} 1 & 1 & \mathbf{0}^\prime_{m-1} & 0 \\ 0 & 1 & \mathbf{0}^\prime_{m-1} & 0 \\ 0 & 0 & \mathbf{0}^\prime_{m-1} & 1 \\ \mathbf{0}_{m-1} & \mathbf{0}_{m-1} & \mathbf{I}_{m-1} & \mathbf{0}_{m-1} \end{pmatrix},
  (\#eq:ETSADAMAAAMatricesTransition)
\end{equation}
where $\mathbf{I}_{m-1}$ is the identity matrix of the size $(m-1) \times (m-1)$ and $\mathbf{0}_{m-1}$ is the vector of zeroes of size $m-1$. The main benefit of using the vector of lags $\boldsymbol{l}$ instead of the conventional mechanism in the transition equation is in the reduction of dimensions of matrices (the transition matrix contains $3\times 3$ elements in the case of \@ref(eq:ETSADAMStateSpacePureAdditive) instead of $(2+m)\times (2+m)$ as in the conventional ETS model). The model \@ref(eq:ETSADAMStateSpacePureAdditive) is more parsimonious than the conventional one and simplifies some of the calculations, making it realistic, for example, to apply models to data with large frequency $m$ (e.g. 24, 48, 52, 365). The main disadvantage of this approach is in the complications arising in the derivation of conditional expectation and variance, which still have closed forms, but are more cumbersome. They are discussed later in this chapter in Section \@ref(pureAdditiveExpectationAndVariance).


## Recursive relation {#adamETSPureAdditiveRecursive}
One of the useful representations of the pure additive model \@ref(eq:ETSADAMStateSpacePureAdditive) is its recursive form, which can be used for further inference.

First, when we produce forecast for $h$ steps ahead, it is important to understand what the actual value $h$ steps ahead might be, given the information on observation $t$ (i.e. in-sample values). In order to get to it, we first consider the model for the actual value $y_{t+h}$:
\begin{equation}
  \begin{aligned}
    & {y}_{t+h} = \mathbf{w}^\prime \mathbf{v}_{t+h-\boldsymbol{l}} + \epsilon_{t+h} \\
    & \mathbf{v}_{t+h} = \mathbf{F} \mathbf{v}_{t+h-\boldsymbol{l}} + \mathbf{g} \epsilon_{t+h}
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion01)
\end{equation}
where $\mathbf{v}_{t+h-\boldsymbol{l}}$ is the vector of previous states, given the lagged values $\boldsymbol{l}$. Now we need to split the measurement and persistence vectors together with the transition matrix into parts for the same lags of components, leading to the following equation:
\begin{equation}
  \begin{aligned}
    & {y}_{t+h} = (\mathbf{w}_{m_1}^\prime + \mathbf{w}_{m_2}^\prime + \dots + \mathbf{w}_{m_d}^\prime) \mathbf{v}_{t+h-\boldsymbol{l}} + \epsilon_{t+h} \\
    & \mathbf{v}_{t+h} = (\mathbf{F}_{m_1} + \mathbf{F}_{m_2} + \dots + \mathbf{F}_{m_d}) \mathbf{v}_{t+h-\boldsymbol{l}} + (\mathbf{g}_{m_1} + \mathbf{g}_{m_2} + \dots \mathbf{g}_{m_d}) \epsilon_{t+h}
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion02)
\end{equation}
where $m_1, m_2, \dots, m_d$ are the distinct lags of the model. So, for example, in the case of an ETS(A,A,A) model on quarterly data (periodicity is equal to four), $m_1=1$, $m_2=4$, leading to $\mathbf{F}_{1} = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ and $\mathbf{F}_{4} = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix}$, where the split of the transition matrix is done column-wise. This split of matrices and vectors into distinct sub matrices and subvectors is needed in order to get the correct recursion and obtain the correct conditional h-steps ahead expectation and variance.

By substituting the values in the transition equation of \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion02) with their previous values until we reach $t$, we get:
\begin{equation}
  \begin{aligned}
    \mathbf{v}_{t+h-\boldsymbol{l}} = & \mathbf{F}_{m_1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{v}_{t} + \sum_{j=1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{F}_{m_1}^{j-1} \mathbf{g}_{m_1} \epsilon_{t+m_1\lceil\frac{h}{m_1}\rceil-j} + \\
    & \mathbf{F}_{m_2}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{v}_{t} + \sum_{j=1}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{F}_{m_2}^{j-1} \mathbf{g}_{m_2} \epsilon_{t+m_2\lceil\frac{h}{m_2}\rceil-j} + \\
    & \dots + \\
    & \mathbf{F}_{m_d}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{v}_{t} + \sum_{j=1}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{F}_{m_d}^{j-1} \mathbf{g}_{m_d} \epsilon_{t+m_d\lceil\frac{h}{m_d}\rceil-j} .
  \end{aligned}
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion03)
\end{equation}
Inserting \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion03) in the measurement equation of \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion02), we get:
\begin{equation}
  \begin{aligned}
    y_{t+h} = & \mathbf{w}_{m_1}^\prime \mathbf{F}_{m_1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{v}_{t} + \mathbf{w}_{m_1}^\prime \sum_{j=1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{F}_{m_1}^{j-1} \mathbf{g}_{m_1} \epsilon_{t+m_1\lceil\frac{h}{m_1}\rceil-j} + \\
    & \mathbf{w}_{m_2}^\prime \mathbf{F}_{m_2}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{v}_{t} + \mathbf{w}_{m_2}^\prime \sum_{j=1}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{F}_{m_2}^{j-1} \mathbf{g}_{m_2} \epsilon_{t+m_2\lceil\frac{h}{m_2}\rceil-j} + \\
    & \dots + \\
    & \mathbf{w}_{m_d}^\prime \mathbf{F}_{m_d}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{v}_{t} + \mathbf{w}_{m_d}^\prime \sum_{j=1}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{F}_{m_d}^{j-1} \mathbf{g}_{m_d} \epsilon_{t+m_d\lceil\frac{h}{m_d}\rceil-j} + \\
    & \epsilon_{t+h} .
  \end{aligned}
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion04)
\end{equation}
This recursion shows how the actual value appears based on the states on observation $t$, values of transition matrix and measurement and persistence vectors, and on the error term for the holdout sample. The latter is typically not known but we can usually estimate its moments (e.g. $\mathrm{E}(\epsilon_t)=0$ and $\mathrm{V}(\epsilon_t)=\sigma^2$), which will help us in getting conditional moments for the actual value $y_{t+h}$.

Substituting the specific values of $m_1, m_2, \dots, m_d$ in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04) will simplify the equation and make it easier to understand. For example, for ETS(A,N,N), $m_1=1$ and all the other lags are equal to zero, so the recursion \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04) simplifies to:
\begin{equation}
    y_{t+h} = \mathbf{w}_{1}^\prime \mathbf{F}_{1}^{h-1} \mathbf{v}_{t} + \mathbf{w}_{1}^\prime \sum_{j=1}^{h-1} \mathbf{F}_{1}^{j-1} \mathbf{g}_{1} \epsilon_{t+h-j} + \epsilon_{t+h} ,
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion06)
\end{equation}
which is the recursion obtained by @Hyndman2008b on page 103.


## Conditional expectation and variance {#pureAdditiveExpectationAndVariance}
Now, why is the recursion \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04) important? This is because we can take the expectation and variance of \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04) conditional on the values of the state vector $\mathbf{v}_{t}$ on the observation $t$ and all the matrices and vectors ($\mathbf{F}$, $\mathbf{w}$, and $\mathbf{g}$), assuming that the basic model assumptions hold (error term is homoscedastic, uncorrelated, and has the expectation of zero, Subsection \@ref(assumptions)), in order to get:
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & \sum_{i=1}^d \left(\mathbf{w}_{m_i}^\prime \mathbf{F}_{m_i}^{\lceil\frac{h}{m_i}\rceil-1} \right) \mathbf{v}_{t} \\
    \sigma^2_{h} = \text{V}(y_{t+h}|t) = & \left( \sum_{i=1}^d \left(\mathbf{w}_{m_i}^\prime \sum_{j=1}^{\lceil\frac{h}{m_i}\rceil-1} \mathbf{F}_{m_i}^{j-1} \mathbf{g}_{m_i} \mathbf{g}^\prime_{m_i} (\mathbf{F}_{m_i}^\prime)^{j-1} \mathbf{w}_{m_i} \right) + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionMeanAndVariance)
\end{equation}
where $\sigma^2$ is the variance of the error term. The formulae \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionMeanAndVariance) are cumbersome, but they give the analytical solutions to the two moments for any model that can be formulated in the pure additive form \@ref(eq:ETSADAMStateSpacePureAdditive). Having obtained both of them, we can construct prediction intervals, assuming, for example, that the error term follows Normal distribution (see Section \@ref(ADAMForecastingPI) for details):
\begin{equation}
    y_{t+h} \in \left( \text{E}(y_{t+h}|t) + z_{\frac{\alpha}{2}} \sqrt{\text{V}(y_{t+h}|t)}, \text{E}(y_{t+h}|t) + z_{1-\frac{\alpha}{2}} \sqrt{\text{V}(y_{t+h}|t)} \right),
  (\#eq:ETSADAMStateSpacePureAdditivePredictionInterval)
\end{equation}
where $z_{\frac{\alpha}{2}}$ is the quantile of standardised Normal distribution for the level $\frac{\alpha}{2}$. When it comes to other distributions (see Section \@ref(ADAMETSAdditiveDistributions)), in order to get the conditional $h$ steps ahead scale parameter, we can first calculate the variance use \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionMeanAndVariance) and then using the relation between the scale and the variance for the specific distribution to get the necessary value (this is discussed in Section \@ref(ADAMETSAdditiveDistributions)).


### Example with ETS(A,N,N)
For example, for the ETS(A,N,N) model, we get:
\begin{equation}
  \begin{aligned}
    \text{E}(y_{t+h}|t) = & \mathbf{w}_{1}^\prime \mathbf{F}_{1}^{h-1} \mathbf{v}_{t} \\
    \text{V}(y_{t+h}|t) = & \left(\mathbf{w}_{1}^\prime \sum_{j=1}^{h-1} \mathbf{F}_{1}^{j-1} \mathbf{g}_{1} \mathbf{g}^\prime_{1} (\mathbf{F}_{1}^\prime)^{j-1} \mathbf{w}_{1} + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSADAMStateSpaceANNRecursionMeanAndVarianceGeneral)
\end{equation}
or by substituting $\mathbf{F}=1$, $\mathbf{w}=1$, $\mathbf{g}=\alpha$ and $\mathbf{v}_t=l_t$:
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = & l_{t} \\
    \sigma^2_{h} = & \left((h-1) \alpha^2 + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSADAMStateSpaceANNRecursionMeanAndVariance)
\end{equation}
which is the same conditional expectation and variance as in the @Hyndman2008b monograph on page 81.


## Stability and forecastability conditions {#stabilityConditionAdditiveError}
Another important aspect of the pure additive model \@ref(eq:ETSADAMStateSpacePureAdditive) is the restriction on the smoothing parameters. This is related to the stability and forecastability conditions of the model, defined by @Hyndman2008b in Chapter 10. The **stability** implies that the weights for observations in a dynamic model decay over time (see example with SES in Section \@ref(whyExponential)). This guarantees that the newer observations will have higher weights than the older ones, thus the impact of the older information on forecasts slowly disappears with the increase of the sample size. The **forecastability** does not guarantee that the weights decay, but it guarantees that the initial value of the state vector will have a constant impact on forecasts, i.e. it will not increase in weight with the increase of the sample size. An example of the non-stable, but forecastable model is ETS(A,N,N) with $\alpha=0$. In this case, it reverts to the global level model (Section \@ref(GlobalMean)), where the initial value impacts the final forecast in the same way as it does for the first observation.

In order to derive both conditions for the ADAM, we need to use a reduced form of the model by inserting the measurement equation in the transition equation via $\epsilon_t= {y}_{t} -\mathbf{w}^\prime \mathbf{v}_{t-\boldsymbol{l}}$:
\begin{equation}
  \begin{aligned}
    \mathbf{v}_{t} = &\mathbf{F} \mathbf{v}_{t-\boldsymbol{l}} + \mathbf{g} \left({y}_{t} -\mathbf{w}^\prime \mathbf{v}_{t-\boldsymbol{l}} \right)\\
    = & \left(\mathbf{F} -\mathbf{g}\mathbf{w}^\prime \right) \mathbf{v}_{t-\boldsymbol{l}} + \mathbf{g} {y}_{t} \\
    = & \mathbf{D} \mathbf{v}_{t-\boldsymbol{l}} + \mathbf{g} {y}_{t} 
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureAdditiveBackRecursion01)
\end{equation}
The matrix $\mathbf{D}=\mathbf{F} -\mathbf{g}\mathbf{w}^\prime$ is called the discount matrix and it shows how the weights diminish over time. It is the main part of the model that determines, whether the model will be stable/forecastable or not.


### Example with ETS(A,N,N)
In order to better understand what we plan to discuss in this section, consider an example of an ETS(A,N,N) model, for which $\mathbf{F}=1$, $\mathbf{w}=1$, $\mathbf{g}=\alpha$, $\mathbf{v}_t=l_t$ and $\boldsymbol{l}=1$. Inserting these values in \@ref(eq:ETSADAMStateSpacePureAdditiveBackRecursion01), we get:
\begin{equation}
  \begin{aligned}
    l_{t} = & \left(1 -\alpha \right) {l}_{t-1} + \alpha {y}_{t},
  \end{aligned}.
  (\#eq:ETSADAMStateSpaceANNBackRecursion01)
\end{equation}
which corresponds to the formula of SES from Section \@ref(SES). The discount matrix, in this case, is $\mathbf{D}=1-\alpha$. If we now substitute the values for the level on the right-hand side of the equation \@ref(eq:ETSADAMStateSpaceANNBackRecursion01) by the previous values of the level, we will obtain the recursion that we have already discussed in Section \@ref(whyExponential), but now in terms of the "true" components and parameters:
\begin{equation}
  \begin{aligned}
    l_{t} = & {\alpha} \sum_{j=0}^{t-1} (1 -{\alpha})^j {y}_{t-j} + (1 -{\alpha})^t l_0
  \end{aligned}.
  (\#eq:ETSADAMStateSpaceANNBackRecursion02)
\end{equation}
The *stability* condition for ETS(A,N,N) is that the discount scalar $1-\alpha$ is less than one by absolute value. This way, the weights will decay in time because of the exponentiation in \@ref(eq:ETSADAMStateSpaceANNBackRecursion02) to the power of $j$. This condition is satisfied when $\alpha \in(0, 2)$, which is the admissible bound discussed in Section \@ref(ETSParametersBounds).

As for the *forecastability* condition, in this case it implies that $\lim\limits_{t\rightarrow\infty}(1 -{\alpha})^t l_0 = \text{const}$, which means that the effect of the initial state on future values stays the same. This is achievable, for example, when $\alpha=0$, but is violated, when $\alpha<0$ or $\alpha\geq 2$. So, the bounds for the smoothing parameters in the ETS(A,N,N) model, guaranteeing the forecastability of the model (i.e. making it useful), are:
\begin{equation}
  \alpha \in [0, 2) .
  (\#eq:ETSADAMStateSpaceANNBounds)
\end{equation}

### Coming back to the general case
In the general case, the logic is the same as with ETS(A,N,N), but it implies the usage of linear algebra. Due to our lagged formulation, the recursion becomes complicated, because the discount matrix $\mathbf{D}$ needs to be split into submatrices similar to how we did it in Section \@ref(adamETSPureAdditiveRecursive):
\begin{equation}
  \begin{aligned}
    \mathbf{v}_{t} = & \mathbf{D}_{m_1}^{\lceil\frac{t}{m_1}\rceil} \mathbf{v}_{0} + \sum_{j=0}^{\lceil\frac{t}{m_1}\rceil-1} \mathbf{D}_{m_1}^{j} y_{t -j m_1} + \\
    & \mathbf{D}_{m_2}^{\lceil\frac{t}{m_2}\rceil} \mathbf{v}_{0} + \sum_{j=0}^{\lceil\frac{t}{m_2}\rceil-1} \mathbf{D}_{m_2}^j y_{t -j m_2} + \\
    & \dots + \\
    & \mathbf{D}_{m_d}^{\lceil\frac{t}{m_d}\rceil} \mathbf{v}_{0} + \sum_{j=0}^{\lceil\frac{t}{m_d}\rceil-1} \mathbf{D}_{m_d}^j y_{t -j m_d}
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion05)
\end{equation}
where $\mathbf{D}_{m_i} = \mathbf{F}_{m_i} -\mathbf{g}_{m_i} \mathbf{w}_{m_i}^\prime$ is the discount matrix for each lag of the model. The stability condition in this case is that the absolute values of all the non-zero eigenvalues of the discount matrices $\mathbf{D}_{m_i}$ are lower than one. This condition can be checked at the model construction stage, ensuring that the selected parameters guarantee the stability of the model. As for the forecastability, as discussed earlier, it will hold if the initial value of the state vector does not have an increasing impact on the last observed value. This is obtained by inserting \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion05) in the measurement equation of the pure additive model:
\begin{equation}
  \begin{aligned}
  y_t = & \mathbf{w}_{m_1}^\prime \mathbf{D}_{m_1}^{\lceil\frac{t-1}{m_1}\rceil} \mathbf{v}_{0} + \mathbf{w}_{m_1}^\prime \sum_{j=0}^{\lceil\frac{t-1}{m_1}\rceil-1} \mathbf{D}_{m_1}^{j} y_{t-1 -j m_1} + \\
        & \mathbf{w}_{m_2}^\prime \mathbf{D}_{m_2}^{\lceil\frac{t-1}{m_2}\rceil} \mathbf{v}_{0} + \mathbf{w}_{m_2}^\prime \sum_{j=0}^{\lceil\frac{t-1}{m_2}\rceil-1} \mathbf{D}_{m_2}^j y_{t-1 -j m_2} + \\
        & \dots + \\
        & \mathbf{w}_{m_d}^\prime \mathbf{D}_{m_d}^{\lceil\frac{t-1}{m_d}\rceil} \mathbf{v}_{0} + \mathbf{w}_{m_d}^\prime \sum_{j=0}^{\lceil\frac{t-1}{m_d}\rceil-1} \mathbf{D}_{m_d}^j y_{t-1 -j m_d} + \epsilon_t
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionForecastability)
\end{equation}
In our case the forecastability condition implies that:
\begin{equation}
  \lim\limits_{t\rightarrow\infty}\left(\mathbf{w}_{m_i}^\prime\mathbf{D}_{m_i}^{\lceil\frac{t-1}{m_i}\rceil} \mathbf{v}_{0}\right) = \text{const for all } i=1, \dots, d.
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionForecastabilityFinal)
\end{equation}
These conditions are general but applicable to any model formulated in the pure additive form \@ref(eq:ETSADAMStateSpacePureAdditive).


## Distributional assumptions in pure additive ADAM {#ADAMETSAdditiveDistributions}
While the conventional ETS assumes that the error term follows Normal distribution, ADAM ETS proposes some flexibility, implementing the following options for the error term distribution in the additive error models:

1. Normal: $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$, meaning that $y_t = \mu_{y,t} + \epsilon_t \sim \mathcal{N}(\mu_{y,t}, \sigma^2)$;
2. Laplace: $\epsilon_t \sim \mathcal{L}(0, s)$, so that $y_t = \mu_{y,t} + \epsilon_t \sim \mathcal{L}(\mu_{y,t}, s)$;
3. Generalised Normal: $\epsilon_t \sim \mathcal{GN}(0, s, \beta)$, leading to $y_t = \mu_{y,t} + \epsilon_t \sim \mathcal{GN}(\mu_{y,t}, s, \beta)$;
4. S (special case of $\mathcal{GN}$ with $\beta=0.5$): $\epsilon_t \sim \mathcal{S}(0, s)$, implying that $y_t = \mu_{y,t} + \epsilon_t \sim \mathcal{S}(\mu_{y,t}, s)$,
<!-- 5. Logistic: $\epsilon_t \sim \mathcal{Logis}(0, s)$; -->
<!-- 6. Student's t: $\epsilon_t \sim \mathcal{t}(\nu)$; -->
<!-- 5. Asymmetric Laplace: $\epsilon_t \sim \mathcal{ALaplace}(0, s, \alpha)$ -->

where $\mu_{y,t} = \mathbf{w}^\prime \mathbf{v}_{t-\boldsymbol{l}}$ is the one step ahead point forecast.

The conditional moments and stability/forecastability conditions do not change for the model with these assumptions. The main element that changes is the scale and the width of prediction intervals. Given that the scales of these distributions are linearly related to the variance, one can calculate the conditional variance as discussed in Section \@ref(pureAdditiveExpectationAndVariance) and then use it in order to obtain the respective scales. Having the scales, it becomes straightforward to calculate the needed quantiles for the prediction intervals. Here are the formulae for the scales of distributions mentioned above:

1. Normal: scale is $\sigma^2_h$;
2. Laplace: $s_h = \sigma_h \sqrt{\frac{1}{2}}$;
4. Generalised Normal: $s_h = \sigma_h \sqrt{\frac{\Gamma(1/\beta)}{\Gamma(3/\beta)}}$;
3. S: $s_h = \sqrt{\sigma_h}\sqrt[4]{\frac{1}{120}}$.

The estimation of pure additive ADAM can be done via the maximisation of the likelihood of the assumed distribution (see Section \@ref(ADAMETSEstimationLikelihood)), which in some cases coincides with the popular loss functions (e.g. Normal and MSE, or Laplace and MAE).

In addition, the following more exotic options for the additive error models are available in ADAM:

1. Log-Normal: $\left(1+\frac{\epsilon_t}{\mu_{y,t}} \right) \sim \text{log}\mathcal{N}\left(-\frac{\sigma^2}{2}, \sigma^2\right)$, implying that $y_t = \mu_{y,t} \left(1+\frac{\epsilon_t}{\mu_{y,t}} \right) = y_t = \mu_{y,t} + \epsilon_t \sim \text{log}\mathcal{N}\left(\log\mu_{y,t} -\frac{\sigma^2}{2}, \sigma^2\right)$.
Here, $\sigma^2$ is the variance of the error term in logarithms and the $-\frac{\sigma^2}{2}$ appears due to the restriction $\text{E}(\epsilon_t)=0$;
2. Inverse Gaussian: $\left(1+\frac{\epsilon_t}{\mu_{y,t}} \right) \sim \mathcal{IG}(1, \sigma^2)$ with $y_t=\mu_{y,t} \left(1+\frac{\epsilon_t}{\mu_{y,t}} \right) \sim \mathcal{IG}\left(\mu_{y,t}, \frac{\sigma^2}{\mu_{y,t}}\right)$;
3. Gamma: $\left(1+\frac{\epsilon_t}{\mu_{y,t}} \right) \sim \mathcal{\Gamma}(\sigma^{-2}, \sigma^2)$, so that $y_t = \mu_{y,t} \left(1+\frac{\epsilon_t}{\mu_{y,t}} \right) \sim \mathcal{\Gamma}(\sigma^{-2}, \sigma^2 \mu_{y,t})$.

The possibility of application of these distributions arises from the reformulation of the original pure additive model \@ref(eq:ETSADAMStateSpacePureAdditive) into:
\begin{equation}
  \begin{aligned}
    {y}_{t} = &\mathbf{w}^\prime \mathbf{v}_{t-\boldsymbol{l}}\left(1 + \frac{\epsilon_t}{\mathbf{w}^\prime \mathbf{v}_{t-\boldsymbol{l}}}\right) \\
    \mathbf{v}_{t} = &\mathbf{F} \mathbf{v}_{t-\boldsymbol{l}} + \mathbf{g} \epsilon_t
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureAdditiveReformulated)
\end{equation}
The connection between the two formulations becomes apparent when opening the brackets in the measurement equation of \@ref(eq:ETSADAMStateSpacePureAdditiveReformulated). Note that in this case, the model assumes that the data is strictly positive, and while it might be possible to fit the model on the data with negative values, the calculation of the scale and the likelihood might become impossible. Using alternative losses (e.g. MSE) is a potential solution in this case.


## Examples of application {#ADAMETSPureAdditiveExamples}
### Non-seasonal data
To see how the pure additive ADAM ETS works, we will try it out using the `adam()` function from the `smooth` package for R on Box-Jenkins sales data. We start with plotting the data:

```{r BJsalesDataPlot, fig.cap="Box-Jenkins sales data."}
plot(BJsales)
```

The series in Figure \@ref(fig:BJsalesDataPlot) seem to exhibit a trend, so we will apply an ETS(A,A,N) model:
```{r}
adamETSBJ <- adam(BJsales, "AAN")
adamETSBJ
```

The model's output summarises which specific model was constructed, what distribution was assumed, how the model was estimated, and also provides the values of smoothing parameters. It also reports the sample size, the number of parameters, degrees of freedom, and produces information criteria [see Section 16.4 of @SvetunkovSBA]. We can compare this model with the ETS(A,N,N) to see which of them performs better in terms of information criteria (e.g. in terms of AICc):
```{r}
adam(BJsales, "ANN")
```
In this situation the AICc for ETS(A,N,N) is higher than for ETS(A,A,N), so we should use the latter for forecasting purposes. We can produce point forecasts and a prediction interval (in this example we will construct 90% and 95% ones) and plot them (Figure \@ref(fig:BJsalesAANForecast)):

```{r BJsalesAANForecast, fig.cap="Forecast for Box-Jenkins sales data from ETS(A,A,N) model."}
forecast(adamETSBJ, h=10,
         interval="prediction", level=c(0.9,0.95)) |>
    plot(main="")
```

Notice that the bounds in Figure \@ref(fig:BJsalesAANForecast) are expanding fast, demonstrating that the components of the model exhibit high uncertainty, which is then reflected in the holdout sample. This is partially due to the high values of the smoothing parameters of ETS(A,A,N), with $\alpha=1$. While we typically want to have lower smoothing parameters, in this specific case this might mean that the maximum likelihood is achieved in the *admissible* bounds (i.e. data exhibits even higher variability than we expected with the usual bounds). We can try it out and see what happens:

```{r}
adamETSBJ <- adam(BJsales, "AAN", bounds="admissible")
adamETSBJ
```

Both smoothing parameters are now higher, which implies that the uncertainty about the future values of states is higher as well, which is then reflected in the slightly wider prediction interval (Figure \@ref(fig:BJsalesAANForecastAdmissible)):

```{r BJsalesAANForecastAdmissible, fig.cap="Forecast for Box-Jenkins sales data from an ETS(A,A,N) model with admissible bounds."}
forecast(adamETSBJ, h=10,
         interval="prediction", level=c(0.9,0.95)) |>
    plot(main="")
```

Although the values of smoothing parameters are higher than one, the model is still stable. In order to see that, we can calculate the discount matrix $\mathbf{D}$ using the objects returned by the function to reflect the formula $\mathbf{D}=\mathbf{F} -\mathbf{g}\mathbf{w}^\prime$:
```{r}
(adamETSBJ$transition - adamETSBJ$persistence %*%
    adamETSBJ$measurement[nobs(adamETSBJ),,drop=FALSE]) |>
    eigen(only.values=TRUE)
```

Notice that the absolute values of both eigenvalues in the matrix are less than one, which means that the newer observations have higher weights than the older ones and that the absolute values of weights decrease over time, making the model stable.

If we want to test ADAM ETS with another distribution, it can be done using the respective parameter in the function (here we use Generalised Normal, estimating the shape together with the other parameters):
```{r}
adamETSBJ <- adam(BJsales, "AAN", distribution="dgnorm")
print(adamETSBJ, digits=3)
```

Similar to the previous cases, we can plot the forecasts from the model:

```{r BJsalesAANForecastGN, fig.cap="Forecast for Box-Jenkins sales data from an ETS(A,A,N) model with Generalised Normal distribution."}
forecast(adamETSBJ, h=10, interval="prediction") |>
    plot(main="")
```

The prediction interval in this case is slightly wider than in the previous one, because the Generalised Normal distribution with $\beta=$ `r round(adamETSBJ$other$shape,2)` has fatter tails than the Normal one (Figure \@ref(fig:BJsalesAANForecastGN)).


### Seasonal data {#ADAMETSPureAdditiveExamplesETSAAA}
```{r AirPassengersPlot, fig.cap="Air passengers data from Box-Jenkins textbook.", echo=FALSE}
plot(AirPassengers)
```

Now we will check what happens in the case of seasonal data. We use `AirPassengers` data, plotted in Figure \@ref(fig:AirPassengersPlot), which apparently has multiplicative seasonality. But for demonstration purposes, we will see what happens when we use the wrong model with additive seasonality. We will withhold the last 12 observations to look closer at the performance of the ETS(A,A,A) model in this case:

```{r}
adamETSAir <- adam(AirPassengers, "AAA", lags=12,
                   h=12, holdout=TRUE)
```

::: remark
In this specific case, the `lags` parameter is not necessary because the function will automatically get the frequency from the `ts` object `AirPassengers`. If we were to provide a vector of values instead of the `ts` object, we would need to specify the correct lag. Note that `1` (lag for level and trend) is unnecessary; the function will always use it anyway.
:::

::: remark
In some cases, the optimiser might converge to the local minimum, so if you find the results unsatisfactory, it might make sense to reestimate the model, tuning the parameters of the optimiser (see Section \@ref(ADAMInitialisation) for details). Here is an example (we increase the number of iterations in the optimisation and set new starting values for the smoothing parameters):
:::

```{r}
adamETSAir$B[1:3] <- c(0.2,0.1,0.3)
adamETSAir <- adam(AirPassengers, "AAA", lags=12,
                   h=12, holdout=TRUE,
                   B=adamETSAir$B, maxeval=1000)
adamETSAir
```

Notice that because we fit the seasonal additive model to the data with multiplicative seasonality, the smoothing parameter $\gamma$ has become large -- the seasonal component needs to be updated frequently to keep up with the changing seasonal profile. In addition, because we use the `holdout` parameter, the function also reports the error measures for the point forecasts on that part of the data. This can be useful when comparing the performance of several models on a time series. And here is how the forecast from ETS(A,A,A) looks on this data:

```{r AirPassengersAAAForecast, fig.cap="Forecast for air passengers data using an ETS(A,A,A) model.", echo=FALSE}
forecast(adamETSAir, h=12, interval="prediction") |>
    plot()
```

Figure \@ref(fig:AirPassengersAAAForecast) demonstrates that while the fit to the data is far from perfect, due to a pure coincidence, the point forecast from this model is decent.

In order to see how the ADAM ETS decomposes the data into components, we can plot it via the `plot()` method with `which=12` parameter:

```{r AirPassengersAAADecomposition, fig.cap="Decomposition of air passengers data using an ETS(A,A,A) model.", echo=TRUE}
plot(adamETSAir, which=12)
```

We can see on the graph in Figure \@ref(fig:AirPassengersAAADecomposition) that the residuals still contain some seasonality, so there is room for improvement. This probably happened because the data exhibits multiplicative seasonality rather than the additive one. For now, we do not aim to fix this issue.
