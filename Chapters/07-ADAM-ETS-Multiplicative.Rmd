# Pure multiplicative ADAM ETS {#ADAMETSPureMultiplicative}

The pure multiplicative ETS implemented in ADAM framework can be formulated using logarithms, similar to how the pure additive ADAM ETS is formulated in \@ref(eq:ETSADAMStateSpace):
\begin{equation}
  \begin{aligned}
		\log y_t = & \mathbf{w}' \log(\mathbf{v}_{t-\mathbf{l}}) + \log(1 + \epsilon_{t}) \\
		\log \mathbf{v}_{t} = & \mathbf{F} \log \mathbf{v}_{t-\mathbf{l}} + \log(\mathbf{1}_k + \mathbf{g} \epsilon_t)
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureMultiplicative)
\end{equation}
where $\mathbf{1}_k$ is the vector of ones, containing $k$ elements (number of components in the model), $\log$ is the natural logarithm, applied element-wise to the vectors and all the other values have been discussed in the previous sections. An example of a pure multiplicative model is ETS(M,M,M), for which we have the following values:
\begin{equation}
  \begin{aligned}
    \mathbf{w} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}, & \mathbf{F} = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}, & \mathbf{g} = \begin{pmatrix} \alpha \\ \beta \\ \gamma \end{pmatrix}, \\
    \mathbf{v}_{t} = \begin{pmatrix} l_t \\ b_t \\ s_t \end{pmatrix}, & \mathbf{l} = \begin{pmatrix} 1 \\ 1 \\ m \end{pmatrix}, & \mathbf{1}_k = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}
  \end{aligned}.
  (\#eq:ETSADAMMMMMatrices)
\end{equation}
By inserting these values in the equation \@ref(eq:ETSADAMStateSpacePureMultiplicative), we obtain model in logarithms:
\begin{equation}
	\begin{aligned}
		\log y_t = & \log l_{t-1} + \log b_{t-1} + \log s_{t-m} + \log \left(1 + \epsilon_{t} \right) \\
		\log l_{t} = & \log l_{t-1} + \log b_{t-1} + \log( 1  + \alpha \epsilon_{t}) \\ 
		\log b_{t} = & \log b_{t-1} + \log( 1  + \beta \epsilon_{t}) \\
		\log s_{t} = & \log s_{t-m} + \log( 1  + \gamma \epsilon_{t}) \\
	\end{aligned} ,
	(\#eq:ETSADAMMMMLogs)
\end{equation}
which after exponentiation becomes equal to the one, discussed in [ETS Taxonomy](#ETSTaxonomyMaths) section:
\begin{equation}
  \begin{aligned}
    y_{t} = & l_{t-1} b_{t-1} s_{t-m} (1 + \epsilon_t) \\
    l_t = & l_{t-1} b_{t-1} (1 + \alpha \epsilon_t) \\
    b_t = & b_{t-1} (1 + \beta \epsilon_t) \\
    s_t = & s_{t-m} (1 + \gamma \epsilon_t) 
  \end{aligned}.
  (\#eq:ETSADAMMMM)
\end{equation}
An interesting observation is that the model \@ref(eq:ETSADAMMMMLogs) will produce values close to the model ETS(A,A,A) applied to the data in logarithms, when the values of smoothing parameters are close to zero. This becomes apparent, when recalling the limit:
\begin{equation}
  \lim\limits_{x \to 0}\log(1+x) = x .
  (\#eq:limitOf1x)
\end{equation}
Based on that, the model will become close to the following one in cases of small values of smoothing parameters:
\begin{equation}
	\begin{aligned}
		\log y_t = & \log l_{t-1} + \log b_{t-1} + \log s_{t-m} + \epsilon_{t} \\
		\log l_{t} = & \log l_{t-1} + \log b_{t-1} + \alpha \epsilon_{t} \\ 
		\log b_{t} = & \log b_{t-1} + \beta \epsilon_{t} \\
		\log s_{t} = & \log s_{t-m} + \gamma \epsilon_{t} \\
	\end{aligned} ,
	(\#eq:ETSADAMMMMLogsEquivalent)
\end{equation}
which is the ETS(A,A,A) applied to the data in logarithms. In many cases the smoothing parameters will be small enough for this limit to hold, so the two models will produce similar forecasts. As a result, model \@ref(eq:ETSADAMMMMLogsEquivalent) can be used instead of \@ref(eq:ETSADAMMMMLogs) in these cases in order to get conditional moments and quantiles of distribution.


## Recursive relation {#adamETSPuremultiplicativeRecursive}
Similarly to [how it was done for the pure additive model](#adamETSPureAdditiveRecursive), we can show what the recursive relation will look like for the pure multiplicative model (the logic here is the same, the main difference is in working with logarithms instead of the original values):
\begin{equation}
  \begin{aligned}
    \log y_{t+h} = & \mathbf{w}_{m_1}' \mathbf{F}_{m_1}^{\lceil\frac{h}{m_1}\rceil-1} \log \mathbf{v}_{t} + \mathbf{w}_{m_1}' \sum_{j=1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{F}_{m_1}^{j-1} \log \left(\mathbf{1}_k + \mathbf{g}_{m_1} \epsilon_{t+m_1\lceil\frac{h}{m_1}\rceil-j}\right) + \\
    & \mathbf{w}_{m_2}' \mathbf{F}_{m_2}^{\lceil\frac{h}{m_2}\rceil-1} \log \mathbf{v}_{t} + \mathbf{w}_{m_2}' \sum_{j=1}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{F}_{m_2}^{j-1} \log \left(\mathbf{1}_k + \mathbf{g}_{m_2} \epsilon_{t+m_2\lceil\frac{h}{m_2}\rceil-j}\right) + \\
    & \dots \\
    & \mathbf{w}_{m_d}' \mathbf{F}_{m_d}^{\lceil\frac{h}{m_d}\rceil-1} \log \mathbf{v}_{t} + \mathbf{w}_{m_d}' \sum_{j=1}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{F}_{m_d}^{j-1} \log \left(\mathbf{1}_k + \mathbf{g}_{m_d} \epsilon_{t+m_d\lceil\frac{h}{m_d}\rceil-j}\right) + \\
    & \log \left(1 + \epsilon_{t+h}\right)
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureMultiplicativeRecursion)
\end{equation}
In order to see how this recursion works, we can take the example of ETS(M,N,N), for which $m_1=1$ and all the other frequencies are equal to zero:
\begin{equation}
    y_{t+h} = \exp\left(\mathbf{w}_{1}' \mathbf{F}_{1}^{h-1} \log\mathbf{v}_{t} + \mathbf{w}_{1}' \sum_{j=1}^{h-1} \mathbf{F}_{1}^{j-1} \log \left(\mathbf{1}_k + \mathbf{g}_{1} \epsilon_{t+h-j}\right) +\log \left(1 + \epsilon_{t+h}\right)\right) ,
  (\#eq:ETSMNNADAMStateSpacePureMultiplicativeRecursion01)
\end{equation}
or after inserting $\mathbf{w}_{1}=1$, $\mathbf{F}_{1}=1$, $\mathbf{v}_{t}=l_t$, $\mathbf{g}_{1}=\alpha$ and $\mathbf{1}_k=1$:
\begin{equation}
    y_{t+h} = l_t \prod_{j=1}^{h-1} \left(1 + \alpha \epsilon_{t+h-j}\right) \left(1 + \epsilon_{t+h}\right) .
  (\#eq:ETSMNNADAMStateSpacePureMultiplicativeRecursion02)
\end{equation}

This recursion is useful in order to understand how the states evolve over time, but in general it cannot be used for the calculation of moments, as the one for [the pure additive ADAM ETS](#adamETSPureAdditiveRecursive).


## The problem with moments in pure multiplicative ETS {#pureMultiplicativeExpectationAndVariance}
The recursion \@ref(eq:ETSADAMStateSpacePureMultiplicativeRecursion) obtained in the previous subsection shows how the logarithms of states are influenced by the previous values. While it is possible to calculate the expectation of the logarithm of the variable $y_{t+h}$, in general this does not allow deriving the expectation of the variable in the original scale. This is because of the convolution of terms $\log(\mathbf{1}_k + \mathbf{g} \epsilon_{t+j})$ for different $j$. In order to better understand this issue, we consider this element for the ETS(M,N,N) model:
\begin{equation}
    \log(1+\alpha\epsilon_t) = \log(1-\alpha + \alpha(1+\epsilon_t)).
  (\#eq:ETSMNNADAMPersistenceIssue)
\end{equation}
Whatever we assume about the distribution of the variable $(1+\epsilon_t)$, the distribution of \@ref(eq:ETSMNNADAMPersistenceIssue) will be more complicated than needed. For example, if we assume that $(1+\epsilon_t)\sim\text{log}\mathcal{N}(0,\sigma^2)$, then the distribution of \@ref(eq:ETSMNNADAMPersistenceIssue) is something like exp three-parameter log normal distribution [@Sangal1970]. The convolution of \@ref(eq:ETSMNNADAMPersistenceIssue) for different $t$ does not follow a known distribution, so it is not possible to calculate the conditional expectation and variance based on \@ref(eq:ETSADAMStateSpacePureMultiplicativeRecursion). Similar issues arrise if we assume any other distribution. The problem is worsened in case of multiplicative trend and / or multiplicative seasonality models, because then the recursion \@ref(eq:ETSADAMStateSpacePureMultiplicativeRecursion) contains several errors on the same observation (e.g. $\log(1+\alpha\epsilon_t)$ and $\log(1+\beta\epsilon_t)$).

The only way to derive the conditional expectation and variance for the pure multiplicative models is to use the formalue in the [ETS Taxonomy](#ETSTaxonomyMaths) and manually derive the values in the original scale. This works well only for the ETS(M,N,N) model, for which it is possible to take conditional expectation and variance of the recursion \@ref(eq:ETSMNNADAMStateSpacePureMultiplicativeRecursion02) in order to obtain:
\begin{equation}
    \begin{aligned}
	    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & l_{t} \\
	    \text{V}(y_{t+h}|t) = & l_{t}^2 \left(  \left(1+ \alpha^2 \sigma^2 \right)^{h-1} (1 + \sigma^2) - 1 \right),
	\end{aligned}
	(\#eq:ETSMNNADAMConditionalValues)
\end{equation}
where $\sigma^2$ is the variance of the error term. For the other models, the conditional moments do not have a general closed forms because of the product of $\log(1+\alpha\epsilon_t)$, $\log(1+\beta\epsilon_t)$ and $\log(1+\gamma\epsilon_t)$. It is still possible to derive the moments for special cases of $h$, but this is a tedious process. In order to see that, we demonstrate here how the recursion looks for ETS(M,Md,M) model:
\begin{equation}
	\begin{aligned}
	    & y_{t+h} = l_{t+h-1} b_{t+h-1}^\phi s_{t+h-m} \left(1 + \epsilon_{t+h} \right) = \\
	    & l_{t} b_{t}^{\sum_{j=1}^h{\phi^j}} s_{t+h-m\lceil\frac{h}{m}\rceil} \prod_{j=1}^{h-1} \left( (1 + \alpha \epsilon_{t+j}) \prod_{i=1}^{j} (1 + \beta \epsilon_{t+i})^{\phi^{j-i}} \right) \prod_{j=1}^{\lceil\frac{h}{m}\rceil} \left(1 + \gamma \epsilon_{t+j}\right) \left(1 + \epsilon_{t+h} \right) .
	\end{aligned}
	(\#eq:ETSMMdMADAMRecursion)
\end{equation}
In general the conditional expectation of the recursion \@ref(eq:ETSMMdMADAMRecursion) does not have a simple form, because of the difficulties in calculating the expectation of $(1 + \alpha \epsilon_{t+j})(1 + \beta \epsilon_{t+i})^{\phi^{j-i}}(1 + \gamma \epsilon_{t+j})$. In a simple example of $h=2$ and $m>h$ the conditional expectation can be simplified to:
\begin{equation}
	\mu_{y,t+2} = l_{t} b_{t}^{\phi+\phi^2} \left(1 + \alpha \beta \sigma^2 \right),
	(\#eq:ETSMMdMADAMRecursionHorizon2)
\end{equation}
introducing the second moment, the variance of the error term $\sigma^2$. The case of $h=3$ implies the appearance of the third moment, the $h=4$ - the fourth etc. This is the reason, why there are no closed forms for the conditional moments for the pure multiplicative models with trend and / or seasonality. In some special cases, when smoothing parameters and the variance of error term are low, it is possible to use approximate formulae proposed by @Hyndman2008b, and in a special case, when all smoothing parameters are equal to zero or when $h=1$, it is also possible to use the point forecast formulae from the [ETS Taxonomy](#ETSTaxonomyMaths). But in general, the best thing that can be done in this case is the simulation of possible paths from the respective ETS model (using the formulae from the taxonomy) and then calculation of mean and variance based on them. In general, it can be shown that:
\begin{equation}
    \check{y}_{t+h} \leq \hat{y}_{t+h} \leq \mu_{y,t+h} ,
    (\#eq:ETSADAMpointValueInequality)
\end{equation}
where $\mu_{y,t+h}$ is the conditional h steps ahead expectation, $\check{y}_{t+h}$ is the conditional h steps ahead geometric expectation and $\hat{y}_{t+h}$ is the [point forecast](#ETSTaxonomyMaths) [@Svetunkov2020ETS].


## Smoothing parameters bounds {#stabilityConditionMultiplicativeError}
Similar to the pure additive ADAM ETS, it is possible to have different types of bounds, including the classical, the usual and the admissible ones. However, in case of pure multiplicative models, the classical and the usual restrictions become more reasonable from the point of view of the model itself, while the derivation of admissible bounds becomes a challenging task. In order to see the former, consider the ETS(M,N,N) model, for which the level is updated using the following relation:
\begin{equation}
    l_t = l_{t-1} (1 + \alpha\epsilon_t) = l_{t-1} (1-\alpha + \alpha(1+\epsilon_t)).
  (\#eq:ETSMNNADAMLevelUpdate)
\end{equation}
As discussed previously, the main benefit of pure multiplicative models is in modelling positive data. So, it is reasonable to assume that $(1 + \epsilon_t)>0$, which then implies that the actual values will always be positive, and that each component of the model should also be positive. This means that $\alpha(1 + \epsilon_t)>0$, which implies that $(1-\alpha + \alpha(1+\epsilon_t))>1-\alpha$ or equivalently based on \@ref(eq:ETSMNNADAMPersistenceIssue) $(1 + \alpha\epsilon_t)>1-\alpha$ should always hold. Now in order for the model to make sense, the condition $(1 + \alpha\epsilon_t)>0$ should hold as well, ensuring that the level is always positive. This leads to the following set of inequalities:
\begin{equation}
	\begin{aligned}
	(1 + \alpha\epsilon_t)> &0 \\
	(1 + \alpha\epsilon_t)> &1-\alpha
	\end{aligned} .
	(\#eq:ETSMNNADAMInequalities01)
\end{equation}
This can only be satisfied in the case of $1-\alpha\geq0$ or $\alpha\leq1$. Another bounds can be obtained by analysing the equation \@ref(eq:ETSMNNADAMLevelUpdate) and using the restriction for positivity of its elements: $(1-\alpha + \alpha(1+\epsilon_t))>0$ which can only be achieved, when $(1+\epsilon_t)>\frac{1-\alpha}{\alpha}$, leading to another two inequalities:
\begin{equation}
	\begin{aligned}
	(1 + \epsilon_t)> &0 \\
	(1 + \epsilon_t)> &\frac{1-\alpha}{\alpha}
	\end{aligned} ,
	(\#eq:ETSMNNADAMInequalities02)
\end{equation}
which can be satisfied only when $\alpha\geq0$, because, as we have already shown, the condition $1-\alpha\geq0$ should hold. So, in general the bounds $[0, 1]$ guarantee that the model ETS(M,N,N) will produce positive values only. The two special cases $\alpha=0$ and $\alpha=1$ make sense, because the level in \@ref(eq:ETSMNNADAMLevelUpdate) will be positive in this case, implying that for the former the model becomes equivalent to the global level, while for the latter the model is equivalent to Random Walk. Using similar logic, it can be shown that the **classical restriction** $\alpha, \beta, \gamma \in [0, 1]$ guarantees that the model will always produce positive values.

The more restrictive condition of the **usual bounds**, discussed in [Parameters Bounds](#ETSParametersBounds) section makes sense as well, although it might be more restrictive than needed, but it has a different idea: guaranteeing that the model exhibits averaging properties.

Finally, the **admissible bounds** might still make sense for the pure multiplicative models, but the condition for parameters bounds becomes more complicated and implies that the distribution of the error term becomes trimmed from below in order to satisfy \@ref(eq:ETSMNNADAMInequalities01) and \@ref(eq:ETSMNNADAMInequalities02). Very crudely, the conventional restriction from pure additive models can be used to get an approximation to the proper admissible bounds, given the limit \@ref(eq:limitOf1x), but this should be used with care, given the discussion above.


## Distributional assumptions in pure multiplicative ETS {#ADAMETSMultiplicativeDistributions}
The conventional assumption for the error term in ETS is that $\epsilon_t\sim\mathcal{N}(0,\sigma^2)$, which guarantees that the conditional expectation of the model will be equal to the point forecasts, when the trend and seasonal components are not multiplicative. In general, ETS works well in many cases with this assumption, mainly when the data is strictly positive and the level of series is high (e.g. thousands of units). However, when dealing with lower level data, this assumption might become unhelpful, because the models may start generating non-positive values, which contradicts the idea of pure multiplicative ETS models. @Akram2009 studied the ETS models with multiplicative error and suggested that applying ETS on data in logarithms is a better approach than just using ETS(M,Y,Y) models (here "Y" stands for non-additive components). However, this approach sidesteps the ETS taxonomy, creating a new group of models. An alternative (also discussed in @Akram2009) is to assume that the error term $1+\epsilon_t$ follows some distribution for positive data. The authors mentioned log Normal, truncated and Gamma distributions, but never explored them further.

@Svetunkov2020ETS discussed several options for the distribution of the $1+\epsilon_t$ in ETS and came to conclusion that the most suitable distribution in this case is the [Inverse Gaussian](#IGDistribution). Having said that, other distributions for positive data can be applied as well, but their usage might become complicated, because they need to meet condition $\text{E}(1+\epsilon_t)=1$ in order for the expectation to coincide with the point forecasts for models with non-multiplicative trend and seasonality. For example, if the error term follows log Normal distribution, then this restriction implies that the location of the distribution should be non-zero, based on \@ref(eq:logNMean): $1+\epsilon_t\sim\text{log}\mathcal{N}\left(-\frac{\sigma^2}{2},\sigma^2\right)$. Based on that the following distributions are supported by ADAM:

1. Inverse Gaussian: $\left(1+\epsilon_t \right) \sim \mathcal{IG}(1, s)$;
2. Gamma: $\left(1+\epsilon_t \right) \sim \mathcal{\Gamma}(s^{-1}, s)$;
3. Log Normal: $\left(1+\epsilon_t \right) \sim \text{log}\mathcal{N}\left(-\frac{\sigma^2}{2}, \sigma^2\right)$.

The MLE of $s$ in $\mathcal{IG}$ is straightforward and is:
\begin{equation}
	\hat{s} = \frac{1}{T} \sum_{t=1}^{T} \frac{e_{t}^2}{1+e_t} ,
	(\#eq:ETSMultiplicativeErrorMLESigmaIG)
\end{equation}
where $e_t$ is the estimate of the error term $\epsilon_t$. However, when it comes to the MLE of scale parameter for the log Normal distribution with the aforementioned restrictions, it is more complicated and is [@Svetunkov2020ETS]:
\begin{equation}
	\hat{\sigma}^2 = 2\left(1-\sqrt{ 1-\frac{1}{T} \sum_{t=1}^{T} \log^2(1+e_{t})}\right).
	(\#eq:ETSMultiplicativeErrorMLESigmaLogN)
\end{equation}
Finally, MLE of $s$ in $\mathcal{\Gamma}$ does not have a closed form. Luckily, as discussed in [section 2.5](#GammaDistribution), method of moments can be used to obtain its value:
\begin{equation}
	\hat{s} = \frac{1}{T} \sum_{t=1}^{T} e_{t}^2 .
	(\#eq:ETSMultiplicativeErrorMLESigmaGamma)
\end{equation}

Even if we assume that we deal with strictly positive high level data and that $\epsilon_t$ can be non-positive, it is not necessary to limit the distribution with Normal only. The following distributions can be applied as well:

1. Normal: $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$, implying that $y_t = \mu_t (1+\epsilon_t) \sim \mathcal{N}(\mu_t, \mu_t^2 \sigma^2)$;
2. Laplace: $\epsilon_t \sim \mathcal{Laplace}(0, s)$, meaning that $y_t = \mu_t (1+\epsilon_t) \sim \mathcal{Laplace}(\mu_t, \mu_t s)$;
3. S: $\epsilon_t \sim \mathcal{S}(0, s)$, so that $y_t = \mu_t (1+\epsilon_t) \sim \mathcal{S}(\mu_t, \sqrt{\mu_t} s)$;
4. Generalised Normal: $\epsilon_t \sim \mathcal{GN}(0, s, \beta)$ and $y_t = \mu_t (1+\epsilon_t) \sim \mathcal{GN}(\mu_t, \mu_t^\beta s)$;
<!-- 5. Logistic: $\epsilon_t \sim \mathcal{Logis}(0, s)$; -->
<!-- 6. Student's t: $\epsilon_t \sim \mathcal{t}(\nu)$; -->
<!-- 5. Asymmetric Laplace: $\epsilon_t \sim \mathcal{ALaplace}(0, s, \alpha)$ with $y_t = \mu_t (1+\epsilon_t) \sim \mathcal{ALaplace}(\mu_t, \mu_t s, \alpha)$. -->

Note that the MLE of scale parameters for these distributions will be calculated differently than in the case of pure additive models. For example, for the normal distribution it is:
\begin{equation}
	\hat{\sigma}^2 = \frac{1}{T}\sum_{t=1}^T \frac{y_t-\hat{\mu}_t}{\hat{\mu}_t} ,
	(\#eq:ETSMultiplicativeErrorMLESigmaNormal)
\end{equation}
where the main difference from the additive error case arises from the measurement equation of the multiplicative error models:
\begin{equation}
	y_t = \mu_t (1+\epsilon_t),
	(\#eq:ETSMultiplicativeErrorMeasurement)
\end{equation}
implying that 
\begin{equation}
	e_t = \frac{y_t-\hat{\mu}_t}{\hat{\mu}_t}.
	(\#eq:ETSMultiplicativeErrorFormula)
\end{equation}
The estimates of scale can then be used in the estimation phase, when parameters are optimised via the maximisation of respective log-likelihood function.


## Examples of application {#ADAMETSMultiplicativeExamples}
### Non-seasonal data
We continue our examples with the same Box-Jenkins sales case by fitting the ETS(M,M,N) model, but this time with a holdout of 10 observations:
```{r}
adamModel <- adam(BJsales, "MMN", h=10, holdout=TRUE)
adamModel
plot(adamModel,7)
```

Note that the function produces the point forecast in this case, which is not equivalent to the conditional expectation! Also, the default distribution for the multiplicative erro models is $\mathcal{IG}$. Similarly, to how it was done in the [previous chapter](#ADAMETSPureAdditiveExamples), the output gives a general summary for the model. We can compare this model with the ETS(A,A,N) via information criteria if we want. For example, here are the AICc for the two models:
```{r}
# ETS(M,M,N)
AICc(adamModel)
# ETS(A,A,N)
AICc(adam(BJsales, "AAN", h=10, holdout=TRUE))
```
The comparison is fair, because both models were estimated via likelihood and both likelihoods are formulated correctly, without omitting any terms (e.g. `ets()` from `forecast` package omits the $-\frac{T}{2} \log\left(2\pi e \frac{1}{T}\right)$ for convenience, which makes it incomparable with other models). In this example, it seems tha the pure additive model is more suitable for the data than the pure multiplicative one. Still, if we want to produce forecasts from the model, we can do it, using the same command as in the previous chapter:
```{r}
plot(forecast(adamModel,h=10,interval="prediction",level=0.95))
```

Note that, when we ask for "prediction" intervals, the `forecast()` function will automatically decide what to use: in case of pure additive model it will use analytical solutions, while in the other cases, it will use simulations. The point forecast obtained from forecast function corresponds to the conditional expectation and is calculated based on the simulations. This also means that it will differ slightly from one run of the function to another (reflecting the uncertainty in the error term), but the difference should be negligible.

We can also compare the performance of ETS(M,M,N) with $\mathcal{IG}$ distribution and the conventional ETS(M,M,N), assuming normality:
```{r}
adamModelNormal <- adam(BJsales, "MMN", h=10, holdout=TRUE, distribution="dnorm")
adamModelNormal
```
which are quite similar on this specific example.


### Seasonal data
The `AirPassengers` data used in the [previous chapter](#ADAMETSPureAdditiveExamples) has (as we discussed) multiplicative seasonality. So, the ETS(M,M,M) model might be more suitable than the pure additive one that we used previously:
```{r}
adamModel <- adam(AirPassengers, "MMM", h=12, holdout=TRUE, silent=FALSE)
adamModel
```

Notice that the smoothing parameter $\gamma=0$ in this case, which reflects the idea that we deal with the data with multiplicative seasonality and apply the correct model. Comparing the information criteria (e.g. AICc) with [the ETS(A,A,A)](#ADAMETSPureAdditiveExamplesETSAAA), this model does a better job at fitting the data. The conditional expectation and prediction interval from this model are better as well:
```{r}
adamForecast <- forecast(adamModel,h=12,interval="prediction")
plot(adamForecast)
```

If we want to calculate the error measures based on the conditional expectation, we can use the `measures()` function from `greybox` package in the following way:
```{r}
measures(adamModel$holdout,adamForecast$mean,actuals(adamModel))
```
And the plot of the time series decomposition according to ETS(M,M,M) is:
```{r}
plot(adamModel,12)
```

It shows that the residuals are more random for the model than for the ETS(A,A,A), but there still might be some structure left. The autocorrelation and partial autocorrelation functions might help in understanding this better:
```{r}
par(mfcol=c(1,2))
plot(adamModel,10:11)
```

The plot shows that there is still some correlation left in the residuals, which could be either due to pure randomness or due to the imperfect estimation of the model. Tuning the parameters of the optimiser or selecting a different model might solve the problem.

Finally, just as an example, we can also fit the most complicated pure multiplicative model, ETS(M,Md,M):
```{r}
adam(AirPassengers, "MMdM", h=12, holdout=TRUE, silent=FALSE)
```
which does not seem to be significantly better than ETS(M,M,M) on this specific time series.
