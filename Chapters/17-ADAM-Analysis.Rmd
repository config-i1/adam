# Model diagnostics
In this chapter we investigate how ADAM can be diagnosed and improved. The majority of topics will build upon the typical model assumptions discussed in Section \@ref(assumptions). Some of the assumptions cannot be diagnosed properly, but for the others there are some existing and well established instruments. We will consider the following assumptions and discuss how to check whether they are violated or not:

1. [Model is correctly specified](#assumptionsCorrectModel):
a. No omitted variables;
b. No redundant variables;
c. The necessary transformation of the variables are applied;
d. No outliers in the model.
2. [Residuals are i.i.d.](#assumptionsResidualsAreIID):
a. They are not autocorrelated;
b. They are homoscedastic;
c. The expectation of residuals is zero, no matter what;
d. The residuals follow the specified distribution;
e. The distribution of residuals does not change over time.
3. [The explanatory variables are not correlated with anything but the response variable](#assumptionsXreg);

All the model diagnostics is aimed at spotting patterns in residuals. If there are some, then something is probably missing in the model. In this chapter we will discuss, which instruments can be used to diagnose different types of assumptions

In order to make this more actionable, we will consider a conventional regression model on `Seatbelts` data. This can be estimated equally well either with `adam()` from `smooth` or `alm()` from `greybox`. In general, I recommend using `alm()`, when no dynamic elements are present in the model, but for illustrative purposes we will do this with `adam()`:
```{r}
adamModelSeat01 <- adam(Seatbelts,"NNN",formula=drivers~PetrolPrice+kms)
plot(adamModelSeat01,7)
```

This model has several issues, and in this chapter we will discuss how to diagnose and fix them.


## Model specification: Omitted variables
We start with one of the most important assumptions for models: model has not omitted important variables. In general this is difficult to diagnose, because typically it is not possible what is missing if we do not have it in front of us. The best thing one can do is a mental experiment, trying to comprise a list of all theoretically possible variables that would impact the variable of interest. If you manage to come up with such a list and realise that some of variables are missing, the next step would be to either collect the variables themselves or their proxies. One way or another, we would need to add the missing information in the model.

In some cases we might be able to diagnose this. For example, with our regression model from the previous section, we have a set of variables that are not included in the model. A simple thing to do would be to see if the residuals of our model are correlated with any of the omitted variables. We can either produce scatterplots or calculate measures of association to see if there is some relation in the residuals that is not explained by the existing structure. I will use `assoc()` and `spread()` functions from `greybox` for this:

```{r}
# Create a new matrix, removing the variables that are already in the model
SeatbeltsWithResiduals <- cbind(as.data.frame(residuals(adamModelSeat01)), Seatbelts[,-c(2,5,6)])
colnames(SeatbeltsWithResiduals)[1] <- "residuals"
# Spread plot
greybox::spread(SeatbeltsWithResiduals)
```

`spread()` function automatically detects the type of variable and produces scatterplot / `boxplot()` / `tableplot()` between them, making the final plot more readable. The plot above tells us that residuals are correlated with `DriversKilled`, `front`, `rear` and `law`, so some of these variables can be added to the model to improve it. `VanKilled` might have a weak relation with `drivers`, but judging by description does not make sense in the model (this is a part of the `drivers` variable). In our case, it is safe to add these variables, because they make sense in explaining the number of injured drivers. However, I would not add `DriversKilled` as it seems not to drive the number of deaths and injuries, but is just correlated with it for obvious reasons (`DriversKilled` is included in `drivers`). We can also calculate measures of association between variables:
```{r}
greybox::assoc(SeatbeltsWithResiduals)
```
Technically speaking, the output of this function tells us that all variables are correlated with residuals and can be considered in the model. I would still prefer not to add `DriversKilled` in the model for the reasons explained earlier. We can construct a new model in the following way:
```{r}
adamModelSeat02 <- adam(Seatbelts,"NNN",formula=drivers~PetrolPrice+kms+front+rear+law)
plot(adamModelSeat02,7)
```

How can we know that we have not omitted any important variables in our new model? Unfortunately, there is no good way of knowing that. In general, we should use judgment in order to decide whether anything else is needed or not. But given that we deal with time series, we can analyse residuals over time and see if there is any structure left:

```{r}
plot(adamModelSeat02,8)
```

This plot shows that the model has not captured seasonality and that there is stil some structure left in the residuals. In order to address this, we will add ETS(A,N,A) element to the model:
```{r}
adamModelSeat03 <- adam(Seatbelts,"ANA",formula=drivers~PetrolPrice+kms+front+rear+law)
par(mfcol=c(1,2))
plot(adamModelSeat03,7:8)
```

This is much better. There is no apparent missing structure in the data and no apparent omitted variables. We can now move to the next steps of diagnostics.


## Model specification: Redundant variables
While there are some ways of testing for omitted variables, the redundant ones are very difficult to diagnose. Yes, we could look at the [significance of variables](#hypothesisTesting) or compare models with and without some variables based on [information criteria](#informationCriteria), but even if our approaches say that a variable is not significant, this does not mean that it is not needed in the model. There can be many reasons, why a test would fail to reject H$_0$ and AIC would prefer a model without the variable under consideration. So, it comes to using judgment, trying to figure out whether a variable is needed in the model or not.

In the example with Seatbelt data, `DriversKilled` would be a redundant variable. Let's see what happens with the model in this case:

```{r}
adamModelSeat04 <- adam(Seatbelts,"NNN",formula=drivers~PetrolPrice+kms+front+rear+law+DriversKilled)
par(mfcol=c(1,2))
plot(adamModelSeat04,7:8)
```

The residuals from this model look adequate, with only issue being the first 45 observations lying below the zero line. The summary of this model is:
```{r}
summary(adamModelSeat04)
```
The uncertainty around the parameter `DriversKilled` is narrow, showing that the variable has a positive impact on the `drivers`. However the issue here is not statistical, but rather fundamental: we have included the variable that is a part of our response variable. It does not explain why drivers get injured and killed, it just reflects a specific part of this relation. So it explains part of the variance, which should have been explained by other variables (e.g. `kms` and `law`), making them statistically not significant. So, based on technical analysis we would be inclined to keep the variable, but based on our understanding of the problem we should not.

If we have redundant variables in the model, then the model might overfit the data, leading to narrower prediction intervals and biased forecasts. The parameters of such model are typically unbiased, but inefficient.


## Model specification: Transformations
The question of appropriate transformations for variables in the model is challenging, because it is difficult to decide, what sort of transformation is needed, if needed at all. In many cases, this comes to selecting between additive linear model and a multiplicative one. This implies that we compare the model:
\begin{equation}
	y_t = a_0 + a_1 x_{1,t} + \dots + a_n x_{n,t} + \epsilon_t,
  (\#eq:additiveModel)
\end{equation}
and
\begin{equation}
	y_t = \exp\left(a_0 + a_1 x_{1,t} + \dots + a_n x_{n,t} + \epsilon_t\right) .
  (\#eq:multiplicativeModel)
\end{equation}
The latter model is equivalent to the so called "log-linear" model, but can also include logarithms of explanatory variables instead of the variables themselves.

There are different ways to diagnose the problem with wrong transformations, which sometimes help in detecting it. The first one is the actuals vs fitted plot:

```{r}
plot(adamModelSeat03,1)
```

The grey dashed line on the plot corresponds to the situation, when actuals and fitted coincide (100% fit). The red line on the plot above is LOESS line, produced by `lowess()` function in R, smoothing the scatterplot to reflect the potential tendencies in the data. In the ideal situation this red line should coinside with the grey line. In addition the variability around the line should not change with the increase of fitted values. In our case there is a slight U-shape in the red line and an insignificant increase in variability around the middle of the data. This could either be due to pure randomness and thus should be ignored, or could indicate a slight non-linearity in the data. After all, we have constructed pure additive model on the data that exhibits seasonality with multiplicative characteristics, which becomes especially apparent at the end of the series, where the drop in level is accompanied by the decrease of variability of the data:

```{r}
plot(adamModelSeat03,7)
```

In order to diagnose this properly, we might use other instruments. One of these is the analysis of standardised residuals. The formula for the standardised residuals will differ depending on the assumed distribution and for some of them comes to the value inside the "$\exp$" part of the [probability density function](#distributions):

1. Normal, $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$: $u_t = \frac{e_t - \bar{e}}{\hat{\sigma}}$;
2. Laplace, $\epsilon_t \sim \mathcal{Laplace}(0, s)$: $u_t = \frac{e_t - \bar{e}}{\hat{s}}$;
3. S, $\epsilon_t \sim \mathcal{S}(0, s)$: $u_t = \frac{e_t - \bar{e}}{\hat{s}^2}$;
4. Generalised Normal, $\epsilon_t \sim \mathcal{GN}(0, s, \beta)$: $u_t = \frac{e_t - \bar{e}}{\hat{s}^{\frac{1}{\beta}}}$;
5. Inverse Gaussian, $1+\epsilon_t \sim \mathcal{IG}(1, s)$: $u_t = \frac{1+e_t}{\bar{e}}$;
6. Gamma, $1+\epsilon_t \sim \mathcal{\Gamma}(s^{-1}, s)$: $u_t = \frac{1+e_t}{\bar{e}}$;
7. Log Normal, $1+\epsilon_t \sim \mathrm{log}\mathcal{N}\left(-\frac{\sigma^2}{2}, \sigma^2\right)$: $u_t = \frac{e_t - \bar{e} +\frac{\hat{\sigma}^2}{2}}{\hat{\sigma}}$.
where $\bar{e}$ is the mean of residuals, which is typically assumed to be zero and $u_t$ is the value of standardised residuals. Note that the scales in the formulae above should be calculated via the formula with the bias correction, i.e. with the division by degrees of freedom, not the number of observations. Also, note that in case of $\mathcal{IG}$, $\Gamma$ and $\mathrm{log}\mathcal{N}$ and additive error models, the formulae for the standardised residuals will be the same, only the assumptions will change (see Section \@ref(ADAMETSAdditiveDistributions)).

Here is an example of a plot of fitted vs standardised residuals in R:
```{r adamModelSeat03Resid, fig.cap="Diagnostics of pure additive ETSX model."}
plot(adamModelSeat03,2)
```

Given that the scale of the original variable is now removed in the standardised residuals, it might be easier to spot the non-linearity. In our case it is still not apparent, but there is a slight U-shape in LOESS line and a slight change in variance. Another plot that we have already used before is standardised residuals over time:

```{r}
plot(adamModelSeat03,8)
```

This plot shows that there is a slight decline in the residuals around year 1977. Still, there is no prominent non-linearity in the residuals, so it is not clear whether any transformations are needed or not.

However, based on my judgment and understanding of the problem, I would expect for the number of injuries and deaths to change proportionally to the change of the level of the data: if after some external interventions the overal level of injuries and deaths would increase, we would expect a percentage decline, not a unit decline with a change of already existing variables in the model. This is why I will try a multiplicative model next:
```{r adamModelSeat05Resid, fig.cap="Diagnostics of pure multiplicative ETSX model."}
adamModelSeat05 <- adam(Seatbelts,"MNM",formula=drivers~PetrolPrice+kms+front+rear+law)
plot(adamModelSeat05,2)
```

The plot shows that the variability is now slightly more uniform across all fitted values, but the difference between Figures \@ref(fig:adamModelSeat03Resid) and \@ref(fig:adamModelSeat05Resid) is not very prominent. One of potential solutions in this situation is to compare the models in terms of information criteria:
```{r}
setNames(c(AICc(adamModelSeat03),AICc(adamModelSeat05)),
         c("Additive model", "Multiplicative model"))
```
Based on this, we would be inclined to select the multiplicative model. My personal judgment in this specific case agrees with the information criterion.


## Model specification: Outliers {#DiagnosticsOutliers}
As we discussed in Section \@ref(assumptionsCorrectModel), one of the important assumptions in forecasting and analytics is the correct specification of the model, which also includes "no outliers in the model" element. Outliers might appear for many different reasons:

1. We missed some important information (e.g. promotion) and did not include a respective variable in the model;
2. There was an error in recordings of the data, i.e. a value of 2000 was recorded as 200;
3. We did not miss anything predictable, we just deal with a distribution with fat tails.

In any of these cases, outliers will impact estimates of parameters of our models. In case of ETS, this will lead to higher than needed smoothing parameters, which leads to wider prediction intervals and potentially biased forecasts. In case of ARIMA, the mechanism is more complicated, but also leads to widened intervals and biased forecasts. So, it is important to identify outliers and deal with them.

### Outliers detection
One of the simplest ways for identifying outliers is based on distributional assumptions. For example, if we assume that our data follows normal distribution, then we would expect 95% of observations lie inside the bounds with approximately $\pm 1.96\sigma$ and 99.8% of them to lie inside the $\pm3.09 \sigma$. Sometimes these values are substituted by heuristic "values lying inside 2 / 3 sigmas", which is not precise and works only for Normal distribution. Still, based on this, we could flag the values outside these bounds and investigate them in order to see if any of them are indeed outliers.

Given that ADAM framework supports [different distributions](#distributions), the heuristics mentioned above is not appropriate. We need to get proper quantiles for each of the assumed distributions. Luckily, this is not difficult to do, because the quantile functions for all the distributions supported by ADAM either have analytical forms or can be obtained numerically.

Here is an example in R with the same multiplicative ETSX model and the standardised residuals vs fitted values with the 95% bounds:

```{r}
plot(adamModelSeat05, 2, level=0.95)
```

Note that in case of $\mathcal{IG}$, $\Gamma$ and $\mathrm{log}\mathcal{N}$, the function will plot $\log u_t$ in order to make the plot more readable. The plot demonstrates that there are outliers, some of which are further away from the bounds. Although the amount of outliers is not big, it would make sense investigating why they happened. Well, we know why - we constructed an incorrect model. Given that we deal with time series, plotting residuals vs time is also sometimes helpful:

```{r}
plot(adamModelSeat05, 8)
```

We see that there is no specific pattern in the outliers, they happen randomly, so they appear not because of the omitted variables or wrong transformations. We have 5 observations lying outside the bounds, which given that the sample size of 192 observations, means that the 95% interval contains $\frac{192-9}{192} \times 100 \mathrm{\%} \approx 95.3\mathrm{\%}$ of observations, which is close to the nominal value.

In some cases, the outliers might impact the scale of distribution and will lead to wrong standardised residuals, distorting the picture. This is where studentised residuals come into play. They are calculated similarly to the standardised ones, but the scale of distribution is recalculated for each observation by considering errors on all but the current observation. So, in a general case, this is an iterative procedure which involves looking through $t=\{1,\dots,T\}$ and which should in theory guarantee that the real outliers do not impact the scale of distribution. Here how they can be analysed in R:

```{r}
par(mfcol=c(1,2))
plot(adamModelSeat05, c(3,9))
```

In many cases (ours included) the standardised and studentised residuals will look very similar, but in some cases of extreme outliers they might differ and the latter might show outliers better than the former. 

Given the situation with outliers in our case, we could investigate when they happen in the original data to better understand whether they need to be taken care of. But instead of manually recording, which of the observations lie beyond the bounds, we can get their ids via the `outlierdummy` method from the package `greybox`, which extracts either standardised or studentised residuals and flags those observations that lie outside the constructed interval, automatically creating dummy variables for these observations. Here how it works:
```{r}
adamModelSeat05Outliers <- outlierdummy(adamModelSeat05,
                                        level=0.95, type="rstandard")
```

The method returns several objects (see documentation for details), including the ids of outliers:
```{r}
adamModelSeat05Outliers$id
```

These ids can be used to produce additional plots. For example:

```{r}
plot(actuals(adamModelSeat05))
points(time(Seatbelts[,"drivers"])[adamModelSeat05Outliers$id],
       Seatbelts[adamModelSeat05Outliers$id,"drivers"],
       col="red", pch=16)
text(time(Seatbelts[,"drivers"])[adamModelSeat05Outliers$id],
     Seatbelts[adamModelSeat05Outliers$id,"drivers"],
     adamModelSeat05Outliers$id, col="red", pos=2)
```

Among all these points, there is one special that happens on observation 170. This is when the law for seatbelts was introduced and the model cannot capture the change in injuries and deaths correctly.

::: remark
As a side note, in R, there are several methods for extracting residuals:

- `resid()` or `residuals()` will extract either $e_t$ or $1+e_t$, depending on the distributional assumptions of the model;
- `rstandard()` will extract the standardised residuals $u_t$;
- `rstudent()` will do the same for the studentised ones.

`smooth` package also introduces `rmultistep` which extracts multiple steps ahead in sample forecast errors. We do not discuss this method here, but we might come back to it later in this textbook.
:::

### Dealing with outliers 
Based on the output of `outlierdummy()` method from the previous example, we can construct a model with explanatory variables to interpolate the outliers and neglect their impact on the model:

```{r}
SeatbeltsWithOutliers <- cbind(as.data.frame(Seatbelts[,-c(1,7)]),adamModelSeat05Outliers$outliers)
SeatbeltsWithOutliers$drivers <- ts(SeatbeltsWithOutliers$drivers,
                                    start=start(Seatbelts),
                                    frequency=frequency(Seatbelts))
adamModelSeat06 <- adam(SeatbeltsWithOutliers,"MNM",lags=12,formula=drivers~.)
```

In order to decide, whether the dummy variables help or not, we can use information criteria, comparing the two models:
```{r}
setNames(c(AICc(adamModelSeat05), AICc(adamModelSeat06)),
         c("ETSX","ETSXOutliers"))
```
Comparing the two values above, I would conclude that adding dummies improves the model. But instead of including all of them, we could try the model with the outlier for the suspicious observation 170, which corresponds to the ninth outlier:

```{r}
adamModelSeat07 <- adam(SeatbeltsWithOutliers,"MNM",lags=12,
                        formula=drivers~PetrolPrice+kms+front+rear+law+outlier9)
plot(adamModelSeat07,2)
AICc(adamModelSeat07)
```

This model is slightly worse than both the one with all outliers in terms of AICc, so there are some other dummy variables that improve the fit that might be considered as well, along with the outlier for the observation 170. We could continue the exploration introducing other dummy variables, but in general we should not do that unless we have good reason for that (e.g. we know that something happened that was not captured by the model).


### An automatic mechanism
A similar automated mechanism is implemented in `adam()` function, which has `outliers` parameter, defining what to do with outliers if there are any with the following three options:

1. "ignore" - do nothing;
2. "use" - create the model with explanatory variables as shown in the previous subsection and see if it is better than the simpler model in terms of an information criterion;
3. "select" - create lags and leads of dummies from `outlierdummy()` and then select the dummies based on the [explanatory variables selection mechanism](#ETSXSelection). Lags and leads are needed for cases, when the effect of outlier is carried over to neighbouring observations.

Here how this works for our case:
```{r}
adamModelSeat08 <- adam(Seatbelts,"MNM",lags=12,
                        formula=drivers~PetrolPrice+kms+front+rear+law,
                        outliers="select",level=0.95)
AICc(adamModelSeat08)
```

This automatic procedure will form a matrix that will include original variables together with the outliers, their lags and leads and then select those of them that minimise AICc in a stepwise procedure (discussed in Section \@ref(ETSXSelection)). In our case, the function throws away some of the important variables and sticks with some of outliers. This might also happen because it could not converge to the optimum on each iteration, so increasing `maxeval` might help. Still, given that this is an automated approach, it is prone to potential mistakes and needs to be treated with care as it might select unnecessary dummy variables and lead to overfitting. I would recommend exploring the outliers manually, when possible and not to rely too much on the automated procedures.


### Final remarks
@Koehler2012 explored the question of the impact of outliers on ETS performance in terms of forecasting accuracy. They found that if outliers happen at the end of the time series then it is important to take them into account in a model. If they happen much earlier, then their impact on the final forecast will be negligible. Unfortunately, the authors did not explore the impact of outliers on the prediction intervals, and based on my experience I can tell that the main impact of outliers is on the width of the interval.


## Residuals are i.i.d.: autocorrelation {#diagnosticsResidualsIIDAuto}
One of the typical characteristics of time series models is the dynamic relation between variables. Even if fundamentally the sales of ice cream on Monday do not impact sales of the same ice cream on Tuesday, they might impact advertising expenses or sales of a competing product on Tuesday, Wednesday or next week. Missing this sort of structure might lead to autocorrelation of residuals, which then would impact the estimates of parameters and final forecasts. Autocorrelations might also arise due to wrong transformations of variables, where the model would systematically underforecast the actuals, producing autocorrelated residuals. In this section, we will see one of the potential ways for the regression diagnostics and try to improve the model in a stepwise fashion, trying out different order of [ARIMA](#ADAMARIMA) model.

As an example, we continue with the same seatbelts data, dropping the dynamic part to see what would happen in this case:
```{r}
adamModelSeat09 <- adam(Seatbelts,"NNN",lags=12,
                        formula=drivers~PetrolPrice+kms+front+rear+law)
AICc(adamModelSeat09)
```
There are different ways to diagnose this model. We start with a basic plot of residuals over time:

```{r}
plot(adamModelSeat09,8)
```

We see that on one hand the residuals still contain seasonality and on the other that they do not look stationary. We could conduct ADF and / or KPSS test to get a formal answer to the stationarity question:

```{r}
tseries::kpss.test(resid(adamModelSeat09))
tseries::adf.test(resid(adamModelSeat09))
```
The tests have opposite null hypothesis, and in our case we would reject H$_0$ for both of them on 5% significance level. This means that they contradict each other and we need to use our judgment. First I will see what happens with the model, when we do take differences:

```{r}
# ARIMAX(0,1,0)(0,0,0)_12
adamModelSeat10 <- adam(Seatbelts,"NNN",lags=12,
                        formula=drivers~PetrolPrice+kms+front+rear+law,
                        orders=list(i=1))
AICc(adamModelSeat10)
```
This leads to an improvement in AICc in comparison with the previous model. The residuals of the model are now also much better behaved:
```{r}
plot(adamModelSeat10,8)
```

In order to see whether there are any other dynamic elements left, we will plt [ACF](#ACF) and [PACF](#PACF) of residuals:

```{r}
par(mfcol=c(1,2))
plot(adamModelSeat10,10:11)
```

In case of `adam()` objects, these plots will always have the range for y-axis from -1 to 1 and will start from lag 1 on x-axis. The red horizontal lines represent the "non-rejection" region: if the point lie inside the region, then they are not statistically different from zero on the selected `level` (the uncertainty around them is so high that it covers zero). The points with numbers are those that are statistically significantly different from zero. So, the ACF / PACF analysis might show the lags that are statistically significant on the selected level (the default one is 0.95). Given that this is a statistical instrument, we would expect for approximately (1-level)% (e.g. 5%) of lags lie outside these bounds, so it is fine if we don't see all point lying inside them. However, we should not see any patterns there and we might need to investigate the suspicious lags (low orders of up to 3 - 5 and the seasonal lags if they appear). In our example we see that there is a suspicious lag 1 on ACF and a suspicious lag 2 on the PACF, which might indicate that some dynamic elements are missing (e.g. MA(1) or AR(2)). Furthermore, there are spikes on lag 12 for both ACF and PACF. While it is not clear, what specifically is needed here, we can try out several models and see which one of them is better in order to deremine the appropriate element:
```{r}
# ARIMAX(0,1,0)(1,0,0)_12
adamModelSeat11 <- adam(Seatbelts,"NNN",lags=12,
                        formula=drivers~PetrolPrice+kms+front+rear+law,
                        orders=list(ar=c(0,1),i=1))
AICc(adamModelSeat11)
# ARIMAX(0,1,0)(0,0,1)_12
adamModelSeat12 <- adam(Seatbelts,"NNN",lags=12,
                        formula=drivers~PetrolPrice+kms+front+rear+law,
                        orders=list(i=1,ma=c(0,1)))
AICc(adamModelSeat12)
# ARIMAX(0,1,0)(1,0,1)_12
adamModelSeat13 <- adam(Seatbelts,"NNN",lags=12,
                        formula=drivers~PetrolPrice+kms+front+rear+law,
                        orders=list(ar=c(0,1),i=1,ma=c(0,1)))
AICc(adamModelSeat13)
```
Based on this analysis, we would be inclined to include both seasonal AR(1) and seasonal MA(1) in the model. Next step in our iterative process - another ACF / PACF plot of the residuals:
```{r}
par(mfcol=c(1,2))
plot(adamModelSeat13,10:11)
```

In this case, there is a big spike on ACF for lag 1, so we can try adding MA(1) component in the model:
```{r}
# ARIMAX(0,1,1)(1,0,1)_12
adamModelSeat14 <- adam(Seatbelts,"NNN",lags=12,
                        formula=drivers~PetrolPrice+kms+front+rear+law,
                        orders=list(ar=c(0,1),i=1,ma=c(1,1)))
AICc(adamModelSeat14)
```
Which leads to further improvement in AICc. We could continue our investigations in order to find the most suitable ARIMAX model for the data using this iterative procedure, but this example should suffice in providing the general idea of how it can be done. What we could do else to simplify the process is to use the automated [ARIMA selection algorithm](#ARIMASelection) in `adam()`, which is built on the principles discussed in this section:

```{r}
adamModelSeat15 <- adam(Seatbelts,"NNN",lags=12,
                        formula=drivers~PetrolPrice+kms+front+rear+law,
                        orders=list(ar=c(3,2),i=c(2,1),ma=c(3,2),select=TRUE))
AICc(adamModelSeat15)
```

This new constructed SARIMAX(0,1,1)(0,1,1)$_{12}$ model has lower AICc than the previous one and should be used instead. In fact, it is even better than the `r adamModelSeat05$model` (model 5) from the previous section in terms of AICc, and its residuals are much better behaved than the ones of model 5 (we might need to analyse the residuals for the [potential outliers](#DiagnosticsOutliers) in this model though):

```{r}
par(mfcol=c(1,3))
plot(adamModelSeat15,c(8,10:11))
```

So for the purposes of analytics and forecasting, we would be inclined to use SARIMAX(0,1,1)(0,1,1)$_{12}$ rather than `r adamModelSeat05$model`.

As a final word for this section, we have focused our discussion on the visual analysis of time series, ignoring the statistical tests (we only used ADF and KPSS). Yes, there is Durbin-Watson [@WikipediaDurbinWatson2021] test for AR(1) in residuals, and yes there are Ljung-Box [@WikipediaLjungBox2021], Box-Pierce and Breuschâ€“Godfrey [@WikipediaBreuschGodfrey2021] tests for multiple AR elements. But visual inspection of time series is not less powerful than hypothesis testing. In fact, it makes you think and analyse the model and its assumptions, while the tests are the lazy way out that might lead to wrong conclusions because they have the [standard limitations of any hypothesis tests](#hypothesisTesting). After all, if you fail to reject H$_0$ it does not mean that the effect does not exist. Having said that, the statistical tests become extremely useful when you need to process many time series at the same time and cannot physically do visual inspection of them. So, if you are in that situation, I would recommend reading more about them, but I do not aim to retell the content of Wikipedia in this textbook.


## Residuals are i.i.d.: heteroscedasticity {#diagnosticsResidualsIIDHetero}
Another important assumption for conventional models is that the residuals are homoscedastic, meaning that the variance of the residuals stays the same (no matter what). Typically, they will be heteroscedastic if the model is not specified correctly. The classical example is the income versus expenditure on meals for different families. If the income is low, then there is not many options what to buy and the variability of expenditures would be low. However, with the increase of the income, the mean expenditures and their variability would increase as well, because there are more options of what to buy, including both cheap and expensive products. If we constructed a basic linear model on such data, then it would violate the assumption of homoscedastic residuals and as a result will have issues discussed in section \@ref(assumptionsResidualsAreIID). But arguably this would typically appear because of the misspecification of the model: if we construct model in logarithms or transform somehow the variables (looking at expenses per family member), then the heteroscedasticity might disappear. Unfortunately, the transformations are not a panacea, so in some cases an analyst would need to construct a model, taking the changing variance into account (e.g. GARCH model).

While in forecasting we are more interested in the holdout performance of models, in econometrics, the parameters of models are typically of the main interest. And, as we discussed [earlier](#assumptionsResidualsAreIID), in case of correctly specified model with heteroscedasticity, the estimates of parameters will be unbiased, but inefficient. So, econometricians would use different approaches to diminish the heteroscedasticity effect: different estimator for a model (such as Weighted Least Squares) or different method for calculation of standard errors of parameters (e.g. Heteroskedasticity-Consistent Standard Errors). This does not resolve the problem, but corrects the parameters of the model.


<!-- ## Residuals are i.i.d.: zero expectation -->
<!-- In ADAM framework, this assumption only works for the [additive error models](#ADAMETSPureAdditive). In case of the multiplicative error models, it is changed to "[expectation of the error term is one](#ADAMETSMultiplicativeDistributions)". It does not make sense to check this assumption unconditionally, because it will hold automatically in sample in case of OLS estimation, but cannot be checked in a more general case. This is because it is about the true model and the asymptotic behaviour rather than the model applied to the data. The only part of this assumption that can be checked is whether the conditional expectation of the residuals is equal to zero (or one) or not. In a way, this comes to making sure that [residuals are i.i.d.](#diagnosticsResidualsIID) and that there are no patterns and thus no consequitive parts of the data, where residuals do not have expectation of zero. -->


<!-- ## Residuals are i.i.d.: distributional assumptions -->
<!-- QQ plot -->
<!-- Shapiro-Wilk and KS tests -->
<!-- ### What if distribution changes? -->


<!-- ## No multicollinearity -->
<!-- cor, assoc, determ -->


<!-- ## Model diagnostics on high frequency data -->
<!-- ## Model diagnostics for intermittent model -->
