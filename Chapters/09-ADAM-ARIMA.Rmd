# Conventional ARIMA {#ARIMA}
Another important dynamic element in ADAM is ARIMA model [developed originally by @Box1976]. ARIMA stands for "AutoRegressive Integrated Moving Average", although the name does not tell much on its own and needs additional explanation, which will be provided in the next sections.

The main idea of the model is that the data might have dynamic relations over time, where the new values depend on the values on the previous observations. This becomes more obvious in case of engineering systems and modelling phisical processes. For example, @Box1976 give an example of a series of CO$_2$ output of a furnace, when the input gas rate changes. In this case, the elements of ARIMA process are natural, as the CO$_2$ cannot just drop to zero, when the gas is switched off - it will leave the furnace, in reducing quantity over time (i.e. leaving $\phi_1\times100\%$ of CO$_2$ in the next minute, where $\phi_1$ is a parameter in the model).

Another example, where AR processes are natural is the temperature in the room, measured with 5 minutes intervals. In this case the temperature at 5:30pm will depend on the one at 5:25pm (if the temperature outside the room is lower, then it will go down slightly due to the loss of heat). So, in these examples, ARIMA model can be considered as a [true model](#intro), but when it comes to time series from the social or business domain, it becomes very difficult to motivate the usage of ARIMA from the from the modelling point of view. 
For example, the demand on products does not reproduce itself and in real live does not depend on the demand on previous observations, unless we are talking about a repetitive purchases by the same group of consumers. So, if we construct ARIMA for such process, we are closing eyes on the fact that the observed time series relations in the data are most probably spurious. At best, ARIMA in this case can be considered as a very crude approximation of a complex true process (demand is typically influenced by price changes, consumer behaviour and promotional activities). Thus, whenever we work with ARIMA models in social or business domain, we should keep in mind that they are wrong even from the philosophical point of view. Nevertheless, they still can be useful, which is why we discuss them in this chapter.

Note that this is a heavy mathematical chapter, and here we will discuss the main theoretical properties of ARIMA processes (i.e. what would happen if the data indeed followed the specified model), moving to more practical aspects in the next chapter.


## Introduction to ARIMA {#ARIMAIntro}
ARIMA contains several elements:

1. AR(p) - the AutoRegressive part, showing how the variable is impacted by its values on the previous observations. It contains $p$ lags. For example, the quantity of the liquid in a vessel with an opened tap on some observation will depend on the quantity on the previous steps. This analogy explains the idea of AR part of the model;
2. I(d) - the number of differences $d$ taken in the model (I stands for "Integrated). Working with differences rather than with the original data means that we deal with changes and rates of changes, not with just values. Technically, differences are needed in order to make data stationary (i.e. with fixed expectation and variance, although there are different definitions of the term *stationarity*);
3. MA(q) - the Moving Average component, explaining how the variable is impacted by the previous white noise. It contains $q$ lags. Once again, in technical systems, the idea that the random error can impact the value, has a relatively simple explanation. For example, when the liquid drips out of a vessel, we might not be able to predict the air fluctations, which would impact the flow and could be perceived as elements of random noise. This randomness might in turn impact the quantity of liquid in a vessel on a next observation, thus introducing the MA elements in the model.

I intentionally do not provide ARIMA examples from the demand forecasting area, as these are much more difficult to motivate and explain than the examples from the more technical areas.

Before we continue our discussion, we should define the term **stationarity**. There are two definitions in the literature, one refers to "strict stationarity", while the other refers to the "weak stationarity":

- Time series is said to be **weak stationary**, when its conditional expectation and variance are constant and the variance is finite for all times;
- Time series is **strong stationary**, when its unconditional joint probability distribution does not change over time. This automatically implies that all its moments are constant (i.e. the process is also weak stationary).

The stationarity is essential in ARIMA context and also plays important role in regression analysis. If the series is not stationary, then it might be difficult to estimate its moments correctly using the conventional methods and in some cases it might be not possible to get the correct parameters at all (e.g. there is infinite combination of parameters that would produce the minimum of the selected loss function). In this case, the series is somehow transformed in order to make sure that the moments are finite and constant (e.g. take logarithms or do Box-Cox transform, take differences or detrend the series) and that the model becomes easier to identify. Note that in contrast with ARIMA, the ETS models are almost always all non-stationary and do not require for the series to be stationary. We will see the connection between the two approaches later in this chapter.


### AR(p) {#AR}
We start with a simple AR(1) model, which is written as:
\begin{equation}
  {y}_{t} = \phi_1 y_{t-1} + \epsilon_t ,
  (\#eq:ARIMA100Example)
\end{equation}
where $\phi_1$ is the parameter of the model. This formula tells us that the value on the previous observation is carried out to the new one in the proportion of $\phi_1$. Typically, the parameter $\phi_1$ is restricted with the region (-1, 1), in order to make the model stationary, but very often in real life $\phi_1$ actually lies in (0, 1) region. If the parameter is equal to 1, then the model becomes equivalent to Random Walk.

The forecast trajectory (conditional expectation several steps ahead) of this model would typically correspond to the exponentially declining curve. Here is a simple example in R of a very basic forecast from AR(1) with $\phi_1=0.9$:
```{r}
y <- vector("numeric", 20)
y[1] <- 100
phi <- 0.9
for(i in 2:length(y)){
    y[i] <- phi * y[i-1]
}
plot(y, type="l", xlab="horizon", ylab="Forecast")
```

If for some reason we get $\phi_1>1$, then the trajectory corresponds to exponential increase, becoming explosive, implying non-stationary behaviour. The model in this case becomes very difficult to work with,  even if the parameter is close to one. So it is typically advised to restrict the parameter with stationarity region (we will discuss this in more detail later in this chapter).

In general, it is possible to imagine the situation, when the value at the moment of time $t$ would depend on several previous values, so the model AR(p) can be written as:
\begin{equation}
  {y}_{t} = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t ,
  (\#eq:ARIMAp00Example)
\end{equation}
where $\phi_i$ is the parameters for the $i$-th lag of the model. So, the model assumes that the data on the recent observations is influenced by the $p$ previous observations. The more lags we introduce in the model, the more complicated the forecasting trajectory becomes, potentially introducing harmonic behaviour. Here is an example of AR(3) model ${y}_{t} = 0.9 y_{t-1} -0.7 y_{t-2} + 0.6 y_{t-3} + \epsilon_t$:
```{r}
y <- vector("numeric", 30)
y[1:3] <- c(100, 75, 30)
phi <- c(0.9,-0.7,0.6)
for(i in 4:30){
    y[i] <- phi[1] * y[i-1] + phi[2] * y[i-2] + phi[3] * y[i-3]
}
plot(y, type="l", xlab="horizon", ylab="Forecast")
```

No matter what the forecast trajectory of AR model is, it will asymptotically converge to zero, as long as the model is stationary.

### MA(q) {#MA}
Before discussing the "Moving Averages" model, we should acknowledge that the name is quite misleading, and that the model has *nothing to do* with Centred Moving Averages used in time series decomposition or Simple Moving Averages (average of several concequitive observation). The idea of the simplest MA(1) model can be summarised in the following mathematical way:
\begin{equation}
  {y}_{t} = \theta_1 \epsilon_{t-1} + \epsilon_t ,
  (\#eq:ARIMA001Example)
\end{equation}
where $\theta_1$ is the parameter of the model, typically lying between (-1, 1), showing what part of the error is carried out to the next observation. Because of the conventional assumption that the error term has a zero mean ($\mathrm{E}(\epsilon_{t})=0$), the forecast trajectory of this model is just a straight line coinsiding with zero starting from the $h=2$. For the one step ahead forecast we have:
\begin{equation}
  \mathrm{E}({y}_{t+1}|t) = \theta_1 \mathrm{E}(\epsilon_{t}|t) + \mathrm{E}(\epsilon_{t+1}|t) = \theta_1 \epsilon_{t}.
  (\#eq:ARIMA001ExampleForecast)
\end{equation}
But starting from $h=2$ there are no observable error terms $\epsilon_t$, so all the values past that are equal to zero:
\begin{equation}
  \mathrm{E}({y}_{t+2}|t) = \theta_1 \mathrm{E}(\epsilon_{t+1}|t) + \mathrm{E}(\epsilon_{t+2}|t) = 0.
  (\#eq:ARIMA001ExampleForecastInSample)
\end{equation}
So, the forecast trajectory for MA(1) model converges to zero, when $h>1$.

More generally, MA(q) model is written as:
\begin{equation}
  {y}_{t} = \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} + \epsilon_t ,
  (\#eq:ARIMA00qExample)
\end{equation}
where $\theta_i$ is the parameters for the $i$-th lag of the error term, which are typically restricted with the so called invertibility region (discussed in the next section). In this case, the model assumes that the recent observation is influenced by several errors on previous observations (your mistakes in the past will haunt you in the future). The more lags we introduce, the more complicated the model becomes. As for the forecast trajectory, it will reach zero, when $h>q$.

### ARMA(p,q) {#ARMA}
Connection the models \@ref(eq:ARIMAp00Example) and \@ref(eq:ARIMA00qExample), we get the more complicated model, ARMA(p,q):
\begin{equation}
  {y}_{t} = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} + \epsilon_t ,
  (\#eq:ARIMAp0q)
\end{equation}
which has the properties of the two models discussed above. The forecast trajectory from this model will have a combination of trajectories for AR and MA for $h \leq q$ and then will correspond to AR(p) for $h>q$.

In order to simplify the work with ARMA models, the equation \@ref(eq:ARIMAp0q) is typically rewritten, by moving all terms with $y_t$ to the left hand side:
\begin{equation}
  {y}_{t} - \phi_1 y_{t-1} - \phi_2 y_{t-2} - \dots - \phi_p y_{t-p} = \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} + \epsilon_t .
  (\#eq:ARIMAp0qLeft)
\end{equation}
Furthermore, in order to make this even more compact, the backshift operator B is introduced, which just shows by how much the subscript of the variable is shifted back in time:
\begin{equation}
  {y}_{t} B^i = {y}_{t-i}.
  (\#eq:backshiftOperator)
\end{equation}
Using \@ref(eq:backshiftOperator), the ARMA model can be written as:
\begin{equation}
  {y}_{t} (1 - \phi_1 B - \phi_2 B^2 - \dots - \phi_p B^p) = \epsilon_t (1 + \theta_1 B + \theta_2 B^2 + \dots + \theta_q B^q) .
  (\#eq:ARIMAp0qCompacter)
\end{equation}
Finally, we can also introduce the AR and MA polynomial functions to make the model even more compact:
\begin{equation}
\begin{aligned}
  & \varphi^p(B) = 1 - \phi_1 B - \phi_2 B^2 - \dots - \phi_p B^p \\ 
  & \vartheta^q(B) = 1 + \theta_1 B + \theta_2 B^2 + \dots + \theta_q B^q .
\end{aligned}
  (\#eq:ARMAPolynomials)
\end{equation}
Inserting the functions \@ref(eq:ARMAPolynomials) in \@ref(eq:ARIMAp0qCompacter) leads to the compact presentation of ARMA model:
\begin{equation}
  {y}_{t} \varphi^p(B) = \epsilon_t \vartheta^q(B) .
  (\#eq:ARIMAp0qCompact)
\end{equation}
The model \@ref(eq:ARIMAp0qCompact) can be considered as a compact form of \@ref(eq:ARIMAp0q). It is more difficult to understand and interpret, but easier to work with from mathematical point of view. In addition, this form permits introducing additional elements, which will be discussed later in this chapter.

Coming back to the ARMA model \@ref(eq:ARIMAp0q), we might notice, that it assumes convergence to zero, the speed of which is regulated via the parameters. In fact, this implies that the data has the mean of zero, and ARMA becomes useful, when the data is somehow pre-processed, so that it is stationary and varies around zero. This means that if you work with non-stationary and / or with non-zero mean data, the pure AR / MA or ARMA will be inappropriate - some prior transformations are in order.

### ARMA with constant {#ARMAConstant}
One of the simpler ways to deal with the issue with zero forecasts is to introduce the constant (or intercept) in ARMA:
\begin{equation}
  {y}_{t} = a_0 + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_p \epsilon_{t-p} + \epsilon_t 
  (\#eq:ARIMAp0qExample)
\end{equation}
or
\begin{equation}
  {y}_{t} \varphi^p(B) = a_0 + \epsilon_t \vartheta^q(B) ,
  (\#eq:ARIMAp0qCompact)
\end{equation}
where $a_0$ is the constant parameter, which in this case also works as the unconditional mean of the series. The forecast trajectory in this case would converge to $a_0$ instead of zero, but with some minor differences from the ARMA without constant. For example, in case of ARMA(1,1) with constant we will have:
\begin{equation}
  {y}_{t} = a_0 + \phi_1 y_{t-1} + \theta_1 \epsilon_{t-1} + \epsilon_t .
  (\#eq:ARIMA101ConstExample01)
\end{equation}
The conditional expectation of $y_{t+h}$ for $h=1$ and $h=2$ can be written as (based on the discussions in previous sections):
\begin{equation}
\begin{aligned}
  & \mathrm{E}({y}_{t+1}|t) = a_0 + \phi_1 y_{t} + \theta_1 \epsilon_{t} \\
  & \mathrm{E}({y}_{t+2}|t) = a_0 + \phi_1 \mathrm{E}(y_{t+1}|t) = a_0 + \phi_1 a_0 + \phi_1^2 y_{t} + \phi_1 \theta_1 \epsilon_t
\end{aligned} ,
  (\#eq:ARIMA101ConstExampleForecasth1)
\end{equation}
or in general for some horizon $h$:
\begin{equation}
  \mathrm{E}({y}_{t+h}|t) = \sum_{j=1}^h a_0\phi_1^{j-1} + \phi_1^h y_{t} + \phi_1^{h-1} \theta_1 \epsilon_{t} .
  (\#eq:ARIMA101ConstExampleForecast)
\end{equation}
So, the forecast trajectory from this model dampens out, similar to the [ETS(A,Ad,N)](#ETSAAdN) model, and the rate of dampening is regulated by the value of $\phi_1$. The following simple example demonstrates this point (I drop the MA(1) part because it does not change the shape of the curve):
```{r}
y <- vector("numeric", 20)
y[1] <- 100
phi <- 0.9
for(i in 2:length(y)){
    y[i] <- 100 + phi * y[i-1]
}
plot(y, type="l", xlab="horizon", ylab="Forecast")
```

The more complicated ARMA(p,q) models with p>1 will have more complicated trajectories with potential harmonics, but the idea of dampening in AR(p) part of the model stays.

Finally, as alternative to adding $a_0$, each actual value of $y_t$ can be centred via $y^\prime_t = y_t - \bar{y}$, making sure that the mean of $y^\prime_t$ is zero and ARMA can be applied to the $y^\prime_t$ data instead of $y_t$. However, this approach introduces additional steps, but the result on stationary data is typically the same.

### I(d) {#Differences}
Based on the previous discussion, we can conclude that ARMA cannot be applied to non-stationary data. So, if we deal with one, we need to make it stationary somehow. The convetional way of doing that is by taking differences of the data. The logic behind this is straight forward: if the data is not stationary, then the mean somehow changes over time. This can be, for example, due to a trend in the data. In this case we should be taking about the change of variable $y_t$ rather than the variable itself. So we should work on the following data instead:
\begin{equation}
  \Delta y_t = y_t - y_{t-1} = y_t (1 - B),
  (\#eq:ARIMADifferencesFirst)
\end{equation}
if the differences have constant mean. The simplest model with differences is I(1), which is also known as the **Random walk**:
\begin{equation}
  \Delta y_t = \epsilon_t,
  (\#eq:ARIMARandomWalk)
\end{equation}
which can be reformulated in a simpler, more interpretable form by inserting \@ref(eq:ARIMADifferencesFirst) in \@ref(eq:ARIMARandomWalk) and regrouping elements:
\begin{equation}
  y_t = y_{t-1} + \epsilon_t.
  (\#eq:ARIMARandomWalk02)
\end{equation}
The model \@ref(eq:ARIMARandomWalk02) can also be perceived as AR(1) with $\phi_1=1$. This is a non-stationary model, meaning that the unconditional mean of $y_t$ is not constant. The forecast from this model corresponds to the Na\"{i}ve method with straight line equal to the last observed actual value (again, assuming that $\mathrm{E}(\epsilon_{t})=0$ and that [other basic assumptions](#assumptions) hold):
\begin{equation}
  \mathrm{E}(y_{t+h}|t) = \mathrm{E}(y_{t+h-1}|t) + \mathrm{E}(\epsilon_{t+h}|t) = y_{t} .
  (\#eq:ARIMARandomWalkForecast)
\end{equation}

Another simple model that relies on differences of the data is called **Random Walk with drift** and is formulated by adding constant $a_0$ to the right hand side of equation \@ref(eq:ARIMARandomWalk):
\begin{equation}
  \Delta y_t = a_0 + \epsilon_t.
  (\#eq:ARIMARandomWalkWithDrift)
\end{equation}
This model has some similarities with the global level model, which is formulated via the actual value rather than differences:
\begin{equation}
  {y}_{t} = a_0 + \epsilon_t.
  (\#eq:ARIMAGlobalMean)
\end{equation}
Using a similar regrouping as with the Random Walk, we can obtain a simpler form of \@ref(eq:ARIMARandomWalkWithDrift):
\begin{equation}
  y_t = a_0 + y_{t-1} + \epsilon_t.
  (\#eq:ARIMARandomWalkWithDrift02)
\end{equation}
which is, again, equivalent to AR(1) model with $\phi_1=1$, but this time with a constant. The term "drift" appears because $a_0$ acts as an additional element, showing what the tendecy in the data will be: if it is positive, the model will exhibit positive trend, if it is negative, the trend will be negative. This can be seen for the conditional mean, for example, for the case of $h=2$:
\begin{equation}
  \mathrm{E}(y_{t+2}|t) = \mathrm{E}(a_0) + \mathrm{E}(y_{t+1}|t) + \mathrm{E}(\epsilon_{t+2}|t) = a_0 + \mathrm{E}(a_0 + y_t + \epsilon_t|t) = 2 a_0 + y_t ,
  (\#eq:ARIMARandomWalkWithDriftForecasth2)
\end{equation}
or in general for the horizon $h$:
\begin{equation}
  \mathrm{E}(y_{t+h}|t) = h a_0 + y_t .
  (\#eq:ARIMARandomWalkWithDriftForecasth)
\end{equation}

In a similar manner we can also introduce second differences of the data (differences of differences) if we suspect that the change of variable over time is not stationary, which would be written as:
\begin{equation}
  \Delta^2 y_t = \Delta y_t - \Delta y_{t-1} = y_t - y_{t-1} - y_{t-1} + y_{t-2},
  (\#eq:ARIMADifferencesSecond)
\end{equation}
which can also be written in a form using backshift operator:
\begin{equation}
  \Delta^2 y_t = y_t(1 - 2B + B^2) = y_t (1-B)^2.
  (\#eq:ARIMADifferencesSecondBackshift)
\end{equation}
In fact, we can introduce higher level differences if we want (but typically we should not) based on the idea of \@ref(eq:ARIMADifferencesSecondBackshift):
\begin{equation}
  \Delta^d = (1-B)^d.
  (\#eq:ARIMADifferences)
\end{equation}
Based on that, the I(d) model is formualted as:
\begin{equation}
  \Delta^d y_t = \epsilon_t.
  (\#eq:ARIMA0d0)
\end{equation}


### ARIMA(p,d,q)
Finally, having made the data stationary via the differences, we can introduce ARMA elements \@ref(eq:ARIMAp0qCompact) to it which would be done on the differenced data, instead of the original $y_t$:
\begin{equation}
  y_t \Delta^d(B) \varphi^p(B) = \epsilon_t \vartheta^q(B) ,
  (\#eq:ARIMApdqCompact)
\end{equation}
or in a more general form \@ref(eq:ARIMAp0qCompacter) with \@ref(eq:ARIMADifferencesSecondBackshift):
\begin{equation}
  y_t (1-B)^d (1 - \phi_1 B - \dots - \phi_p B^p) = \epsilon_t (1 + \theta_1 B + \dots + \theta_q B^q),
  (\#eq:ARIMApdq)
\end{equation}
which is ARIMA(p,d,q) model. This model allows producing trends with some values of differences and also inherits the trajectories from both AR(p) and MA(q). This implies that the point forecasts from the model can exhibit quite complicated trajectories, depending on the values of parameters of the model.

The model \@ref(eq:ARIMApdq) is difficult to interpret in a general form, but opening the brackets and moving all elements but $y_t$ to the right hand side typically helps in understanding of each specific model.


### Parameters bounds {#ARIMABounds}
ARMA models have two conditions that need to be satisfied in order for them to be useful and to work appropriately:

1. Stationarity,
2. Invertibility.

The condition (1) has already been discussed [earlier in this chapter](#ARIMAIntro), and is imposed on AR parameters of the model, making sure that the forecast trajectories do not exhibit explosive behaviour (in terms of both mean and variance). (2) is equivalent to the [stability condition](#stabilityConditionAdditiveError) in ETS and refers to the MA parameters: it guarantees that the old observations do not have increasing impact on the recent ones. The term "invertibility" comes from the idea that any MA process can be represented as an infinite AR process via the inversion of the parameters. For example, MA(1) model, which is written as:
\begin{equation}
  y_t = \epsilon_t (1 + \theta_1 B) = \epsilon_t + \theta_1 \epsilon_{t-1} ,
  (\#eq:ARIMA100Example01)
\end{equation}
can be rewritten as:
\begin{equation}
  y_t (1 + \theta_1 B)^{-1} = \epsilon_t,
  (\#eq:ARIMA100Example02)
\end{equation}
or in a slightly easier to digest form (based on \@ref(eq:ARIMA100Example01) and the idea that $\epsilon_{t} = y_{t} - \theta_1 \epsilon_{t-1}$, implying that $\epsilon_{t-1} = y_{t-1} - \theta_1 \epsilon_{t-2}$):
\begin{equation}
  y_t = \theta_1 y_{t-1} - \theta_1^2 \epsilon_{t-2} + \epsilon_t = \theta_1 y_{t-1} - \theta_1^2 y_{t-2} + \theta_1^3 \epsilon_{t-2} + \epsilon_t = \sum_{j=1}^\infty -1^{j-1} \theta_1^j y_{t-j} + \epsilon_t.
  (\#eq:ARIMA100Example03)
\end{equation}
The recursion in \@ref(eq:ARIMA100Example03) shows that the recent actual value $y_t$ in fact depends on the previous infinite number of values of $y_{t-j}$ for $j=\{1,\dots,\infty\}$. The parameter $\theta_1$ in this case is exponentiated and defines the exponential distribution of weights in this infinite series (reminds [SES](#SES) doesn't it?). The *invertibility* condition makes sure that those weights decline over time with the increase of $j$, so that the older observations do not have an increasing impact on the most recent $y_t$.

There are different ways how to check both conditions, the conventional of which is calculating the roots of the polynomial equations:
\begin{equation}
\begin{aligned}
  & \varphi^p(B) = 0 \text{ for AR} \\
  & \vartheta^q(B) = 0 \text{ for MA}
\end{aligned} ,
  (\#eq:ARIMApdqConditionsCompact)
\end{equation}
or expanding the functions in \@ref(eq:ARIMApdqConditionsCompact) and substituting $B$ with an unknown variable $x$:
\begin{equation}
\begin{aligned}
  & 1 - \phi_1 x - \phi_2 x^2 - \dots - \phi_p x^p = 0 \text{ for AR} \\
  & 1 + \theta_1 x + \theta_2 x^2 + \dots + \theta_q x^q = 0 \text{ for MA}
\end{aligned} .
  (\#eq:ARIMApdqConditions)
\end{equation}
Solving the first equation for $x$ in \@ref(eq:ARIMApdqConditions) we will get $p$ roots (some of them might be complex numbers). In order for the model to be stationary all the roots need to be greater than one by absolute value. Similarly, if all the roots of the second equation in \@ref(eq:ARIMApdqConditions) are greater than one by absolute value, then the model is invertible (aka stable). A special case for both conditions is for the sums of parameters to lie between 0 and 1:
\begin{equation}
\begin{aligned}
  & 0 < \sum_{j=1}^p \phi_j < 1 \\
  & 0 < \sum_{j=1}^q \theta_j < 1
\end{aligned} .
  (\#eq:ARIMApdqConditionsSpecial)
\end{equation}
In a special case with AR(p) model and $\sum_{j=1}^p \phi_j = 1$, we end up with the moving weighted average, which is a non-stationary model. This becomes apparent from the connection between Simple Moving Average and AR processes [@Svetunkov2017].

Note that the condition \@ref(eq:ARIMApdqConditionsSpecial) is rather restrictive and not genuinly applicable for all ARIMA models. Still, if the condition \@ref(eq:ARIMApdqConditionsSpecial) is satisfied, then the respective stationarity and invertibility conditions would be satisfied as well, which could be used in the model estimation (calculating roots of polynomials is a more difficult task).


## Seasonal ARIMA
### Single seasonal ARIMA
When it comes to the real data, we typically have not only relations between consequitive observations, but also between observations happening with some fixed lags. In ETS framework, these relations are taken care of via seasonal indices, which are repeated every $m$ observations. In ARIMA framework, this is done via introducing lags in the elements of the model. For example, seasonal AR(P) with lag $m$ can be written similar to AR(p), but with some minor modifications:
\begin{equation}
  {y}_{t} = \phi_{m,1} y_{t-m} + \dots + \phi_{m,P} y_{t-Pm} + \varepsilon_t ,
  (\#eq:SARIMAP00Example)
\end{equation}
where $\phi_{m,i}$ is the parameter for the lagged $im$ actual value in the model and $\varepsilon_t$ is the error term of the seasonal AR model. We use the underscore "m" just to show that the parameters here refer to the seasonal part of the model. This will be specifically useful, when we will merge the seasonal and non-seasonal parts lof ARIMA. The idea of the model \@ref(eq:SARIMAP00Example) on example of monthly data is that the current observation is influence by the one the same month a year ago, then the same month two years ago etc. This is hard to justify from the theoretical point of view, but this model allows capturing complex relations in the data.

Similarly to seasonal AR(P), we can have seasonal MA(Q):
\begin{equation}
    {y}_{t} = \theta_{m,1} \varepsilon_{t-m} + \dots + \theta_{m,Q} \varepsilon_{t-Qm} + \varepsilon_t ,
  (\#eq:SARIMA00QExample)
\end{equation}
where $\theta_{m,i}$ is the parameter for the lagged error term in the model. This model is even more difficult to justify than the MA(q), because it is difficult to explain, how the white noise the same month last year can impact the actual value this year. Still, this is a useful instrument for forecasting purposes.

Finally, we have the seasonal differences, I(D), which are easier to present using the backshift operator:
\begin{equation}
  y_t (1-B^m)^D = \varepsilon_t.
  (\#eq:SARIMA0D0Example)
\end{equation}
The seasonal differences allow dealing with the seasonality that changes its amplitude in the data, i.e. model the multiplicative seasonality via ARIMA by making the seasonality itself stationary.

Combining \@ref(eq:SARIMAP00Example), \@ref(eq:SARIMA00QExample) and \@ref(eq:SARIMA0D0Example) we get pure seasonal ARIMA(P,D,Q)$_m$ model in the compact notation, similar to the one we had for ARIMA(p,d,q):
\begin{equation}
  y_t (1-B^m)^D (1 - \phi_{m,1} B^m - \dots - \phi_{m,P} B^{Pm}) = \varepsilon_t (1 + \theta_{m,1} B^m + \dots + \theta_{m,Q} B^{Qm}),
  (\#eq:SARIMAPDQ)
\end{equation}
or if we introduce the polynomial functions for seasonal AR and MA and use notation similar to \@ref(eq:ARIMADifferencesSecondBackshift):
\begin{equation}
  y_t \Delta^D(B^m) \varphi^P(B^m) = \varepsilon_t \vartheta^Q(B^m),
  (\#eq:SARIMAPDQCompact)
\end{equation}
where
\begin{equation}
\begin{aligned}
  & \Delta^D(B^m) = (1-B^m)^D \\
  & \varphi^P(B^m) = 1 - \phi_{m,1} B^m  - \dots - \phi_{m,P} B^{Pm} \\ 
  & \vartheta^Q(B^m) = 1 + \theta_{m,1} B^m + \dots + \theta_{m,Q} B^{Qm} .
\end{aligned}
  (\#eq:SARIMAPolynomials)
\end{equation}
Now that we have taken care of the seasonal part of the model, we should not forget that there is a non-seasonal part. If it is in the data, then $\varepsilon_t$ would not be just a white noise, but could be modelled using a non-seasonal ARIMA(p,d,q):
\begin{equation}
  \varepsilon_t \Delta^d(B) \varphi^p(B) = \epsilon_t \vartheta^q(B),
  (\#eq:ARIMApdqForError)
\end{equation}
implying that:
\begin{equation}
  \varepsilon_t = \epsilon_t \frac{\vartheta^q(B)}{\Delta^d(B) \varphi^p(B)}.
  (\#eq:ARIMApdqForErrorRewritten)
\end{equation}
Inserting \@ref(eq:ARIMApdqForErrorRewritten) into \@ref(eq:SARIMAPDQCompact), we get the final SARIMA(p,d,q)(P,D,Q)$_m$ model in the compact form after regrouping the polynomials:
\begin{equation}
  y_t \Delta^D(B^m) \varphi^P(B^m) \Delta^d(B) \varphi^p(B) = \epsilon_t \vartheta^Q(B^m) \vartheta^q(B) .
  (\#eq:SARIMApdqPDQCompact)
\end{equation}
The equation \@ref(eq:SARIMApdqPDQCompact) does not tell us much about what happens in the model, it just shows how different elements interact with each other in it. In order to understand, what SARIMA really means, we need to take an example and see what impacts the current actual value. For example, here what we will have in case of SARIMA(1,0,1)(1,0,1)$_4$ (i.e. applied to quarterly data):
\begin{equation}
  y_t \Delta^0(B^4) \varphi^1(B^4) \Delta^0(B) \varphi^1(B) = \epsilon_t \vartheta^1(B^4) \vartheta^1(B) .
  (\#eq:SARIMA101101Example01)
\end{equation}
Inserting the values of polynomials \@ref(eq:SARIMAPolynomials), \@ref(eq:ARIMADifferences) and \@ref(eq:ARMAPolynomials) in \@ref(eq:SARIMA101101Example01), we get:
\begin{equation}
  y_t (1 - \phi_{4,1} B^4)(1 - \phi_{1} B) = \epsilon_t (1 + \theta_{4,1} B^4) (1 + \theta_{1} B),
  (\#eq:SARIMA101101Example02)
\end{equation}
which is slightly easier to understand, but still does not explain how the past values impact the present one. So, we open the brackets and move all the elements except for $y_t$ to the right hand side of the equation to get:
\begin{equation}
  y_t = \phi_{1} y_{t-1} + \phi_{4,1} y_{t-4} - \phi_{1} \phi_{4,1} y_{t-5} + \theta_1 \epsilon_{t-1} + \theta_{4,1} \epsilon_{t-4} + \theta_{1} \theta_{4,1} \epsilon_{t-5} + \epsilon_t .
  (\#eq:SARIMA101101Example03)
\end{equation}
So, now we see that SARIMA(1,0,1)(1,0,1)$_4$ implies that the present values is impacted by the value in the previous quarter, the value last year on the same quarter and the value from last year on the previous quarter, which introduces a much more complicated interaction than just an ETS model does. Howver, this complexity is obtained with a minimum number of parameters: we have three lagged actual values and three lagged error terms, but we only have four parameters to estimate, not six. The more complicated SARIMA models would have even more complicated interactions, making it more challenging to interpret the model, but all of that comes with a benefit of having a parsimonious model with just $p+q+P+Q$ parameters to estimate.

When it comes to forecasting from such model as SARIMA(1,0,1)(1,0,1)$_4$, the forecasting trajectories would have elements of the classical ARMA model, [discussed earlier](#ARMA), converging to zero as long as there is no constant and the model is stationary. The main difference would be in having the seasonal element. Here is an R example of a prediction for such a model for $h>m+1$ (MA part is dropped because the expectation of the error terms is assumed to be equal to zero):
```{r}
y <- vector("numeric", 20)
y[1:5] <- c(97,87,85,94,95)
phi <- c(0.6,0.8)
for(i in 6:length(y)){
    y[i] <- phi[1] * y[i-1] + phi[2] * y[i-4] - phi[1] * phi[2] * y[i-5]
}
plot(y, type="l", xlab="horizon", ylab="Forecast")
```

As we see, the values converge to zero due to $0<\phi_1<1$ and the seasonality disappears because $0<\phi_{4,1}<1$ as well. So, this is the forecast implied by the SARIMA without differences. If the differences are introduced, then the model would produce non-stationary and seasonaly non-stationary trajectories.

### SARIMA with constant
In addition, it is possible to add the constant term to the SARIMA model, and it will have a more complex effect on the forecast trajectory, depending on the order of the model. In case of zero differences, the effect will be similar to [ARMA](#ARMAConstant), introducing the dampening trajectory, here is an example:
```{r}
y <- vector("numeric", 100)
y[1:5] <- c(97,87,85,94,95)
phi <- c(0.6,0.8)
for(i in 6:length(y)){
    y[i] <- phi[1] * y[i-1] + phi[2] * y[i-4] - phi[1] * phi[2] * y[i-5] + 8
}
plot(y, type="l", xlab="horizon", ylab="Forecast")
```

In case of the model with the differences, the constant would have a two-fold effect: working as a drift for the non-seasonal part and increasing the amplitude of seasonality for the seasonal one. Here is an example from SARIMA(1,0,0)(1,1,0)$_4$ with constant:
\begin{equation}
  y_t (1 - \phi_{4,1} B^4)(1 - \phi_{1} B) (1 - B^4) = \epsilon_t + a_0 ,
  (\#eq:SARIMA101110Example01)
\end{equation}
which can be reformulated as (after opening brackets and moving elements to the right hand side):
\begin{equation}
  y_t = \phi_{1} y_{t-1} + (1+\phi_{4,1}) y_{t-4} + - (1+\phi_{4,1}) \phi_{1} y_{t-5} - \phi_{4,1} y_{t-8} + \phi_1 \phi_{4,1} y_{t-9} + a_0 + \epsilon_t .
  (\#eq:SARIMA101110Example02)
\end{equation}
This formula can then be used to see, what the trajectory from such model will be:
```{r}
y <- vector("numeric", 100)
y[1:9] <- c(96,87,85,94,97,88,86,95,98)
phi <- c(0.6,0.8)
for(i in 10:length(y)){
    y[i] <- phi[1] * y[i-1] + (1+phi[2]) * y[i-4] - (1+ phi[2]) *phi[1] * y[i-5] -
      phi[2] * y[i-8] + phi[1] * phi[2] * y[i-9] + 0.1
}
plot(y, type="l", xlab="horizon", ylab="Forecast")
```

As we see, the trajectory exhibits a drift, coming from the non-seasonal part of the model and a stable seasonality (the amplitude of which does not converge to zero anymore). More complex behaviours for the future trajectories can be obtained with higher orders of seasonal and non-seasonal parts of SARIMA model.

### Multiple seasonal ARIMA
Using the same approach as with the conventional SARIMA, we can introduce more terms [similar to how it was done by @Taylor2003a] with several seasonal frequencies. For example, we can have an hour of day, a day of week and a week of year frequencies in the data. Given that we work with the hourly data in this case, we should introduce three seasonal ARIMA elements with seasonalities $m_1=24$, $m_2=24 \times 7$ and $m_3=24 \times 7 \times 365$. In this example we would have AR, I and MA polynomials for each seasonal part of the model, introducing a triple seasonal ARIMA, which is not even easy to formulate in the compact form. This type of model with multiple seasonal components can be called "Multiple Seasonal ARIMA", MSARIMA. In general, the compact form of the MSARIMA model can be written as:
\begin{equation}
  y_t \Delta^{D_n}(B^{m_n}) \varphi^{P_n}(B^{m_n}) \dots \Delta^{D_0}(B^{m_0}) \varphi^{P_0}(B^{m_0}) = \epsilon_t \vartheta^{Q_n}(B^{m_n}) \dots \vartheta^{Q_0}(B^{m_0}) ,
  (\#eq:MSARIMACompact)
\end{equation}
where $n$ is the number of seasonal cycles, and $D_0=d$, $P_0=p$, $Q_0=q$ and $m_0=1$ for convenience. The slightly more compact and even less comprehensible form of \@ref(eq:MSARIMACompact) is:
\begin{equation}
  y_t \prod_{j=0}^n \Delta^{D_j} (B^{m_j}) \varphi^{P_j}(B^{m_j}) = \epsilon_t \prod_{j=0}^n \vartheta^{Q_j}(B^{m_j}) ,
  (\#eq:MSARIMACompactFinal)
\end{equation}
Conceptually, the model \@ref(eq:MSARIMACompactFinal) is neat, as it captures all the complex relations in the data, but it is not easy to understand and work with, not to mention the potential estimation and order selection problems. In order to understand what the forecast from such model can be, we would need to take a special case, multiply the polynomials and move all the past elements on the right hand side, leaving only $y_t$ on the left hand side one. We have done this before for some examples of ARIMA and SARIMA, so we will not repeat this exercise here. It is worth noting that `msarima()` function from `smooth` package implements the model \@ref(eq:MSARIMACompactFinal), although not in this form, but in the state space form, discussed in the next chapter.

### Parameters bounds for MSARIMA {#MSARIMABounds}
When it comes to parameters bounds of SARIMA, the logic stays quite similar to the process discussed for the case of [non-seasonal model](#ARIMABounds), with the only difference being that instead of analysing the polynomials of a specific part of a model, we need to consider the product of polynomials. So, the *stationarity* condition for the MSARIMA is for the roots of the following polynomial to be greater than one by absolute value (lie outside the unit circle):
\begin{equation}
  \prod_{j=0}^n \varphi^{P_j}(B^{m_j}) = 0,
  (\#eq:MSARIMABoundsStationarity)
\end{equation}
while the invertibility condition is for the roots of the following polynomial to lie outside the unit circle:
\begin{equation}
  \prod_{j=0}^n \vartheta^{Q_j}(B^{m_j}) = 0.
  (\#eq:MSARIMABoundsInvertibility)
\end{equation}
Both of these conditions are difficult to check, especially for high frequencies $m_j$: the polynomial equation of order $n$ has $n$ complex roots, so if you fit a multiple seasonal ARIMA on hourly data, where the maximum frequency is $24\times 7\times 365 = 61,320$, then the equation will have at least 61,320 roots (this number will increase, if there are lower frequency or non-seasonal orders of the model). Finding all of them is not a trivial task even for modern computers (for example, `polyroot()` function from `base` package cannot handle this). So, when considering ARIMA on high frequency data with high seasonal frequency values, it might make sense to find other ways of checking the stationarity and stability conditions. The `msarima()` and `adam()` functions in `smooth` package use the state space form of ARIMA (discussed in the next chapter) and rely on a slightly different principles of checking the same conditions, and they do that more efficiently than in the case of the convetional approach of finding the roots of polynomials \@ref(eq:MSARIMABoundsStationarity) and \@ref(eq:MSARIMABoundsInvertibility).


## Box-Jenkins approach {#BJApproach}
Know that we are more or less familiar with the idea of ARIMA models, we can move to practicalities. One of the issues with the model as it might become apparent from the previous sections, is the identification of orders p, d, q, P$_j$, D$_j$, Q$_j$ etc. Back in the 20th century, when computers were slow, this was a very difficult task to do, so George Box and Gwilym Jenkins [@Box1976] developed a methodology for the identification and estimation of ARIMA models. While nowadays there are more efficient ways of order selection for ARIMA, some of their principles are still used in time series analysis and in forecasting. We briefly outline the idea in this section, not purpoting to give the detailed explanation of the approach.

### Identifying stationarity
Before doing any sort of time series analysis, we need to make the data stationary, which in the context of ARIMA is done via the [differences](#Differences). But before doing anything, we need to understand, whether the data is stationary or not in the first place: over-differencing typically is harmful for the model and would lead to misspecification issues, while in case of under-differencing it might not be possible to correctly identify the model.

There different ways of understanding, whether the data is stationary or not. The simples of them is just looking at the data: in some cases it becomes obvious that the mean of the data changes or that there is a trend in the data, so the conclusion would be relatively easy to make. If it is not stationary, then taking differences and analysing the differenced data again would be the way to go, just to make sure that the second differences are not needed.

The more formal approach would be to conduct statistical tests, such as ADF (`adf.test()` from `tseries` package) or KPSS (`kpss.test()` from `tseries` package). Note that they test different hypotheses:

- In case of ADF it is H$_0$: the data is **not** stationary; H$_1$: the data is stationary;
- In case of KPSS, H$_0$: the data is stationary; H$_1$: the data is **not** stationary;

I do not plan discussing, how the tests are conducted and what they imply, but it should suffice to say that ADF is based on estimating parameters of AR model and then testing the hypothesis for those parameters, while KPSS includes the component of Random Walk in a model (with potential trend) and checks, whether the variance of that component is zero or not. Both tests have their own advantages and disadvantages and sometimes might contradict each other. No matter, what test you choose, do not forget what conducting testing a statistical hypothesis means: if you fail to reject H$_0$, it does not mean that it is true.

Note that even if you select the test-based appraoch, the procedure should be iterative as well: test the hypothesis, take differences if needed, test hypothesis again etc. This way we can determine the order of differences I(d).

When you work with seasonal data, the situation becomes more complicated. Yes, you can probably spot seasonality doing a visual analysis of the data, but it is not easy to conclude, whether the seasonal differences are needed or not. Canova-Hansen test (`ch.test()` in `uroot` package) can be used in this case to formally test the hypothesis similar to the one in KPSS test.

Only after making sure that the data is stationary, we can move to the analysis of the Autocorrelation and Partial Autocorrelation functions.

### Autocorrelation function (ACF)
In the core of the Box-Jenkins approach, lies th eidea of autocorrelation and partial autocorrelation functions. **Autocorrelation** is the correlation of a variable with itself from a different period of time. Here is an example of aucorrelation coefficient for lag 1:
\begin{equation}
  \rho(1) = \frac{\sigma_{y_t,y_{t-1}}}{\sigma_{y_t}\sigma_{y_{t-1}}} = \frac{\sigma_{y_t,y_{t-1}}}{\sigma_{y_t}^2},
  (\#eq:autoCorrelation)
\end{equation}
where $\rho(1)$ is the "true" autocorrelation coefficient, $\sigma_{y_t,y_{t-1}}$ is the covariance between $y_t$ and $y_{t-1}$, while $\sigma_{y_t}$ and $\sigma_{y_{t-1}}$ are the "true" standard deviations of $y_t$ and $y_{t-1}$. Note that $\sigma_{y_t}=\sigma_{y_{t-1}}$, because we are talking about one and the same variable, thus the simpler formula on the right hand side of \@ref(eq:autoCorrelation). As you see, the formula \@ref(eq:autoCorrelation) corresponds to the classical correlation coefficient, so the interpretation of this is the same as for the classical one: the value of $\rho(1)$ shows the closeness of the lagged relation to linear. If it is close to one, then this means that variable has a strong linear relation with itself on the previous observation. It obviously does not tell you anything about the causality, just shows a technical relation between variables, even if in the real life it is spurious.

Using the formula \@ref(eq:autoCorrelation), we can calculate the autocorrelation coefficients for other lags as well, just substituting $y_{t-1}$ with $y_{t-2}$, $y_{t-3}$, $\dots$, $y_{t-\tau}$ etc. In a way, $\rho(\tau)$ can be considered as a function of a lag $\tau$, which is in fact called "Autocorrelation function" (ACF). If we know the order of ARIMA process we deal with, then we can plot the values of ACF on y-axis, by changing the $\tau$ on x-axis. In fact, @Box1976 discuss different theoretical functions for several special cases of ARIMA, which we do not plan to fully repeat here. But, for example, they show that if you deal with AR(1) process, then the $\rho(1)=\phi_1$, $\rho(2)=\phi_1^2$ etc. This can be seen on the example of $\rho(1)$ by calculating the covariance:
\begin{equation}
  \sigma_{y_t,y_{t-1}} = \mathrm{cov}(y_t,y_{t-1}) = \mathrm{cov}(\phi_1 y_{t-1} + \epsilon_t, y_{t-1}) = \mathrm{cov}(\phi_1 y_{t-1}, y_{t-1}) = \phi_1 \sigma_{y_t}^2 ,
  (\#eq:autoCovarianceAR1)
\end{equation}
which when inserted in \@ref(eq:autoCorrelation) leads to $\rho(1)=\phi_1$. The ACF for AR(1) with a positive $\phi_1$ will have the following shape (on the example of $\phi_1=0.9$):
```{r echo=FALSE}
y <- vector("numeric",21)
y[] <- 0.9^c(0:(length(y)-1))
plot(0:(length(y)-1), y, type="l", xlab="Lag", ylab="ACF")
```

Note that $\rho(0)=1$ just because the value is correlated with itself in this case, so lag 0 is typically dropped as not being useful. The declinign shape of the ACF tells us that if $y_t$ is correlated with $y_{t-1}$, then the correlation between $y_{t-1}$ and $y_{t-2}$ will be exactly the same, also implying that $y_{t}$ is somehow correlated with $y_{t-2}$, even if there is no true correlation between them. In fact, it is difficult to say anything for AR process based on ACF exactly because of this temporal relation of the variable with itself. On the other hand, ACF can be used to judge the order of MA(q) process. For example, if we consider [MA(1)](#MA), then the $\rho(1)$ will depend on the following covariance:
\begin{equation}
  \sigma_{y_t,y_{t-1}} = \mathrm{cov}(y_t,y_{t-1}) = \mathrm{cov}(\theta_1 \epsilon_{t-1} + \epsilon_t, \theta_1 \epsilon_{t-2} + \epsilon_{t-1}) = \mathrm{cov}(\theta_1 \epsilon_{t-1}, \epsilon_{t-1}) = \theta_1 \sigma^2 ,
  (\#eq:autoCovarianceMA1)
\end{equation}
where $\sigma^2$ is the variance of the error term. However, the covariance between $y_t$ and $y_{t-2}$ will be equal to zero for the pure MA(1) (given that the [usual assumptions](#assumptions) hold). In fact, @Box1976 showed that for the moving averages, ACF tells more about the order of the model than for the autoregressive one: *ACF will drop rapidly right after the specific lag q for the MA(q) process*.

When it comes to seasonal models, in case of seasonal AR(P), ACF would decrease exponentially from season to season (e.g you would see a decrease on lags 4, 8, 12 etc for SAR(1) and $m=4$), while in case of seasonal MA(Q), ACF would drop abruptly, starting from the lag $(Q+1)m$ (so, the next seasonal lag from the one that the process has, e.g. on lag 8, if we deal with SMA(1) with $m=4$).


### Partial autocorrelation function (PACF)
The other instrument useful for the time series analysis with respect to ARIMA is called "partial ACF". The idea of this follows from ACF directly. As we have spotted, if the process we deal with follows AR(1), then $\rho(2)=\phi_1^2$ just because of the temporal element. In order to get rid of this temporal effect, when calculating the correlation between $y_t$ and $y_{t-2}$ we could remove the correlation $\rho(1)$ in order to get the clean effect of $y_{t-2}$ on $y_t$. This type of correlation is called "partial" and we will denote it as $\varrho(\tau)$. There are different ways how to do that, one of the simplest is to estimate the following regression model:
\begin{equation}
  y_t = a_1 y_{t-1} + a_2 y_{t-2} + \dots + a_\tau y_{t-\tau} + \epsilon_t,
  (\#eq:PACFRegression)
\end{equation}
where $a_i$ is the parameter for the $i$-th lag of the model. In this regression, all the relations between $y_t$ and $y_{t-\tau}$ are captured separately, so the last parameter $a_k$ is clean of all the temporal effects discussed above. We can then use the value $\varrho(\tau) = a_\tau$ as the coefficient, showing this relation. In order to obtain the PACF, we would need to construct and estimate regressions \@ref(eq:PACFRegression) for each lag $\tau=\{1, 2, \dots, p\}$ and get the respective parameters $a_1$, $a_2$, ..., $a_p$, which would correspond to $\varrho(1)$, $\varrho(2)$, ..., $\varrho(p)$.

Just to show what this implies, we consider calculating PACF for AR(1) process. In this case, the true model is:
\begin{equation*}
  y_t = \phi_1 y_{t-1} + \epsilon_t.
\end{equation*}
For the first lag we estimate exactly the same model, so that $\varrho(1)=\phi_1$. For the second lag we estimate the model:
\begin{equation*}
  y_t = a_1 y_{t-1} + a_2 y_{t-2} + \epsilon_t.
\end{equation*}
But we know that for AR(1), $a_2=0$, so when estimated in population this would result in $\varrho(2)=0$ (in case of a sample, this would be a value very close to zero). If we continue with other lags, we will come to the same conclusion: for all lags $\tau>1$ for the AR(1) we will have $\varrho(\tau)=0$. In fact, this is one of the properties of PACF: *if we deal with AR(p) process, then PACF drops rapidly to zero right after the lag $p$*.

When it comes to MA(q) process, PACF behaves differently. In order to understand how it would behave, we take an example of MA(1) model, which is formulated as:
\begin{equation*}
  y_t = \theta_1 \epsilon_{t-1} + \epsilon_t.
\end{equation*}
As it was [discussed earlier](#ARIMABounds), MA process can be also represented as an infinite AR (see \@ref(eq:ARIMA100Example03) for derivation):
\begin{equation*}
  y_t = \sum_{j=1}^\infty -1^{j-1} \theta_1^j y_{t-j} + \epsilon_t.
\end{equation*}
If we construct and estimate the regression \@ref(eq:PACFRegression) for any lag $\tau$ for such process we will get $\varrho(\tau)=-1^{\tau-1} \theta_1^\tau$. This would correspond to the exponentially decreasing curve (if the parameter $\theta_1$ is positive, then this will be an alternating series of values), similar to the one we have seen for the AR(1) and ACF. More generally PACF will decline exponentially for MA(q) process, starting from the $\varrho(q)=\theta_q$.

When it comes to seasonal ARIMA models, the behaviour of PACF would resemble the one of the non-seasonal models, but with lags, multiple to the seasonality $m$. e.g., for the SAR(1) process with $m=4$, the $\varrho(4)=\phi_{4,1}$, while $\varrho(8)=0$.

Summarising the discussions in this section, we can conclude that:

1. For AR(p) process, ACF will decrease either exponentially or alternating (depending on the parameters' values), starting from the lag $p$;
2. For AR(p) process, PACF will drop abruptly right after the lag $p$;
3. For MA(q) process, ACF will drop abruptly right after the lag $q$;
4. For MA(q) process, PACF will decline either exponentially or alternating (based on the specific values of parameters), starting from the lag $q$.

These rules are tempting to use, when determining the appropriate order of ARMA model. However these rules are not necessarily bi-directional: e.g. *if we deal with MA(q), ACF drops abruptly right after the lag q, but if ACF drops abruptly after the lag q, then this does not necessarily mean that we are dealing with MA(q)*. The former follows directly from the assumed "true" model, while the latter refers to the identification of the model on the data, and there can be different reasons for the ACF to behave in a way it does. This small discrepancy led to issues in ARIMA identification over the years. You should not rely fully on Box-Jenkins approach, when identifying the orders of ARIMA, there are more appropriate methods for order selection, which can be used in the context of ARIMA, and we will discuss them in the next chapter. Still, ACF and PACF could be helpful in order to see if anything important is missing in the model, but together with some additional instruments.


## ARIMA and ETS
@Box1976 showed in their textbook that several exponential smoothing methods can be considered as special cases of ARIMA model. Because of that, statisticians have thought for many years that ARIMA is a superior model and payed no attention to the exponential smoothing. It took many years, many papers and a lot of effort [see, for example, @Makridakis1982, @Fildes1998, @Makridakis2000] to show that this is not correct, and that if you are interested in forecasting, then exponential smoothing, being a simpler model, typically does a better job than ARIMA. In fact, it was only after @Ord1997 that the statisticians have started considering ETS as a separate model with its own properties. Furthermore, it seems that some of the conclusions from the previous competitions mainly apply to the [Box-Jenkins approach](#BJApproach) [for example, see @Makridakis1997], pointing out that selecting the correct order of ARIMA models is much more challenging task than the statisticians have thought before.

Still, there is a connection between ARIMA and ETS models, which can be beneficial for both models, so it is worth discussing this in a separate section of the textbook.

### ARIMA(0,1,1) and ETS(A,N,N)
@Muth1960 was one of the first authors who showed that [Simple Exponential Smoothing](#SES) has an underlying ARIMA(0,1,1) model. This becomes apparent, when we study the error correction form of SES:
\begin{equation*}
  \hat{y}_{t} = \hat{y}_{t-1} + \hat{\alpha} e_{t-1}.
\end{equation*}
Recalling that $e_t=y_t-\hat{y}_t$, this equation can be rewritten as:
\begin{equation*}
  y_{t} = y_{t-1} - e_{t-1} + \hat{\alpha} e_{t-1} + e_t,
\end{equation*}
or after regrouping elements:
\begin{equation*}
  y_{t} - y_{t-1} = e_t + (\hat{\alpha} -1) e_{t-1}.
\end{equation*}
Finally, using the backshift operator for ARIMA, substituting the estimated values by their "true" values, we get the ARIMA(0,1,1) model:
\begin{equation*}
  y_{t}(1 - B) = \epsilon_t(1 + (\alpha -1) B) = \epsilon_t(1 + \theta_1 B),
\end{equation*}
where $\theta_1 = \alpha-1$. This relation was one of the first hints that $\alpha$ in SES should lie in a wider interval: based on the fact that $\theta \in (-1, 1)$, the smoothing parameter $\alpha \in (0, 2)$. This is exactly the same region we get, when we deal with admissible bounds of ETS(A,N,N) model. This connection between the parameters of ARIMA(0,1,1) and ETS(A,N,N) is useful on its own, because we can transfer the properties of ETS to ARIMA. For example, we know that the level of ETS(A,N,N) will change slowly, when $\alpha$ is close to zero. The similar behaviour would be observed in ARIMA(0,1,1) with $\theta_1$ close to -1. In addition, we know that ETS(A,N,N) reverts to Random Walk, when $\alpha=1$, which corresponds to $\theta_1=0$. So, the closer $\theta_1$ to zero, the more abrupt behaviour the ARIMA model exhibits. In cases of $\theta_1>0$, the behaviour of the model becomes even more uncertain. In a way, this relation gives us the idea of what to expect from more complicated ARIMA(p,d,q) models, when the parameters for moving average are negative - the model should typically behave smoother.

The main conceptual difference between ARIMA(0,1,1) and ETS(A,N,N) is that the latter still makes sense, when $\alpha=0$, while in case of ARIMA(0,1,1) the condition $\theta_1=-1$ is unacceptable.

### ARIMA(0,2,2) and ETS(A,A,N)

### ARIMA(1,1,2) and ETS(A,Ad,N)

### Seasonal ETS

### ETS + ARIMA
