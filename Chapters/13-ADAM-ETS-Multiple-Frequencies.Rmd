# Multiple frequencies in ADAM ETS

@Taylor2003 proposed an exponential smoothing model with double seasonality and applied it to energy data. Since then, the topic was developed by @Gould2008, @Taylor2008, @Taylor2010, @DeLivera2010 and @DeLivera2011. In this chapter we will discuss some of the proposed models, how they relate to the ADAM framework and can be implemented. Roughly, the idea of a model with multiple seasonalities is in introducing additional seasonal components. For the general framework this means that the state vector (for example, in a model with trend and seasonality) becomes:
\begin{equation}
  \mathbf{v}_t' =
    \begin{pmatrix}
    l_t & b_t & s_{1,t} & s_{2,t} & \dots & s_{n,t}
    \end{pmatrix},
  (\#eq:ETSADAMSeasonalMultiStateVector)
\end{equation}
where $n$ is the number of seasonal components (e.g. hour of day, hour of week and hour of year components). The lag matrix in this case becomes:
\begin{equation}
  \mathbf{l}'=\begin{pmatrix}1 & 1 & m_1 & m_2 & \dots & m_n \end{pmatrix},
  (\#eq:ETSADAMSeasonalMultiStateVectorLags)
\end{equation}
where $m_i$ is the $i$-th seasonal periodicity. While, in theory there can be combinations between additive and multiplicative seasonal components, we argue that such a mixture does not make sense, and the components should align with each other. This means that in case of ETS(M,N,M), all seasonal components should be multiplicative, while in ETS(A,A,A) they should be additive. This results fundamentally in two types of models:

1. Additive seasonality:
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \check{y}_t + s_{1,t-m_1} + \dots + s_{n,t-m_n} \epsilon_t \\
    & \vdots \\
    & s_{1,t} = s_{1,t-m_1} + \gamma_1 \epsilon_t \\
    & \vdots \\
    & s_{n,t} = s_{n,t-m_n} + \gamma_n \epsilon_t
  \end{aligned},
  (\#eq:ETSADAMAdditiveSeasonality)
\end{equation}
where $\check{y}_t$ is the point value based on all non-seasonal components (e.g. $\check{y}_t=l_{t-1}$ in case of no trend model) and $\gamma_i$ is the $i$-th seasonal smoothing parameter.

2. Multiplicative seasonality:
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \check{y}_t \times s_{1,t-m_1} \times \dots \times s_{n,t-m_n} \times(1+\epsilon_t) \\
    & \vdots \\
    & s_{1,t} = s_{1,t-m_1} (1 + \gamma_1 \epsilon_t) \\
    & \vdots \\
    & s_{n,t} = s_{n,t-m_n} (1+ \gamma_n \epsilon_t)
  \end{aligned}.
(\#eq:ETSADAMMultiplicativeSeasonality)
\end{equation}

Depending on a specific model, the number of seasonal components can be 1, 2, 3 or more (although more than 3 might not make much sense from modelling point of view). @DeLivera2010 introduced components based on fourier terms, updated over time via smoothing parameters. This feature is not yet fully supported in `adam()`, but it is possible to substitute some of seasonal components (especially those that have fractional periodicity) with fourier terms via explanatory variables and update them over time. The explanatory variables idea was discussed in the [previous chapter](#ETSX).


## Estimation of multiple seasonal model
### ADAM ETS issues
Estimating a multiple seasonal ETS model is a challenging task, because the number of parameters becomes large. The number of parameters related to seasonal components is equal in general to $\sum_{j=1}^n m_j + n$. For example, in case of hourly data, a triple seasonal model for hours of day, hours of week and hours of year will have: $m_1 = 24$, $m_2 = 24 \times 7 = 168$ and $m_3= 7 \times 24 \times 365 = 61320$, resulting overall in $24 + 168 + 61320 + 3 = 61498$ parameters related to seasonal components to estimate. This is not a trivial task and would take hours to converge to optimum, unless the [pre-initials](#ADAMInitialisation) are already close to optimum. So, if you want to construct multiple seasonal ADAM ETS model, it makes sense to use a different initialisation, reducing the number of estimated parameters. A possible solution in this case is [backcasting](#ADAMInitialisationOptAndBack). The number of parameters in our example would reduce from 61498 to 3, substantially speeding up the model estimation process.

Another consideration is fitting model to the data. In the conventional ETS, the size of transition matrix is equal to the number of initial parameters, which makes it too slow to be practical on high frequency data (multiplying a matrix $61498 \times 61498$ matrix by a vector with rows is a difficult task even for modern computers). But due to the [lagged structure of ADAM models](#ADAMETSIntroduction), construction of multiple seasonal models does not take as much time, because we end up multiplying a matrix of $3 \times 3$ by a vector with 3 rows (skipping level and trend, which would add two more elements). So, in ADAM, the main computational burden comes from recursive relation in transition equation of the state space model, because this operation needs to be repeated at least $T$ times, whatever the sample size $T$ is. As a result, you would want to get to the optimum with as fewer iterations as possible, not needing to refit the model with different parameters to the same data many times. This gives another motivation for reducing the number of parameters to estimate (and thus for using backcasting).

Another potential simplification would be to use deterministic seasonality for some of seasonal frequencies. The possible solution in this case is to use [explanatory variables](#ADAMX) for the higher frequency states (see discussion in the (next section)[#ETSXMultipleSeasonality]) or use multiple seasonal ETS, setting some of smoothing parameters equal to zero.

Finally, given that we deal with large samples of data, some of states of ETS might become more reactive than needed, having higher than needed smoothing parameters. One of possible ways to overcome this limitation is by using [multistep loss functions](#multistepLosses). For example, @kourentzes2018smoothing showed that using such loss functions as [TMSE](#multistepLossesTMSE) in the estimation of ETS models on high frequency data leads to improvements in accuracy due to the shrinkage of parameters towards zero, mitigating the potential overfitting issue. The only problem with this approach is that it is more computationally expensive and thus would take more time (at least $h$ times more, where $h$ is the length of the forecast horizon).

### ADAM ARIMA issues
It is also possible to fit multiple seasonal ARIMA to the high frequency data, and, for example, @Taylor2010 used [triple seasonal ARIMA](#MSARIMA) on example of two time series, and demonstrated that it produced more accurate forecasts than other ARIMAs under consideration, even slightly outperforming ETS. The main issue with ARIMA arises in the model selection direction. While in case of ETS, one can decide, what model to use based on judgment (e.g. there is no obvious trend, and the amplitude increases with the increase of level, so we will fit ETS(M,N,M) model), ARIMA requires more careful consideration of possible orders of the model. Selecting appropriate orders of ARIMA is not a trivial task on its own, but selecting the orders on high frequency data (where correlations might appear significant just because of the sample size) becomes even more challenging task than usual. Furthremore, while on monthly data we typically maximum AR and MA orders of the model with 3 or 5, in case of high frequency data this does not look natural anymore. If the first seasonal component has lag of 24, then in theory anything up until 24 might be useful for the model. Long story short, be prepared for the lengthy investigation of appropriate ARIMA orders. While ADAM ARIMA implements an efficient [order selection mechanism for ARIMA](#ARIMASelection), it does not guarantee that the most appropriate model will be applied to the data. Inevitably, you would need to analyse the residuals, add higher orders and see if there is an improvement in performance of the model.

The related issue to this in context of [ADAM ARIMA](#StateSpaceARIMA) is the dimensionality problem. The more orders you introduce in the model, the bigger transition matrix becomes. This leads to the same issues as in the ADAM ETS, discussed in the previous subsection. There is no unique recipe in this difficult situation, but using [backcasting](#ADAMInitialisationOptAndBack) addresses some of these issues. You might also want to fine tune the optimiser to get a balance between speed and accuracy in the estimation of parameters (see discussion in [Subection 12.4](#ADAMInitialisationOptAndBack)).


## Using explanatory variables for multiple seasonalities {#ETSXMultipleSeasonality}
The conventional way of introducing several seasonal components has several issues:

1. It only works with the data with fixed periodicity (the problem sometimes referred to as "fractional frequency"): if $m_i$ is not fixed and changes from period to period, the model becomes disaligned. An example of such problem is fitting ETS on daily data with $m=365$, while there are leap years that contain 366 days;
2. If the model is fit on high frequency data, the problem of parameters estimation becomes non-trivial. Indeed, on daily data with $m=365$, we need to estimate 364 initial seasonal indices together with the other parameters;
3. Different seasonal indices would "compete" with each other for each observation, thus making the model overfit the data. An example is the daily data with $m_1=7$ and $m_2=365$, where both seasonal components are updated on each observation based on the same error, but with different smoothing parameters.

The situation becomes even more complicated, when the model has more than two seasonal components. But there are at least two ways of resolving these issues in ADAM framework.

The first is based on the idea of @DeLivera2010 and the [dynamic ETSX](#ETSXDynamic). In this case we need to generate fourier series and use them as explanatory variables in the model, switching on the mechanism of adaptation. For example, for the pure additive model, in this case, we will have:
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \check{y}_t + \sum_{i=1}^p a_{i,t-1} x_{i,t} + \epsilon_t \\
    & \vdots \\
    & a_{i,t} = a_{i,t-1} + \delta_i \frac{\epsilon_t}{x_{i,t}} \text{ for each } i \in \{1, \dots, p\}
  \end{aligned},
  (\#eq:ETSXADAMMultipleSeasonalityFourier)
\end{equation}
where $p$ is the number of fourier harmonics. In this case, we can introduce the conventional seasonal part of the model for the fixed periodicity (e.g. days of week) in $\check{y}_t$ and use the updated harmonics for the non-fixed one. This approach is not the same as the one in @DeLivera2010, but might lead to similar results. The only issue here is in the selection of the number of harmonics, which can be done via the [variables selection mechanism](#ETSXSelection), but would inevitably increase computational time.

The second option is based on the idea of [dynamic model with categorical variables](#ETSXDynamicCategories). In this case, instead of trying to fix the problem with days of year, we first introduce the categorical variables for days of week and then for the weeks of year (or months of year if we can assume that the effects of months are more appropriate). After that we can introduce both categorical variables in the model, using the similar adaptation mechanism to \@ref(eq:ETSXADAMMultipleSeasonalityFourier). In fact, if some of variables have fixed periodicity, we can substitute them with the conventional seasonal components. So, for example, in this case, ETSX(M,N,M)[7]{D} could be written as:
\begin{equation}
  \begin{aligned}
    & {y}_{t} = l_{t-1} s_{t-7} \times \prod_{i=1}^q \exp(a_{i,t-1} x_{i,t}) (1 + \epsilon_t) \\
    & l_t = l_{t-1} (1 + \alpha\epsilon_t) \\
    & s_t = s_{t-7} (1 + \gamma\epsilon_t) \\
    & a_{i,t} = a_{i,t-1} + \left \lbrace \begin{aligned}
      &\delta \log(1+\epsilon_t) \text{ for each } i \in \{1, \dots, q\}, \text{ if } x_{i,t} = 1 \\
      &0 \text{ otherwise }
    \end{aligned} \right.
  \end{aligned},
(\#eq:ETSXADAMMultipleSeasonalityCategories)
\end{equation}
where $q$ is the number of levels in the categorical variable (for weeks of year, this should be 53). The number of parameters to estimate in this case might be greater than the number of harmonics in the first case, but this type of model resolves all three issues as well and does not have the dilema about selecting the number of harmonics.


## Dealing with daylight saving and leap years
Another problem that arises in case of data with high frequency is the change of local time due to daylight saving (DST). This happens in some countries two times a year: in Spring the time is moved one hour forward (typically at 1am to 2am), while in the Autumn it is moved back one hour. The implications of this are terrifying from forecasting point of view, because one day of year has 23 hours, while the other one has 25 hours, while all the business processes are aligned to the local time. This means that if the conventional seasonal ETS model with $m=24$ is fit to the data, it will only work correctly in a half of year. Well, it will adapt to the new patterns after some times, but this implies that the smoothing parameter $\gamma$ will be higher than needed.

There are two solutions to this problem:
1. Shift the periodicity for one day, when the time changes from 24 to either 23, or 25, depending on the time of year;
2. Introduce categorical variables for factors, which will mark specific hours of day;

The former is more difficult to formalise mathematically and implement in software, but the latter relies on the already discussed mechanism of [ETSX{D} with categorical variables](#ETSXDynamicCategories) and should be more straightforward. Given the connection between seasonality in the conventional ETS model and the ETSX{D} with categorical variables for seasonality, both approaches should be equivalent in terms of parameters estimation and final forecasts.

Similarly, the problem with leap years can be solved either using the shift from $m=365$ to $m=366$ on 29th February in a spirit of the option (1), or using the categorical variables, approach (2). There is a difference, however: the former would be suitable for the data with only one leap year, where the estimation of the seasonal index for 29th February might be difficult, while the latter assumes the separate estimation of the parameter (so it has one more parameter to estimate). However, given the discussion in [the previous section](ETSXMultipleSeasonality), maybe we should not bother with $m=365$ in the first place and rethink the problem, if possible. Having 52 / 53 weeks in a year has similar difficulties, but at least does not involve the estimation of so many initial seasonal states.


## Examples of application
```{r include=FALSE}
load("data/adamModelETSMNMTaylor.Rdata")
```
### ADAM ETS
In order to see how ADAM can be applied to high frequency data, we will use `taylor` series from `forecast` package. This is half-hourly electricity demand in England and Wales from Monday 5 June 2000 to Sunday 27 August 2000, used in @Taylor2003a.

```{r}
y <- forecast::taylor
plot(y)
```
The series does not exhibit an obvious trend, but has two seasonal cycles: half-hour of day and day of week. Seasonality seems to be multiplicative. We will try several different models and see how they compare. In all the cases below, we will use backcasting as initialisation of the model. We will use the last 336 observations ($48 \times 7$) as the holdout, just to see whether models perform adequately or not.

First, it is ADAM ETS(M,N,M) with `lags=c(48,7*48)`:
```{r}
adamModelETSMNM <- adam(y, "MNM", lags=c(1,48,336), initial="back",
                        h=336, holdout=TRUE)
adamModelETSMNM
plot(adamModelETSMNM,7)
```

As you might notice the model was constructed in `r round(adamModelETSMNM$timeElapsed,2)` seconds, and while it might not be the most accurate model for the data, it fits the data well and produces reasonable forecasts. So, it is a good starting point. If we want to improve upon it, we can try one of multistep estimators, for example [GTMSE](#multistepLossesGTMSE):

```{r eval=FALSE}
adamModelETSMNMGTMSE <- adam(y, "MNM", lags=c(1,48,336), initial="back",
                             h=336, holdout=TRUE, loss="GTMSE")
```
This time the function will take much more time (on my computer it takes around 1.5 minutes), but hopefully will produce more accurate forecasts due to shrinkage of smoothing parameters:
```{r}
adamModelETSMNMGTMSE
```

Comparing, for example, RMSSE of the two models, we can conclude that the one with TMSE was more accurate than the one estimated using the conventional likelihood.

Another potential way of improvement for the model is the inclusion of AR(1) term, as for example done by @Taylor2010. This will take more time, but might lead to some improvements in the accuracy:
```{r eval=FALSE}
adamModelETSMNMAR <- adam(y, "MNM", lags=c(1,48,336), initial="back", orders=c(1,0,0),
                          h=336, holdout=TRUE, maxeval=1000)
```

Note that estimating ETS+ARIMA models is a complicated task, but by default the number of iterations would be restricted by 160, which might not be enough to get to the minimum of the loss. This is why I increased the number of iterations in the code above to 1000. If you want to get more feedback on how the optimisation has been carried out, you can ask function to print details via `print_level=41`.

```{r}
adamModelETSMNMAR
```

In this specific example, we see that the ADAM ETS(M,N,M)+AR(1) leads to a small improvement in accuracy.


<!-- ### ADAM ETSX -->
<!-- Another option of dealing with multiple seasonalities, as discussed above, is ETSX model. We start with a static model, which captures half-hours of day via its seasonal component and days of week frequency via explanatory variable. We will use `temporaldummy()` function from `greybox` package for this. This function works much better, when the data contains proper time stamps and, for example, is of class `zoo` or `xts`. So we will convert the original variable `y` in zoo object: -->
<!-- ```{r} -->
<!-- library(zoo) -->
<!-- y <- zoo(y, order.by=as.POSIXct("2000/06/05")+(c(1:length(y))-1)*60*30) -->
<!-- x1 <- temporaldummy(y,type="day",of="week",factors=TRUE) -->
<!-- # x2 <- temporaldummy(y,type="hour",of="day",factors=TRUE) -->
<!-- taylorData <- data.frame(y=y,x=x1) -->
<!-- ``` -->

<!-- We can now fit the ADAM ETSX model with dummy variables for days of week: -->
<!-- ```{r} -->
<!-- adamModelETSXMNM <- adam(taylorData, "MNM", lags=c(1,48), h=336, holdout=TRUE) -->
<!-- adamModelETSXMNM -->
<!-- ``` -->

<!-- We can spot that the forecasts are off, which is mainly because the smoothing parameter $\alpha$ is too high. In order to make sure that we do not overfit the data, we can use a multistep estimator, to shrink the parameter: -->
<!-- ```{r} -->
<!-- adamModelETSXMNMGTMSE <- adam(taylorData, "MNM", lags=c(1,48), h=336, holdout=TRUE, loss="GTMSE") -->
<!-- adamModelETSXMNMGTMSE -->
<!-- plot(adamModelETSXMNMGTMSE,7) -->
<!-- ``` -->

### ADAM ARIMA
Another model we can try on this data is ARIMA. We have not yet discussed the order selection mechanism for ARIMA, so I will construct a model based on my judgment. Keeping in mind that ETS(A,N,N) is equivalent to ARIMA(0,1,1), and that the changing seasonality in ARIMA context can be modelled with seasonal differences, I will construct ARIMA(0,1,1)(0,1,1)$_{336}$, skipping the frequencies for half-hour of day. Hopefully, this will be enough to model: (a) changing level of data; (b) changing seasonal amplitude. Here how we can construct this model using `adam()`:
```{r}
adamModelARIMA <- adam(y, "NNN", lags=c(1,336), initial="back", orders=list(i=c(1,1),ma=c(1,1)),
                       h=336, holdout=TRUE)
adamModelARIMA
plot(adamModelARIMA,7)
```

This model is directly comparable with ADAM ETS via information criteria, and as we can see is slightly worse than ADAM ETS(M,N,M)+AR(1), but is better than multiple seasonal ETS(M,N,M) in terms of AICc. In fact, it is better even in terms of RMSSE, producing more accurate forecasts. We could analyse the residuals of this model and iteratively test, whether the addition of AR terms and halfhour of day seasonality improves the accuracy of the model. We could also try ARIMA models with different distributions, compare them and select the most appropriate one. The reader is encouraged to do this task on their own.
