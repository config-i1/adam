# ADAM: Pure additive ETS

The ETS model implemented in ADAM framework is built upon the conventional one but has several important differences. First it is formulated using lags of components rather than the transition of them over time, so the original model \@ref(eq:ETSConventionalStateSpace) is written in the following way:
\begin{equation}
  \begin{aligned}
  {y}_{t} = &w(\mathbf{v}_{t-\mathbf{l}}) + r(\mathbf{v}_{t-\mathbf{l}}) \epsilon_t \\
  \mathbf{v}_{t} = &f(\mathbf{v}_{t-\mathbf{l}}) + g(\mathbf{v}_{t-\mathbf{l}}) \epsilon_t
  \end{aligned},
  (\#eq:ETSADAMStateSpace)
\end{equation}
where $\mathbf{v}_{t-\mathbf{l}}$ is the vector of lagged components and $\mathbf{l}$ is the vector of lags, and all the other functions corresponds to the ones used in \@ref(eq:ETSConventionalStateSpace). So, for example, for the ETS(A,A,A) model the lags will be $\mathbf{l}'=\begin{pmatrix}1 & 1 & m\end{pmatrix}$, where $m$ is the seasonal periodicity of the data, leading to $\mathbf{v}_{t-\mathbf{l}}'=\begin{pmatrix} l_{t-1} & b_{t-1} & s_{t-m}\end{pmatrix}$. The model \@ref(eq:ETSADAMStateSpace) updates the states exactly in the same way as \@ref(eq:ETSConventionalStateSpace) and produces exactly the same values. The main benefit of doing that is that the transition matrix becomes smaller, containing $3\times 3$ elements in case of ETS(A,A,A) instead of $(2+m)\times (2+m)$ as for the conventional model. The main disadvantage of this approach is in the complications arrising in the derivation of conditional expectation and mean, which still have closed forms, but are more cumbersome. They are discussed later in this chapter for the example of [pure additive ETS](#ADAMETSPureAdditive).

Furthermore, ADAM ETS introduces more flexibility, allowing the error term $\epsilon_t$ to follow non-normal distributions. This impacts the likelihood function and prediction interval, but does not change the mechanism of the update of states.

Based on all this, in the next few chapters we will discuss several special cases of the ADAM ETS model and explain how they can be extended.


## Model formulation {#ADAMETSPureAdditive}
The pure additive case is interesting, because this is the group of models that has closed forms for both conditional mean and variance. It is formulated in the following way:
\begin{equation}
  \begin{aligned}
    {y}_{t} = &\mathbf{w}' \mathbf{v}_{t-\mathbf{l}} + \epsilon_t \\
    \mathbf{v}_{t} = &\mathbf{F} \mathbf{v}_{t-\mathbf{l}} + \mathbf{g} \epsilon_t
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditive)
\end{equation}
where $\mathbf{w}$ is the measurement vector, $\mathbf{F}$ is the transition matrix and $\mathbf{g}$ is the persistence vector. An example of a pure additive model is ETS(A,A,A), for which we have the following values:
\begin{equation}
  \begin{aligned}
    \mathbf{w} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} & \mathbf{F} = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \\
    \mathbf{g} = \begin{pmatrix} \alpha \\ \beta \\ \gamma \end{pmatrix} & \mathbf{v}_{t} = \begin{pmatrix} l_t \\ b_t \\ s_t \end{pmatrix} & \mathbf{l} = \begin{pmatrix} 1 \\ 1 \\ m \end{pmatrix}
  \end{aligned}.
  (\#eq:ETSADAMAAAMatrices)
\end{equation}
By inserting these values in the equation \@ref(eq:ETSADAMStateSpacePureAdditive), we will obtain the model discussed in the [ETS Taxonomy](#ETSTaxonomyMaths) section:
\begin{equation}
  \begin{aligned}
    y_{t} = & l_{t-1} + b_{t-1} + s_{t-m} + \epsilon_t \\
    l_t = & l_{t-1} + b_{t-1} + \alpha \epsilon_t \\
    b_t = & b_{t-1} + \beta \epsilon_t \\
    s_t = & s_{t-m} + \gamma \epsilon_t 
  \end{aligned}.
  (\#eq:ETSADAMAAA)
\end{equation}
Just to compare, the conventinal ETS(A,A,A), formulated according to \@ref(eq:ETSConventionalStateSpace) would have the following transition matrix:
\begin{equation}
  \mathbf{F} = \begin{pmatrix} 1 & 1 & \mathbf{0}'_{m-1} & 0 \\ 0 & 1 & \mathbf{0}'_{m-1} & 0 \\ 0 & 0 & \mathbf{0}'_{m-1} & 1 \\ \mathbf{0}_{m-1} & \mathbf{0}_{m-1} & \mathbf{I}_{m-1} & \mathbf{0}_{m-1} \end{pmatrix},
  (\#eq:ETSADAMAAAMatrices)
\end{equation}
where $\mathbf{I}_{m-1}$ is the identity matrix of the size $(m-1) \times (m-1)$ and $\mathbf{0}_{m-1}$ is the vector of zeroes of size $m-1$. The model \@ref(eq:ETSADAMStateSpacePureAdditive) is more parsimonious and simplifies some of the calculations, making it realistic, for example, to apply models to data with large frequency $m$ (e.g. 24, 48, 52, 365).


## Recursive relation {#adamETSPureAdditiveRecursive}
A useful thing that can be derived from the pure additive model \@ref(eq:ETSADAMStateSpacePureAdditive) is the recursive value, which can be used in several important aspects. First, when we produce forecast for $h$ steps ahead, it is important to understand what the actual value $h$ steps ahead might be, given all the information we have on the observation $t$:
\begin{equation}
  \begin{aligned}
    {y}_{t+h} = &\mathbf{w}' \mathbf{v}_{t-h_\mathbf{l}} + \epsilon_{t+h} \\
    \mathbf{v}_{t+h} = &\mathbf{F} \mathbf{v}_{t-h_\mathbf{l}} + \mathbf{g} \epsilon_{t+h}
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion01)
\end{equation}
where $\mathbf{v}_{t-h_\mathbf{l}}$ is the vector of previous states, given the lagged values $\mathbf{l}$. In order to obtain the recursion, we need to split the measurement and persisitence vectors together with the transition matrix into parts for the same lags of components, leading to the following transition equation in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion01):
\begin{equation}
  \begin{aligned}
    {y}_{t+h} = &(\mathbf{w}_{m_1}' + \mathbf{w}_{m_2}' + \dots + \mathbf{w}_{m_d}') \mathbf{v}_{t-h_\mathbf{l}} + \epsilon_{t+h} \\
    \mathbf{v}_{t+h} = &(\mathbf{F}_{m_1} + \mathbf{F}_{m_2} + \dots + \mathbf{F}_{m_d}) \mathbf{v}_{t-h_\mathbf{l}} + (\mathbf{g}_{m_1} + \mathbf{g}_{m_2} + \dots \mathbf{g}_{m_d}) \epsilon_{t+h}
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion02)
\end{equation}
where $m_1, m_2, \dots, m_d$ are the distinct seasonal frequencies. So, for example, in case of ETS(A,A,A) model on quarterly data (periodicity is equal to four), $m_1=1$, $m_d=4$, leading to $\mathbf{F}_{m_1} = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ and $\mathbf{F}_{m_2} = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix}$. This split of matrices and vectors into distinct sub matrices and subvectors is needed in order to get the correct recursion and obtain the correct conditional expectation and variance. By substituting the values in the transition equation of \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion02) by their previous values until we reach $t$, we get:
\begin{equation}
  \begin{aligned}
    \mathbf{v}_{t-h_\mathbf{l}} = & \mathbf{F}_{m_1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{v}_{t} + \sum_{j=1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{F}_{m_1}^{j-1} \mathbf{g}_{m_1} \epsilon_{t+m_1\lceil\frac{h}{m_1}\rceil-j} + \\
    & \mathbf{F}_{m_2}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{v}_{t} + \sum_{j=1}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{F}_{m_2}^{j-1} \mathbf{g}_{m_2} \epsilon_{t+m_2\lceil\frac{h}{m_2}\rceil-j} + \\
    & \dots \\
    & \mathbf{F}_{m_d}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{v}_{t} + \sum_{j=1}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{F}_{m_d}^{j-1} \mathbf{g}_{m_d} \epsilon_{t+m_d\lceil\frac{h}{m_d}\rceil-j}
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion03)
\end{equation}
Inserting \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion03) in the measurement equation of \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion02), we will get:
\begin{equation}
  \begin{aligned}
    y_{t+h} = & \mathbf{w}_{m_1}' \mathbf{F}_{m_1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{v}_{t} + \mathbf{w}_{m_1}' \sum_{j=1}^{\lceil\frac{h}{m_1}\rceil-1} \mathbf{F}_{m_1}^{j-1} \mathbf{g}_{m_1} \epsilon_{t+m_1\lceil\frac{h}{m_1}\rceil-j} + \\
    & \mathbf{w}_{m_2}' \mathbf{F}_{m_2}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{v}_{t} + \mathbf{w}_{m_2}' \sum_{j=1}^{\lceil\frac{h}{m_2}\rceil-1} \mathbf{F}_{m_2}^{j-1} \mathbf{g}_{m_2} \epsilon_{t+m_2\lceil\frac{h}{m_2}\rceil-j} + \\
    & \dots \\
    & \mathbf{w}_{m_d}' \mathbf{F}_{m_d}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{v}_{t} + \mathbf{w}_{m_d}' \sum_{j=1}^{\lceil\frac{h}{m_d}\rceil-1} \mathbf{F}_{m_d}^{j-1} \mathbf{g}_{m_d} \epsilon_{t+m_d\lceil\frac{h}{m_d}\rceil-j} + \\
    & \epsilon_{t+h}
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion04)
\end{equation}
Substituting the specific values of $m_1, m_2, \dots, m_d$ in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04) will simplify the equation and make it easier to understand. For example, for ETS(A,N,N) $m_1=1$ and all the other frequencies are equal to zero, so the recursion \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04) simplifies to:
\begin{equation}
    y_{t+h} = \mathbf{w}_{1}' \mathbf{F}_{1}^{h-1} \mathbf{v}_{t} + \mathbf{w}_{1}' \sum_{j=1}^{h-1} \mathbf{F}_{1}^{j-1} \mathbf{g}_{1} \epsilon_{t+h-j} + \epsilon_{t+h} ,
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion05)
\end{equation}
which is the recursion obtained by @Hyndman2008b.


## Conditional expectation and variance {#pureAdditiveExpectationAndVariance}
Now, why is the recursion \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04) important? This is because we can take the expectation and variance of \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04) conditional on the values of the state vector $\mathbf{v}_{t}$ on the observation $t$ (assuming that the error term is homoscedastic, uncorrelated and has the expectation of zero) in order to get:
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & \sum_{i=1}^d \left(\mathbf{w}_{m_i}' \mathbf{F}_{m_i}^{\lceil\frac{h}{m_i}\rceil-1} \right) \mathbf{v}_{t} \\
    \text{V}(y_{t+h}|t) = & \left( \sum_{i=1}^d \left(\mathbf{w}_{m_i}' \sum_{j=1}^{\lceil\frac{h}{m_i}\rceil-1} \mathbf{F}_{m_i}^{j-1} \mathbf{g}_{m_i} \mathbf{g}'_{m_i} (\mathbf{F}_{m_i}')^{j-1} \mathbf{w}_{m_i} \right) + 1 \right) \sigma^2
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionMeanAndVariance)
\end{equation}
These two formulae are cumbersome, but they give the analytical solutions to the two statistics. Having obtained both of them, we can construct prediction intervals, assuming, for example, that the error term follows normal distribution:
\begin{equation}
    y_{t+h} \in \text{E}(y_{t+h}|t) \pm z_{\frac{\alpha}{2}} \sqrt{\text{V}(y_{t+h}|t)} ,
  (\#eq:ETSADAMStateSpacePureAdditivePredictionInterval)
\end{equation}
where $z_{\frac{\alpha}{2}}$ is quantile of standardised normal distribution for the level $\alpha$.


### Example with ETS(A,N,N)
For example, for the ETS(A,N,N) model, discussed above, we get:
\begin{equation}
  \begin{aligned}
    \text{E}(y_{t+h}|t) = & \mathbf{w}_{1}' \mathbf{F}_{1}^{h-1} \mathbf{v}_{t} \\
    \text{V}(y_{t+h}|t) = & \left(\mathbf{w}_{1}' \sum_{j=1}^{h-1} \mathbf{F}_{1}^{j-1} \mathbf{g}_{1} \mathbf{g}'_{1} (\mathbf{F}_{1}')^{j-1} \mathbf{w}_{1} + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSADAMStateSpaceAANRecursionMeanAndVarianceGeneral)
\end{equation}
or by substituting $\mathbf{F}=1$, $\mathbf{w}=1$, $\mathbf{g}=\alpha$ and $\mathbf{v}_t=l_t$:
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & l_{t} \\
    \text{V}(y_{t+h}|t) = & \left((h-1) \alpha^2 + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSADAMStateSpaceAANRecursionMeanAndVariance)
\end{equation}
which is the same conditional expectation and variance as in the [ETS Taxonomy section](#ETSTaxonomyMaths) and in the @Hyndman2008b textbook.


## Stability and forecastability conditions {#stabilityConditionAdditiveError}
Another important aspect of the pure additive model \@ref(eq:ETSADAMStateSpacePureAdditive) is the restriction on the smoothing parameters. This is related to the stability and forecastability conditions of the model. The **stability** implies that [the weights for observations decay](#whyExponential), guaranteeing that the newer ones will have higher weights than the older ones. If this condition holds, then the model behaves "steadily", forgetting eventually the past values. The **forecastability** does not guarantee that the weights will decay, but it guarantees that the initial value of the state vector will have a constant impact on forecasts, i.e. will not increase in weight with the increase of the forecast horizon. An example of the non-stable, but forecastable model is ETS(A,N,N) with $\alpha=0$. In this case it reverts to the global level model, where the initial value impacts the forecast, but does not change with the increase of the forecast horizon.

In order to obtain both conditions, we need to use a reduced form of ETS by inserting the measurement equation in the transition equation via $\epsilon_t= {y}_{t} - \mathbf{w}' \mathbf{v}_{t-\mathbf{l}}$:
\begin{equation}
  \begin{aligned}
    \mathbf{v}_{t} = &\mathbf{F} \mathbf{v}_{t-\mathbf{l}} + \mathbf{g} \left({y}_{t} - \mathbf{w}' \mathbf{v}_{t-\mathbf{l}} \right)\\
    = & \left(\mathbf{F} - \mathbf{g}\mathbf{w}' \right) \mathbf{v}_{t-\mathbf{l}} + \mathbf{g} {y}_{t} \\
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureAdditiveBackRecursion01)
\end{equation}
The matrix $\mathbf{D}=\mathbf{F} - \mathbf{g}\mathbf{w}'$ is called the discount matrix and it shows how the weights diminish over time. It is the main part of the model that determines, whether the model will be stable / forecastable or not.


### Example with ETS(A,N,N)
In order to better understand what we plan to discuss later, we can take an example of ETS(A,N,N) model, for which $\mathbf{F}=1$, $\mathbf{w}=1$, $\mathbf{g}=\alpha$, $\mathbf{v}_t=l_t$ and $\mathbf{l}=1$. Inserting these values into \@ref(eq:ETSADAMStateSpacePureAdditiveBackRecursion01), we get:
\begin{equation}
  \begin{aligned}
    l_{t} = & \left(1 - \alpha \right) {l}_{t-1} + \alpha {y}_{t},
  \end{aligned}.
  (\#eq:ETSADAMStateSpaceANNBackRecursion01)
\end{equation}
which corresponds to the formula of Simple Exponential Smoothing \@ref(eq:BrownMethod). The discount matrix in this case is $\mathbf{D}=1-\alpha$. If we now substitute the values for the level on the right hand side of the equation \@ref(eq:ETSADAMStateSpaceANNBackRecursion01) by the previous values of the level, we will obtain the recursion that we have already discussed in [a previous section](#whyExponential), but now in terms of the "true" components and parameters:
\begin{equation}
  \begin{aligned}
    l_{t} = & {\alpha} \sum_{j=0}^{t-1} (1 -{\alpha})^j {y}_{t-j} + (1 -{\alpha})^t l_0,
  \end{aligned}.
  (\#eq:ETSADAMStateSpaceANNBackRecursion02)
\end{equation}
The *stability* condition for ETS(A,N,N) is that the discount matrix $\mathbf{D}=1-\alpha$ is less than or equal to one by absolute value. This way the weights will decay in time because of the exponentiation in \@ref(eq:ETSADAMStateSpaceANNBackRecursion02). This condition is satisfied, when $\alpha \in(0, 2)$. As for the *forecastability* condition, in this case it implies that $\lim\limits_{t\rightarrow\infty}(1 -{\alpha})^t = \text{const}$. This is achievable, for example, when $\alpha=0$, but is violated, when $\alpha<0$ or $\alpha\geq 2$. So, the bounds for the smoothing parameters in the ETS(A,N,N) model, guaranteeing the forecastability of the model (i.e. making it useful) are:
\begin{equation}
  \alpha \in [0, 2) .
  (\#eq:ETSADAMStateSpaceANNBounds)
\end{equation}

### Comming back to the general case
In general, the logic is the same as with ETS(A,N,N), but it implies the usage of linear algebra. Due to our lagged formulation, the recursion becomes more complicated:
\begin{equation}
  \begin{aligned}
    \mathbf{v}_{t} = & \mathbf{D}_{m_1}^{\lceil\frac{t}{m_1}\rceil} \mathbf{v}_{0} + \sum_{j=0}^{\lceil\frac{t}{m_1}\rceil-1} \mathbf{D}_{m_1}^{j} y_{t - j m_1} + \\
    & \mathbf{D}_{m_2}^{\lceil\frac{t}{m_2}\rceil} \mathbf{v}_{0} + \sum_{j=0}^{\lceil\frac{t}{m_2}\rceil-1} \mathbf{D}_{m_2}^j y_{t - j m_2} + \\
    & \dots \\
    & \mathbf{D}_{m_d}^{\lceil\frac{t}{m_d}\rceil} \mathbf{v}_{0} + \sum_{j=0}^{\lceil\frac{t}{m_d}\rceil-1} \mathbf{D}_{m_d}^j y_{t - j m_d}
  \end{aligned},
  (\#eq:ETSADAMStateSpacePureAdditiveRecursion04)
\end{equation}
where $\mathbf{D}_{m_i} = \mathbf{F}_{m_i} - \mathbf{g}_{m_i} \mathbf{w}_{m_i}'$ is the discount matrix for each lagged part of the model. The stability condition in this case is that the absolute values of all the non-zero eigenvalues of the discount matrices $\mathbf{D}_{m_i}$ are lower than one. This condition can be checked at the model construction stage, ensuring that the selected parameters guarantee the stability of the model. As for the forecastability, in our case this implies that $\lim\limits_{t\rightarrow\infty}\left(\mathbf{w}_{m_i}'\mathbf{D}_{m_i}\right)^{t} = \text{const}$ for all $i=1,\dots,d$.


## Distributional assumptions in pure additive ETS
While the conventional ETS assumes that the error term follows Normal distribution, ADAM ETS proposes some flexibility, implementing the following options for the error term [distribution](#distributions) in the additive error models:

1. Normal: $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$;
2. Laplace: $\epsilon_t \sim \mathcal{Laplace}(0, s)$;
3. S: $\epsilon_t \sim \mathcal{S}(0, s)$;
4. Generalised Normal: $\epsilon_t \sim \mathcal{GN}(0, s, \beta)$;
5. Logistic: $\epsilon_t \sim \mathcal{N}(0, s)$;
6. Student's t: $\epsilon_t \sim \mathcal{t}(\nu)$;
7. Asymmetric Laplace: $\epsilon_t \sim \mathcal{ALaplace}(0, s, \alpha)$

The conditional expectation and stability / forecastability conditions do not change for the model with these new assumptions. The main element that changes is the scale and the width of prediction intervals. Given that scales of these distributions are linearly related to the variance, one can calculate the conditional variance as [discussed earlier](#pureAdditiveExpectationAndVariance) and then use the formulae from the [theory of distributions](#distributions) section in order to obtain the respective scales. Having the scales it becomes straightforward to calculate the needed quantiles for the prediction intervals.

The estimation of pure additive ETS models can be done via the maximisation of the likelihood of the assumed distribution, which in some cases coincide with the popular losses (e.g. Normal and MSE, or Laplace and MAE, or Asymmetric Laplace and pinball).

In addition, the following more exotic options for the additive error models are available in ADAM ETS:

1. Log Normal: $\left(1+\frac{\epsilon_t}{\mu_{y,t}} \right) \sim \text{log}\mathcal{N}\left(-\frac{\sigma^2}{2}, \sigma^2\right)$.
Here $\mu_{y,t} = \mathbf{w}' \mathbf{v}_{t-\mathbf{l}}$, $\sigma^2$ is the variance of the error term in logarithms and the $-\frac{\sigma^2}{2}$ appears due to the restriction $\text{E}(\epsilon_t)=0$.
2. Inverse Gaussian: $\left(1+\frac{\epsilon_t}{\mu_{y,t}} \right) \sim \mathcal{IG}(1, s)$;
  
The possibility of using these distributions arrises from a reformulation of the original pure additive model \@ref(eq:ETSADAMStateSpacePureAdditive) into:
\begin{equation}
  \begin{aligned}
    {y}_{t} = &\mathbf{w}' \mathbf{v}_{t-\mathbf{l}}\left(1 + \frac{\epsilon_t}{\mathbf{w}' \mathbf{v}_{t-\mathbf{l}}}\right) \\
    \mathbf{v}_{t} = &\mathbf{F} \mathbf{v}_{t-\mathbf{l}} + \mathbf{g} \epsilon_t
  \end{aligned}.
  (\#eq:ETSADAMStateSpacePureAdditiveReformulated)
\end{equation}
The connection between the two formulations becomes apparent, when opening the brackets in the measurement equation of \@ref(eq:ETSADAMStateSpacePureAdditiveReformulated). Note that in this case the model assumes that the data is strictly positive and while it might be possible to fit the model on the data with negative values, the calculation of the scale and the likelihood might become impossible. Using alternative losses (e.g. MSE) is a possible solution in this case.


## Examples of application {#ADAMETSPureAdditiveExamples}
### Non-seasonal data
In order to see how the pure additive ADAM ETS works, we will try it out using `adam()` function from `smooth` package for R on Box-Jenkins sales data. We start with plotting the data:

```{r}
plot(BJsales)
```

The series seem to exhibit trend, so we will apply ETS(A,A,N) model:
```{r}
adamModel <- adam(BJsales, "AAN")
adamModel
```

The output of the model summarises, which specific model was estimated, assuming what distribution, how it was estimated, what are the values of smoothing parameters, the sample size, degrees of freedom and also produces [information criteria](#modelSelection). We can compare this model with the ETS(A,N,N) in order to see, which of them performs better in terms of information criteria (e.g. in terms of AICc):
```{r}
adam(BJsales, "ANN")
```
In this situation the information criteria for ETS(A,N,N) are higher than for ETS(A,A,N), so we should use the latter for forecasting purposes. We can produced point forecasts and prediction interval (in this example we will constract 90% and 95% ones) and plot them:
```{r}
plot(forecast(adamModel,h=10,interval="prediction",level=c(0.9,0.95)))
```

Notice that the smoothing parameters of ETS(A,A,N) are very high, with $\alpha=1$. This might mean that the maximum of the likelihood is achieved in the *admissible* bounds. We can try it out and see what happens:
```{r}
adamModel <- adam(BJsales, "AAN", bounds="admissible")
adamModel
plot(forecast(adamModel,h=10,interval="prediction",level=c(0.9,0.95)))
```

Both smoothing parameters are now higher, which implies that the uncertainty about the future values of states is higher as well, which is then reflected in the slightly wider prediction interval. Although the values are larger than one, the model is still stable. In order to see that we can calculate the discount matrix using the objects returned by the function:
```{r}
discountMatrix <- adamModel$transition - adamModel$persistence %*% adamModel$measurement[nobs(adamModel),,drop=FALSE]
eigen(discountMatrix)$values
```
Notice that the absolute values of the both eigenvalues in the matrix are less than one, which means that the newer observations have higher weights than the older ones and that the absolute values of weights decrease over time, making the model stable.

If we want to test ADAM ETS with another distribution, it can be done using the respective parameter (here we use Generalised Normal, estimating the shape together with the other parameters):
```{r}
adamModel <- adam(BJsales, "AAN", distribution="dgnorm")
print(adamModel,digits=3)
plot(forecast(adamModel,h=10,interval="prediction"))
```

The prediction interval in this case is slightly weider than in the previous one, because $\mathcal{GN}$ distribution with $\beta=$ `r round(adamModel$other$beta,2)` has fatter tails than the normal distribution.


### Seasonal data {#ADAMETSPureAdditiveExamplesETSAAA}
Now we will check what happens in the case of seasonal data. We use `AirPassengers`, which actually has multiplicative seasonality, but for purposes of demonstration we will see what happens, when we use the wrong model. We will withhold 12 observations to look closer at the performance of the ETS(A,A,A) model in this case:
```{r}
adamModel <- adam(AirPassengers, "AAA", lags=12, h=12, holdout=TRUE)
```

```{block, type="remark"}
The `lags` parameter in this specific case is not necessary, because the function will get the frequency from the `ts` object automatically. If we were to provide a vector of values instead of the `ts` object, we would need to specify the correct lag. Note that `1` (lag for level and trend) is not important either - the function will always use it anyway.
```

```{block, type="remark"}
In some cases, the optimiser might converge to the local minimum, so if you find the results unsatisfactory, it might make sense to reestimate the model tuning the parameters of the optimiser. Here is an example (we increase the number of iterations in the optimisation and set new starting values for the smoothing parameters):
```

```{r}
adamModel$B[1:3] <- c(0.2,0.1,0.3)
adamModel <- adam(AirPassengers, "AAA", lags=12, h=12, holdout=TRUE, B=adamModel$B, maxeval=1000)
adamModel
```

Notice that because we fit the additive seasonal model to the data with multiplicative seasonality, the smoothing parameter $\gamma$ has become quite big - the seasonal component needs to be updated in order to keep up for the changing seasonal profile. In addition, because we used `holdout` parameter, the function now also reports the errors for the point forecasts on that test part of data. This can be useful, when you want to compare the performance of several models on a time series. Here how the forecast from ETS(A,A,A) looks on this data:
```{r}
plot(forecast(adamModel,h=12,interval="prediction"))
```

While the fit to the data is far from perfect, due to a pure coincidence the point forecast from this model is quite decent.

In order to see how the ADAM ETS decomposes the data into components, we can plot it via the `plot()` method with `which` parameter:
```{r}
plot(adamModel,which=12)
```

We can see on this graph that the residuals still contain some seasonality in them, so there is a room for improvement. Most probably, this happened because the data exhibits multiplicative seasonality rather than the additive one. For now, we do not aim to fix this issue.
