# Introduction to ETS {#ETSConventional}
Now that we know how time series can be decomposed into components, we can discuss the ETS model, its connection with exponential smoothing methods, focusing on the most popular of those. We do not discuss in detail how the methods were originally derived and how to work with them. Instead, we focus on the main ideas behind the conventional ETS, as formulated by @Hyndman2008b, and the connection between Exponential Smoothing and ETS.

The reader interested in the history of exponential smoothing, how it was developed, and what papers contributed to the field can refer to the reviews of @Gardner1985 and @Gardner2006. They summarise all the progress in exponential smoothing up until 1985 and until 2006 respectively.


## ETS taxonomy {#ETSTaxonomy}
Building on the idea of time series components (from Section \@ref(tsComponents)), we can move to the ETS taxonomy. ETS stands for "Error-Trend-Seasonality" and defines how specifically the components interact with each other. Based on the type of error, trend and seasonality, @Pegels1969 proposed a taxonomy, which was then developed further by @Hyndman2002 and refined by @Hyndman2008b. According to this taxonomy, error, trend and seasonality can be:

1. Error: "Additive" (A), or "Multiplicative" (M);
2. Trend: "None" (N), or "Additive" (A), or "Additive damped" (Ad), or "Multiplicative" (M), or "Multiplicative damped" (Md);
3. Seasonality: "None" (N), or "Additive" (A), or "Multiplicative" (M).

In this taxonomy, the model \@ref(eq:PureAdditive) is denoted as ETS(A,A,A) while the model \@ref(eq:PureMultiplicative) is denoted as ETS(M,M,M), and \@ref(eq:MixedAdditiveTrend) is ETS(M,A,M).

The components in the ETS taxonomy have clear interpretations: level shows average value per time period, trend reflects the change in the value, while seasonality corresponds to periodic fluctuations (e.g. increase in sales each January). Based on the the types of the components above, it is theoretically possible to devise 30 ETS models with different types of error, trend and seasonality. Figure \@ref(fig:ETSTaxonomyAdditive) shows examples of different time series with deterministic (they do not change over time) level, trend, seasonality and with the additive error term.

```{r ETSTaxonomyAdditive, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Time series corresponding to the additive error ETS models"}
modelsList <- c("ANN","AAN","AAdN","AMN","AMdN","ANA","AAA","AAdA","AMA","AMdA","ANM","AAM","AAdM","AMM","AMdM",
                "MNN","MAN","MAdN","MMN","MMdN","MNA","MAA","MAdA","MMA","MMdA","MNM","MAM","MAdM","MMM","MMdM")
level <- 500
trend <- c(100,1.05)
seasonality <- list((c(1.3,1.1,0.9,0.75)-1)*2*level,c(1.3,1.1,0.9,0.75))
generatedData <- vector("list", length(modelsList))
scale <- 0.05
for(i in 1:length(modelsList)){
  initial <- switch(substr(modelsList[i],2,2),
                    "A"=c(level,trend[1]),
                    "M"=c(level,trend[2]),
                    level*2);
  initialSeason <- switch(substr(modelsList[i],nchar(modelsList[i]),nchar(modelsList[i])),
                    "A"=seasonality[[1]],
                    "M"=seasonality[[2]],
                    NULL);
  if(nchar(modelsList[i])==4){
    phi <- 0.95;
  }
  else{
    phi <- 1;
  }
  sdValue <- switch(substr(modelsList[i],1,1),
                    "A"=sqrt(level^2*(exp(scale^2)-1)*exp(scale)),
                    "M"=scale)
  meanValue <- switch(substr(modelsList[i],1,1),
                      "A"=0,
                      "M"=1)
  generatedData[[i]] <- smooth::sim.es(modelsList[i], obs=36, frequency=4, persistence=0, phi=phi,
                                       initial=initial, initialSeason=initialSeason, mean=meanValue, sd=sdValue)$data
}

# Prepare the canvas
par(mfcol=c(5,3),mar=c(2,3,2,1))
# The matrix that corresponds to the i
ylimValues <- matrix(c(1:15),5,3)
for(i in 1:15){
  if(i>15){
    ylimRow <- which(ylimValues == i-15,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]+15]))
  }
  else{
    ylimRow <- which(ylimValues == i,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]]))
  }
  plot(generatedData[[i]], main=paste0("ETS(",modelsList[i],")"),ylim=ylim,ylab="")
}
```

Things to note from the plots in Figure \@ref(fig:ETSTaxonomyAdditive):

1. When seasonality is multiplicative, its amplitude increases with the increase of the level of the data, while with additive seasonality, the amplitude is constant. Compare, for example, ETS(A,A,A) with ETS(A,A,M): for the former, the distance between the highest and the lowest points in the first year is roughly the same as in the last year. In the case of ETS(A,A,M) the distance increases with the increase in the level of series;
2. When the trend is multiplicative, data exhibits exponential growth/decay;
3. The damped trend slows down both additive and multiplicative trends;
4. It is practically impossible to distinguish additive and multiplicative seasonality if the level of series does not change because the amplitude of seasonality will be constant in both cases (compare ETS(A,N,A) and ETS(A,N,M)).

Figure \@ref(fig:ETSTaxonomyMultiplicative) demonstrates a similar plot for the multiplicative error models.

```{r ETSTaxonomyMultiplicative, echo=FALSE, warning=FALSE, fig.cap="Time series corresponding to the multiplicative error ETS models"}
# Prepare the canvas
par(mfcol=c(5,3),mar=c(2,3,2,1))
# The matrix that corresponds to the i
ylimValues <- matrix(c(1:15),5,3)
for(i in 16:30){
  if(i>15){
    ylimRow <- which(ylimValues == i-15,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]+15]))
  }
  else{
    ylimRow <- which(ylimValues == i,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]]))
  }
  plot(generatedData[[i]], main=paste0("ETS(",modelsList[i],")"),ylim=ylim,ylab="")
}
```

The plots in Figure \@ref(fig:ETSTaxonomyMultiplicative) show roughly the same idea as the additive case, the main difference being that the variance of the error increases with the increase of the level of the data -- this becomes clearer on ETS(M,A,N) and ETS(M,M,N) data. This property is called heteroscedasticity in statistics, and @Hyndman2008b argue that the main benefit of the multiplicative error models is in capturing this feature.

We will discuss the most important ETS family members in the following chapters. Note that not all the models in this taxonomy are sensible, and some are typically ignored entirely (this applies mainly to models with a mixture of additive and multiplicative components). ADAM implements the entire taxonomy, but we need to be aware of the potential issues with some of models, and we will discuss what to expect from them in different situations in the next Chapters.


## Mathematical models in the ETS taxonomy {#ETSTaxonomyMaths}
I hope that it becomes more apparent to the reader how the ETS framework is built upon the idea of time series decomposition (from Section \@ref(tsComponents)). By introducing different components, defining their types, and adding the equations for their update, we can construct models that would work better in capturing the key features of the time series. The equations discussed in Section \@ref(tsComponents) represent so-called "measurement" or "observation" equations of the ETS models. But we should also consider the potential change in components over time. The "transition" or "state" equations are supposed to reflect this change: they explain how the level, trend or seasonal components evolve.

As discussed in Section \@ref(ETSTaxonomy), given different types of components and their interactions, we end up with 30 models in the taxonomy. Tables \@ref(tab:ETSAdditiveError) and \@ref(tab:ETSMultiplicativeError) summarise mathematically all 30 ETS models shown graphically on Figures \@ref(fig:ETSTaxonomyAdditive) and \@ref(fig:ETSTaxonomyMultiplicative), presenting formulae for measurement and transition equations.

```{r ETSAdditiveError, echo=FALSE}
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\begin{landscape}')
}
# T="N"
etsAdditiveTable <- c("$\\begin{aligned} &y_{t} = l_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\alpha \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\alpha \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1}}
    \\end{aligned}$",
# T="A"
    "$\\begin{aligned} &y_{t} = l_{t-1} + b_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned}
      &y_{t} = l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1}) s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + b_{t-1}}
    \\end{aligned}$",
# T="Ad"
    "$\\begin{aligned} &y_{t} = l_{t-1} + \\phi b_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = \\phi b_{t-1} + \\beta \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = \\phi b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + \\phi b_{t-1}}
    \\end{aligned}$",
# T="M"
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}}
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}}
    \\end{aligned}$",
# T="Md"
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}}
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}}
    \\end{aligned}$")
etsAdditiveTable <- matrix(etsAdditiveTable, 5, 3, byrow=TRUE,
                           dimnames=list(c("No trend","Additive","Additive damped",
                                           "Multiplicative","Multiplicative damped"),
                                         c("Nonseasonal","Additive","Multiplicative")))
kableTable <- kableExtra::kable(etsAdditiveTable, escape=FALSE, caption="Additive error ETS models",
                                col.names=c("Nonseasonal","Additive","Multiplicative"))
kable_styling(kableTable, font_size=12, protect_latex=TRUE, latex_options="scale_down")
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\end{landscape}')
}
```


```{r ETSMultiplicativeError, echo=FALSE}
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\begin{landscape}')
}
# T="N"
etsMultiplicativeTable <- c("$\\begin{aligned} &y_{t} = l_{t-1}(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1}(1 + \\alpha \\epsilon_t)
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} s_{t-m}(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\
      &s_t = s_{t-m}(1 + \\gamma \\epsilon_t)
    \\end{aligned}$",
# T="A"
    "$\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1})(1 + \\epsilon_t) \\\\
      &l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\
      &l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} + \\beta (l_{t-1} + b_{t-1}) \\epsilon_t \\\\
      &s_t = s_{t-m} (1 + \\gamma \\epsilon_t)
    \\end{aligned}$",
# T="Ad"
    "$\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1})(1 + \\epsilon_t) \\\\
      &l_t = (l_{t-1} + \\phi b_{t-1})(1 + \\alpha \\epsilon_t) \\\\
      &b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} (1 + \\alpha \\epsilon_t) \\\\
      &b_t = \\phi b_{t-1} + \\beta (l_{t-1} + \\phi b_{t-1}) \\epsilon_t \\\\
      &s_t = s_{t-m}(1 + \\gamma \\epsilon_t)
    \\end{aligned}$",
# T="M"
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} (1 + \\beta \\epsilon_t)
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} s_{t-m} (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\\\
      &s_t = s_{t-m} (1 + \\gamma \\epsilon_t)
    \\end{aligned}$",
# T="Md"
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi (1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1}^\\phi (1 + \\beta \\epsilon_t)
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} b_{t-1}^\\phi + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi \\left(1 + \\alpha \\epsilon_t\\right) \\\\
      &b_t = b_{t-1}^\\phi \\left(1 + \\beta \\epsilon_t\\right) \\\\
      &s_t = s_{t-m} \\left(1 + \\gamma \\epsilon_t\\right)
    \\end{aligned}$")
etsMultiplicativeTable <- matrix(etsMultiplicativeTable, 5, 3, byrow=TRUE,
                           dimnames=list(c("No trend","Additive","Additive damped",
                                           "Multiplicative","Multiplicative damped"),
                                         c("Nonseasonal","Additive","Multiplicative")))
kableTable <- kableExtra::kable(etsMultiplicativeTable, escape=FALSE, caption="Multiplicative error ETS models",
                                col.names=c("Nonseasonal","Additive","Multiplicative"))
kable_styling(kableTable, font_size=12, protect_latex=TRUE, latex_options="scale_down")
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\end{landscape}')
}
```

From a statistical point of view, formulae in Tables \@ref(tab:ETSAdditiveError) and \@ref(tab:ETSMultiplicativeError) correspond to the "true models" (see Section \@ref(modelsMethods)), they explain the models underlying potential data, but when it comes to their construction and estimation, the $\epsilon_t$ is substituted by the estimated $e_t$ (which is calculated differently depending on the error type), and time series components and smoothing parameters are also replaced by their estimates (e.g. $\hat{\alpha}$ instead of $\alpha$). However, if the values of these models' parameters were known, it would be possible to produce point forecasts and conditional h steps ahead expectations from these models, which are summarised in Table \@ref(tab:ETSModelsForecasts) with the following elements:

- Conditional one step ahead expectation $\mu_{y,t} \equiv \mu_{y,t|t-1}$;
- Multiple steps ahead point forecast $\hat{y}_{t+h}$;
- Conditional multiple steps ahead expectation $\mu_{y,t+h|t}$;

In the case of the additive error models, the point forecasts correspond to the expectations only when the expectation of the error term is zero, i.e. $\text{E}(\epsilon_t)=0$. In contrast, in the case of the multiplicative models, the condition is changed to $\text{E}(1+\epsilon_t)=1$.

::: remark
**Not all point forecasts of ETS models correspond to conditional expectations**. This issue applies to the models with multiplicative trend and/or multiplicative seasonality. This is because the ETS model assumes that different states are correlated (they have the same source of error), and as a result, multiple steps ahead values (when h>1) of states introduce products of error terms. So, the conditional expectations in these cases might not have analytical forms ("n.c.f." in Table \@ref(tab:ETSModelsForecasts) stands for "No Closed Form"), and when working with these models, simulations might be required. This does not apply to the one-step-ahead forecasts, for which all the classical formulae work. This issue is discussed in Section \@ref(pureMultiplicativeExpectationAndVariance).
:::

```{r ETSModelsForecasts, echo=FALSE}
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\begin{landscape}')
}
# T="N"
etsAdditiveTable <- c("$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{aligned}$",
# T="A"
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} + b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} + h b_t \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} + b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + h b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = (l_{t-1} + b_{t-1}) s_{t-m} \\\\
      &\\hat{y}_{t+h} = \\left(l_{t} + h b_{t-1}\\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{aligned}$",
# T="Ad"
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_t \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} \\\\
      &\\hat{y}_{t+h} = \\left(l_{t} + \\sum_{j=1}^h \\phi^j b_t \\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{aligned}$",
# T="M"
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} b_t^h \\\\
      &\\mu_{y,t+h|t} \\text{ -- n.c.f. for } h>1
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^h + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ -- n.c.f. for } h>1
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} b_{t-1} s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^h s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ -- n.c.f. for } h>1
    \\end{aligned}$",
# T="Md"
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi \\\\
      &\\hat{y}_{t+h} = l_{t} b_t^{\\sum_{j=1}^h \\phi^j} \\\\
      &\\mu_{y,t+h|t} \\text{ -- n.c.f. for } h>1
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ -- n.c.f. for } h>1
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ -- n.c.f. for } h>1
    \\end{aligned}$")
etsAdditiveTable <- matrix(etsAdditiveTable, 5, 3, byrow=TRUE,
                           dimnames=list(c("No trend","Additive","Additive damped",
                                           "Multiplicative","Multiplicative damped"),
                                         c("Nonseasonal","Additive","Multiplicative")))
kableTable <- kableExtra::kable(etsAdditiveTable, escape=FALSE, caption="Point forecasts and expectations of ETS models. n.c.f. stands for \"No Closed Form\".",
                                col.names=c("Nonseasonal","Additive","Multiplicative"))
kable_styling(kableTable, font_size=12, protect_latex=TRUE, latex_options="scale_down")
if (knitr:::is_latex_output()) {
    knitr::asis_output('\\end{landscape}')
}
```

The multiplicative error models have the same one step ahead expectations and point forecasts as the additive error ones. However, due to the multiplication by the error term, the multiple steps ahead conditional expectations between the two types of models might differ, specifically for the multiplicative trend and multiplicative seasonal models. These values do not have closed forms and can only be obtained via simulations.

Although there are 30 potential ETS models, not all of them are sensible. So, Rob Hyndman has reduced the pool of models under consideration in the `ets()` function of `forecast` package to the following 19: ANN, AAN, AAdN, ANA, AAA, AAdA, MNN, MAN, MAdN, MNA, MAA, MAdA, MNM, MAM, MAdM, MMN, MMdN, MMM, MMdM. In addition, the multiplicative trend models are unstable in data with outliers, so they are switched off in the `ets()` function by default, which reduces the pool of models further to the first 15.

The `es()` function from the `smooth` package implements the conventional ETS, supporting all 30 models and implementing some features, discussed in the original @Hyndman2008b book (e.g. explanatory variables and cumulative over the lead time forecasts).


## ETS and SES {#SESandETS}
Taking a step back, in this Section we discuss one of the basic ETS models, the local level model and the exponential smoothing method related to it.

### ETS(A,N,N)
There have been several tries to develop statistical models underlying SES, and we know now that the model has underlying ARIMA(0,1,1) [@Muth1960], local level MSOE [Multiple Source of Error, @Muth1960] and SSOE [Single Source of Error, @Snyder1985] models. According to @Hyndman2002, the ETS(A,N,N) model also underlies the SES method. To see the connection and to get to it from SES, we need to recall two things: how in general, the actual value relates to the forecast error and the fitted value, and the error correction form of SES from Subsection \@ref(SESEC):
\begin{equation}
  \begin{aligned}
    & y_t = \hat{y}_{t} + e_t \\
    & \hat{y}_{t+1} = \hat{y}_{t} + \hat{\alpha} e_{t}
  \end{aligned} .
  (\#eq:ETSANNDerivation01)
\end{equation}
In order to get to the SSOE state space model for SES, we need to substitute $\hat{y}_t=\hat{l}_{t-1}$, implying that the fitted value is equal to the level of the series:
\begin{equation}
  \begin{aligned}
    & y_t = \hat{l}_{t-1} + e_t \\
    & \hat{l}_{t} = \hat{l}_{t-1} + \hat{\alpha} e_{t}
  \end{aligned} .
  (\#eq:ETSANN01)
\end{equation}
If we now substitute the sample estimates of level, smoothing parameter and forecast error by their population values, we will get the ETS(A,N,N), which was discussed in Section \@ref(ETSTaxonomyMaths):
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + \epsilon_t \\
    & l_t = l_{t-1} + \alpha \epsilon_t
  \end{aligned} ,
  (\#eq:ETSANN)
\end{equation}
where, as we know from Section \@ref(tsComponents), $l_t$ is the level of the data, $\epsilon_t$ is the error term, and $\alpha$ is the smoothing parameter. Note that we use $\alpha$ without the "hat" symbol, which implies that there is a "true" value of the parameter (which could be obtained if we had all the data in the world or just knew it for some reason). The main benefit of having the model \@ref(eq:ETSANN) instead of just the method \@ref(eq:SESErrorCorrection) is in having a flexible framework, which allows adding other components, selecting the most appropriate ones (Section \@ref(ETSSelection)), consistently estimating parameters (Chapter \@ref(ADAMETSEstimation)), producing prediction intervals (Section \@ref(ADAMForecastingPI)) etc. In a way, this model is the basis of ADAM.

In order to see the data that corresponds to the ETS(A,N,N) we can use `sim.es()` function from the `smooth` package. Here are several examples with different smoothing parameters values:

```{r}
# list with generated data
y <- vector("list",6)
# Parameters for DGP
initial <- 1000
meanValue <- 0
sdValue <- 20
alphas <- c(0.1,0.3,0.5,0.75,1,1.5)
# Go through all alphas and generate respective data
for(i in 1:length(alphas)){
  y[[i]] <- sim.es("ANN", 120, 1, 12, persistence=alphas[i],
                   initial=initial, mean=meanValue, sd=sdValue)
}
```

The generated data can be plotted the following way:

```{r DGPetsANNExample, fig.cap="Local level data corresponding to ETS(A,N,N) model with different smoothing parameters."}
par(mfrow=c(3,2), mar=c(2,2,2,1))
for(i in 1:6){
  plot(y[[i]], main=paste0("alpha=",y[[i]]$persistence),
       ylim=initial+c(-500,500))
}
```

This simple simulation shows that the smoothing parameter in ETS(A,N,N) controls the variability in the data (Figure \@ref(fig:DGPetsANNExample)): the higher $\alpha$ is, the higher variability is and less predictable the data becomes. With the higher values of $\alpha$, the level changes faster, leading to increased uncertainty about the future values.

When it comes to the application of this model to the data, the conditional h steps ahead mean corresponds to the point forecast and is equal to the last observed level:
\begin{equation}
    \mu_{y,t+h|t} = \hat{y}_{t+h} = l_{t} ,
  (\#eq:ETSANNForecast)
\end{equation}
this holds because it is assumed (see Section \@ref(assumptions)) that $\mathrm{E}(\epsilon_t)=0$, which implies that the conditional h steps ahead expectation of the level in the model is (from the second equation in \@ref(eq:ETSANN)):
\begin{equation}
  \mathrm{E}(l_{t+h-1}|t) = l_t + \mathrm{E}(\alpha\sum_{j=1}^{h-2}\epsilon_{t+j}|t) = l_t .
  (\#eq:ETSANNForecasthStepsAhead)
\end{equation}

Here is an example of a forecast from ETS(A,N,N) with automatic parameter estimation using `es()` function from the `smooth` package:

```{r ETSANNExample, fig.cap="An example of ETS(A,N,N) applied to the data generated from the same model."}
# Generate the data
y <- sim.es("ANN", 120, 1, 12, persistence=0.3, initial=1000)
# Apply ETS(A,N,N) model
esModel <- es(y$data, "ANN", h=12, holdout=TRUE)
# Produce forecasts
esModel |> forecast(h=12, interval="pred") |>
    plot(main=paste0("ETS(A,N,N) with alpha=",
                     round(esModel$persistence,4)))
```

As we see from Figure \@ref(fig:ETSANNExample), the true smoothing parameter is 0.3, but the estimated one is not exactly 0.3, which is expected because we deal with an in-sample estimation. Also, notice that with such a smoothing parameter, the prediction interval widens with the increase of the forecast horizon. If the smoothing parameter were lower, the bounds would not increase, but this might not reflect the uncertainty about the level correctly. Here is an example with $\alpha=0.01$ on the same data (Figure \@ref(fig:ETSANNExamplealpha01))

```{r ETSANNExamplealpha01, fig.cap="ETS(A,N,N) with $\\hat{\\alpha}=0.01$ applied to the data generated from the same model with $\\alpha=0.3$."}
es(y$data, "ANN", h=12,
   holdout=TRUE, persistence=0.01) |>
    forecast(h=12, interval="pred") |>
    plot(main="ETS(A,N,N) with alpha=0.01")
```

Figure \@ref(fig:ETSANNExamplealpha01) shows that the prediction interval does not expand, but at the same time is wider than needed, and the forecast is biased -- the model does not keep up to the fast-changing time series. So, it is essential to correctly estimate the smoothing parameters not only to approximate the data but also to produce a less biased point forecast and a more appropriate prediction interval.

### ETS(M,N,N) {#ETSMNNSES}
@Hyndman2008b demonstrate that there is another ETS model, underlying SES. It is the model with multiplicative error, which is formulated in the following way, as mentioned in Chapter \@ref(ETSTaxonomyMaths):
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1}(1 + \epsilon_t) \\
    & l_t = l_{t-1}(1 + \alpha \epsilon_t)
  \end{aligned} ,
  (\#eq:ETSMNN)
\end{equation}
where $(1+\epsilon_t)$ corresponds to the $\varepsilon_t$ discussed in Section \@ref(tsComponents). In order to see the connection of this model with SES, we need to revert to the estimation of the model on the data again:
\begin{equation}
  \begin{aligned}
    & y_{t} = \hat{l}_{t-1}(1 + e_t) \\
    & \hat{l}_t = \hat{l}_{t-1}(1 + \hat{\alpha} e_t)
  \end{aligned} .
  (\#eq:ETSMNNEstimation)
\end{equation}
where one step ahead forecast is (Section \@ref(ETSTaxonomyMaths)) $\hat{y}_t = \hat{l}_{t-1}$ and $e_t=\frac{y_t -\hat{y}_t}{\hat{y}_t}$. Substituting these values in second equation of \@ref(eq:ETSMNNEstimation) we obtain:
\begin{equation}
    \hat{y}_{t+1} = \hat{y}_t \left(1 + \hat{\alpha} \frac{y_t -\hat{y}_t}{\hat{y}_t} \right)
  (\#eq:ETSMNNEstimation2)
\end{equation}
Finally, opening the brackets, we get the SES in the form similar to \@ref(eq:SESErrorCorrection):
\begin{equation}
    \hat{y}_{t+1} = \hat{y}_t + \hat{\alpha} (y_t -\hat{y}_t).
  (\#eq:ETSMNNEstimation4)
\end{equation}

This example again demonstrates the difference between a forecasting method and a model. When we use SES, we ignore the distributional assumptions, which restricts the usefulness of the method. When we work with a model, we assume a specific structure, which on the one hand, makes it more restrictive, but on the other hand, gives it additional features. The main ones in the case of ETS(M,N,N) in comparison with ETS(A,N,N) are:

1. The variance of the actual values in ETS(M,N,N) increases with the increase of the level $l_{t}$. This allows modelling heteroscedasticity situation in the data;
2. If $(1+\epsilon_t)$ is always positive, then the ETS(M,N,N) model will always produce only positive forecasts (both point and interval). This makes this model applicable in principle to the data with low levels.

An alternative to \@ref(eq:ETSMNN) would be the ETS(A,N,N) model \@ref(eq:ETSANN) applied to the data in logarithms (assuming that the data we work with is always positive), implying that:
\begin{equation}
  \begin{aligned}
    & \log y_{t} = l_{t-1} + \epsilon_t \\
    & l_t = l_{t-1} + \alpha \epsilon_t
  \end{aligned} .
  (\#eq:ETSANNLogs)
\end{equation}
However, to produce forecasts from \@ref(eq:ETSANNLogs), exponentiation is needed, making the application of the model more difficult than needed. The ETS(M,N,N), on the other hand, does not rely on exponentiation, making it more practical and safe in cases when the model produces very high values (e.g. `exp(1000)` returns infinity in R).

Finally, the conditional h steps ahead mean of ETS(M,N,N) corresponds to the point forecast and is equal to the last observed level, but only if $\mathrm{E}(1+\epsilon_t)=1$:
\begin{equation}
    \mu_{y,t+h|t} = \hat{y}_{t+h} = l_{t} .
  (\#eq:ETSMNNForecast)
\end{equation}

And here is an example with the ETS(M,N,N) data (Figure \@ref(fig:ETSMNNExample)):

```{r ETSMNNExample, fig.cap="ETS(M,N,N) model applied to the data generated from the same model."}
y <- sim.es("MNN", 120, 1, 12, persistence=0.3, initial=1000)
esModel <- es(y$data, "MNN", h=12, holdout=TRUE)
forecast(esModel, h=12, interval="pred") |>
    plot(main=paste0("ETS(M,N,N) with alpha=",
                     round(esModel$persistence,4)))
```

Conceptually, the data in Figure \@ref(fig:ETSMNNExample) looks very similar to the one from ETS(A,N,N) (Figure \@ref(fig:ETSANNExample)), but demonstrating the changing variance of the error term with the change of the level. The model itself would in general produce wider prediction interval than its additive error counterpart keeping the same smoothing parameter.


## Several examples of ETS and related exponential smoothing methods {#ETSExamples}
There are other exponential smoothing methods, which include more components, as discussed in Section \@ref(tsComponents). This includes but is not limited to: Holt's [@Holt2004b, originally proposed in 1957], Holt-Winter's [@Winters1960], multiplicative trend [@Pegels1969], Damped trend (originally proposed by @Roberts1982 and then picked up by @Gardner1985a), Damped trend Holt-Winters [@Gardner1989] and damped multiplicative trend [@Taylor2003] methods. We will not discuss them here one by one, as we will not use them further in this monograph. Instead, we will focus on the ETS models underlying them.

We already understand that there can be different components in time series and that they can interact either in an additive or a multiplicative way, which gives us the taxonomy discussed in Section \@ref(ETSTaxonomy). This section considers several examples of ETS models and their relations to the conventional exponential smoothing methods.

### ETS(A,A,N) {#ETSAAN}
This is also sometimes known as the local trend model and is formulated similar to ETS(A,N,N), but with addition of the trend equation. It underlies **Holt's method** [@Ord1997]:
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + b_{t-1} + \epsilon_t \\
    & l_t = l_{t-1} + b_{t-1} + \alpha \epsilon_t \\
    & b_t = b_{t-1} + \beta \epsilon_t
  \end{aligned} ,
  (\#eq:ETSAAN)
\end{equation}
where $\beta$ is the smoothing parameter for the trend component. It has a similar idea as ETS(A,N,N): the states evolve, and the speed of their change depends on the values of $\alpha$ and $\beta$. The trend is not deterministic in this model: both the intercept and the slope change over time. The higher the smoothing parameters are, the more uncertain the level and the slope will be, thus, the higher the uncertainty about the future values is.

Here is an example of the data that corresponds to the ETS(A,A,N) model:

```{r ETSAANExample, fig.cap="Data generated from ETS(A,A,N) model."}
sim.es("AAN", 120, 1, 12, persistence=c(0.3,0.1),
       initial=c(1000,20), mean=0, sd=20) |>
    plot()
```

The series in Figure \@ref(fig:ETSAANExample) demonstrates a trend that changes over time. If we need to produce forecasts for this data, we will capture the dynamics of the trend component via ETS(A,A,N) and then use the last values for the several steps ahead prediction. 

The point forecast h steps ahead from this model is a straight line with a slope $b_t$ (as shown in Table \@ref(tab:ETSModelsForecasts) from Section \@ref(ETSTaxonomyMaths)):
\begin{equation}
    \mu_{y,t+h|t} = \hat{y}_{t+h} = l_{t} + h b_t.
  (\#eq:ETSAANForecast)
\end{equation}
This becomes apparent if one takes the conditional expectations E$(l_{t+h}|t)$ and E$(b_{t+h}|t)$ in the second and third equations of \@ref(eq:ETSAAN) and then inserts them in the measurement equation. Graphically it will look as shown in Figure \@ref(fig:ETSAANExampleForecast):

```{r ETSAANExampleForecast, fig.cap="ETS(A,A,N) and a point forecast produced from it."}
es(y, h=10) |>
    plot(which=7)
```

If you want to experiment with the model and see how its parameters influence the fit and forecast, you can use the following R code:

```{r eval=FALSE}
es(y$data, h=10, persistence=c(0.2,0.1)) |>
    plot(which=7)
```
where `persistence` is the vector of smoothing parameters (first $\hat\alpha$, then $\hat\beta$). By changing their values, we will make the model less/more responsive to the changes in the data.

In a special case, ETS(A,A,N) corresponds to the Random Walk with drift (Subsection \@ref(RWWithDrift)), when $\beta=0$ and $\alpha=1$:
\begin{equation*}
  \begin{aligned}
    & y_{t} = l_{t-1} + b_{t-1} + \epsilon_t \\
    & l_t = l_{t-1} + b_{t-1} + \epsilon_t \\
    & b_t = b_{t-1}
  \end{aligned} ,
\end{equation*}
or
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + b_{0} + \epsilon_t \\
    & l_t = l_{t-1} + b_{0} + \epsilon_t
  \end{aligned} .
  (\#eq:ETSAANRWDrift)
\end{equation}
The connection between the two models becomes apparent, when substituting the first equation into the second one in \@ref(eq:ETSAANRWDrift) to obtain:
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + b_{0} + \epsilon_t \\
    & l_t = y_{t}
  \end{aligned} 
\end{equation}
or after inserting the second equation into the first one:
\begin{equation}
  y_{t} = y_{t-1} + b_{0} + \epsilon_t .
\end{equation}

Finally, in another special case, when both $\alpha$ and $\beta$ are zero, the model reverts to the Global Trend discussed in Subsection \@ref(GlobalTrend) and becomes:
\begin{equation*}
  \begin{aligned}
    & y_{t} = l_{t-1} + b_{t-1} + \epsilon_t \\
    & l_t = l_{t-1} + b_{t-1} \\
    & b_t = b_{t-1}
  \end{aligned} ,
\end{equation*}
or
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + b_{0} + \epsilon_t \\
    & l_t = l_{t-1} + b_{0}
  \end{aligned} ,
  (\#eq:ETSAANGT2)
\end{equation}
The main difference of \@ref(eq:ETSAANGT2) with the Global Trend model \@ref(eq:GlobalTrend) is that the latter explicitly includes the trend component $t$ and is formulated in one equation, while the former splits equation into two parts and has an explicit level component. They both fit the data in the same way and produce the same forecasts when $a_0 = l_0$ and $a_1 = b_0$ in the Global Trend model.

Due to its flexibility, the ETS(A,A,N) model is considered as one of the good benchmarks in the case of trended time series.


### ETS(A,Ad,N) {#ETSAAdN}
This is the model that underlies **Damped trend method** [@Roberts1982; @Gardner1985a]:
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + \phi b_{t-1} + \epsilon_t \\
    & l_t = l_{t-1} + \phi b_{t-1} + \alpha \epsilon_t \\
    & b_t = \phi b_{t-1} + \beta \epsilon_t
  \end{aligned} ,
  (\#eq:ETSAAdN)
\end{equation}
where $\phi$ is the dampening parameter, typically lying between 0 and 1. If it is equal to zero, the model reduces to ETS(A,N,N), \@ref(eq:ETSANN). If it is equal to one, it becomes equivalent to ETS(A,A,N), \@ref(eq:ETSAAN). The dampening parameter slows down the trend, making it non-linear. An example of data that corresponds to ETS(A,Ad,N) is provided in Figure \@ref(fig:ETSAAdNExample).

```{r ETSAAdNExample, fig.cap="An example of ETS(A,Ad,N) data."}
sim.es("AAdN", 120, 1, 12, persistence=c(0.3,0.1),
       initial=c(1000,20), phi=0.95, mean=0, sd=20) |>
    plot()
```

Visually it is typically challenging to distinguish ETS(A,A,N) from ETS(A,Ad,N) data. So, some other model selection techniques are recommended (see Section \@ref(ETSSelection)).

The point forecast from this model is a bit more complicated than the one from ETS(A,A,N) (see Section \@ref(ETSTaxonomyMaths)):
\begin{equation}
    \mu_{y,t+h|t} = \hat{y}_{t+h} = l_{t} + \sum_{j=1}^h \phi^j b_t.
  (\#eq:ETSAAdNForecast)
\end{equation}
It corresponds to the slowing down trajectory, as shown in Figure \@ref(fig:ETSAAdNExampleForecast).

```{r ETSAAdNExampleForecast, fig.cap="A point forecast from ETS(A,Ad,N).", echo=FALSE}
es(y, h=10) |>
    plot(which=7)
```

As can be seen in Figure \@ref(fig:ETSAAdNExampleForecast), the forecast trajectory from the ETS(A,Ad,N) has a slowing down element in it. This is because of the $\phi=0.95$ in our example.


### ETS(A,A,M) {#ETSAAMModel}
Finally, this is an exotic model with additive error and trend, but multiplicative seasonality. It can be considered exotic because of the misalignment of the error and seasonality. Still, we list it here, because it underlies the **Holt-Winters method** [@Winters1960]:
\begin{equation}
  \begin{aligned}
    & y_{t} = (l_{t-1} + b_{t-1}) s_{t-m} + \epsilon_t \\
    & l_t = l_{t-1} + b_{t-1} + \alpha \frac{\epsilon_t}{s_{t-m}} \\
    & b_t = b_{t-1} + \beta \frac{\epsilon_t}{s_{t-m}} \\
    & s_t = s_{t-m} + \gamma \frac{\epsilon_t}{l_{t-1}+b_{t-1}}
  \end{aligned} ,
  (\#eq:ETSAAM)
\end{equation}
where $s_t$ is the seasonal component and $\gamma$ is its smoothing parameter. This is one of the potentially unstable models, which due to the mix of components, might produce unreasonable forecasts because the seasonal component might become negative, while it should always be positive. Still, it might work on the strictly positive high-level data. Figure \@ref(fig:ETSAAMExample) shows how the data for this model can look.

```{r ETSAAMExample, fig.cap="An example of ETS(A,A,M) data."}
sim.es("AAM", 120, 1, 4, persistence=c(0.3,0.05,0.2),
       initial=c(1000,20), initialSeason=c(0.9,1.1,0.8,1.2),
            mean=0, sd=20) |>
    plot()
```

The data in Figure \@ref(fig:ETSAAMExample) exhibits an additive trend with increasing seasonal amplitude, which are the two characteristics of the model.

Finally, the point forecast from this model builds upon the ETS(A,A,N), introducing seasonal component:
\begin{equation}
    \hat{y}_{t+h} = (l_{t} + h b_t) s_{t+h-m\lceil\frac{h}{m}\rceil},
  (\#eq:ETSAAAForecast)
\end{equation}
where $\lceil\frac{h}{m}\rceil$ is the rounded up value of the fraction in the brackets, implying that the seasonal index from the previous period is used (e.g. previous January value). The point forecast from this model is shown in Figure \@ref(fig:ETSAAMExampleForecast).

```{r ETSAAMExampleForecast, fig.cap="A point forecast from ETS(A,A,M).", echo=FALSE}
esModel <- es(y, h=10)
plot(esModel, 7)
```


::: remark
The point forecasts produced from this model do not correspond to the conditional expectations. This will be discussed in Section \@ref(ADAMETSMixedModelsGroup3).
:::

@Hyndman2008b argue that in ETS models, the error term should be aligned with the seasonal component because it is difficult to motivate why the amplitude of seasonality should increase with the increase of level, while the variability of the error term should stay the same. So, they recommend using ETS(M,A,M) instead of ETS(A,A,M) if you deal with positive high volume data. This is a reasonable recommendation, but keep in mind that both models might break if you deal with the low volume data and the trend component becomes negative.


## ETS basic assumptions and principles
Several assumptions need to hold for the conventional ETS models to work properly. Some of them have already been discussed in Section \@ref(assumptions), and we will come back to them in Chapter \@ref(diagnostics). What is important in our context is that the conventional ETS assumes that the error term $\epsilon_t$ follows the Normal distribution with zero mean and variance $\sigma^2$. There are several points related to this that need to be clarified:

1. If the mean was not equal to zero then, for example, the level models would act as models with drift (see Subsection \@ref(RWWithDrift)). This implies that the architecture of the model should change, and the conventional ETS models cannot be efficiently applied to such data. Furthermore, correctly estimating such models would not be straightforward, because ETS exhibits "pull to centre" effect, where the predicted value gets closer to the actual one based on the forecast error of the model. As a result, it would be challenging to capture the non-zero mean of the error term. So, **the zero mean assumption is essential** for such dynamic models as ETS. For the multiplicative error models, this translates into $\mathrm{E}(1+\epsilon_t)=1$.
2. As it is well known, the Normal distribution is defined for positive, negative and zero values. This is not a big deal for additive models, which assume that the actual value can be anything, and it is not an issue for the multiplicative models when we deal with high-level positive data (e.g. thousands of units): in this case, the variance of the error term will be small enough, protecting it from becoming negative. However, if the level of the data is low, then the variance of the error term can be large enough for the normally distributed error to cover negative values. And if the error term $1+\epsilon_t$ becomes negative the model will break. This is a potential flaw in the conventional ETS model with the multiplicative error term. So, what the standard multiplicative error ETS model actually assumes, is that **the data we work with is strictly positive and has high-level values**.

Based on the assumption of normality of error term, the ETS model can be estimated via the maximisation of likelihood (discussed in Section \@ref(ADAMETSEstimationLikelihood)), which is equivalent to the minimisation of the mean squared one step ahead forecast error $e_t$. Note that in order to apply the ETS models to the data, we also need to know the initial values of components, $\hat{l}_0, \hat{b}_0, \hat{s}_{-m+2}, \hat{s}_{-m+3}, \dots, \hat{s}_{0}$. The conventional approach is to estimate these values together with the smoothing parameters during likelihood maximisation. As a result, the optimisation might involve a large number of parameters. In addition, the variance of the error term is considered as an additional parameter in the maximum likelihood estimation, so the number of parameters for different models is (here "*" stands for any type):

1. ETS(\*,N,N) -- 3 parameters: $\hat{l}_0$, $\hat{\alpha}$ and $\hat{\sigma}^2$;
2. ETS(\*,\*,N) -- 5 parameters: $\hat{l}_0$, $\hat{b}_0$, $\hat{\alpha}$, $\hat{\beta}$ and $\hat{\sigma}^2$;
3. ETS(\*,\*d,N) -- 6 parameters: $\hat{l}_0$, $\hat{b}_0$, $\hat{\alpha}$, $\hat{\beta}$, $\hat{\phi}$ and $\hat{\sigma}^2$;
4. ETS(\*,N,\*) -- 4+m-1 parameters: $\hat{l}_0$, $\hat{s}_{-m+2}, \hat{s}_{-m+3}, \dots, \hat{s}_{0}$, $\hat{\alpha}$, $\hat{\gamma}$ and $\hat{\sigma}^2$;
5. ETS(\*,\*,\*) -- 6+m-1 parameters: $\hat{l}_0$, $\hat{b}_0$, $\hat{s}_{-m+2}, \hat{s}_{-m+3}, \dots, \hat{s}_{0}$, $\hat{\alpha}$, $\hat{\beta}$, $\hat{\gamma}$ and $\hat{\sigma}^2$;
6. ETS(\*,\*d,\*) -- 7+m-1 parameters: $\hat{l}_0$, $\hat{b}_0$, $\hat{s}_{-m+2}, \hat{s}_{-m+3}, \dots, \hat{s}_{0}$, $\hat{\alpha}$, $\hat{\beta}$, $\hat{\gamma}$, $\hat{\phi}$ and $\hat{\sigma}^2$.

::: remark
In the case of seasonal models, we typically make sure that the initial seasonal indices are normalised, so we only need to estimate $m-1$ of them, the last one is calculated based on the linear combination of the others. For example, for the additive seasonality, it is equal to $-\sum_{j=1}^{m-1} s_j$ because the sum of all the indices should be equal to zero.
:::

When it comes to selecting the most appropriate model, the conventional approach involves the application of all models to the data and then selecting the most appropriate of them based on an information criterion (Section \@ref(ETSSelection)). This was first proposed by @Hyndman2002. In the case of the conventional ETS model, this relies on the likelihood value of Normal distribution used in the estimation of the model.

Finally, the assumption of normality is used to generate prediction interval from the model. There are typically two ways of doing that:

1. Calculating the variance of multiple steps ahead forecast error and then using it for the intervals construction (see Chapter 6 of @Hyndman2008b or Sections \@ref(pureAdditiveExpectationAndVariance) and \@ref(ADAMForecastingMoments));
2. Generating thousands of possible paths for the components of the series and the actual values and then taking the necessary quantiles for the prediction intervals (see Section \@ref(ADAMForecastingSimulations) for details).

Typically, (1) is applied for the pure additive models, where the closed forms for the variances are known, and the assumption of normality holds for several steps ahead. In some special cases of mixed models, approximations for variances work on short horizons [see Section 6.4 of @Hyndman2008b]. But in all the other cases, (2) should be used, despite being typically slower than (1) and producing bounds that differ slightly from run to run due to randomness.


## State space form of ETS {#ETSConventionalModel}
One of the main advantages of the ETS model is its state space form, which gives it the flexibility. @Hyndman2008b use the following general formulation of the model with the first equation called "measurement equation" and the second one "transition equation":
\begin{equation}
  \begin{aligned}
  & {y}_{t} = w(\mathbf{v}_{t-1}) + r(\mathbf{v}_{t-1}) \epsilon_t \\
  & \mathbf{v}_{t} = f(\mathbf{v}_{t-1}) + g(\mathbf{v}_{t-1}) \epsilon_t
  \end{aligned},
  (\#eq:ETSConventionalStateSpace)
\end{equation}
where $\mathbf{v}_t$ is the state vector, containing the components of series (level, trend and seasonal), $w(\cdot)$ is the measurement, $r(\cdot)$ is the error, $f(\cdot)$ is the transition and $g(\cdot)$ is the persistence functions. Depending on the types of components these functions can have different values.

::: remark
Note that @Hyndman2008b use $\mathbf{x}_{t}$ instead of $\mathbf{v}_{t}$. I do not use their notation because I find it confusing: $x$ is typically used to denote explanatory variables (especially in regression context), and when we use $\mathbf{x}_{t}$ in ETS context, the states are sometimes perceived as related to explanatory variables. However, this is not the case. They relate more to time-varying parameters rather than exogenous variables in the regression context. This aspect is discussed on an example of seasonal model in Section \@ref(ETSXDynamicCategories).
:::

1. Depending on the types of trend and seasonality $w(\mathbf{v}_{t-1})$ will be equal either to the addition or multiplication of components. The special cases were presented in tables \@ref(tab:ETSAdditiveError) and \@ref(tab:ETSMultiplicativeError) in the Section \@ref(ETSTaxonomyMaths). For example, in case of ETS(M,M,M) it is: $w(\mathbf{v}_{t-1}) = l_{t-1} b_{t-1} s_{t-m}$;
2. If the error is additive, then $r(\mathbf{v}_{t-1})=1$, otherwise (in case of multiplicative error) it is $r(\mathbf{v}_{t-1})=w(\mathbf{v}_{t-1})$. For example, for ETS(M,M,M) it will be $r(\mathbf{v}_{t-1}) = l_{t-1} b_{t-1} s_{t-m}$;
3. The transition function $f(\cdot)$ will produce values depending on the types of trend and seasonality and will correspond to the first parts in the Tables \@ref(tab:ETSAdditiveError) and \@ref(tab:ETSMultiplicativeError) of the transition equations (dropping the error term). This function records how components interact with each other and how they change from one observation to another (thus the term "transition"). An example is the ETS(M,M,M) model, for which the transition function will produce three values: $l_{t-1}b_{t-1}$, $b_{t-1}$ and $s_{t-m}$ respectively for the level, trend and seasonal components. So, if we drop the persistence function $g(\cdot)$ and the error term $\epsilon_t$ for a moment, the second equation in \@ref(eq:ETSConventionalStateSpace) will be:
\begin{equation}
  \begin{aligned}
  & {l}_{t} = l_{t-1} b_{t-1} \\
  & b_t = b_{t-1} \\
  & s_t = s_{t-m}
  \end{aligned},
  (\#eq:ETSMMMTransitionFunction)
\end{equation}
4. Finally, the persistence function will differ from one model to another, but in some special cases it can either be: $g(\mathbf{v}_{t-1})=\mathbf{g}$ if all components are additive, or $g(\mathbf{v}_{t-1})=f(\mathbf{v}_{t-1})\mathbf{g}$ if they are all multiplicative. $\mathbf{g}$ is the vector of smoothing parameters, called in the ETS context the "persistence vector". An example of persistence function is the ETS(M,M,M) model, for which it is: $l_{t-1}b_{t-1}\alpha$, $b_{t-1}\beta$ and $s_{t-m}\gamma$ respectively for the level, trend and seasonal components. Uniting this with the transition function \@ref(eq:ETSMMMTransitionFunction) we get the equation from the table \@ref(tab:ETSMultiplicativeError):
\begin{equation}
  \begin{aligned}
  & {l}_{t} = l_{t-1} b_{t-1} + l_{t-1} b_{t-1} \alpha\epsilon_t \\
  & b_t = b_{t-1} + b_{t-1} \beta\epsilon_t \\
  & s_t = s_{t-m} + s_{t-m} \gamma\epsilon_t
  \end{aligned},
  (\#eq:ETSMMMTransitionEquation01)
\end{equation}
which can be simplified to:
\begin{equation}
  \begin{aligned}
  & {l}_{t} = l_{t-1}b_{t-1} (1+\alpha\epsilon_t)\\
  & b_t = b_{t-1} (1+\beta\epsilon_t)\\
  & s_t = s_{t-m} (1+\gamma\epsilon_t)
  \end{aligned} .
  (\#eq:ETSMMMTransitionEquation)
\end{equation}
Some of mixed models have more complicated persistence function values. For example, for ETS(A,A,M) it is:
\begin{equation}
  g(\mathbf{v}_{t-1}) =
  \begin{pmatrix}
      \alpha \frac{1}{s_{t-m}} \\
      \beta \frac{1}{s_{t-m}} \\
      \gamma \frac{1}{l_{t-1} + b_{t-1}}
  \end{pmatrix} ,
\end{equation}
which results in the state space model discussed in subsection \@ref(ETSAAMModel).

The compact form \@ref(eq:ETSConventionalStateSpace) is thus convenient, it underlies all the 30 ETS models discussed in the Sections \@ref(ETSTaxonomy) and \@ref(ETSTaxonomyMaths). Unfortunately, they cannot be used directly for deriving conditional values, so they are needed just for the general understanding of ETS and can be used in programming.


### Pure additive state space model {#ETSConventionalModelAdditive}
The more useful state space model in ETS framework is the pure additive one, which, based on the discussion above, is formulated as:
\begin{equation}
  \begin{aligned}
  & {y}_{t} = \mathbf{w}' \mathbf{v}_{t-1} + \epsilon_t \\
  & \mathbf{v}_{t} = \mathbf{F} \mathbf{v}_{t-1} + \mathbf{g} \epsilon_t
  \end{aligned},
  (\#eq:ETSConventionalStateSpaceAdditive)
\end{equation}
where $\mathbf{w}$ is the measurement vector, showing how the components form the structure, $\mathbf{F}$ is the transition matrix, showing how components interact with each other and change over time (e.g. level is equal to the previous level plus trend) and $\mathbf{g}$ is the persistence vector, containing smoothing parameters. The conditional expectation and variance can be derived based on \@ref(eq:ETSConventionalStateSpaceAdditive), together with bounds on the smoothing parameters for any model that can be formulated in this way. And, as mentioned above, any pure additive ETS model can be written in the form \@ref(eq:ETSConventionalStateSpaceAdditive), which means that all of them have relatively simple analytical formulae for the statistics mentioned above. For example, the h steps ahead conditional expectation and variance of the model \@ref(eq:ETSConventionalStateSpaceAdditive) are [@Hyndman2008b, Chapter 6]:
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \mathrm{E}(y_{t+h}|t) = & \mathbf{w}^\prime \mathbf{F}^{h-1} \mathbf{v}_{t} \\
    \sigma^2_{h} = \mathrm{V}(y_{t+h}|t) = & \left(\mathbf{w}^\prime \mathbf{F}^{j-1} \mathbf{g} \mathbf{g}^\prime \mathbf{F}^\prime \mathbf{w} + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSConventionalStateSpaceAdditiveExpectation)
\end{equation}
where $\sigma^2$ is the variance of the error term. Formulae \@ref(eq:ETSConventionalStateSpaceAdditiveExpectation) can be used for generation of respective moments from any pure additive ETS model. The conditional expectation can also be used for some mixed models as an approximation for the true conditional mean.


## Parameters bounds {#ETSParametersBounds}
While many practitioners and academics accept that the smoothing parameters of exponential smoothing methods should lie between zero and one, this is not entirely true for the ETS models. There are, in fact, several possible restrictions on smoothing parameters, and it is worth discussing them separately:

1. **Classical or conventional** bounds are $\alpha, \beta, \gamma \in (0,1)$. The idea behind them originates from the simple exponential smoothing method (Section \@ref(SES)), where it is logical to restrict the bounds with this region because then the smoothing parameters regulate what weight the actual value $y_t$ will have and what weight will be assigned to the predicted one $\hat{y}_t$. @Hyndman2008b showed that this condition is sometimes too loose and, in other cases, is too restrictive to some ETS models. @Brenner1968 was one of the first to show that the bounds are wider than this region for many exponential smoothing methods. Still, the conventional restriction is the most often used in practice, just because it is easy to interpret.

2. **Usual or traditional** bounds are those that satisfy the set of the following equations:
\begin{equation}
  \begin{aligned}
  &\alpha \in [0, 1)\\
  &\beta \in [0, \alpha) \\
  &\gamma \in [0, 1-\alpha)
  \end{aligned},
  (\#eq:ETSUsualBounds)
\end{equation}
This set of restrictions guarantees that the weights decline over time exponentially (see Section \@ref(whyExponential)), and the ETS models have the property of "averaging" the values over time. In the lower boundary condition, the model's components become deterministic, and we can say that they are calculated as the global averages of the values over time.

3. **Admissible** bounds, satisfying stability condition. The idea here is that the most recent observation should have a higher weight than the older ones, which is regulated via the smoothing parameters. However, in this case, we do not impose the restriction of exponential decay of weights on the models, so they can oscillate or decay harmonically as long as their absolute values decrease over time. The condition is more complicated mathematically than the previous two. It will be discussed later in the monograph for the pure additive models (see Section \@ref(ADAMETSPureAdditive)), but here are several examples for bounds, satisfying this condition [from Chapter 10 of @Hyndman2008b]:

- ETS(A,N,N): $\alpha \in (0, 2)$;
- ETS(A,A,N): $\alpha \in (0, 2); \beta \in (0, 4-2\alpha)$;
- ETS(A,N,A): $\alpha \in \left(\frac{-2}{m-1}, 2-\gamma\right); \gamma \in (\max(-m\alpha, 0), 2-\alpha)$;

As you see, the admissible bounds are much wider than the conventional and usual ones. In fact, in this case, smoothing parameters can become either negative or greater than one in some cases for some models, which is hard to interpret but might indicate that the data is difficult to predict. Furthermore, the admissible bounds correspond to the restrictions of the parameters for ARIMA models, underlying some of pure additive ETS models (see discussion in Section \@ref(ARIMAandETS)). In a way, they are more natural for the ETS models than the other two because they follow the formulation and arise naturally. However, their usage in practice has been met with mixed success, with only a handful of papers using them instead of (1) or (2) [e.g. @Gardner2008; mention that they appear in some cases and @Snyder2017 use them in their model].

<!-- The admissible bounds are calculated based on the discount matrix in the R code, which will be discussed in the context of pure additive ADAM ETS models in Chapter \@ref(ADAMETSPureAdditive). -->
