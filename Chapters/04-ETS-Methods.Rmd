# Conventional Exponential Smoothing {#ETSConventional}

In this chapter, we discuss the most popular exponential smoothing methods and their connection with the ETS model. We do not go into many details of how the methods were originally derived and how to work with them. Instead, we focus on their connection with ETS and then on the main ideas behind the conventional ETS.

The reader interested in the topic of the history of exponential smoothing, how it was developed and what papers contributed towards the development of the field, can refer to the reviews of [@Gardner1985] and [@Gardner2006]. They summmarise all the progress in the area of exponential smoothing up until 1985 and then until 2006.


## Simple Exponential Smoothing {#SES}
We start our discussion of exponential smoothing with the original Simple Exponential Smoothing (SES) forecasting method, which was formulated by [@Brown1956]:
\begin{equation}
  \hat{y}_{t+1} = \hat{\alpha} {y}_{t} + (1 - \hat{\alpha}) \hat{y}_{t},
  (\#eq:BrownMethod)
\end{equation}
where $\hat{\alpha}$ is the smoothing parameter, defined by analyst and which is typically restricted with (0, 1) region (this region is actually arbitrary and we will see later what is the correct one). This is one of the simplest forecasting methods, and the smoothing parameter in it is typically interpretted as a weight between the actual value and the one-step-ahead predicted one. If the smoothing parameter is close to zero, then more weight is given to the previous fitted value $\hat{y}_{t}$ and the new information is neglected. When it is close to one, then mainly the actual value ${y}_{t}$ is taken into account. By changing the smoothing parameter value, the forecaster can decide how to approximate the data and filter out the noise.

Also, notice that this is a recursive method, meaning that there needs to be some starting point $\hat{y}_1$ in order to apply \@ref(eq:BrownMethod) to the existing data. Different initialisation and estimation methods for SES have been discussed in the literature, but the sttate of the art one is to estimate $\hat{\alpha}$ and $\hat{y}_{1}$ together by minimising some loss function. Typically [MSE](#errorMeasures) is used as one, minimising the one step ahead forecast error.

### Examples of application
Here is an example of how this method works on different time series. We start with generating a stationary series and using `es()` function from `smooth` package. Although it implements the ETS model, we will see later the connection between SES and ETS(A,N,N). We start with the stationary time series and $\hat{\alpha}=0$:
```{r SESExample1}
y <- rnorm(100,100,10)
ourModel <- es(y, model="ANN", h=10, persistence=0)
plot(ourModel, 7, main=paste0("SES with alpha=",ourModel$persistence))
```

The SES works well in this case, capturing the deterministic level of the series and filtering out the noise. In this case, it works like a global average applied to the data. As mentioned before, the method is flexible, so if we have a level shift in the data and increase the smoothing parameter, it will adapt and get to the new level. Here is an example:

```{r SESExample2}
y <- c(rnorm(50,100,10),rnorm(50,130,10))
ourModel <- es(y, model="ANN", h=10, persistence=0.1)
plot(ourModel, 7, main=paste0("SES with alpha=",ourModel$persistence))
```

With $\hat{\alpha}=0.1$, it manages to get to the new level, but now the method starts adapting to noise a little bit - it follows the peaks and troughs and repeats them, but with much smaller magnitude. If we increase the smoothing parameter, it will react to the changes much faster, but it will also react more to noise:
```{r echo=FALSE}
par(mfcol=c(3,1))
ourModel <- es(y, model="ANN", h=10, persistence=0.2)
plot(ourModel, 7, main=paste0("SES with alpha=",ourModel$persistence))
ourModel <- es(y, model="ANN", h=10, persistence=0.3)
plot(ourModel, 7, main=paste0("SES with alpha=",ourModel$persistence))
ourModel <- es(y, model="ANN", h=10, persistence=0.5)
plot(ourModel, 7, main=paste0("SES with alpha=",ourModel$persistence))
```

If we set $\hat{\alpha}=1$, we will end up with Naive forecasting method, which is not appropriate for our example:
```{r echo=FALSE}
par(mfcol=c(1,1))
ourModel <- es(y, model="ANN", h=10, persistence=1)
plot(ourModel, 7, main=paste0("SES with alpha=",ourModel$persistence))
```

So, when working with SES, we need to make sure that the reasonable smoothing parameter is selected. This can be done automatically via minimising the MSE:
```{r SESExample3}
ourModel <- es(y, model="ANN", h=10, loss="MSE")
plot(ourModel, 7, main=paste0("SES with alpha=",round(ourModel$persistence,3)))
```

This approach won't guarantee that we will get the most appropriate $\hat{\alpha}$, but it has been shown in the literature that the optimisation of smoothing parameter on average leads to improvements in terms of forecasting.


### Why "exponential"? {#whyExponential}
Now, **why is it called "exponential"**? Because the same method can be represented in a different form, if we substitute $\hat{y}_{t}$ in right hand side \@ref(eq:BrownMethod) by the formula for the previous step:
\begin{equation}
  \hat{y}_{t+1} = \hat{\alpha} {y}_{t} + (1 -\hat{\alpha}) \left( \hat{\alpha} {y}_{t-1} + (1 -\hat{\alpha}) \hat{y}_{t-1} \right).
  (\#eq:BrownMethodExponential1)
\end{equation}
By repeating this procedure for each $\hat{y}_{t-1}$, $\hat{y}_{t-2}$ etc, we will obtain a different form of the method:
\begin{equation}
  \hat{y}_{t+1} = \hat{\alpha} {y}_{t} + \hat{\alpha} (1 -\hat{\alpha}) {y}_{t-1} + \hat{\alpha} (1 -\hat{\alpha})^2 {y}_{t-2} + \dots + (1 -\hat{\alpha})^t \hat{y}_1 
  (\#eq:BrownMethodExponential2)
\end{equation}
or equivalently:
\begin{equation}
  \hat{y}_{t+1} = \hat{\alpha} \sum_{j=0}^{t-1} (1 -\hat{\alpha})^j {y}_{t-j} + (1 -\hat{\alpha})^t \hat{y}_1 .
  (\#eq:BrownMethodExponential3)
\end{equation}
Now each actual observation has a weight infront of it. For the most recent observation it is $\hat{\alpha}$, for the previous one it is $\hat{\alpha} (1 -\hat{\alpha})$, then $\hat{\alpha} (1 -\hat{\alpha})^2$ etc. These form the geometric series or an exponential curve. Here, for example, how it looks, when $\hat{\alpha} =0.25$ for a sample of 30 observations:
```{r}
plot(0.25*(1-0.25)^c(0:30), type="b", xlab="Time lags", ylab="Weights")
```

This explains the name "exponential". The term "smoothing" comes from the idea that the parameter $\hat{\alpha}$ should be selected so that the method smoothes the original time series.

### Error correction form of SES
Finally, an alternative form of SES is known as error correction form and involves some simple permutations, taking that $e_t=y_t-\hat{y}_t$ is the one step ahead forecast error:
\begin{equation}
  \hat{y}_{t+1} = \hat{y}_{t} + \hat{\alpha} e_{t}.
  (\#eq:SESErrorCorrection)
\end{equation}
In this form, the smoothing parameter $\hat{\alpha}$ regulates how much the model reacts to the forecast error. In this interpretation it no longer needs to be restricted with (0, 1) region, but we would still typically want it to be closer to zero, in order to filter out the noise, not to adapt to it.

As you see, this is a very simple method. It is easy to explain it to practitioners and it is very easy to implement in practice. However, this is just a [forecasting method](#intro), so it just gives a way of generating point forecasts, but does not explain where the error comes from and how to generate prediction intervals.


## SES and ETS {#SESandETS}
### ETS(A,N,N)
There have been several tries to develop statistical models, underlying SES, and we know now that it has underlying ARIMA(0,1,1), local level MSOE (Multiple Source of Error) model [@Muth1960] and SSOE (Single Source of Error) model [@Snyder1985]. According to [@Hyndman2002], the ETS(A,N,N) model also underlies the SES method. It can be formulated in the following way, [as discussed earlier](#ETSTaxonomyMaths):
\begin{equation}
  \begin{split}
    y_{t} &= l_{t-1} + \epsilon_t \\
    l_t &= l_{t-1} + \alpha \epsilon_t
  \end{split} ,
  (\#eq:ETSANN)
\end{equation}
where, as we know from [the previous section](#tsComponents), $l_t$ is the level of the data, $\epsilon_t$ is the error term and $\alpha$ is the smoothing parameter. Note that we use $\alpha$ without the "hat" symbol, which implies that there is a "true" value of the parameter (which could be obtained if we had all the data in the world or just knew it for some reason). It is easy to show that ETS(A,N,N) underlies SES. In order to see this, we need to take move towards estimation phase and use $\hat{l}_{t-1}=l_{t-1}$ and move to estimates $\hat{\alpha}$ and $e_t$ (the estimate of the error term $\epsilon_t$):
\begin{equation}
  \begin{split}
    y_{t} &= \hat{l}_{t-1} + e_t \\
    \hat{l}_t &= \hat{l}_{t-1} + \hat{\alpha} e_t
  \end{split} ,
  (\#eq:ETSANNEstimation)
\end{equation}
and also take that $\hat{y}_t=\hat{l}_{t-1}$:
\begin{equation}
  \begin{split}
    y_{t} &= \hat{y}_{t} + e_t \\
    \hat{y}_{t} &= \hat{y}_{t-1} + \hat{\alpha} e_{t-1}
  \end{split} .
  (\#eq:ETSANNEstimation2)
\end{equation}
Inserting the second equation in the first one and substituting $y_t$ with $\hat{y}_t+e_t$ we get:
\begin{equation}
    \hat{y}_t+e_t = \hat{y}_{t-1} + \hat{\alpha} e_{t-1} + e_t ,
  (\#eq:ETSANNEstimation3)
\end{equation}
cancelling out $e_t$ and shifting everything by one step ahead, we obtain the error correction form \@ref(eq:SESErrorCorrection) of SES.

But now, the main benefit of having the model \@ref(eq:ETSANN) instead of just the method \@ref(eq:SESErrorCorrection) is in having a flexible framework, which allows adding other components, selecting the most appropriate ones, estimating parameters in a [consistent](#intro) way, producing prediction intervals etc.

In order to see the data that corresponds to the ETS(A,N,N) we can use `sim.es()` function from smooth package. Here are several examples with different smoothing parameters:
```{r}
y <- vector("list",6)
initial <- 1000
meanValue <- 0
sdValue <- 20
alphas <- c(0.1,0.3,0.5,0.75,1,1.5)
for(i in 1:length(alphas)){
  y[[i]] <- sim.es("ANN", 120, 1, 12, persistence=alphas[i], initial=initial, mean=meanValue, sd=sdValue)
}

par(mfcol=c(3,2))
for(i in 1:6){
  plot(y[[i]], main=paste0("alpha=",y[[i]]$persistence), ylim=initial+c(-500,500))
}
```

This simple simulation shows that the higher $\alpha$ is, the higher variability is in the data and less predictable the data becomes. This is related with the higher values of $\alpha$, the level changes faster, also leading to the increased uncertainty about the future values of the level in the data.

When it comes to the application of this model to the data, the point forecast corresponds to the conditional h steps ahead mean and is equal to the last observed level:
\begin{equation}
    \mu_{y,t+h|t} = \hat{y}_{t+h} = l_{t} ,
  (\#eq:ETSANNForecast)
\end{equation}
this holds because it is [assumed](#assumptions) that $\text{E}(\epsilon_t)=0$, which implies that the conditional h steps ahead expectation of the level in the model is $\text{E}(l_{t+h}|t)=l_t+\alpha\sum_{j=1}^{h-1}\epsilon_{t+j} = l_t$.

Here is an example with automatic parameter estimation in ETS(A,N,N) using `es()` function from `smooth` package:
```{r ETSANNExample}
y <- sim.es("ANN", 120, 1, 12, persistence=0.3, initial=1000)
es(y$data, "ANN", h=12, interval=TRUE, holdout=TRUE, silent=FALSE)
```

As we see, the true smoothing parameter is 0.3, but the estimated one is not exactly 0.3, which is expected, because we deal with an in-sample estimation. Also, notice that with such a high smoothing parameter, the prediction interval is widening with the increase of the forecast horizon. If the smoothing parameter would be lower, then the bounds would not increase, but this might not reflect the uncertainty about the level correctly. Here is an example with $\alpha=0.01$:
```{r ETSANNExamplealpha0.1}
ourModel <- es(y$data, "ANN", h=12, interval=TRUE, holdout=TRUE, silent=FALSE, persistence=0.01)
```
In this case, the prediction interval is wider than needed and the forecast is biased - the model does not keep up to the fast changing time series. So, it is important to correctly estimate the smoothing parameters not only to approximate the data, but also to produce less biased point forecast and more appropriate prediction interval.

### ETS(M,N,N)
@Hyndman2008b also demonstrate that there is another ETS model, underlying SES. It is the model with multiplicative error, which is formulated in the following way, [as mentioned in a previous chapter](#ETSTaxonomyMaths):
\begin{equation}
  \begin{split}
    y_{t} &= l_{t-1}(1 + \epsilon_t) \\
    l_t &= l_{t-1}(1 + \alpha \epsilon_t)
  \end{split} ,
  (\#eq:ETSMNN)
\end{equation}
where $(1+\epsilon_t)$ corresponds to the $\varepsilon_t$ discussed in [the previous section](#tsComponents). In order to see the connection of this model with SES, we need to revert to the estimation of the model on the data again:
\begin{equation}
  \begin{split}
    y_{t} &= \hat{l}_{t-1}(1 + e_t) \\
    \hat{l}_t &= \hat{l}_{t-1}(1 + \hat{\alpha} e_t)
  \end{split} ,
  (\#eq:ETSMNNEstimation)
\end{equation}
where $\hat{y}_t = \hat{l}_{t-1}$ and $e_t=\frac{y_t - \hat{y}_t}{\hat{y}_t}$. Substituting these values in \@ref(eq:ETSMNNEstimation) we obtain:
\begin{equation}
  \begin{split}
    y_{t} &= \hat{y}_t (1 + e_t) \\
    \hat{y}_{t+1} &= \hat{y}_t \left(1 + \hat{\alpha} \frac{y_t - \hat{y}_t}{\hat{y}_t} \right)
  \end{split} .
  (\#eq:ETSMNNEstimation2)
\end{equation}
Substituting $y_t$ with $\hat{y}_t(1+e_t)$, shifting the indices one step ahead and inserting the second equation to the first one, we get:
\begin{equation}
    \hat{y}_{t+1} = \hat{y}_t \left(1 + \hat{\alpha} \frac{y_t - \hat{y}_t}{\hat{y}_t} \right).
  (\#eq:ETSMNNEstimation3)
\end{equation}
Finally, opening the brackets, we get the SES in the form similar to \@ref(eq:SESErrorCorrection):
\begin{equation}
    \hat{y}_{t+1} = \hat{y}_t + \hat{\alpha} (y_t - \hat{y}_t).
  (\#eq:ETSMNNEstimation4)
\end{equation}

This example demonstratesonce again the difference between the forecasting method and the forecasting model. When we use SES, we ignore the distributional assumptions, which restricts the usage of the method. The main features of ETS(M,N,N) model in comparison with ETS(A,N,N) are:

1. The variance of the actual values in ETS(M,N,N) increases with the increase of the level $l_{t}$. This allows modelling heteroscedasticity situation in the data;
2. If $(1+\epsilon_t)$ is always positive, then the ETS(M,N,N) model will always produce only positive forecasts (both point and interval). This makes this model applicable to the data with low level;

An alternative to \@ref(eq:ETSMNN) would be the model \@ref(eq:ETSANN) applied to the data in logarithms (assuming that the data we work with is always positive). However, the ETS(M,N,N) does not rely on exponentiation of the values, making it safe in cases, when very high values are produced by the model (e.g. `exp(1000)` returns infinity in R).

Finally, the point forecast of ETS(M,N,N) corresponds to the conditional h steps ahead mean and is equal to the last observed level, but only if $\text{E}(1+\epsilon_t)=1$:
\begin{equation}
    \mu_{y,t+h|t} = \hat{y}_{t+h} = l_{t} .
  (\#eq:ETSMNNForecast)
\end{equation}

And here is an example with the ETS(M,N,N) data, which is very similar to the ETS(A,N,N) one:
```{r ETSMNNExample}
y <- sim.es("MNN", 120, 1, 12, persistence=0.3, initial=1000)
ourModel <- es(y$data, "MNN", h=12, interval=TRUE, holdout=TRUE, silent=FALSE)
ourModel
```


## Several examples of exponential smoothing methods and ETS {#ETSExamples}
There are other exponential smoothing, which include more components, as discussed in [the previous section](#tsComponents). This includes but is not restricted with: Holt's [@Holt2004b, originally proposed in 1957], Holt-Winter's [@Winters1960], multiplicative trend [@Pegels1969], Damped trend (originally proposed by @Roberts1982 and then picked up by @Gardner1985a), Damped trend Holt-Winters [@Gardner1989] and damped multiplicative trend methods [@Taylor2003]. We will not discuss them here one by one, as we will not use them further in this textbook. More importantly, all of them have underlying ETS models, so we will focus on them instead.

We already understand that there can be different components in time series and that they can interact with each other either in an additive or a multiplicative way, which gives us the [aforementioned taxonomy](#ETSTaxonomy). Here are several examples of ETS models with several components and their relations to the conventional exponential smoothing methods.

### ETS(A,A,N) {#ETSAAN}
This is also sometimes known as local trend model and is formulated as ETS(A,N,N), but with addition of the trend equation. It underlies Holt's method:
\begin{equation}
  \begin{split}
    y_{t} &= l_{t-1} + b_{t-1} + \epsilon_t \\
    l_t &= l_{t-1} + b_{t-1} + \alpha \epsilon_t \\
    b_t &= b_{t-1} + \beta \epsilon_t
  \end{split} ,
  (\#eq:ETSAAN)
\end{equation}
where $\beta$ is the smoothing parameter for the trend component. It has a similar idea as ETS(A,N,N): the states evolve over time, and the speed of their change depends on the values of $\alpha$ and $\beta$. 

Here is an example of the data that corresponds to the ETS(A,A,N) model:
```{r}
y <- sim.es("AAN", 120, 1, 12, persistence=c(0.3,0.1), initial=c(1000,20), mean=0, sd=20)
plot(y)
```

As you might notice, the trend is not deterministic in this model: both the intercept and the slope change over time. The higher the smoothing parameters are, the more uncertain it is, what the level and the slope will be, thus higher the uncertainty about the future values is.

The point forecast h steps ahead from this model is a straight line with a slope $b_t$:
\begin{equation}
    \mu_{y,t+h|t} = \hat{y}_{t+h} = l_{t} + h b_t.
  (\#eq:ETSAANForecast)
\end{equation}
This becomes apparent if one takes the conditional expectations E$(l_{t+h}|t)$ and E$(b_{t+h}|t)$ in the second and third equations of \@ref(eq:ETSAAN). Graphically it will look like this:
```{r}
esModel <- es(y, h=10, silent=FALSE)
```

If you want to experiment with the model and see how its parameters influence the fit and forecast, you can use the following R code:
```{r eval=FALSE}
esModel <- es(y$data, h=10, silent=FALSE, persistence=c(0.2,0.1))
```
where `persistence` is the vector of smoothing parameters (first $\alpha$, then $\beta$).


### ETS(A,Ad,N) {#ETSAAdN}
This is the model that underlies Damped trend method [@Roberts1982]:
\begin{equation}
  \begin{split}
    y_{t} &= l_{t-1} + \phi b_{t-1} + \epsilon_t \\
    l_t &= l_{t-1} + \phi b_{t-1} + \alpha \epsilon_t \\
    b_t &= \phi b_{t-1} + \beta \epsilon_t
  \end{split} ,
  (\#eq:ETSAAdN)
\end{equation}
where $\phi$ is the dampening parameter, typically lying between 0 and 1. If it is equal to zero, then the model \@ref(eq:ETSAAdN) reduces to \@ref(eq:ETSANN). If it is equal to one, then it becomes equivalent to \@ref(eq:ETSAAN). The dampening parameter slows down the trend, making it non-linear. The typical data that corresponds to ETS(A,Ad,N) is:
```{r}
y <- sim.es("AAdN", 120, 1, 12, persistence=c(0.3,0.1), initial=c(1000,20), phi=0.95, mean=0, sd=20)
plot(y)
```

The point forecast from this model is a bit more complicated:
\begin{equation}
    \mu_{y,t+h|t} = \hat{y}_{t+h} = l_{t} + \sum_{j=1}^h \phi^j b_t.
  (\#eq:ETSAANForecast)
\end{equation}
It corresponds to the slowing down trajectory:
```{r}
esModel <- es(y, h=10, silent=FALSE)
```


### ETS(A,A,M)
Finaly, this is an exotic model with additive error and trend, but multiplicative seasonality. Still, we list it here, because it underlies the Holt-Winters method [@Winters1960]:
\begin{equation}
  \begin{split}
    y_{t} &= (l_{t-1} + b_{t-1}) s_{t-m} + \epsilon_t \\
    l_t &= l_{t-1} + b_{t-1} + \alpha \frac{\epsilon_t}{s_{t-m}} \\
    b_t &= b_{t-1} + \beta \frac{\epsilon_t}{s_{t-m}} \\
    s_t &= s_{t-m} + \gamma \frac{\epsilon_t}{l_{t-1}+b_{t-1}}
  \end{split} ,
  (\#eq:ETSAAM)
\end{equation}
where $s_t$ is the seasonal component and $\gamma$ is its smoothing parameter. This is one of the potentially unstable models, which due to the mix of components might produce unreasonable forecasts. Still, it might work on the strictly positive high level data. Here how the data for this model can look like:
```{r}
y <- sim.es("AAM", 120, 1, 4, persistence=c(0.3,0.1,0.2), initial=c(1000,20), initialSeason=c(0.9,1.1,0.8,1.2), mean=0, sd=20)
plot(y)
```

Finally, the point forecast from this model are based on the ETS(A,A,N):
\begin{equation}
    \hat{y}_{t+h} = (l_{t} + h b_t) s_{t+h-m\lceil\frac{h}{m}\rceil},
  (\#eq:ETSAANForecast)
\end{equation}
where $\lceil\frac{h}{m}\rceil$ is the rounded up value of the fraction in the brackets.

::: remark
The point forecasts produced from this model do not correspond to the conditional expectations. This was discussed in the [previous chapter](#ETSTaxonomyMaths).
:::

## ETS assumptions, estimation and selection
There are several assumptions that need to hold for the conventional ETS models in order for them to be used in practice appropriately. Some of them have already been discussed in [one of the previous sections](#assumptions), and we will not discuss them here again. What is important in our context is that the conventional ETS assumes that the error term $\epsilon_t$ follows normal distribution with zero mean and variance $\sigma^2$. As discussed [earlier](#distributions), normal distribution is defined for positive, negative and zero values. This is not a big deal for additive models, which assume that the actual value can be anything. And it is not an issue for the multiplicative models, when we deal with high level positive data (e.g. thousands of units): the variance of the error term will be small enough for the $\epsilon_t$ not to become less than minus one. However, if the level of the data is low, then the variance of the error term can be large enough for the normally distributed error to cover negative values, less than minus one. This implies that the error term $1+\epsilon_t$ can become negative, and the model will break. This is a potential flaw in the conventional ETS model with the multiplicative error term. So, what the conventional multiplicative error ETS model assumes in fact is that **the data we work with is strictly positive and has high level values**.

Based on the assumption of normality of error term, the ETS model can be estimated via the maximisation of likelihood, which is equivalent to the minimisation of the mean squared forecast error $e_t$. Note that in order to apply the ETS models to the data, we also need to know the initial values of components, $\hat{l}_0, \hat{b}_0, \hat{s}_{-m+2}, \hat{s}_{-m+3}, \dots, \hat{s}_{0}$. The conventional approach is to estimate these values together with the smoothing parameters during the maximisation of likelihood. As a result, the optimisation might involve a large number of parameters. In addition, the variance of the error term is considered as an additional parameter in the maximum likelihood estimation, so the number of parameters for different models is (here "*" stands for any type):

1. ETS(\*,N,N) - 3 parameters: $\hat{l}_0$, $\hat{\alpha}$ and $\hat{\sigma}^2$;
2. ETS(\*,\*,N) - 5 parameters: $\hat{l}_0$, $\hat{b}_0$, $\hat{\alpha}$, $\hat{\beta}$ and $\hat{\sigma}^2$;
3. ETS(\*,\*d,N) - 6 parameters: $\hat{l}_0$, $\hat{b}_0$, $\hat{\alpha}$, $\hat{\beta}$, $\hat{\phi}$ and $\hat{\sigma}^2$;
4. ETS(\*,N,\*) - 4+m-1 parameters: $\hat{l}_0$, $\hat{s}_{-m+2}, \hat{s}_{-m+3}, \dots, \hat{s}_{0}$, $\hat{\alpha}$, $\hat{\gamma}$ and $\hat{\sigma}^2$;
5. ETS(\*,\*,\*) - 6+m-1 parameters: $\hat{l}_0$, $\hat{b}_0$, $\hat{s}_{-m+2}, \hat{s}_{-m+3}, \dots, \hat{s}_{0}$, $\hat{\alpha}$, $\hat{\beta}$, $\hat{\gamma}$ and $\hat{\sigma}^2$;
6. ETS(\*,\*d,\*) - 7+m-1 parameters: $\hat{l}_0$, $\hat{b}_0$, $\hat{s}_{-m+2}, \hat{s}_{-m+3}, \dots, \hat{s}_{0}$, $\hat{\alpha}$, $\hat{\beta}$, $\hat{\gamma}$, $\hat{\phi}$ and $\hat{\sigma}^2$.

Note that in case of seasonal models we typically make sure that the initial seasonality indices are normalised, so we only need to estimate $m-1$ of them, the last one is calculated based on the linear combination of the others.

When it comes to the selection of the most appropriate model, the conventional approach involves the application of all models to the data and then selecting the most appropriate of them based on [an information cretiria](#modelSelection). In case of the conventional ETS model, this relies on the likelihood value of normal distribution, used in the estimation of the model.

Finally, the assumption of normality is used for the generation of prediction intervals from the model. There are typically two ways of doing that:

1. Calculating the variance of multiple steps ahead forecast error and then using it for the intervals calculation;
2. Generating thousands of possible paths for the components of the series and the actual values and then taking the necessary quantiles for the prediction intervals;

Typically, (1) is applied for pure additive models, where the closed forms for the variances are known and the assumption of normality holds for several steps ahead. In some special cases of mixed models, there are approximations for variances that work on small horizons. But in all the other cases (2) should be used, despite being typically slower than (1) and producing bounds that differ from run to run due to randomness.


## State space form of ETS
One of the main advantages of the ETS model is its state space form, which gives it the flexibility. We would need to revert to linear algebra in this section in order to understand how any ETS model can be presented in a compact state space form.

@Hyndman2008b use the following general formulation of the model with the first equation called "measurement equation" and the second one "transition equation":
\begin{equation}
  \begin{aligned}
  {y}_{t} = &w(\mathbf{v}_{t-1}) + r(\mathbf{v}_{t-1}) \epsilon_t \\
  \mathbf{v}_{t} = &f(\mathbf{v}_{t-1}) + g(\mathbf{v}_{t-1}) \epsilon_t
  \end{aligned},
  (\#eq:ETSConventionalStateSpace)
\end{equation}
where $\mathbf{v}_t$ is the state vector, containing the components of series (level, trend and seasonal), $w(\cdot)$ is the measurement,$r(\cdot)$ is the error, $f(\cdot)$ is the transition and $g(\cdot)$ is the persistence functions. Depending on the types of components these functions can have different values:

1. Depending on the types of trend and seasonality $w(\mathbf{v}_{t-1})$ will be equal either to the addition or multiplication of components. The special cases were presented in tables \@ref(tab:ETSAdditiveError) and \@ref(tab:ETSMultiplicativeError) in the [ETS Taxonomy section](#ETSTaxonomyMaths). For example, in case of ETS(M,M,M) it is: $w(\mathbf{v}_{t-1}) = l_{t-1} b_{t-1} s_{t-m}$;
2. If the error is additive, then $r(\mathbf{v}_{t-1})=1$, otherwise (in case of multiplicative error) it is $r(\mathbf{v}_{t-1})=w(\mathbf{v}_{t-1})$;
3. The transition function $f(\cdot)$ will produce values depending on the types of trend and seasonality and will correspond to the first parts in the tables \@ref(tab:ETSAdditiveError) and \@ref(tab:ETSMultiplicativeError) of the transition equations (dropping the error term). This function records how components interact with each other and how they change from one observation to another (thus the term "transition"). An example is the ETS(M,M,M) model, for which the transition function will produce three values: $l_{t-1}b_{t-1}$, $b_{t-1}$ and $s_{t-m}$ respectively for the level, trend and seasonal components. So, if we drop the persistence function $g(\cdot)$ and the error term $\epsilon_t$ for a moment, the second equation in \@ref(eq:ETSConventionalStateSpace) will be:
\begin{equation}
  \begin{aligned}
  {l}_{t} = &l_{t-1}b_{t-1} \\
  b_t = &b_{t-1} \\
  s_t = &s_{t-m}
  \end{aligned},
  (\#eq:ETSMMMTransitionFunction)
\end{equation}
4. Finally, the persistence function will differ from one model to another, but in some special cases it can either be: $g(\mathbf{v}_{t-1})=\mathbf{g}$, if the error term is additive and $g(\mathbf{v}_{t-1})=f(\mathbf{v}_{t-1})\mathbf{g}$ if it is multiplicative. $\mathbf{g}$ is the vector of smoothing parameters, called in the ETS context the "persistence vector". An example of persistence function is the ETS(M,M,M) model, for which it is: $l_{t-1}b_{t-1}\alpha$, $b_{t-1}\beta$ and $s_{t-m}\gamma$ respectively for the level, trend and seasonal components. Uniting this with the transition function \@ref(eq:ETSMMMTransitionFunction) we get the equation from the table \@ref(tab:ETSMultiplicativeError):
\begin{equation}
  \begin{aligned}
  {l}_{t} = &l_{t-1}b_{t-1} (1+\alpha\epsilon_t)\\
  b_t = &b_{t-1} (1+\beta\epsilon_t)\\
  s_t = &s_{t-m} (1+\gamma\epsilon_t)
  \end{aligned},
  (\#eq:ETSMMMTransitionEquation)
\end{equation}

The compact form \@ref(eq:ETSConventionalStateSpace) is thus comfortable to work with and underlies all the 30 ETS models discussed in the sections [5.1](#ETSTaxonomy) and [5.2](#ETSTaxonomyMaths). Unfortunately, they cannot be used directly for the derivation of conditional values, so they are needed just for the general understanding of ETS.

<!-- Several special cases of ETS models and the respective values for the functions will be discussed later in the next chapters in the context of ADAM ETS. The most useful and important cases are pure additive and pure multiplicative ETS models, which then can be formulated via the form that allows deriving conditional expectation and variance. -->


## Parameters bounds {#ETSParametersBounds}
While, it is accepted by many practitioners and academics that the smoothing parameters of ETS models should lie between zero and one, this is not entirely true for the models. There are, in fact, several possible restrictions on smoothing parameters, and it is worth discussing them separately:

1. **Classical or conventional** bounds are $\alpha, \beta, \gamma \in (0,1)$. The idea behind them originates from the [exponential smoothing methods](#SES), where it is logical to restrict the bounds with this region, because then the smoothing parameters regulate, what weight the actual value $y_t$ will have and what weight will be asigned to the predicted one $\hat{y}_t$. @Hyndman2008b showed that this condition is sometimes too loose and in other cases is too restrictive to some ETS models. @Brenner1968 was one of the first to show that the bounds are wider than this region for many exponential smoothing methods. Still, the conventional restriction is the most often used, just because it is nice to work with.

2. **Usual or traditional** bounds are those that satisfy the set of the following equations:
\begin{equation}
  \begin{aligned}
  &\alpha \in [0, 1)\\
  &\beta \in [0, \alpha) \\
  &\gamma \in [0, 1-\alpha)
  \end{aligned},
  (\#eq:ETSUsualBounds)
\end{equation}
This set of restrictions guarantees that the weights [decline over time exponentially](#whyExponential) and the ETS models have the property of "averaging" the values over time. In the lower boundary condition, the components of the model become deterministic and we can say that they are calculated as the simple averages of the values over time.

3. **Admissible** bounds, satisfying stability condition. The idea here is that the most recent observation should have higher weight than the older ones, which is regulated via the smoothing parameters. However, in this case we do not impose the restriction of [exponential decay](whyExponential) of weights over time on the models, so they can oscillate or decay harmonially, as long as their absolute values decrease over time. The condition is more complicated mathematically than the previous two and will be discussed later in the textbook for the [pure additive models](#ADAMETSPureAdditive), but here are several examples for bounds, satisfying this condition:

- ETS(A,N,N): $\alpha \in (0, 2)$;
- ETS(A,A,N): $\alpha \in (0, 2); \beta \in (0, 4-2\alpha)$;
- ETS(A,N,A): $\alpha \in \left(\frac{-2}{m-1}, 2-\gamma\right); \gamma \in (\max(-m\alpha, 0), 2-\alpha)$;

As you see, the admissible bounds are much wider than the conventional and usual ones. In fact, smoothing parameters can become either negative or greater than one in some cases for some models. Furthermore, the admissible bounds correspond to the parameters restrictions for ARIMA models, underlying some of pure additive ETS models. In a way, they are more natural for the ETS models, because they follow from the formulation and arise naturally. However, their usage in practice has been met with mixed success, with only handful of papers using them instead of (1) or (2) (e.g. @Gardner2008 mention that they appear in some cases and @Snyder2017 use them in their model).

In the R code, the admissible bounds are calculated based on the discount matrix, which will be discussed in the context of [pure additive ADAM ETS models](#ADAMETSPureAdditive) in the next chapter.
