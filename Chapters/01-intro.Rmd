# Introduction {#intro}
I started writing this book in 2020 during the COVID-19 pandemic, having figured out that it had been more than 10 years since the publishing of the fundamental textbook of [@Hyndman2008b], who discuss ETS (Error-Trend-Seasonality) framework in the Single Source of Error (SSOE) form and that the topic has not been updated substantially since then. If you are interested in learning more about exponential smoothing, then this is must-read material on the topic.

However, there has been some progress in the area since 2008, and I have developed some models and functions based on SSOE, making the framework more flexible and general. Given that publication of all aspects of these models in peer-reviewed journals would be very time consuming, I have decided to summarise all progress in this book, showing what happens inside the models and how to use the functions in different cases, so that there is a source to refer to.

Many parts of this textbook rely on such topics as model, scales of information, model uncertainty, likelihood, information criteria and model building. All these topics are discussed in detail in the textbook of @SvetunkovSBA. You are recommended to familiarise yourself with them before moving to the more advanced modelling topics of ADAM.

In this chapter, we explain what is forecasting, how it is different from planning and analytics and what are the main forecasting principles one should follow in order not to fail in trying to predict the future.


## Forecasting, planning and analytics {#forecastingPlanningAnalytics}
While there are many definitions of what is forecast, I like the following, proposed by Sergey Svetunkov [@Svetunkov2014Textbook]: **Forecast is a scientifically justified assertion about possible states of an object in future**. This definition does not have the word "probability" in it, because in some cases forecasts do not rely on rigorous statistical methods and theory of probabilities. For example, Delphi method allows obtaining judgmental forecasts, typically focusing on what to expect, not on the probability side of things. An important word in the definition is "scientific". If a prediction is done based on coffee grounds, then it is not a forecast. Judgmental predictions on the other hand can be considered as forecasts if a person has a reason behind them. If they do not, then they would be called "guesses", not forecasts. As for the definition of **forecasting**, it is a process of producing forecasts - as simple as that.

Forecasting is a very important activity, carried by many companies, some of which do that unconsciously or label it as "demand planning" or "predictive analytics". There is a difference, however, between the terms "forecasting" and "planning". The latter relies on the former and implies actions made by the company in order to adjust its trajectory. For example, if we forecast that the sales will go down, a company should make some marketing decisions in order to increase the demand on the product. The first part relates to forecasting, while the second one relates to planning. This also means that if a company does not like a forecast, it should change something in its activities, not in the forecast itself. It is important not to confuse these terms in practice, when important decisions are made.

Another important thing to keep in mind is that any forecasting activity should be done to inform decisions. Forecasting for the sake of forecasting is pointless. Yes, we can forecast the overall number of hospitalisations due to SARS-CoV-2 virus in the world for the next decade, but what decisions can be made based on that? If there are some decisions, then this exercise is useful. If not, then this is just a waste of time.

Related to this is the question of forecasts accuracy. In reality, the accurate forecasts do not always translate to good decisions. This is because there are many different aspects of reality that need to be taken into account, and forecasting focuses only on one of them. Capturing the variability of demand correctly is sometimes more useful than producing very accurate forecasts - this is because many decisions are based on distributions of values rather than on point forecasts. The classical example of this situation is the inventory management, where the ordering decisions are made based on quantiles of distribution to form safety stock. Furthermore, the orders are typically done in pallets, so it is not important, whether the expected demand is 99 or 95 units, if a pallet includes 100 units of a product. This means that whenever we produce forecasts, we need to keep in mind how they will be used and by whom.

In some cases, the very accurate forecasts might go to waste if people make decisions differently and / or do not trust to what they see. For example, a demand planner might decide that a straight line is not a good point forecast and would start changing the values, introducing noise. This might happen due to lack of experience, expertise or trust, and this means that it is important to understand who will use the forecasts and how.

Finally, in practice, not everything can be solved with forecasting. In some cases companies can make decisions based on other reasons. For example, promotional decisions can be dictated by the existing stock of the product that needs to be moved out. Another case, if the holding costs for a product are very low, then there is no need in spending time on forecasting the demand on it - a company can implement a simple replenishment policy, ordering, when stock reaches some threshold. And in times of crysis, some decisions are dictated by the financial situation of a company, not by forecasts: you do not need to predict demand on products that are sold out of prestige if they are not profitable and a company needs to cut the costs.

Summarising this discussion, before diving into forecasting, it makes sense to find out what decisions will be made based on it, by whom and how. There is no need to waste time and effort on improving the forecasting accuracy if the process in company is flawed and forecasts are then ignored, not needed or amended inadequately.

As for analytics, this is a relatively new term, which implies a set of activities based on analysis, forecasting and optimisation to support informed managerial decisions. The term is large and relies on many research areas. In this textbook, we will focus on the forecasting side, occasionally discussing how to analyse the existing processes and how various models could help in making adequate practical decisions.


## Forecasting principles
If you have decided that you need to forecast something, then it makes sense to keep several important forecasting principles in mind.

First, as discussed earlier, you need to understand why the forecast is needed, how it will be used and by whom. Answers to these questions will guide you in deciding, what technique to use, how specifically to do forecasting and what should be reported. For example, if a client does not know machine learning, it might be unwise to use Neural Networks for forecasting - they will not trust the technique and thus will not trust the forecasts, switching to simpler methods. If the final decision is to order some number of units, then it would be more reasonable to produce cumulative forecasts over the lead time (time between the order and product delivery) and form safety stock based on the model and assumed distribution.

When you have an understanding of what to forecast and how, the second principle comes into play. Select the relevant error measures. You need to decide how to measure the accuracy of forecasting methods, and that accuracy needs to be as close to the final decision, as possible. For example, if you need to decide the number of nurses for a specific day in the A&E department based on the patients attendance, then it would be more reasonable to compare models in terms of their quantile performance (see Section \@ref(uncertainty)) rather than expectation or median. Thus, it would be more appropriate to calculate pinball loss instead of MAE or RMSE (see details in Section \@ref(forecastsEvaluation)).

Third, you should always test your models on a sample of data not seen by them. Train your model on one part of a sample and test it on another one. This way you can have some guarantees that the model will not overfit the data and when you need to produce a final forecast, it will be reasonable. Yes, there are cases, when you do not have enough data to do that. All you can do in these situations, is use simpler, robust models (for example, such as damped trend exponential smoothing by @Roberts1982 and @Gardner1985a or Theta by @Assimakopoulos2000) and to use judgment in deciding, whether the final forecasts are reasonable or not. But in all the other cases, you should test model on the data they are not aware of. The recommended approach in this case is rolling origin, discussed in more detail in Section \@ref(rollingOrigin).

Fourth, you need to have benchmark models. Always compare forecasts from your favourite approach with those from Naïve, global average and / or regression - depending on what you do specifically. If your fancy Neural Network performs worse than Naïve, then it does not bring value and should not be used in practice. Comparing one Neural Network with another is also not a good idea, because Simple Exponential Smoothing (see Section \@ref(SES)), being much simpler model, might beat both networks, and you would never find out about that. If possible, also compare forecasts from the proposed approach with forecasts of other well established benchmarks, such as ETS [@Hyndman2008b], ARIMA [@Box1976] and Theta [@Assimakopoulos2000].

Finally, when comparing forecasts from different models, you might end up with several very similar performing approaches. If the difference between them is not significant, then the general recommendation is to select the one that is faster and simpler. This is because simpler models are more difficult to break and those that work faster are more attractive in practice due to reduced energy consumption (let's save the planet!).

These forecasting principles do not guarantee that you will end up with the most accurate results, but at least you will not end up with the unreasonable ones.


## Types of forecasts
Depending on circumstances, we might require different types of forecasts with different characteristics. It is important to understand what your model produces in order to measure its performance correctly (see Section \@ref(errorMeasures)) and make correct decisions in practice. There are several things that are typically produced for forecasting purposes. We start with the most popular one.

### Point forecasts
The classical and most often produced thing is the point forecast, which corresponds to some trajectory from a model. This however might align with different types of statistics depending on the model and its assumptions. In case of pure additive model (such as linear regression), the point forecasts correspond to the conditional expectation (**mean**) from the model. The conventional interpretation of this value is that it shows what to expect on average if the situation would repeat itself many times (e.g. if we have the day with similar conditions, then the average temperature will be 10 degrees Celsius). In case of time series, this interpretation is difficult to digest, given that time does not repeat itself, but this is the best we can have. The technicalities of producing conditional expectations from ADAM will be discussed in Section \@ref(ADAMForecastingExpectation).

Another type of point forecast is the (conditional) geometric expectation (**geometric mean**). It typically arises, when the model is applied to the data in logarithms and the final forecast is only exponentiated. This becomes apparent from the following definition of geometric mean:
\begin{equation}
    \check{y} = \sqrt[T]{\prod_{t=1}^T y_t} = \exp \left(\frac{1}{T} \sum_{t=1}^T \log(y_t) \right) ,
    (\#eq:GeoMean)
\end{equation}
where $y_t$ is the actual value and $T$ is the sample size. In order to use the geometric mean, it is assumed that the actual values can only be positive, otherwise the root in c might produce imaginary units (due to, for example, taking a square root out of a negative number) or be equal to zero (if one of the values is zero). In general, the arithmetic and geometric means are related via the following inequality:
\begin{equation}
    \check{y} \leq \mu ,
    (\#eq:GeoAndArithMeans)
\end{equation}
where $\check{y}$ is the geometric mean and $\mu$ is the arithmetic one. Although geometric mean makes sense in many contexts, it is more difficult to explain than the arithmetic one to decision makers.

Finally, sometimes **medians** are used in place of point forecasts. In this case we can say that the forecast splits the sample in two halves and shows the level, below which 50% of observations will lie in the future.

### Quantiles and prediction intervals
As some forecasters say, all point forecasts are wrong. They will never correspond to the actual values, because they only capture the mean (or median) performance of the model, as discussed in the previous subsection. Everything that is not included in the point forecast can be considered as an uncertainty of demand. For example, we never will be able to say specifically how many cups of coffee we will sell next Monday, but we can at least capture the main tendencies and the uncertainty around our point forecast.

```{r adamExampleNormal, fig.cap="An example of a well behaved data, point forecast and a 95% prediction interval.", echo=FALSE}
testAdam <- adam(rnorm(100,100,10), "ANN", persistence=0, h=10, holdout=TRUE)
testAdamForecast <- forecast(testAdam, h=10, interval="approx")
plot(testAdamForecast, main="")
```

Figure \@ref(adamExampleNormal) shows an example, with a well behaved demand, for which the best point forecast is the straight line. In order to capture the uncertainty of demand, we can construct the prediction interval, which will tell in which bound the demand will lie in $1-\alpha$ percent of cases. The interval in Figure \@ref(adamExampleNormal) has the width of 95% ($\alpha=0.05$) and shows that if the situation is repeated many times, the actual demand will be between `r round(testAdamForecast$lower[1],2)` and `r round(testAdamForecast$upper[1],2)`. Capturing the uncertainty correctly is important, because the real life decisions need to be made based on the full information, not only on the point forecasts.

We will discuss how to produce prediction intervals in more detail in Section \@ref(ADAMForecastingPI).

Another way to capture the uncertainty (related to the prediction interval) is via specific quantiles of distribution. The prediction interval typically has two sides, leaving $\frac{\alpha}{2}$ values on the left and the same on the right, outside the bounds of the interval. Instead of producing the interval, in some cases we might need just a specific quantile, essentially producing the one-sided prediction interval (see Section \@ref(forecastingADAMOtherOneSided) for technicalities). The bound in this case will show the specific value, below which the pre-selected percentage of cases would lie. This becomes especially useful, in such contexts as safety stock calculation (because we are not interested in knowing the lower bound, we want to have products to satisfy some proportion of demand).


### Forecast horizon
Finally, an important aspect in forecasting is the horizon, for which we need to produce forecasts. Depending on the context, we might need:

1. Only a specific value h steps ahead, e.g. what the temperature next Monday will be.
2. All values from 1 to h steps ahead, e.g. how many patients we will have each day next week.
3. Cumulative values for the period from 1 to h steps ahead, e.g. what the cumulative demand over the lead time (the time between the order and product delivery) will be (see discussion in Section \@ref(forecastingADAMOtherCumulative)).

It is important to understand how decisions are made in practice and align them with the forecast horizon. In combination with the point forecasts and prediction intervals discussed above, this will give us an understanding of what to produce from the model and how. For example, in case of safety stock calculation it would be more reasonable to produce quantile of the cumulative over the lead time demand than to produce point forecasts from the model.
