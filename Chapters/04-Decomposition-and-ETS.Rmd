# From time series components to ETS {#tsDecomposition}

Before we turn to state space models, ETS, ARIMA and other things we need to discuss time series decomposition and the ETS taxonomy. These topics lie at the heart of ETS models and are essential for the understanding of the further material.

In this chapter we start with a discussion of time series components, then move to the idea of decomposing time series into distinct components and then move to the conventional ETS taxonomy, as formulated by @Hyndman2008b, demonstrating its connection with the previous topics.

## Time series components {#tsComponents}
The main idea behind many forecasting techniques is that any time series can contain several unobservable components, such as:

1. **Level** of the series - the average value for specific period of time,
2. **Growth** of the series - the average increase or decrease of the value over a period of time,
3. **Seasonality** - a pattern that repeats itself with a fixed periodicity. This pattern need not literally be seasonal, like beer sales being higher in summer than they are in winter (season of year). Any pattern with a fixed periodicity works: the number of hospital visitors is higher on Mondays than on Saturdaya or Sundays because people tend to stay at home over the weekend (day of week seasonality), and sales are higher during daytime than they are at night (hour of the day seasonality).
4. **Error** - unexplainable white noise.

Each textbook and paper will use slightly different names to refer to these components. For example, in classical decomposition [@Persons1919] it is assumed that (1) and (2) jointly represent a "trend" component so a model will contain error, trend and seasonality. There are modifications of this, which also contain cyclical component(s). When it comes to ETS, the growth component (2) is called "trend", so the model consists of the four components. We will use the ETS formulation in this textbook. According to this formulation the components can interact with each other in one of two ways: additively or multiplicatively. The pure additive model in this case can be summarised as:
\begin{equation}
    y_t = l_{t-1} + b_{t-1} + s_{t-m} + \epsilon_t ,
    (\#eq:PureAdditive)
\end{equation}
where $l_{t-1}$ is the level, $b_{t-1}$ is the trend, $s_{t-m}$ is the seasonal component with periodicity $m$ (e.g. 12 for months of year data, implying that something is repeated every 12 months) - all these components are produced on the previous observations and are used on the current one. Finally, $\epsilon_t$ is the error term, which follows some distribution and has zero mean. Similarly, the pure multiplicative model is:
\begin{equation}
    y_t = l_{t-1} b_{t-1} s_{t-m} \varepsilon_t ,
    (\#eq:PureMultiplicative)
\end{equation}
where $\varepsilon_t$ is the error term that has mean of one. The interpretation of the model \@ref(eq:PureAdditive) is that the different components add up to each other, so, for example, the sales in January typically increase by the amount $s_{t-m}$, and that there is still some randomness that is not taken into account in the model. The pure additive models can be applied to data that can have positive, negative and zero values. In case of the model \@ref(eq:PureMultiplicative), the interpretation is similar, but the sales change by $(s_{t-m}-1)$% from the baseline. These models only work on data with strictly positive values (data with purely negative values are also possible but rare in practice).

It is also possible to define mixed models in which, for example, the trend is additive but the other components are multiplicative:
\begin{equation}
    y_t = (l_{t-1} + b_{t-1}) s_{t-m} \varepsilon_t
    (\#eq:MixedAdditiveTrend)
\end{equation}
These models work well in practice when the data has large values far from zero. In other cases, however, they might produce strange results (e.g. negative values on positive data) so the conventional decomposition techniques only consider the pure models.

## Classical Seasonal Decomposition {#ClassicalDecomposition}
### How to do?
One of the classical textbook methods for decomposing the time series into unobservable components is called "Classical Seasonal Decomposition" [@Persons1919]. It assumes either a pure additive or pure multiplicative model, is done using centred moving averages and is focused on approximation, not on forecasting. The idea of the method can be summarised in the following steps:

1. Decide, which of the models to use based on the type of seasonality in the data: additive \@ref(eq:PureAdditive) or multiplicative \@ref(eq:PureMultiplicative)
2. Smooth the data using a centred moving average (CMA) of order equal to the periodicity of the data $m$. If $m$ is the an number then the formula is:
\begin{equation}
    d_t = \frac{1}{m}\sum_{i=-(m-1)/2}^{(m-1)/2} y_{t+i},
    (\#eq:CMAOdd)
\end{equation}
which means that, for example, the value on Thursday is the average of values from Monday to Sunday. If $m$ is an even number then a different weighting scheme is typically used, involving the inclusion of additional an value:
\begin{equation}
    d_t = \frac{1}{m}\left(\frac{1}{2}\left(y_{t+(m-1)/2}+y_{t-(m-1)/2}\right) + \sum_{i=-(m-2)/2}^{(m-2)/2} y_{t+i}\right),
    (\#eq:CMAEven)
\end{equation}
which means that we use half of the December of the previous year and half of the December of the current year in order to calculate the centred moving average in June. The values $d_t$ are placed in the middle of the window going through the series (e.g. on Thursday the average will contain values from Monday to Sunday).

The resulting series is deseasonalised. When we average e.g. sales in a year we automatically remove the potential seasonality, which can be observed individually in each month. A drawback of using CMA is that we inevitably lose $\frac{m}{2}$ observations at the beginning and the end of the series.

In R, the `ma()` function from the `forecast` package implements CMA.

3. De-trend the data:
- For the additive decomposition this is done using: ${y^\prime}_t = y_t - d_t$;
- For the multiplicative decomposition, it is: ${y^\prime}_t = \frac{y_t}{d_t}$;
4. If the data is seasonal, then the average value for each period is calculated based on the de-trended series. e.g. we produce average seasonal indices for each January, February, etc. This will give us the set of seasonal indices $s_t$;
5. Calculate the residuals based on what you assume in the model:
- additive seasonality: $e_t = y_t - d_t - s_t$;
- multiplicative seasonality: $e_t = \frac{y_t}{d_t s_t}$;
- no seasonality: $e_t = {y^\prime}_t$.

Note that the functions in R typically allow you to select between additive and multiplicative seasonality. There is no option for "none" and so even if the data is not seasonal you will nonetheless get values for $s_t$ in the output. Also, notice that the classical decomposition assumes that there is a deseasonalised series $d_t$ but does not make any further split of this variable into level $l_t$ and trend $b_t$.

### A couple of examples
An example of the classical decomposition in R is the `decompose()` function from `stats` package. Here is an example with pure multiplicative model and `AirPassengers` data:
```{r decomposeAirPassengers}
ourDecomposition <- decompose(AirPassengers, type="multiplicative")
plot(ourDecomposition)
```

We can see that the function has smoothed the original series and produced the seasonal indices. Note that the trend component has gaps at the beginning and at the end. This is because the method relies on CMA (see above). Note also that the error term still contains some seasonal elements, which is a downside of such a simple decomposition procedure. However, the lack of precision in this method is compensated by the simplicity and speed of calculation. Note again that the trend component in `decompose()` function is in fact $d_t = l_{t}+b_{t}$.

Here is an example of decomposition of the **non-seasonal data** (we assume pure additive model in this example):
```{r decomposeRandomNoise}
y <- ts(c(1:100)+rnorm(100,0,10),frequency=12)
ourDecomposition <- decompose(y, type="additive")
plot(ourDecomposition)
```

As you can see, the original data is not seasonal but the decomposition assumes that it is and proceeds with the default approach returning a seasonal component. You get what you ask for.

### Other techniques
There are other techniques that decompose series into error, trend and seasonal components but make different assumptions about each component. The general procedure, however, always remains the same: (1) smooth the original series, (2) extract the seasonal components, (3) smooth them out. The methods differ in the smoother they use (LOESS, e.g., uses a bisquare function instead of CMA) and in some cases multiple rounds of smoothing are performed to make sure that the components are split correctly.

There are many functions in R that implement seasonal decomposition. Here is a small selection:

- `decomp()` from the `tsutils` package does classical decomposition and fills in the tail and head of the smoothed trend with forecasts from exponential smoothing;
- `stl()` from the `stats` package uses a different approach - seasonal decomposition via LOESS. It is an iterative algorithm that smoothes the states and allows them to evolve over time. So, for example, the seasonal component in STL can change;
- `mstl()` from the `forecast` package does the STL for data with several seasonalities;
- `msdecompose()` from the `smooth` package does a classical decomposition for multiple seasonal series.

### "Why bother?"

"Why decompose?" you may wonder at this point. Understanding the idea behind decompositions and how to perform them helps in understanding ETS, which relies on it. From a practical point of view it can be useful if you want to see if there is a trend in the data and whether the residuals contain outliers or not. It will _not_ show you if the data is seasonal as the seasonality is _assumed_ in the decomposition (I stress this because many students think otherwise). Additionally, when seasonality cannot be added to the model under consideration decomposing the series, predicting the trend and then reseasonalising can be a viable solution. Finally, the values from the decomposition can be used as starting points for the estimation of components in ETS or other dynamic models relying on the error-trend-seasonality.


## ETS taxonomy {#ETSTaxonomy}
Building on the idea of [time series components](#tsComponents) we can move to the ETS taxonomy. ETS stands for "Error-Trend-Seasonality" and defines how specifically the components interact with each other. Based on the type of error, trend and seasonality, [@Pegels1969] proposed a taxonomy, which was then developed further by [@Hyndman2002] and refined by [@Hyndman2008b]. According to this taxonomy, error, trend and seasonality can be:

1. Error: either "Additive" (A), or "Multiplicative" (M);
2. Trend: either "None" (N), or "Additive" (A), or "Additive damped" (Ad), or "Multiplicative" (M), or "Multiplicative damped" (Md);
3. Seasonality: either "None" (N), or "Additive" (A), or "Multiplicative" (M).

According to this taxonomy, the model \@ref(eq:PureAdditive) is denoted as ETS(A,A,A) while the model \@ref(eq:PureMultiplicative) is denoted as ETS(M,M,M), and \@ref(eq:MixedAdditiveTrend) is ETS(M,A,M).

The main advantages of the ETS taxonomy are that the components have clear interpretations and that it is flexible, allowing to have 30 models with different types of error, trend and seasonality. The figure below shows examples of different time series with deterministic (they do not change over time) level, trend and seasonality, based on how they interact in the model. The first one shows the additive error case:
```{r ETSTaxonomyAdditive, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Time series corresponding to the additive error ETS models"}
modelsList <- c("ANN","AAN","AAdN","AMN","AMdN","ANA","AAA","AAdA","AMA","AMdA","ANM","AAM","AAdM","AMM","AMdM",
                "MNN","MAN","MAdN","MMN","MMdN","MNA","MAA","MAdA","MMA","MMdA","MNM","MAM","MAdM","MMM","MMdM")
level <- 500
trend <- c(100,1.05)
seasonality <- list((c(1.3,1.1,0.9,0.75)-1)*2*level,c(1.3,1.1,0.9,0.75))
generatedData <- vector("list", length(modelsList))
scale <- 0.05
for(i in 1:length(modelsList)){
  initial <- switch(substr(modelsList[i],2,2),
                    "A"=c(level,trend[1]),
                    "M"=c(level,trend[2]),
                    level*2);
  initialSeason <- switch(substr(modelsList[i],nchar(modelsList[i]),nchar(modelsList[i])),
                    "A"=seasonality[[1]],
                    "M"=seasonality[[2]],
                    NULL);
  if(nchar(modelsList[i])==4){
    phi <- 0.95;
  }
  else{
    phi <- 1;
  }
  sdValue <- switch(substr(modelsList[i],1,1),
                    "A"=sqrt(level^2*(exp(scale^2)-1)*exp(scale)),
                    "M"=scale)
  meanValue <- switch(substr(modelsList[i],1,1),
                      "A"=0,
                      "M"=1)
  generatedData[[i]] <- smooth::sim.es(modelsList[i], obs=36, frequency=4, persistence=0, phi=phi,
                                       initial=initial, initialSeason=initialSeason, mean=meanValue, sd=sdValue)$data
}

# Prepare the canvas
par(mfcol=c(5,3),mar=c(2,3,2,1))
# The matrix that corresponds to the i
ylimValues <- matrix(c(1:15),5,3)
for(i in 1:15){
  if(i>15){
    ylimRow <- which(ylimValues == i-15,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]+15]))
  }
  else{
    ylimRow <- which(ylimValues == i,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]]))
  }
  plot(generatedData[[i]], main=paste0("ETS(",modelsList[i],")"),ylim=ylim,ylab="")
}
```

Things to note from this plot:

1. When the seasonality is multiplicative its amplitude increases with the level of the data while with additive seasonality the amplitude is constant. Compare, e.g., ETS(A,A,A) with ETS(A,A,M): the distance between the highest and the lowest points for the former in the first year is roughly the same as in the last year. In the case of ETS(A,A,M) the distance increases with the increase in the level.
2. When the trend is multiplicative data with exponential growth / decay result. With ETS(A,M,N), for example, we say that there is roughly 5% growth in the data;
3. The damped trend models slow down both additive and multiplicative trends;
4. It is practically impossible to distinguish additive and multiplicative seasonality if the series does not trend because what distinguishes the two -- see (1) -- is not relevant (compare ETS(A,N,A) and ETS(A,N,M)).

Here is a similar plot for the multiplicative error models:
```{r ETSTaxonomyMultiplicative, echo=FALSE, warning=FALSE, fig.cap="Time series corresponding to the multiplicative error ETS models"}
# Prepare the canvas
par(mfcol=c(5,3),mar=c(2,3,2,1))
# The matrix that corresponds to the i
ylimValues <- matrix(c(1:15),5,3)
for(i in 16:30){
  if(i>15){
    ylimRow <- which(ylimValues == i-15,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]+15]))
  }
  else{
    ylimRow <- which(ylimValues == i,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]]))
  }
  plot(generatedData[[i]], main=paste0("ETS(",modelsList[i],")"),ylim=ylim,ylab="")
}
```

They show roughly the same picture as the additive case, the main difference being that the variance of the error increases with the increase of the level of the data - this becomes clearer on ETS(M,A,N) and ETS(M,M,N) data. This property is called heteroscedasticity in statistics and [@Hyndman2008b] argue that the main benefit of the multiplicative error models is being able to capture this feature.

In the next chapters we will discuss the most important members of the ETS taxonomy. Not all the models in this taxonomy are particularly sensible and some are typically ignored entirely. Although ADAM implements the entire taxonomy we will discuss potential issues and what to expect from them.


## Mathematical models in the ETS taxonomy {#ETSTaxonomyMaths}
I hope that it becomes clearer to the reader how the ETS framework is built upon the idea of [time series decomposition](#tsComponents). By introducing different components and defining their types and by adding the equations for their update, we can construct models that would work better on the time series at hands. The equations discussed in [the previous section](#tsComponents) represent so called "measurement" or "observation" equations of the ETS models. But we should also take into account the potential change in components over time. The "transition" or "state" equation is supposed to reflect this change: they explain, how the level, trend or seasonal components change over time.

As discussed in [the previous section](#ETSTaxonomy), given different types of components and their interactions, we end up with 30 models in the taxonomy. Tables \@ref(tab:ETSAdditiveError) and \@ref(tab:ETSMultiplicativeError) summarise mathematically all 30 ETS models shown graphically on Figures \@ref(fig:ETSTaxonomyAdditive) and \@ref(fig:ETSTaxonomyMultiplicative) in the [ETS Taxonomy chapter](#ETSTaxonomy), presenting formulae for:

- Measurement equation;
- Transition equation;
- Conditional one step ahead expectation $\mu_{y,t} = \mu_{y,t|t-1}$;
- Multiple steps ahead point forecast $\hat{y}_{t+h}$;
- Conditional multiple steps ahead expectation $\mu_{y,t+h|t}$;

In case of the additive error models, the point forecasts correspond to the expectations only when the expectation of the error term is zero, $\text{E}(\epsilon_t)=0$, while in case of the multiplicative one the condition is typically that $\text{E}(1+\epsilon_t)=1$.

::: remark
However, **note that not all the point forecasts correspond to the conditional expectations**. This issue applies to the models with multiplicative trend and / or multiplicative seasonality. This is because SSOE models assume that different states are correlated (they have the same source of error) and as a result multiple steps ahead values (when h>1) of states introduce products of error terms. So, the conditional expectations in these cases might not have an analytical forms, and when working with these models, simulations might be required. This does not apply to the one step ahead forecasts, for which the classical formulae work.
:::

```{r ETSAdditiveError, echo=FALSE}
# T="N"
etsAdditiveTable <- c("$\\begin{split}
      &y_{t} = l_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\alpha \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\alpha \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &\\mu_{y,t} = l_{t-1} s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{split}$",
# T="A"
    "$\\begin{split}
      &y_{t} = l_{t-1} + b_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} + b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} + h b_t \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} + b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + h b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + b_{t-1}) s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + b_{t-1}} \\\\
      &\\mu_{y,t} = (l_{t-1} + b_{t-1}) s_{t-m} \\\\
      &\\hat{y}_{t+h} = \\left(l_{t} + h b_{t-1}\\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{split}$",
# T="Ad"
    "$\\begin{split}
      &y_{t} = l_{t-1} + \\phi b_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_t \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = \\phi b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + \\phi b_{t-1}} \\\\
      &\\mu_{y,t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} \\\\
      &\\hat{y}_{t+h} = \\left(l_{t} + \\sum_{j=1}^h \\phi^j b_t \\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{split}$",
# T="M"
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &\\mu_{y,t} = l_{t-1} b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} b_t^h \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form for} h>1
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^h + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form for} h>1
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1} s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}} \\\\
      &\\mu_{y,t} = l_{t-1} b_{t-1} s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^h s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form for} h>1
    \\end{split}$",
# T="Md"
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1}^\\phi + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi \\\\
      &\\hat{y}_{t+h} = l_{t} b_t^{\\sum_{j=1}^h \\phi^j} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form for} h>1
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form for} h>1
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}} \\\\
      &\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form for} h>1
    \\end{split}$")
etsAdditiveTable <- matrix(etsAdditiveTable, 5, 3, byrow=TRUE,
                           dimnames=list(c("**No trend**","**Additive trend**","**Additive damped trend**",
                                           "**Multiplicative trend**","**Multiplicative damped trend**"),
                                         c("N","A","M")))
kableTable <- kableExtra::kable(etsAdditiveTable, escape=FALSE, caption="Additive error ETS models",
                                col.names=c("Nonseasonal","Additive seasonality","Multiplicative seasonality"))
kable_styling(kableTable, font_size=12, protect_latex=TRUE)
```

The multiplicative error models have the same one step ahead expectations as the additive error ones, but due to the multiplication by the error term, the multiple steps ahead conditional expectations between the two models might differ, specifically for the multiplicative trend and multiplicative seasonal models.

```{r ETSMultiplicativeError, echo=FALSE}
# T="N"
etsMultiplicativeTable <- c("$\\begin{split}
      &y_{t} = l_{t-1}(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\
      &\\mu_{y,t} = l_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} s_{t-m}(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\
      &s_t = s_{t-m}(1 + \\gamma \\epsilon_t) \\\\
      &\\mu_{y,t} = l_{t-1} s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{split}$",
# T="A"
    "$\\begin{split}
      &y_{t} = (l_{t-1} + b_{t-1})(1 + \\epsilon_t) \\\\
      &l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} + b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} + h b_t \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} + b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + h b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\
      &l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} + \\beta (l_{t-1} + b_{t-1}) \\epsilon_t \\\\
      &s_t = s_{t-m} (1 + \\gamma \\epsilon_t) \\\\
      &\\mu_{y,t} = (l_{t-1} + b_{t-1}) s_{t-m} \\\\
      &\\hat{y}_{t+h} = \\left(l_{t} + h b_{t-1}\\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{split}$",
# T="Ad"
    "$\\begin{split}
      &y_{t} = (l_{t-1} + \\phi b_{t-1})(1 + \\epsilon_t) \\\\
      &l_t = (l_{t-1} + \\phi b_{t-1})(1 + \\alpha \\epsilon_t) \\\\
      &b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_t \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + \\phi b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} (1 + \\alpha \\epsilon_t) \\\\
      &b_t = \\phi b_{t-1} + \\beta (l_{t-1} + \\phi b_{t-1}) \\epsilon_t \\\\
      &s_t = s_{t-m}(1 + \\gamma \\epsilon_t) \\\\
      &\\mu_{y,t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} \\\\
      &\\hat{y}_{t+h} = \\left(l_{t} + \\sum_{j=1}^h \\phi^j b_t \\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{split}$",
# T="M"
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1} (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\\\
      &\\mu_{y,t} = l_{t-1} b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} b_t^h \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^h + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1} s_{t-m} (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\\\
      &s_t = s_{t-m} (1 + \\gamma \\epsilon_t) \\\\
      &\\mu_{y,t} = l_{t-1} b_{t-1} s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^h s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
# T="Md"
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1}^\\phi (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi (1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1}^\\phi (1 + \\beta \\epsilon_t) \\\\
      &\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi \\\\
      &\\hat{y}_{t+h} = l_{t} b_t^{\\sum_{j=1}^h \\phi^j} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} b_{t-1}^\\phi + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t \\\\
      &\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi \\left(1 + \\alpha \\frac{\\epsilon_t}{s_{t-m}}\\right) \\\\
      &b_t = b_{t-1}^\\phi \\left(1 + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}}\\right) \\\\
      &s_t = s_{t-m} \\left(1 + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}}\\right) \\\\
      &\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$")
etsMultiplicativeTable <- matrix(etsMultiplicativeTable, 5, 3, byrow=TRUE,
                           dimnames=list(c("**No trend**","**Additive trend**","**Additive damped trend**",
                                           "**Multiplicative trend**","**Multiplicative damped trend**"),
                                         c("N","A","M")))
kableTable <- kableExtra::kable(etsMultiplicativeTable, escape=FALSE, caption="Multiplicative error ETS models",
                                col.names=c("Nonseasonal","Additive seasonality","Multiplicative seasonality"))
kable_styling(kableTable, font_size=12, protect_latex=TRUE)
```

The formulae summarised above explain the models underlying potential data, but when it comes to their construction and estimation, the $\epsilon_t$ is substituted by the estimated $e_t$ (which is calculated differently depending on the error type), and time series components and smoothing parameters are also substituted by their estimated analogues (e.g. $\hat{\alpha}$ instead of $\alpha$).

Although there are 30 potential ETS models, not all of them are stable So, Rob Hyndman has reduced the pool of models under consideration in the `ets()` function of `forecast` package to the following 19: ANN, AAN, AAdN, ANA, AAA, AAdA, MNN, MAN, MAdN, MNA, MAA, MAdA, MNM, MAM, MAdM, MMN, MMdN, MMM, MMdM. In addition, the multiplicative trend models are difficult and are unstable in cases of data with outliers, so they are switched off in the `ets()` function by default, which reduces the pool of models further to the first 15.
