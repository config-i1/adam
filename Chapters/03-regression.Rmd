# Regression analysis {#regression}
While we do not expect to cover the regression analysis in its fullness, I think that it is important to have some basic understanding of what regression model is, how it can be estimated and used for forecasting. This chapter introduces the regression starting from the simple linear model and then moving to more advanced topics of multiple linear regression, model estimation, regression assumptions, dummy variables, variables transformations, ARDL and model for scale of distribution.


## Simple Linear Regression {#simpleLinearRegression}
When we want to analyse some relations between variables, we can do [graphical](#dataAnalysisGraphical) and [correlations](#correlations) analysis. But this will not provide us sufficient information about what happens with the response variable with the change of explanatory variable. So it makes sense to consider the possible relations between variables, and the basis for this is Simple Linear Regression, which can be represented in the form:
\begin{equation}
    y_t = a_0 + a_1 x_t + \epsilon_t ,
    (\#eq:SLRFormula)
\end{equation}
where $a_0$ is the intercept (constant term), $a_1$ is the coefficient for the slope parameter and $\epsilon_t$ is the error term. The regression model is a basic [statistical model](#modelsMethods) that captures the relation between an explanatory variable $x_t$ and the response variable $y_t$. The parameters of the models are typically denoted as $\beta_0$ and $\beta_1$ in econometrics literature, but we use $a_0$ and $a_1$ because we will use $\beta$ for other purposes later in this textbook.

In order to better understand what simple linear regression implies, consider the scatterplot (we discussed it earlier in Section \@ref(dataAnalysisGraphical)) shown in Figure \@ref(fig:scatterWeightMPG2).

```{r scatterWeightMPG2, fig.cap="Scatterplot diagram between weight and mileage."}
slmMPGWt <- lm(mpg~wt,mtcarsData)
plot(mtcarsData$wt, mtcarsData$mpg,
     xlab="Weight", ylab="Mileage",
     xlim=c(0,6), ylim=c(0,40))
abline(h=0, col="grey")
abline(v=0, col="grey")
abline(slmMPGWt,col="red")
text(4,35,paste0(c("mpg=",round(coef(slmMPGWt),2),"wt+et"),collapse=""))
```

The line drawn on the plot is the regression line, parameters of which were estimated based on the available data. In this case the intercept $\hat{a}_0$=`r round(coef(slmMPGWt)[1],2)`, meaning that this is where the red line crosses the y-axis, while the parameter of slope $\hat{a}_1$=`r round(coef(slmMPGWt)[2],2)` shows how fast the values change (how steep the line is). I've added hat symbols on the parameters to point out that they were estimated based on a sample of data. If we had all the data in the universe (population) and estimated a correct model on it, we would not need the hats. In simple linear regression, the re line will always go through the cloud of points, showing the averaged out tendencies. The one that we observe above can be summarise as "with the increase of weight, on average the mileage of cars goes down". Note that we might find some specific points, where the increase of weight would not decrease mileage (e.g. the two furthest left points show this), but this can be considered as a random fluctuation, so overall, the average tendency is as described above.


### Ordinary Least Squares (OLS) {#OLS}
For obvious reasons, we do not have the values of parameters from the population. This means that we will never know what the true intercept and slope are. Luckily, we can estimate them based on the sample of data. There are different ways of doing that, and the most popular one is called "Ordinary Least Squares" method. This is the method that was used in the estimation of the model in Figure \@ref(fig:scatterWeightMPG2). So, how does it work?

```{r scatterWeightMPG3, fig.cap="Scatterplot diagram between weight and mileage.", echo=FALSE}
slmMPGWt <- lm(mpg~wt,mtcarsData)
plot(mtcarsData$wt, mtcarsData$mpg,
     xlab="Weight", ylab="Mileage",
     xlim=c(0,6), ylim=c(0,40))
abline(h=0, col="grey")
abline(v=0, col="grey")
abline(slmMPGWt,col="red")
lines(rep(mtcarsData$wt[20],2),c(fitted(slmMPGWt)[20],mtcarsData$mpg[20]), lty=2)
text(mtcarsData$wt[20]+0.15,mean(c(fitted(slmMPGWt)[20],mtcarsData$mpg[20])),TeX("$e_t$"))
points(mtcarsData$wt[20],mtcarsData$mpg[20], pch=16)
points(mtcarsData$wt[20],fitted(slmMPGWt)[20], pch=3)
```

When we estimate the simple linear regression model, the model \@ref(eq:SLRFormula) transforms into:
\begin{equation}
    y_t = \hat{a}_0 + \hat{a}_1 x_t + e_t .
    (\#eq:SLRFormulaEstimated)
\end{equation}
This is because we do not know the true values of parameters and thus they are substituted by their estimates. This also applies to the error term for which in general $e_t \neq \epsilon_t$ because of the sample estimation. Now consider the same situation with weight vs mileage in Figure \@ref(fig:scatterWeightMPG3) but with some arbitrary line with unknown parameters. Each point on the plot will typically lie above or below the line, and we would be able to calculate the distances from those points to the line. They would correspond to $e_t = y_t - \hat{y}_t$, where $\hat{y}_t$ is the value of the regression line (aka "fitted" value) for each specific value of explanatory variable. For example, for the weight of car of `r mtcarsData$wt[20]` tones, the actual mileage is `r mtcarsData$mpg[20]`, while the fitted value is `r round(fitted(slmMPGWt)[20],3)`. The resulting error (or residual of model) is `r round(residuals(slmMPGWt)[20],3)`. We could collect all these errors of the model for all available cars based on their weights and this would result in a vector of positive and negative values like this:

```{r echo=FALSE}
residuals(slmMPGWt)
```

This corresponds to the formula:
\begin{equation}
    e_t = y_t - \hat{a}_0 - \hat{a}_1 x_t.
    (\#eq:SLRFormulaEstimatedError)
\end{equation}
If we needed to estimate parameters $\hat{a}_0$ and $\hat{a}_1$ of the model, we would need to minimise those distances by changing the parameters of the model. The problem is that some errors are positive, while the others are negative. If we just sum them up, they will cancel each other out, and we would loose the information about the distance. The simplest way to get rid of sign and keep the distance is by taking squares of each error and calculating Sum of Squared Errors for the whole sample $T$:
\begin{equation}
    \mathrm{SSE} = \sum_{t=1}^T e_t^2 .
    (\#eq:OLSCriterion)
\end{equation}
If we now minimise SSE by changing values of parameters $\hat{a}_0$ and $\hat{a}_1$, we will find those parameters that would guarantee that the line goes somehow through the cloud of points. Luckily, we do not need to use any fancy optimisers for this, as this has analytical solution (in order to get it, insert \@ref(eq:SLRFormulaEstimatedError) in \@ref(eq:OLSCriterion), take derivatives with respect to the parameters $\hat{a}_0$ and $\hat{a}_1$ and equate the resulting values to zero):
\begin{equation}
    \begin{aligned}
        \hat{a}_1 = & \frac{\mathrm{cov}(x,y)}{\mathrm{V}(x)} \\
        \hat{a}_0 = & \bar{y} - \hat{a}_1 \bar{x}
    \end{aligned} ,
    (\#eq:OLSSLREstimates)
\end{equation}
where $\bar{x}$ is the mean of the explanatory variable $x_t$ and $\bar{y}$ is the mean of the response variables $y_t$. Note that if for some reason $\hat{a}_1=0$ (for example, because the covariance between $x$ and $y$ is zero, implying that they are not correlated), then the intercept $\hat{a}_0 = \bar{y}$, meaning that the global average of the data is the best predictor of the variable $y_t$. This method of estimation of parameters based on the minimisation of SSE, is called "Ordinary Least Squares". It is simple and does not require any specific assumptions: we just minimise the overall distance by changing the values of parameters.

Another thing to note is the connection between the parameter $\hat{a}_1$ and the correlation coefficient. We have already briefly discussed this in Section \@ref(correlationCoefficient), we could estimate two models given the pair of variable $x$ and $y$:

1. Model \@ref(eq:SLRFormulaEstimated);
2. The inverse model $x_t = \hat{b}_0 + \hat{b}_1 y_t + u_t$.

We could then extract the slope parameters of the two models via \@ref(eq:OLSSLREstimates) and get the value of correlation coefficient as a geometric mean of the two:
\begin{equation}
    r_{x,y} = \mathrm{sign}(\hat{b}_1) \sqrt{\hat{a}_1 \hat{b}_1} = \mathrm{sign}(\mathrm{cov}(x,y)) \sqrt{\frac{\mathrm{cov}(x,y)}{\mathrm{V}(x)} \frac{\mathrm{cov}(x,y)}{\mathrm{V}(y)}} = \frac{\mathrm{cov}(x,y)}{\sqrt{V(x)V(y)}} ,
    (\#eq:correlationDerivationPearson)
\end{equation}
which is the formula \@ref(eq:measuresAssociationPearson). This is how the correlation coefficient was originally derived.

While we can do some inference based on simple linear regression, we know that the bivariate relations are not often met in practice: typically a variable is influenced by a set of variables, not just by one. This implies that the correct model would typically include many explanatory variables. This is why we will discuss inference in the next section.


## Multiple Linear Regression {#linearRegression}
While simple linear regression provides a basic understanding of the idea of capturing the relations between variables, it is obvious that in reality there are more than one external variable that would impact the response variable. This means that instead of \@ref(eq:SLRFormula) we should have:
\begin{equation}
    y_t = a_0 + a_1 x_{1,t} + a_2 x_{2,t} + \dots + a_{k-1} x_{k-1,t} + \epsilon_t ,
    (\#eq:MLRFormula)
\end{equation}
where $a_j$ is a $j$-th parameter for the respective $j$-th explanatory variable and there is $k-1$ of them in the model, meaning that when we want to estimate this model, we will have $k$ unknown parameters. The regression line of this model in population (aka expectation conditional on the values of explanatory variables) is:
\begin{equation}
    \mu_{y,t} = \mathrm{E}(y_t | \mathbf{x}_t) = a_0 + a_1 x_{1,t} + a_2 x_{2,t} + \dots + a_{k-1} x_{k-1,t} ,
    (\#eq:MLRExpectation)
\end{equation}
while in case of a sample estimation of the model we will use:
\begin{equation}
    \hat{y}_t = \hat{a}_0 + \hat{a}_1 x_{1,t} + \hat{a}_2 x_{2,t} + \dots + \hat{a}_{k-1} x_{k-1,t} .
    (\#eq:MLRExpectationSample)
\end{equation}
While the simple linear regression can be represented as a line on the plane with an explanatory variable and a response variable, the multiple linear regression cannot be easily represented in the same way. In case of two explanatory variables the plot becomes three dimensional and the regression line transforms into regression plane.

```{r scatterplot3dmtcars, fig.cap="3D scatterplot of Mileage vs Weight of a car and its Engine Horsepower.", echo=FALSE}
fit <- lm(mpg ~ wt+hp, data=mtcars)
s3d <- scatterplot3d::scatterplot3d(mtcars$wt, mtcars$hp, mtcars$mpg, pch=16, highlight.3d=TRUE,
                    type="h", xlab="Weight", ylab="Horsepower", zlab="Mileage", main="")
s3d$plane3d(fit, lty.box="solid", col="darkblue", draw_polygon=TRUE, polygon_args=list(col=rgb(0,0,0.8,0.2)))
```

Figure \@ref(fig:scatterplot3dmtcars) demonstrates a three dimensional scatterplot with the regression plane, going through the points, similar to how the regression line went through the two dimensional scatterplot \@ref(fig:scatterWeightMPG2). These sorts of plots are already difficult to read, but the situation becomes even more challenging, when more than two explanatory variables are under consideration: plotting 4D, 5D etc is not a trivial task. Still, what can be said about the parameters of the model even if we cannot plot it in the same way, is that they represent slopes for each variable, in a similar manner as $a_1$ did in the [simple linear regression](#simpleLinearRegression).


### OLS estimation
In order to show how the estimation of multiple linear regression is done, we need to present it in a more compact form. In order to do that we will introduce the following vectors:
\begin{equation}
    \mathbf{x}'_t = \begin{pmatrix}1 & x_{1,t} & \dots & x_{k-1,t} \end{pmatrix},
    \boldsymbol{a} = \begin{pmatrix}a_0 \\ a_{1} \\ \vdots \\ a_{k-1} \end{pmatrix} ,
    (\#eq:MLRVectors)
\end{equation}
where $'$ symbol is the transposition. This can then be substituted in \@ref(eq:MLRFormula) to get:
\begin{equation}
    y_t = \mathbf{x}'_t \boldsymbol{a} + \epsilon_t .
    (\#eq:MLRFormulaCompacter)
\end{equation}
But this is not over yet, we can make it even more compact, if we pack all those values with index $t$ in vectors and matrices:
\begin{equation}
    \mathbf{X} = \begin{pmatrix} \mathbf{x}'_1 \\ \mathbf{x}'_2 \\ \vdots \\ \mathbf{x}'_T \end{pmatrix}, 
    \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_T \end{pmatrix}, 
    \boldsymbol{\epsilon} = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_T \end{pmatrix} ,
    (\#eq:MLRMatrices)
\end{equation}
where $T$ is the sample size. This leads to the following compact form of multiple linear regression:
\begin{equation}
    \mathbf{y} = \mathbf{X} \boldsymbol{a} + \boldsymbol{\epsilon} .
    (\#eq:MLRFormulaCompactest)
\end{equation}
Now that we have this compact form of multiple linear regression, we can estimate it using linear algebra. Many statistical textbooks explain how the following result is obtained (this involves taking derivative of SSE \@ref(eq:OLSCriterion) with respect to $\boldsymbol{a}$ and equating it to zero):
\begin{equation}
    \hat{\boldsymbol{a}} = \left(\mathbf{X}' \mathbf{X}\right)^{-1} \mathbf{X}' \mathbf{y} .
    (\#eq:MLROLS)
\end{equation}
The formula \@ref(eq:MLROLS) is used in all the statistical software, including `lm()` function from `stats` package for R. Here is an example with the same `mtcars` dataset:

```{r}
mtcarsModel01 <- lm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb, mtcars)
```

The simplest plot that we can produce from this model is fitted values vs actuals, plotting $\hat{y}_t$ on x-axis and $y_t$ on the y-axis:

```{r}
plot(fitted(mtcarsModel01),actuals(mtcarsModel01))
```

The same plot is produced via `plot()` method if we use `alm()` function from `greybox` instead:

```{r mtcarsModel02Plot, fig.cap="Actuals vs fitted values for multiple linear regression model on mtcars data."}
mtcarsModel02 <- alm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb, mtcars)
plot(mtcarsModel02,1)
```

This plot can be used for diagnostic purposes and in ideal situation the red line (LOWESS line) should coincide with the grey one, which would mean that we have correctly capture the tendencies in the data, so that all the regression assumptions are satisfied (see Section \@ref(assumptions)). We will come back to the model diagnostics in Section \@ref(diagnostics).


### Quality of a fit {#linearRegressionQualityOfFit}
In order to get a general impression about the performance of the estimated model, we can calculate several in-sample measures, which could provide us insights about the fit of the model.

The first one is based on the OLS criterion, \@ref(eq:OLSCriterion) and is called either "Root Mean Squared Error" (RMSE) or a "standard error" or a "standard deviation of error" of the regression:
\begin{equation}
    \mathrm{RMSE} = \sqrt{\frac{1}{T-k} \sum_{t=1}^T e_t^2 }.
    (\#eq:RMSERegression)
\end{equation}
Note that it is divided by the number of degrees of freedom in the model, $T-k$, not on the number of observations. This is needed to correct the in-sample [bias](#estimatesPropertiesBias) of the measure. RMSE does not tell us about the in-sample performance but can be used to compare several models with the same response variable between each other: the lower RMSE is, the better the model fits the data. Note that this measure is not aware of the randomness in [the true model](#modelsMethods) and thus will be equal to zero in a model that fits the data perfectly (thus ignoring the existence of error term). This is a potential issue, as we might end up with a poor model that would seem like the best one.

Here is how this can be calculated for our model, estimated using `alm()` function:
```{r}
sigma(mtcarsModel02)
```

Another measure is called "Coefficient of Determination" and is calculated based on the following sums of squares:
\begin{equation}
    \mathrm{R}^2 = 1 - \frac{\mathrm{SSE}}{\mathrm{SST}} = \frac{\mathrm{SSR}}{\mathrm{SST}},
    (\#eq:Determination)
\end{equation}
where SSE$=\sum_{t=1}^T e_t^2 $ is the OLS criterion defined in \@ref(eq:OLSCriterion),
\begin{equation}
    \mathrm{SST}=\sum_{t=1}^T (y_t - \bar{y})^2,
    (\#eq:SST)
\end{equation}
is the total sum of squares (where $\bar{y}$ is the in-sample mean) and
\begin{equation}
    \mathrm{SSR}=\sum_{t=1}^T (\hat{y}_t - \bar{y})^2,
    (\#eq:SSR)
\end{equation}
is the sum of squares of the regression line. SSE, as discussed above, shows the overall distance of actual values from the regression line. The SST has an apparent connection with the variance of the response variable:
\begin{equation}
    \mathrm{V}(y) = \frac{1}{T-1} \sum_{t=1}^T (y_t - \bar{y})^2 = \frac{1}{T-1} \mathrm{SST} .
    (\#eq:dataVariance)
\end{equation}
Finally, SSR characterises the deviation of the regression line from the mean. In *the linear regression* (this is important! This property might be violated in other models), the three sums are related via the following equation:
\begin{equation}
    \mathrm{SST} = \mathrm{SSE} + \mathrm{SSR},
    (\#eq:SSTSum)
\end{equation}
which explains why the coefficient of determination \@ref(eq:Determination) can be calculated using two different formulae. If we want to interpret the coefficient of determination $\mathrm{R}^2$, we can imagine the following situations:

1. The model fits the data in the same way as a straight line (mean). In this case SSE would be equal to SST and SSR would be equal to zero (because $\hat{y}_t=\bar{y}$) and as a result the R$^2$ would be equal to zero.
2. The model fits the data perfectly, without any errors. In this situation SSE would be equal to zero and SSR would be equal to SST, because the regression would go through all points (i.e. $\hat{y}_t=y_t$). This would make R$^2$ equal to one.

In the linear regression model due to \@ref(eq:SSTSum), the coefficient of determination would always lie between zero and one, where zero means that the model does not explain the data at all and one means that it overfits the data. The value itself is usually interpreted as a percentage of variability in data explained by the model. This definition above provides us an important point about the coefficient of determination: it should not be equal to one, and it is alarming if it is very close to one - because in this situation we are implying that there is no randomness in the data, but this contradicts our definition of the statistical model (see Section \@ref(modelsMethods)). So, in practice we should not maximise R$^2$ and should be careful with models that have very high values of it. At the same time, too low values of R$^2$ are also alarming, as they tell us that the model is not very different from the global mean. So, coefficient of determination in general is not a very good measure for assessing performance of a model.

Here how this measure can be calculated in R based on the estimated model:
```{r}
1 - sigma(mtcarsModel02)^2*(nobs(mtcarsModel02)-nparam(mtcarsModel02)) /
    (var(actuals(mtcarsModel02))*(nobs(mtcarsModel02)-1))
```
Note that in this formula we used the relation between SSE and RMSE and between SST and V$(y)$, multiplying the values by $n-k$ and $n-1$ respectively. The resulting value tells us that the model has explained 94.7% deviations in the data.

Based on coefficient of determination, we can also calculate the coefficient of multiple correlation, which we have already discussed in Section \@ref(correlationsMixed):
\begin{equation}
    R = \sqrt{R^2} = \sqrt{\frac{\mathrm{SSR}}{\mathrm{SST}}} .
    (\#eq:multipleCorrelation)
\end{equation}

Furthermore, the value of coefficient of determination would always increase with the increase of number of variables included in the model. This is because every variable will explain some proportion of the data due to randomness. So, if we add redundant variables, the fit will improve, but the quality of model will decrease. Here is an example:
```{r}
mtcarsData$noise <- rnorm(nrow(mtcarsData),0,10)
mtcarsModel02WithNoise <- alm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb+noise,
                                   mtcarsData)
```
And here is the value of determination coefficient of the new model:
```{r}
1 - sigma(mtcarsModel02WithNoise)^2*(nobs(mtcarsModel02WithNoise)-nparam(mtcarsModel02WithNoise)) /
    (var(actuals(mtcarsModel02WithNoise))*(nobs(mtcarsModel02WithNoise)-1))
```
The value in the new model will always be higher than in the previous one, no matter how we generate the random fluctuations. This means that some sort of penalisation of the number of variables in the model is required in order to make the measure more reasonable. This is what adjusted coefficient of determination is supposed to do:
\begin{equation}
    R^2_{adj} = 1 - \frac{\mathrm{MSE}}{\mathrm{V}(y)} = 1 - \frac{(n-1)\mathrm{SSE}}{(n-k)\mathrm{SST}},
    (\#eq:DeterminationAdjusted)
\end{equation}
where MSE is the Mean Squared Error (square of RMSE \@ref(eq:RMSERegression)). So, instead of dividing sums of squares, in the adjusted R$^2$ we divide the entities that are based on degrees of freedom. Given the presence of $k$ in the formula \@ref(eq:DeterminationAdjusted), the coefficient will not necessarily increase with the addition of variables - when the variable does not contribute in the reduction of SSE of model substantially, R$^2$ will not go up.

Here how it can be calculated for a model in R:
```{r}
setNames(c(1 - sigma(mtcarsModel02)^2 / var(actuals(mtcarsModel02)),
           1 - sigma(mtcarsModel02WithNoise)^2 / var(actuals(mtcarsModel02WithNoise))),
         c("R^2-adj","R^2-adj, Noise"))
```
What we hope to see in the output above is that the model with the noise will have a lower value of adjusted R$^2$ than the model without it. However, given that we deal with randomness, if you reproduce this example many times, you will see different situation, including those, where introducing noise still increases the value of the parameter. So, you should not fully trust R$^2_{adj}$ either. When constructing a model or deciding what to include in it, you should always use your judgement - make sure that the variables included in the model are meaningful. Otherwise you can easily overfit the data, which would lead to inaccurate forecasts and inefficient estimates of parameters (see Section \@ref(assumptions) for details).


### Interpretation of parameters
Finally, we come to the discussion of parameters of a model. As mentioned earlier, each one of them represents the slope of the model. But there is more to the meaning of parameters of the model. Consider the coefficients of the previously estimated model:
```{r}
coef(mtcarsModel02)
```

Each of the parameters of this model shows an **average** effect of each variable on the mileage. They have a simple interpretation and show how the response variable will change **on average** with the increase of a variable by 1 unit, keeping all the other variables constant. For example, the parameter for `wt` (weight) shows that with the increase of weight of a car by 1000 pounds, the mileage would decrease **on average** by `r round(abs(coef(mtcarsModel02)["wt"]),3)` miles per gallon, if all the other variables do not change. I have made the word "average" boldface three times in this paragraph for a reason. This is a very important point to keep in mind - the parameters will not tell you how variable will change for any specific observation. They do not show how it will change for each point. The regression model capture average tendencies and thus the word "average" is very important in the interpretation. In each specific case, the increase of weight by 1 will lead to different decreases (and even increases in some cases). But if we take the arithmetic mean of those individual effects, it will be close to the value of the parameter in the model. This however is only possible if all the assumptions of regression hold (see Section \@ref(assumptions)).


## Regression uncertainty
Coming back to the example of mileage vs weight of cars, the estimated simple linear regression on the data was `r paste0(c("mpg=",round(coef(slmMPGWt),2),"wt+et"),collapse="")`. But what would happen if we estimate the same model on a different sample of data (e.g. 15 first observations instead of 32)?

```{r scatterWeightMPG4, fig.cap="Weight vs mileage and two regression lines."}
slmMPGWt <- lm(mpg~wt,mtcarsData)
slmMPGWt2 <- lm(mpg~wt,mtcarsData,subset=c(1:15))
plot(mtcarsData$wt, mtcarsData$mpg,
     xlab="Weight", ylab="Mileage",
     xlim=c(0,6), ylim=c(0,40))
abline(h=0, col="grey")
abline(v=0, col="grey")
abline(slmMPGWt,col="red")
text(4,35,paste0(c("mpg=",round(coef(slmMPGWt),2),"wt+et"),collapse=""), col="darkred")
abline(slmMPGWt2,col="darkblue")
text(1,20,paste0(c("mpg=",round(coef(slmMPGWt2),2),"wt+et"),collapse=""), col="darkblue")
legend("bottomleft",legend=c("Small subsample","Full sample"), lwd=1, col=c("blue","red"))
```

Figure \@ref(fig:scatterWeightMPG4) shows the two lines: the red one corresponds to the larger sample, while the blue one corresponds to the small one. We can see that these lines have different intercepts and slope parameters. So, which one of them is correct? An amateur analyst would say that the one that has more observations is the correct model. But a more experienced statistician would tell you that none of the two is correct. They are both estimated on a sample of data and they both inevitably inherit the uncertainty of the data, making them both incorrect if we compare them to the hypothetical [true model](#modelsMethods). This means that whatever regression model we estimate on a sample of data, it will be incorrect as well.

This uncertainty about the regression line actually comes to the uncertainty of estimates of parameters of the model. In order to see it more clearly, consider the example with Speed and Stopping Distances of Cars dataset from `datasets` package (`?cars`):

```{r scatterSpeedDistance, fig.cap="Speed vs stopping distance of cars"}
slmSpeedDistance <- alm(dist~speed,cars)
plot(cars$speed, cars$dist,
     xlab="Speed", ylab="Stopping Distance")
abline(h=0, col="grey")
abline(v=0, col="grey")
abline(slmSpeedDistance,col="red")
```

While the linear relation between these variables might be not the the most appropriate, it suffices for demonstration purposes. What we will do for this example is fit the model and then use a simple bootstrap technique to get estimates of parameters of the model. We will do that using `coefbootstrap()` method from `greybox` package. The bootstrap technique implemented in the function applies the same model to subsamples of the original data and returns a matrix with parameters. This way we get an idea about the empirical uncertainty of parameters:
```{r eval=FALSE}
slmSpeedDistance <- alm(dist~speed,cars)
slmSpeedDistanceBootstrap <- coefbootstrap(slmSpeedDistance)
```

```{r include=FALSE}
load("data/slmSpeedDistance.Rdata")
```

Based on that we can plot the histograms of the estimates of parameters.

```{r slmSpeedDistanceBoot, fig.cap="Distribution of bootstrapped parameters of a regression model"}
par(mfcol=c(1,2))
hist(slmSpeedDistanceBootstrap$coefficients[,1],
     xlab="Intercept", main="")
hist(slmSpeedDistanceBootstrap$coefficients[,2],
     xlab="Slope", main="")
```

Figure \@ref(fig:slmSpeedDistanceBoot) shows the uncertainty around the estimates of parameters. These distributions look similar to the normal distribution. In fact, if we repeated this example thousands of times, the distribution of estimates of parameters would indeed follow the normal one due to CLT (if the assumptions hold, see Sections \@ref(CLT) and \@ref(assumptions)). As a result, when we work with regression we should take this uncertainty about the parameters into account. This applies to both parameters analysis and forecasting.


### Confidence intervals
In order to take this uncertainty into account, we could construct confidence intervals for the estimates of parameters, using the principles discussed in Section \@ref(confidenceIntervals). This way we would hopefully have some idea about the uncertainty of the parameters, and not just rely on average values. If we assume that [CLT](#CLT) holds, we could use the t statistics for the calculation of the quantiles of distribution (we need to use t because we do not know the variance of estimates of parameters). But in order to do that, we need to have variances of estimates of parameters. One of possible ways of getting them would be the bootstrap used in the example above. However, this is a computationally expensive operation, and there is a more efficient procedure, which however only works with linear regression models either estimated using OLS or via Maximum Likelihood Estimation assuming Normal distribution (see Section \@ref(likelihoodApproach)). In these conditions the covariance matrix of parameters can be calculated using the following formula:
\begin{equation}
    \mathrm{V}(\hat{\mathbf{a}}) = \frac{1}{T-k} \sum_{t=1}^T e_t^2 \times \left(\mathbf{X}' \mathbf{X}\right)^{-1}.
    (\#eq:MLRcovarianceMatrix)
\end{equation}
This matrix will contain variances of parameters on the diagonal and covariances between the parameters on off-diagonals. In this specific case, we only need the diagonal elements. We can take square root of them to obtain standard errors of parameters, which can then be used to construct confidence intervals for each parameter $j$ via:
\begin{equation}
    a_j \in (\hat{a}_j + t_{\alpha/2}(T-k) s_{\hat{a}_j}, \hat{a}_j + t_{1-\alpha/2}(T-k) s_{\hat{a}_j}),
    (\#eq:MLRcovarianceMatrix)
\end{equation}
where $s_{\hat{a}_j}$ is the standard error of the parameter $\hat{a}_j$. All modern software does all these calculations automatically, so we do not need to do them manually. Here is an example:

```{r}
vcov(slmSpeedDistance)
```
This is the covariance matrix of parameters, the diagonal elements of which are then used in the `confint()` method:

```{r}
confint(slmSpeedDistance)
```
The confidence interval for speed above shows, for example, that if we repeat the construction of interval many times, the true value of parameter speed will lie in 95% of cases between 3.08 and 4.78. This gives an idea about the real effect in the population. We can also present all of this in the following summary (this is based on the `alm()` model, the other functions will produce different summaries):
```{r}
summary(slmSpeedDistance)
```

This summary provide all the necessary information about the estimates of parameters: their mean values in the column "Estimate", their standard errors in "Std. Error", the bounds of confidence interval and finally a star if the interval does not contain zero. This typically indicates that we are certain on the selected confidence level (95% in our example) about the sign of the parameter and that the effect really exists.


<!-- ### Hypothesis testing -->

<!-- ### Variance of regression -->


<!-- ## Dummy variables {#dummyVariables} -->

<!-- ### Interpretation of parameters -->


<!-- ## Variables transformations -->

<!-- ### Interpretation of parameters -->


## Likelihood Approach {#likelihoodApproach}
We will use different estimation techniques throughout this book, one of the main of which is **Maximum Likelihood Estimate** (MLE). The very rough idea of the approach is to maximise the chance that each observation in the sample follows a pre-selected distribution with specific set of parameters. In a nutshell, what we try to do when using likelihood for estimation, is fit the distribution function to the data. In order to demonstrate this idea, we start in a non-conventional way, with an example in R. We will then move to the mathematical side of the problem.

### An example in R
We consider a simple example, when we want to estimate the model $y_t = \mu_y + \epsilon_t$ (global average), assuming that the error term follows normal distribution: $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$, which means that $y_t \sim \mathcal{N}(\mu_{y}, \sigma^2)$. In this case we want to estimate two parameters using likelihood: $\hat{\mu}_y$ and $\hat{\sigma}^2$. First, we generate the random variable in R and plot its distribution:
```{r}
y <- rnorm(1000, 100, 10)
hist(y, xlim=c(50,150), main="", probability=TRUE)
```

As expected, the distribution of this variable (1000 observations) has the bell shape of Normal distribution. In order to estimate the parameters, for the distribution, we will try them one by one and see how the likelihood and the shape of the fitted curve to this histogram change. We start with $\hat{\mu}_y=80$ and $\hat{\sigma}=10$ just to see how the probability density function of normal distribution fits the data:

```{r MLENormalExample01, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=80$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),80,10),col="red",lwd=2)
abline(v=80,col="red",lwd=2)
```

and we get the following log-likelihood value (we will discuss how this formula can be obtained later):
```{r}
sum(dnorm(y,80,10,log=T))
```
In order for the normal distribution on \@ref(fig:MLENormalExample01) to fit the data well, we need to shift the estimate of $\mu_y$ to the right, thus increasing the value to, let's say, $\hat{\mu}_y=90$:

```{r MLENormalExample02, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=90$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),90,10),col="orange",lwd=2)
abline(v=90,col="orange",lwd=2)
```

Now, in Figure \@ref(fig:MLENormalExample02), the normal curve is much closer to the data, but it is still a bit off. The log-likelihood value in this case is `r round(sum(dnorm(y,90,10,log=T)),3)`, which is higher than the previous one, indicating that we are moving towards the maximum of the likelihood function. Moving it further, setting $\hat{\mu}_y=100$, we get:

```{r MLENormalExample03, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=100$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),100,10),col="green3",lwd=2)
abline(v=100,col="green3",lwd=2)
```

Figure \@ref(fig:MLENormalExample02) demonstrates a much better fit than in the previous cases with the log-likelihood of `r round(sum(dnorm(y,100,10,log=T)),3)`, which is even higher than in the previous case. We are almost there. In fact, in order to maximise this likelihood, we just need to calculate the sample mean of the variable (this is the MLE of the location parameter in normal distribution) and insert it in the function to obtain:

```{r MLENormalExample04, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=\\bar{y}$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),mean(y),10),col="darkgreen",lwd=2)
abline(v=mean(y),col="darkgreen",lwd=2)
```

So the value of $\hat{\mu}_y=\bar{y}=$ `r round(mean(y),3)` (where $\bar{y}$ is the sample mean) maximises the likelihood function, resulting in log-likelihood of `r round(sum(dnorm(y,mean(y),10,log=T)),3)`.

In a similar fashion we can get the MLE of the scale parameter $\sigma^2$ of the model. In this case, we will be changing the height of the distribution. Here is an example with $\hat{\mu}_y=$ `r round(mean(y),3)` and $\hat{\sigma}=15$:

```{r MLENormalExample05, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=\\bar{y}$ and $\\hat{\\sigma}=15$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),mean(y),15),col="royalblue",lwd=2)
abline(v=mean(y),col="royalblue",lwd=2)
```
Figure \@ref(fig:MLENormalExample05) demonstrates that the curve is located lower than needed, which implies that the scale parameter $\hat{\sigma}$ is too high. The log-likelihood value in this case is `r round(sum(dnorm(y,mean(y),15,log=T)),3)`. In order to get a better fit of the curve to the data, we need to reduce the $\hat{\sigma}$. Here how the situation would look for the case of $\hat{\sigma}=10$:

```{r MLENormalExample06, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=\\bar{y}$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),mean(y),10),col="darkblue",lwd=2)
abline(v=mean(y),col="darkblue",lwd=2)
```

The fit on Figure \@ref(fig:MLENormalExample06) is better than on Figure \@ref(fig:MLENormalExample05), which is also reflected in the log-likelihood value being equal to `r round(sum(dnorm(y,mean(y),10,log=T)),3)` instead of `r round(sum(dnorm(y,mean(y),15,log=T)),3)`. The best fit and the maximum of the likelihood is obtained, when the scale parameter is estimated using the formula $\hat{\sigma}^2 = \frac{1}{T}\sum_{t=1}^T\left(y_t - \bar{y}\right)^2$, resulting in log-likelihood of `r round(sum(dnorm(y,mean(y),sqrt(mean((y-mean(y))^2)),log=T)),3)`. Note that if we use the unbiased estimate of the variance $\hat{s}^2 = \frac{1}{T-1}\sum_{t=1}^T\left(y_t - \bar{y}\right)^2$, the log-likelihood will not reach the maximum and will be equal to `r round(sum(dnorm(y,mean(y),sd(y),log=T)),3)`. In our special case the difference between the two is infinitesimal, because of the large sample (1000 observations), but it will be more substantial on small samples. Still, the two likelihood values are diffrent, which can be checked in R via the following commands:
```{r, eval=FALSE}
# The maximum log-likelihood with the biased variance
logLik01 <- sum(dnorm(y,mean(y),sqrt(mean((y-mean(y))^2)),log=TRUE))
# The log-likelihood value with the unbiased variance
logLik02 <- sum(dnorm(y,mean(y),sd(y),log=TRUE))
# The difference between the two
logLik01 - logLik02
```

All of this is great, but so far we have discussed a very special case, when the data follows normal distribution and we fit the respective model. But what if the model is wrong (no kidding!)? In that case the idea stays the same: we need to find the parameters of the normal distribution, that would guarantee the best possible fit to the non-normal data. Here is an example with MLE of parameters of Normal distribution for the data following Log Normal one:

```{r MLENormalExample07, fig.cap="ML example with Normal curve on Log Normal data"}
y <- rlnorm(1000, log(80), 0.4)
hist(y, main="", probability=T, xlim=c(0,300))
lines(c(0:300),dnorm(c(0:300),mean(y),sd(y)),col="blue",lwd=2)
```

Figure \@ref(fig:MLENormalExample07) shows that the Normal model does not fit the Log Normal data properly, but this is the best we can get, given our assumptions. The log-likelihood in this case is `r round(sum(dnorm(y,mean(y),sd(y),log=TRUE)),3)`. The much better model would be the Log Normal one:

```{r MLENormalExample08, fig.cap="ML example with Log Normal curve on Log Normal data"}
hist(y, main="", probability=T, xlim=c(0,300))
lines(c(0:300),dlnorm(c(0:300),mean(log(y)),sd(log(y))),col="red",lwd=2)
```

The model in Figure \@ref(fig:MLENormalExample08) has the log likelihood of `r round(sum(dlnorm(y,mean(log(y)),sd(log(y)),log=TRUE)),3)`. This indicates that the Log Normal model is more appropriate for the data and gives us an idea that it is possible to compare different distributions via the likelihood, finding the better fit to the data. This idea is explored further in the [next section](#modelSelection).

As a final word, when it comes to more complicated models with more parameters and dynamic structure, the specific curves and data become more complicated, but the logic of the likelihood approach stays the same.

### Mathematical explanation {#likelihoodApproachMaths}
Now we can discuss the same idea from the mathematical point of view. We use an example of [normal distribution](#distributionsNormal) and a simple model as before:
\begin{equation}
    y_t = \mu_{y} + \epsilon_t,
    (\#eq:MLESimpleRegression)
\end{equation}
where $\mu_{y}$ is the population location parameter (the true parameter, the global mean). The typical assumption in regression context is that $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$, which means that $y_t \sim \mathcal{N}(\mu_{y}, \sigma^2)$. We can use this assumption in order to calculate the point likelihood value for each observation based on the [PDF of Normal distribution](#distributionsNormal):
\begin{equation}
    \mathcal{L} (\mu_{y}, \sigma^2 | y_t) = f(y_t | \mu_{y}, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{\left(y_t - \mu_{y,t} \right)^2}{2 \sigma^2} \right).
    (\#eq:MLEPointLik)
\end{equation}
Very roughly, what the value \@ref(eq:MLEPointLik) shows is the chance that the specific observation comes from the assumed model with specified parameters (we know that in real world the data does not come from any model, but this interprertation is easier to work with). Note that the likelihood is not the same as probability, because for any continuous random variables the probability for it to be equal to any specific number is equal to zero. However, the idea of likelihood has some similarities with the probability, so we prefer to refer to it as a "chance". The point likelihood \@ref(eq:MLEPointLik) is not very helpful on its own, but we can get $T$ values like that, based on our sample of data. We can then summarise it in one number, that would characterise the whole sample, given the assumed distribution, applied model and selected values of parameters:
\begin{equation}
    \mathcal{L} (\boldsymbol{\theta} | \mathbf{y}) = \mathcal{L} (\mu_{y}, \sigma^2 | \mathbf{y}) = \prod_{t=1}^T f(y_t | \mu_{y}, \sigma^2),
    (\#eq:MLEFullLik)
\end{equation}
where $\boldsymbol{\theta}$ is the vector of all parameters in the model (in our example, it is just the two of them). We take the product of likelihoods in \@ref(eq:MLEFullLik) because we need to get the joint likelihood for all observations and because we can typically assume that the point likelihoods are independent of each other (for example, the value on observation $t$ will not be influenced by the value on $t-1$). The value \@ref(eq:MLEFullLik) shows the summary chance that the data comes from the assumed model with specified parameters. Having this value, we can change the values of parameters of the model, getting different value of \@ref(eq:MLEFullLik) (as we did in the example above). Using an iterative procedure, we can get such estimates of parameters that would maximise the likelihood \@ref(eq:MLEFullLik), which are called Maximum Likelihood Estimates (MLE) of parameters. However, working with the products in that formula is difficult, so typically we linearise it using natural logarithm, obtaining log-likelihood:
\begin{equation}
    \ell (\boldsymbol{\theta} | \mathbf{y}) = \log \mathcal{L} (\boldsymbol{\theta} | \mathbf{y}) = -\frac{T}{2} \log(2 \pi \sigma^2) -\sum_{t=1}^T \frac{\left(y_t - \mu_{y,t} \right)^2}{2 \sigma^2} .
    (\#eq:MLEFullLogLik)
\end{equation}
Based on that, we can find some of parameters of the model analytically. For example, we can take derivative of \@ref(eq:MLEFullLogLik) with respect to the scale $\hat{\sigma}^2$ (which is an estimate of the true parameter $\sigma^2$) and equate it to zero in order to find the value that maximises the log-likelihood function in our sample:
\begin{equation}
    \frac{d \ell (\boldsymbol{\theta} | \mathbf{y})}{d \hat{\sigma}^2} = -\frac{T}{2} \frac{1}{\hat{\sigma}^2} + \frac{1}{2 \hat{\sigma}^4}\sum_{t=1}^T \left(y_t - \mu_{y,t} \right)^2 =0 , 
    (\#eq:MLEFullLogLikScale01)
\end{equation}
which after multiplication of both sides by $2 \hat{\sigma}^4$ leads to:
\begin{equation}
    T \hat{\sigma}^2 = \sum_{t=1}^T \left(y_t - \mu_{y,t} \right)^2 , 
    (\#eq:MLEFullLogLikScale02)
\end{equation}
or
\begin{equation}
    \hat{\sigma}^2 = \frac{1}{T}\sum_{t=1}^T \left(y_t - \mu_{y,t} \right)^2 .
    (\#eq:MLEFullLogLikScale)
\end{equation}
The value \@ref(eq:MLEFullLogLikScale) is in fact a [Mean Squared Error](#errorMeasures) (MSE) of the model. If we calculate the value of $\hat{\sigma}^2$ using the formula \@ref(eq:MLEFullLogLikScale), we will maximise the likelihood with respect to the scale parameter. In fact, we can insert \@ref(eq:MLEFullLogLikScale) in \@ref(eq:MLEFullLogLik) in order to obtain the so called concentrated (or profile) log-likelihood for the normal distribution:
\begin{equation}
    \ell^* (\boldsymbol{\theta}, \hat{\sigma}^2 | \mathbf{y}) = -\frac{T}{2}\left( \log(2 \pi e) + \log \hat{\sigma}^2 \right) .
    (\#eq:MLEFullLogLikConcentrated)
\end{equation}
This function is useful because it simplifies some calculations and also demonstrates the condition, for which the likelihood is maximised: the first part on the right hand side of the formula does not depend on the parameters of the model, it is only the $\log \hat{\sigma}^2$ that does. So, the maximum of the concentrated log-likelihood \@ref(eq:MLEFullLogLikConcentrated) is obtained, when $\hat{\sigma}^2$ is minimised, implying the minimisation of MSE, which is the mechanism behind the "Ordinary Least Squares" (OLS)
) estimation method. By doing this, we have just demonstrated that if we assume normality in the model, then the estimates of its parameters obtained via the maximisation of the likelihood coincide with the values obtained from OLS. So, why bother with MLE, when we have OLS?

First, the finding above holds for normal distribution only. If we assume a different [distribution](#distributions), we would get different estimates of parameters. In some cases, it might not be possible or reasonable to use OLS, but MLE would be a plausible option (for example, logistic, Poisson and any other non-standard model).

Second, the MLE of parameters have good statistical properties: they are [consistent](#estimatesPropertiesConsistency) and [efficient](#estimatesPropertiesEfficiency). These properties hold almost universally for many likelihoods under very mild conditions. Note that the MLE of parameters are not necessarily [unbiased](#estimatesPropertiesBias), but after estimating the model, one can de-bias some of them (for example, calculate the standard deviation of the error via devision of the sum of squared errors by the number of degrees of freedom $T-k$ instead of $T$).

Third, likelihood can be used for the model assessment, even when the standard statistics, such as $R^2$ or F-test are not available. We do not discuss these aspects in this textbook.

Finally, it permits the [model selection](#modelSelection) via information criteria. In general, this is not possible to do unless you assume a distribution and maximise the respective likelihood. In some statistical literature, you can notice that information criteria are calculated for the models estimated via OLS, but what the authors of such resources do not tell you is that there is still an assumption of normality behind this (see the link between OLS and MLE of Normal distribution above).

Note that the likelihood approach assumes that all parameters of the model are estimated, including location, scale, shape, shift etc of distribution. So typically it has more parameters to estimate than, for example, the OLS does. This is discussed in some detail later in the [next section](#statisticsNumberOfParameters).


## Calculating number of parameters in models {#statisticsNumberOfParameters}
When performing model selection and calculating different statistics, it is important to know how many parameters were estimated in the model. While this might seems trivial there are a number of edge cases and wrinkles that are seldom discussed in detail.

When it comes to inference based on regression models, the general idea is to calculate the number of **all the independent estimated parameters** $k$. This typically includes all initial components and all coefficients of the model together with the scale, shape and shift parameters of the assumed distribution (e.g. variance in the Normal distribution).

```{example}
In a simple regression model: $y_t = \beta_0 + \beta_1 x_t + \epsilon_t$ - assuming Normal distribution for $\epsilon_t$, using the MLE will result in the estimation of $k=3$: the two parameters of the model ($\beta_0$ and $\beta_1$) and the variance of the error term $\sigma^2$.
```

If likelihood is not used, then the number of parameters might be different. For example, if we estimate the model via the minimisation of MSE (similar to OLS), then the number of all estimated parameters does not include the variance anymore - it is obtained as a by product of the estimation. This is because the likelihood needs to have all the parameters of distribution in order to be maximised, but with MSE, we just minimise the mean of squared errors, and the variance of the distribution is obtained automatically. While the values of parameters might be the same, the logic is slightly different.

```{example}
This means that for the same simple linear regression, estimated using OLS, the number of parameters is equal to 2: estimates of $\beta_0$ and $\beta_1$.
```

In addition, all the restrictions on the parameters can reduce the number of estimated parameters, when they get to the boundary values.

```{example}
If we know that the parameter $\beta_1$ lies between 0 and 1, and in the estimation process it gets to the value of 1 (due to how the optimiser works), it can be considered as a restriction $\beta_1=1$. So, when estimated via the minimum of MSE with this restriction, this would imply that $k=1$.
```

In general, if a parameter is provided in the model, then it does not count towards the number of all estimated parameters. So, setting $b_1=1$ acts in the same fashion.

Finally, if a parameter is just a function of another one, then it does not count towards the $k$ as well.

```{example}
If we know that in the same simple linear regression $\beta_1 = \frac{\beta_0}{\sigma^2}$, then the number of all the estimated parameter via the maximum likelihood is 2: $\beta_0$ and $\sigma^2$.
```

We will come back to the number of parameters later in this textbook, when we discuss specific models.

A final note: typically, the standard maximum likelihood estimators for the scale, shape and shift parameters are biased in small samples and do not coincide with the OLS estimators. For example, in case of Normal distribuiton, OLS estimate of variance has $T-k$ in the denominator, while the likelihood one has just $T$. This needs to be taken into account, when the variance is used in forecasting.




## Typical assumptions of statistical models {#assumptions}
In order for a statistical model to work adequately and not to fail on data, several assumptions about it, when it is *applied* to the data, should hold. If they do not, then the model might lead to biased or inefficient estimates of parameters and forecasts. Here we briefly discuss the main of them, united in three big groups:

1. Model is correctly specified;
2. Residuals are independent and identicaly distributed (i.i.d.);
3. The explanatory variables are not correlated with anything but the response variable;

In Section \@ref(diagnostics) we also discuss how to diagnose the constructed models and fix the issues in cases, when the assumptions are violated.

### Model is correctly specified {#assumptionsCorrectModel}
This implies that:

1. We have not omitted important variables in the model (underfitting the data);
2. We do not have redundant variables in the model (overfitting the data);
3. The necessary transformation of the variables are applied;
4. We do not have outliers in the model.

(1): if there are some important variables that we did not include in the model, then the estimates of the parameters might be *biased* and in some cases quite seriously (e.g. positive sign instead of the negative one). This also means that the point forecasts from the model might be *biased* as well (systematic under or over forecasting).

(2): if there are redundant variables that are not needed in the model, then the estimates of parameters and point forecasts might be *unbiased*, but *inefficient.* This implies that the variance of parameters can be lower than needed and the prediction intervals can be narrower than needed.

(3): this means that, for example, instead of using a multiplicative model, we apply an additive one. The estimates of parameters and the point forecasts might be *biased* in this case as well: the model will produce linear trajectory of the forecast, when a non-linear one is needed.

(4): in a way, this is similar to (1), the presence of outliers might mean that we have missed some important information, meaning that the estimates of parameters and forecasts would be *biased* as well. There can be other reasons for outliers as well. For example, we might be using a wrong distributional assumptions. If so, this would imply that the prediction intervals from the model are narrower than needed.


### Residuals are i.i.d. {#assumptionsResidualsAreIID}
There are five assumptions in this group:

1. There is no autocorrelation in the residuals;
2. The residuals are homoscedastic;
3. The expectation of residuals is zero, no matter what;
4. The variable follows the specified distribution;
5. More generally speaking, distribution of residuals does not change over time.

(1): we expect that the model captures all the important aspects, so if the residuals are autocorrelated, then something is neglected by the applied model. Typically, this leads to *inefficient* estimates of parameters and in some cases they can also become *biased*. As a result, the point forecasts can be less accurate than expected and the prediction intervals might be wrong (wider or narrower than needed).

(2): if this is violated, then we say that there is a **heteroscedasticity** in the model. This means that with a change of variable, the variance of the residuals changes as well. If the model neglects this, then typically the estimates of parameters become *inefficient* and prediction intervals are wrong: they are wider than needed in some cases and narrower than needed in the other ones.

(3): while in sample, this holds automatically in many cases (e.g. when using Least Squares method for regression model estimation), this assumption might be violated in the holdout sample. In this case the point forecasts would be *biased*, because they typically do not take the non-zero mean of forecast error into account, and the prediction interval might be off as well, because of the wrong estimation of the scale of distribution (e.g. variance is higher than needed). This assumption also implies that the expectation of residuals is zero even conditional on the explanatory variables in the model. If it is not, then this might mean that there is still some important information omitted in the applied model.

Note that some models in ADAM framework assume that the expectation of residuals is equal to one instead of zero (e.g. multiplicative error models). The idea of the assumption stays the same, it is only the value that changes.

(4): in some cases we are interested in using methods that imply specific distributional assumptions about the model and its residuals. For example, it is assumed in the classical linear model that the error term follows Normal distribution. Estimating this model using MLE with the probability density function of Normal distribution or via minimisation of [Mean Squared Error](#errorMeasures) (MSE) would give *efficient* and *consistent* estimates of parameters. If the assumption of normality does not hold, then the estimates might be *inefficient* and in some cases *inconsistent*. When it comes to forecasting, the main issue in the wrong distributional assumption appears, when prediction intervals are needed: they might rely on a wrong distribution and be narrower or wider than needed. Finally, if we deal with the wrong distribution, then the model selection mechanism might be flawed and would lead to the selection of an inappropriate model.

(5): this assumption aligns with (4), but in this specific context implies that all the parameters of distribution stay the same and the shape of distribution does not change. If the former is violated then we might have one of the issues discussed above. If the latter is violated then we might produce *biased* forecasts and underestimate / overestimate the uncertainty about the future.


### The explanatory variables are not correlated with anything but the response variable {#assumptionsXreg}
There are two cases here as well:

1. No multicollinearity;
2. No endogeneity;

(1): the effect of **multicollinearity** implies that the variables included in the model are linearly dependent from each other. In this case, it becomes difficult to distinguish the effect of one variables from the other one. As a result, the estimates of parameters become *inefficient* and might become *biased* in some sever cases. In case of forecasting, the effect is not as straight forward, and in some cases might not damage the point forecasts, but can lead to prediction intervals of an incorrect width. It is important to note that this is in a way an assumption about the estimation of the model rather than the model itself: it is unreasonable to assume that explanatory variables are independent - reality is more complicated than we would want it to be, so inevitably some variables will be correlated. The main issue of multicollinearity comes to the difficulties in the model estimation in a sample. If we had all the data in the world, then the issue would not exist.

(2): **endogeneity** applies to the situation, when the dependent variable $y_t$ influences the explanatory variable $x_t$ in the model on the same observation. The relation in this case becomes bi-directional, meaning that the basic model is not appropriate in this situation any more. The parameters and forecasts will typically be *biased*, and a different estimation method is needed or maybe a different model would need to be constructed in order to fix this.

In many cases, in our discussions in this textbook, we assume that all of these assumptions hold. In some of the cases, we will say explicitly, which are violated and what needs to be done in those situations. In Section \@ref(diagnostics) we will discuss how these assumptions can be checked and how the issues caused by their violation can be fixed.


<!-- ## ARDL model -->

<!-- ## Scale model -->
