# Regression analysis {#regression}
While we do not expect to cover the regression analysis in its fullness, I think that it is important to have some basic understanding of what regression model is, how it can be estimated and used for forecasting. This chapter introduces the regression starting from the simple linear model and then moving to more advanced topics of multiple linear regression, model estimation, regression assumptions, dummy variables, variables transformations, ARDL and model for scale of distribution.


## Simple Linear Regression {#simpleLinearRegression}
When we want to analyse some relations between variables, we can do [graphical](#dataAnalysisGraphical) and [correlations](#correlations) analysis. But this will not provide us sufficient information about what happens with the response variable with the change of explanatory variable. So it makes sense to consider the possible relations between variables, and the basis for this is Simple Linear Regression, which can be represented in the form:
\begin{equation}
    y_t = a_0 + a_1 x_t + \epsilon_t ,
    (\#eq:SLRFormula)
\end{equation}
where $a_0$ is the intercept (constant term), $a_1$ is the coefficient for the slope parameter and $\epsilon_t$ is the error term. The regression model is a basic [statistical model](#modelsMethods) that captures the relation between an explanatory variable $x_t$ and the response variable $y_t$. The parameters of the models are typically denoted as $\beta_0$ and $\beta_1$ in econometrics literature, but we use $a_0$ and $a_1$ because we will use $\beta$ for other purposes later in this textbook.

In order to better understand what simple linear regression implies, consider the scatterplot (we discussed it earlier in Section \@ref(dataAnalysisGraphical)) shown in Figure \@ref(fig:scatterWeightMPG2).

```{r scatterWeightMPG2, fig.cap="Scatterplot diagram between weight and mileage."}
slmMPGWt <- lm(mpg~wt,mtcarsData)
plot(mtcarsData$wt, mtcarsData$mpg,
     xlab="Weight", ylab="Mileage",
     xlim=c(0,6), ylim=c(0,40))
abline(h=0, col="grey")
abline(v=0, col="grey")
abline(slmMPGWt,col="red")
text(4,35,paste0(c("mpg=",round(coef(slmMPGWt),2),"wt+et"),collapse=""))
```

The line drawn on the plot is the regression line, parameters of which were estimated based on the available data. In this case the intercept $\hat{a}_0$=`r round(coef(slmMPGWt)[1],2)`, meaning that this is where the red line crosses the y-axis, while the parameter of slope $\hat{a}_1$=`r round(coef(slmMPGWt)[2],2)` shows how fast the values change (how steep the line is). I've added hat symbols on the parameters to point out that they were estimated based on a sample of data. If we had all the data in the universe (population) and estimated a correct model on it, we would not need the hats. In simple linear regression, the re line will always go through the cloud of points, showing the averaged out tendencies. The one that we observe above can be summarise as "with the increase of weight, on average the mileage of cars goes down". Note that we might find some specific points, where the increase of weight would not decrease mileage (e.g. the two furthest left points show this), but this can be considered as a random fluctuation, so overall, the average tendency is as described above.


### Ordinary Least Squares (OLS) {#OLS}
For obvious reasons, we do not have the values of parameters from the population. This means that we will never know what the true intercept and slope are. Luckily, we can estimate them based on the sample of data. There are different ways of doing that, and the most popular one is called "Ordinary Least Squares" method. This is the method that was used in the estimation of the model in Figure \@ref(fig:scatterWeightMPG2). So, how does it work?

```{r scatterWeightMPG3, fig.cap="Scatterplot diagram between weight and mileage.", echo=FALSE}
slmMPGWt <- lm(mpg~wt,mtcarsData)
plot(mtcarsData$wt, mtcarsData$mpg,
     xlab="Weight", ylab="Mileage",
     xlim=c(0,6), ylim=c(0,40))
abline(h=0, col="grey")
abline(v=0, col="grey")
abline(slmMPGWt,col="red")
lines(rep(mtcarsData$wt[20],2),c(fitted(slmMPGWt)[20],mtcarsData$mpg[20]), lty=2)
text(mtcarsData$wt[20]+0.15,mean(c(fitted(slmMPGWt)[20],mtcarsData$mpg[20])),TeX("$e_t$"))
points(mtcarsData$wt[20],mtcarsData$mpg[20], pch=16)
points(mtcarsData$wt[20],fitted(slmMPGWt)[20], pch=3)
```

When we estimate the simple linear regression model, the model \@ref(eq:SLRFormula) transforms into:
\begin{equation}
    y_t = \hat{a}_0 + \hat{a}_1 x_t + e_t .
    (\#eq:SLRFormulaEstimated)
\end{equation}
This is because we do not know the true values of parameters and thus they are substituted by their estimates. This also applies to the error term for which in general $e_t \neq \epsilon_t$ because of the sample estimation. Now consider the same situation with weight vs mileage in Figure \@ref(fig:scatterWeightMPG3) but with some arbitrary line with unknown parameters. Each point on the plot will typically lie above or below the line, and we would be able to calculate the distances from those points to the line. They would correspond to $e_t = y_t - \hat{y}_t$, where $\hat{y}_t$ is the value of the regression line (aka "fitted" value) for each specific value of explanatory variable. For example, for the weight of car of `r mtcarsData$wt[20]` tones, the actual mileage is `r mtcarsData$mpg[20]`, while the fitted value is `r round(fitted(slmMPGWt)[20],3)`. The resulting error (or residual of model) is `r round(residuals(slmMPGWt)[20],3)`. We could collect all these errors of the model for all available cars based on their weights and this would result in a vector of positive and negative values like this:

```{r echo=FALSE}
residuals(slmMPGWt)
```

This corresponds to the formula:
\begin{equation}
    e_t = y_t - \hat{a}_0 - \hat{a}_1 x_t.
    (\#eq:SLRFormulaEstimatedError)
\end{equation}
If we needed to estimate parameters $\hat{a}_0$ and $\hat{a}_1$ of the model, we would need to minimise those distances by changing the parameters of the model. The problem is that some errors are positive, while the others are negative. If we just sum them up, they will cancel each other out, and we would loose the information about the distance. The simplest way to get rid of sign and keep the distance is by taking squares of each error and calculating Sum of Squared Errors for the whole sample $T$:
\begin{equation}
    \mathrm{SSE} = \sum_{t=1}^T e_t^2 .
    (\#eq:OLSCriterion)
\end{equation}
If we now minimise SSE by changing values of parameters $\hat{a}_0$ and $\hat{a}_1$, we will find those parameters that would guarantee that the line goes somehow through the cloud of points. Luckily, we do not need to use any fancy optimisers for this, as this has analytical solution (in order to get it, insert \@ref(eq:SLRFormulaEstimatedError) in \@ref(eq:OLSCriterion), take derivatives with respect to the parameters $\hat{a}_0$ and $\hat{a}_1$ and equate the resulting values to zero):
\begin{equation}
    \begin{aligned}
        \hat{a}_1 = & \frac{\mathrm{cov}(x,y)}{\mathrm{V}(x)} \\
        \hat{a}_0 = & \bar{y} - \hat{a}_1 \bar{x}
    \end{aligned} ,
    (\#eq:OLSSLREstimates)
\end{equation}
where $\bar{x}$ is the mean of the explanatory variable $x_t$ and $\bar{y}$ is the mean of the response variables $y_t$. Note that if for some reason $\hat{a}_1=0$ (for example, because the covariance between $x$ and $y$ is zero, implying that they are not correlated), then the intercept $\hat{a}_0 = \bar{y}$, meaning that the global average of the data is the best predictor of the variable $y_t$. This method of estimation of parameters based on the minimisation of SSE, is called "Ordinary Least Squares". It is simple and does not require any specific assumptions: we just minimise the overall distance by changing the values of parameters.

Another thing to note is the connection between the parameter $\hat{a}_1$ and the correlation coefficient. We have already briefly discussed this in Section \@ref(correlationCoefficient), we could estimate two models given the pair of variable $x$ and $y$:

1. Model \@ref(eq:SLRFormulaEstimated);
2. The inverse model $x_t = \hat{b}_0 + \hat{b}_1 y_t + u_t$.

We could then extract the slope parameters of the two models via \@ref(eq:OLSSLREstimates) and get the value of correlation coefficient as a geometric mean of the two:
\begin{equation}
    r_{x,y} = \mathrm{sign}(\hat{b}_1) \sqrt{\hat{a}_1 \hat{b}_1} = \mathrm{sign}(\mathrm{cov}(x,y)) \sqrt{\frac{\mathrm{cov}(x,y)}{\mathrm{V}(x)} \frac{\mathrm{cov}(x,y)}{\mathrm{V}(y)}} = \frac{\mathrm{cov}(x,y)}{\sqrt{V(x)V(y)}} ,
    (\#eq:correlationDerivationPearson)
\end{equation}
which is the formula \@ref(eq:measuresAssociationPearson). This is how the correlation coefficient was originally derived.

While we can do some inference based on simple linear regression, we know that the bivariate relations are not often met in practice: typically a variable is influenced by a set of variables, not just by one. This implies that the correct model would typically include many explanatory variables. This is why we will discuss inference in the next section.


## Multiple Linear Regression {#linearRegression}
While simple linear regression provides a basic understanding of the idea of capturing the relations between variables, it is obvious that in reality there are more than one external variable that would impact the response variable. This means that instead of \@ref(eq:SLRFormula) we should have:
\begin{equation}
    y_t = a_0 + a_1 x_{1,t} + a_2 x_{2,t} + \dots + a_{k-1} x_{k-1,t} + \epsilon_t ,
    (\#eq:MLRFormula)
\end{equation}
where $a_j$ is a $j$-th parameter for the respective $j$-th explanatory variable and there is $k-1$ of them in the model, meaning that when we want to estimate this model, we will have $k$ unknown parameters. The regression line of this model in population (aka expectation conditional on the values of explanatory variables) is:
\begin{equation}
    \mu_{y,t} = \mathrm{E}(y_t | \mathbf{x}_t) = a_0 + a_1 x_{1,t} + a_2 x_{2,t} + \dots + a_{k-1} x_{k-1,t} ,
    (\#eq:MLRExpectation)
\end{equation}
while in case of a sample estimation of the model we will use:
\begin{equation}
    \hat{y}_t = \hat{a}_0 + \hat{a}_1 x_{1,t} + \hat{a}_2 x_{2,t} + \dots + \hat{a}_{k-1} x_{k-1,t} .
    (\#eq:MLRExpectationSample)
\end{equation}
While the simple linear regression can be represented as a line on the plane with an explanatory variable and a response variable, the multiple linear regression cannot be easily represented in the same way. In case of two explanatory variables the plot becomes three dimensional and the regression line transforms into regression plane. Unfortunately, it becomes close to impossible to plot relations for more than two explanatory variables. But what can be said about the parameters of the model nonetheless is that they represent slopes for each variable, in a similar manner as $a_1$ did in the simple linear regression.


### OLS estimation
In order to show how the estimation of multiple linear regression is done, we need to present it in a more compact form. In order to do that we will introduce the following vectors:
\begin{equation}
    \mathbf{x}'_t = \begin{pmatrix}1 & x_{1,t} & \dots & x_{k-1,t} \end{pmatrix},
    \boldsymbol{a} = \begin{pmatrix}a_0 \\ a_{1} \\ \vdots \\ a_{k-1} \end{pmatrix} ,
    (\#eq:MLRVectors)
\end{equation}
where $'$ symbol is the transposition. This can then be substituted in \@ref(eq:MLRFormula) to get:
\begin{equation}
    y_t = \mathbf{x}_t \boldsymbol{a} + \epsilon_t .
    (\#eq:MLRFormulaCompacter)
\end{equation}
But this is not over yet, we can make it even more compact, if we pack all those values with index $t$ in vectors and matrices:
\begin{equation}
    \mathbf{X} = \begin{pmatrix} \mathbf{x}'_1 \\ \mathbf{x}'_2 \\ \vdots \\ \mathbf{x}'_T \end{pmatrix}, 
    \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_T \end{pmatrix}, 
    \boldsymbol{\epsilon} = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_T \end{pmatrix} ,
    (\#eq:MLRMatrices)
\end{equation}
where $T$ is the sample size. This leads to the following compact form of multiple linear regression:
\begin{equation}
    \mathbf{y} = \mathbf{X} \boldsymbol{a} + \boldsymbol{\epsilon} .
    (\#eq:MLRFormulaCompactest)
\end{equation}
Now that we have this compact form of multiple linear regression, we can estimate it using linear algebra. Many statistical textbooks explain how the following result is obtained (this involves taking derivative of SSE \@ref(eq:OLSCriterion) with respect to $\boldsymbol{a}$ and equating it to zero):
\begin{equation}
    \hat{\boldsymbol{a}} = \left(\mathbf{X}' \mathbf{X}\right)^{-1} \mathbf{X}' \mathbf{y} .
    (\#eq:MLROLS)
\end{equation}
The formula \@ref(eq:MLROLS) is used in all the statistical software, including `lm()` function from `stats` package for R. Here is an example with the same `mtcars` dataset:

```{r}
mtcarsModel01 <- lm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb, mtcars)
```

The simplest plot that we can produce from this model is fitted values vs actuals, plotting $\hat{y}_t$ on x-axis and $y_t$ on the y-axis:

```{r}
plot(fitted(mtcarsModel01),actuals(mtcarsModel01))
```

The same plot is produced via `plot()` method if we use `alm()` function from `greybox` instead:

```{r mtcarsModel02Plot, fig.cap="Actuals vs fitted values for multiple linear regression model on mtcars data."}
mtcarsModel02 <- alm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb, mtcars)
plot(mtcarsModel02,1)
```

This plot can be used for diagnostic purposes and in ideal situation the red line (LOWESS line) should coincide with the grey one, which would mean that we have correctly capture the tendencies in the data, so that all the regression assumptions are satisfied (see Section \@ref(assumptions)). We will come back to the model diagnostics in Section \@ref(diagnostics).


<!-- ### Quality of a fit -->

<!-- ### Interpretation of parameters -->

<!-- ## Regression uncertainty -->

<!-- ### Uncertainty of linear regression -->

<!-- ### Confidence intervals -->
<!-- One additional useful thing is that if we use OLS for estimation (or a maximum likelihood of Normal distribution, see in Section \@ref(likelihoodApproach)), then the following formula can be used to obtain covariance matrix of parameters: -->

<!-- ### Hypothesis testing -->


<!-- ## Dummy variables {#dummyVariables} -->

<!-- ### Interpretation of parameters -->


<!-- ## Variables transformations -->

<!-- ### Interpretation of parameters -->


## Likelihood Approach {#likelihoodApproach}
We will use different estimation techniques throughout this book, one of the main of which is **Maximum Likelihood Estimate** (MLE). The very rough idea of the approach is to maximise the chance that each observation in the sample follows a pre-selected distribution with specific set of parameters. In a nutshell, what we try to do when using likelihood for estimation, is fit the distribution function to the data. In order to demonstrate this idea, we start in a non-conventional way, with an example in R. We will then move to the mathematical side of the problem.

### An example in R
We consider a simple example, when we want to estimate the model $y_t = \mu_y + \epsilon_t$ (global average), assuming that the error term follows normal distribution: $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$, which means that $y_t \sim \mathcal{N}(\mu_{y}, \sigma^2)$. In this case we want to estimate two parameters using likelihood: $\hat{\mu}_y$ and $\hat{\sigma}^2$. First, we generate the random variable in R and plot its distribution:
```{r}
y <- rnorm(1000, 100, 10)
hist(y, xlim=c(50,150), main="", probability=TRUE)
```

As expected, the distribution of this variable (1000 observations) has the bell shape of Normal distribution. In order to estimate the parameters, for the distribution, we will try them one by one and see how the likelihood and the shape of the fitted curve to this histogram change. We start with $\hat{\mu}_y=80$ and $\hat{\sigma}=10$ just to see how the probability density function of normal distribution fits the data:

```{r MLENormalExample01, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=80$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),80,10),col="red",lwd=2)
abline(v=80,col="red",lwd=2)
```

and we get the following log-likelihood value (we will discuss how this formula can be obtained later):
```{r}
sum(dnorm(y,80,10,log=T))
```
In order for the normal distribution on \@ref(fig:MLENormalExample01) to fit the data well, we need to shift the estimate of $\mu_y$ to the right, thus increasing the value to, let's say, $\hat{\mu}_y=90$:

```{r MLENormalExample02, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=90$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),90,10),col="orange",lwd=2)
abline(v=90,col="orange",lwd=2)
```

Now, in Figure \@ref(fig:MLENormalExample02), the normal curve is much closer to the data, but it is still a bit off. The log-likelihood value in this case is `r round(sum(dnorm(y,90,10,log=T)),3)`, which is higher than the previous one, indicating that we are moving towards the maximum of the likelihood function. Moving it further, setting $\hat{\mu}_y=100$, we get:

```{r MLENormalExample03, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=100$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),100,10),col="green3",lwd=2)
abline(v=100,col="green3",lwd=2)
```

Figure \@ref(fig:MLENormalExample02) demonstrates a much better fit than in the previous cases with the log-likelihood of `r round(sum(dnorm(y,100,10,log=T)),3)`, which is even higher than in the previous case. We are almost there. In fact, in order to maximise this likelihood, we just need to calculate the sample mean of the variable (this is the MLE of the location parameter in normal distribution) and insert it in the function to obtain:

```{r MLENormalExample04, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=\\bar{y}$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),mean(y),10),col="darkgreen",lwd=2)
abline(v=mean(y),col="darkgreen",lwd=2)
```

So the value of $\hat{\mu}_y=\bar{y}=$ `r round(mean(y),3)` (where $\bar{y}$ is the sample mean) maximises the likelihood function, resulting in log-likelihood of `r round(sum(dnorm(y,mean(y),10,log=T)),3)`.

In a similar fashion we can get the MLE of the scale parameter $\sigma^2$ of the model. In this case, we will be changing the height of the distribution. Here is an example with $\hat{\mu}_y=$ `r round(mean(y),3)` and $\hat{\sigma}=15$:

```{r MLENormalExample05, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=\\bar{y}$ and $\\hat{\\sigma}=15$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),mean(y),15),col="royalblue",lwd=2)
abline(v=mean(y),col="royalblue",lwd=2)
```
Figure \@ref(fig:MLENormalExample05) demonstrates that the curve is located lower than needed, which implies that the scale parameter $\hat{\sigma}$ is too high. The log-likelihood value in this case is `r round(sum(dnorm(y,mean(y),15,log=T)),3)`. In order to get a better fit of the curve to the data, we need to reduce the $\hat{\sigma}$. Here how the situation would look for the case of $\hat{\sigma}=10$:

```{r MLENormalExample06, fig.cap="ML example with Normal curve and $\\hat{\\mu}_y=\\bar{y}$ and $\\hat{\\sigma}=10$"}
hist(y, xlim=c(50,150), main="", probability=TRUE)
lines(c(50:150),dnorm(c(50:150),mean(y),10),col="darkblue",lwd=2)
abline(v=mean(y),col="darkblue",lwd=2)
```

The fit on Figure \@ref(fig:MLENormalExample06) is better than on Figure \@ref(fig:MLENormalExample05), which is also reflected in the log-likelihood value being equal to `r round(sum(dnorm(y,mean(y),10,log=T)),3)` instead of `r round(sum(dnorm(y,mean(y),15,log=T)),3)`. The best fit and the maximum of the likelihood is obtained, when the scale parameter is estimated using the formula $\hat{\sigma}^2 = \frac{1}{T}\sum_{t=1}^T\left(y_t - \bar{y}\right)^2$, resulting in log-likelihood of `r round(sum(dnorm(y,mean(y),sqrt(mean((y-mean(y))^2)),log=T)),3)`. Note that if we use the unbiased estimate of the variance $\hat{s}^2 = \frac{1}{T-1}\sum_{t=1}^T\left(y_t - \bar{y}\right)^2$, the log-likelihood will not reach the maximum and will be equal to `r round(sum(dnorm(y,mean(y),sd(y),log=T)),3)`. In our special case the difference between the two is infinitesimal, because of the large sample (1000 observations), but it will be more substantial on small samples. Still, the two likelihood values are diffrent, which can be checked in R via the following commands:
```{r, eval=FALSE}
# The maximum log-likelihood with the biased variance
logLik01 <- sum(dnorm(y,mean(y),sqrt(mean((y-mean(y))^2)),log=TRUE))
# The log-likelihood value with the unbiased variance
logLik02 <- sum(dnorm(y,mean(y),sd(y),log=TRUE))
# The difference between the two
logLik01 - logLik02
```

All of this is great, but so far we have discussed a very special case, when the data follows normal distribution and we fit the respective model. But what if the model is wrong (no kidding!)? In that case the idea stays the same: we need to find the parameters of the normal distribution, that would guarantee the best possible fit to the non-normal data. Here is an example with MLE of parameters of Normal distribution for the data following Log Normal one:

```{r MLENormalExample07, fig.cap="ML example with Normal curve on Log Normal data"}
y <- rlnorm(1000, log(80), 0.4)
hist(y, main="", probability=T, xlim=c(0,300))
lines(c(0:300),dnorm(c(0:300),mean(y),sd(y)),col="blue",lwd=2)
```

Figure \@ref(fig:MLENormalExample07) shows that the Normal model does not fit the Log Normal data properly, but this is the best we can get, given our assumptions. The log-likelihood in this case is `r round(sum(dnorm(y,mean(y),sd(y),log=TRUE)),3)`. The much better model would be the Log Normal one:

```{r MLENormalExample08, fig.cap="ML example with Log Normal curve on Log Normal data"}
hist(y, main="", probability=T, xlim=c(0,300))
lines(c(0:300),dlnorm(c(0:300),mean(log(y)),sd(log(y))),col="red",lwd=2)
```

The model in Figure \@ref(fig:MLENormalExample08) has the log likelihood of `r round(sum(dlnorm(y,mean(log(y)),sd(log(y)),log=TRUE)),3)`. This indicates that the Log Normal model is more appropriate for the data and gives us an idea that it is possible to compare different distributions via the likelihood, finding the better fit to the data. This idea is explored further in the [next section](#modelSelection).

As a final word, when it comes to more complicated models with more parameters and dynamic structure, the specific curves and data become more complicated, but the logic of the likelihood approach stays the same.

### Mathematical explanation {#likelihoodApproachMaths}
Now we can discuss the same idea from the mathematical point of view. We use an example of [normal distribution](#distributionsNormal) and a simple model as before:
\begin{equation}
    y_t = \mu_{y} + \epsilon_t,
    (\#eq:MLESimpleRegression)
\end{equation}
where $\mu_{y}$ is the population location parameter (the true parameter, the global mean). The typical assumption in regression context is that $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$, which means that $y_t \sim \mathcal{N}(\mu_{y}, \sigma^2)$. We can use this assumption in order to calculate the point likelihood value for each observation based on the [PDF of Normal distribution](#distributionsNormal):
\begin{equation}
    \mathcal{L} (\mu_{y}, \sigma^2 | y_t) = f(y_t | \mu_{y}, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{\left(y_t - \mu_{y,t} \right)^2}{2 \sigma^2} \right).
    (\#eq:MLEPointLik)
\end{equation}
Very roughly, what the value \@ref(eq:MLEPointLik) shows is the chance that the specific observation comes from the assumed model with specified parameters (we know that in real world the data does not come from any model, but this interprertation is easier to work with). Note that the likelihood is not the same as probability, because for any continuous random variables the probability for it to be equal to any specific number is equal to zero. However, the idea of likelihood has some similarities with the probability, so we prefer to refer to it as a "chance". The point likelihood \@ref(eq:MLEPointLik) is not very helpful on its own, but we can get $T$ values like that, based on our sample of data. We can then summarise it in one number, that would characterise the whole sample, given the assumed distribution, applied model and selected values of parameters:
\begin{equation}
    \mathcal{L} (\boldsymbol{\theta} | \mathbf{y}) = \mathcal{L} (\mu_{y}, \sigma^2 | \mathbf{y}) = \prod_{t=1}^T f(y_t | \mu_{y}, \sigma^2),
    (\#eq:MLEFullLik)
\end{equation}
where $\boldsymbol{\theta}$ is the vector of all parameters in the model (in our example, it is just the two of them). We take the product of likelihoods in \@ref(eq:MLEFullLik) because we need to get the joint likelihood for all observations and because we can typically assume that the point likelihoods are independent of each other (for example, the value on observation $t$ will not be influenced by the value on $t-1$). The value \@ref(eq:MLEFullLik) shows the summary chance that the data comes from the assumed model with specified parameters. Having this value, we can change the values of parameters of the model, getting different value of \@ref(eq:MLEFullLik) (as we did in the example above). Using an iterative procedure, we can get such estimates of parameters that would maximise the likelihood \@ref(eq:MLEFullLik), which are called Maximum Likelihood Estimates (MLE) of parameters. However, working with the products in that formula is difficult, so typically we linearise it using natural logarithm, obtaining log-likelihood:
\begin{equation}
    \ell (\boldsymbol{\theta} | \mathbf{y}) = \log \mathcal{L} (\boldsymbol{\theta} | \mathbf{y}) = -\frac{T}{2} \log(2 \pi \sigma^2) -\sum_{t=1}^T \frac{\left(y_t - \mu_{y,t} \right)^2}{2 \sigma^2} .
    (\#eq:MLEFullLogLik)
\end{equation}
Based on that, we can find some of parameters of the model analytically. For example, we can take derivative of \@ref(eq:MLEFullLogLik) with respect to the scale $\hat{\sigma}^2$ (which is an estimate of the true parameter $\sigma^2$) and equate it to zero in order to find the value that maximises the log-likelihood function in our sample:
\begin{equation}
    \frac{d \ell (\boldsymbol{\theta} | \mathbf{y})}{d \hat{\sigma}^2} = -\frac{T}{2} \frac{1}{\hat{\sigma}^2} + \frac{1}{2 \hat{\sigma}^4}\sum_{t=1}^T \left(y_t - \mu_{y,t} \right)^2 =0 , 
    (\#eq:MLEFullLogLikScale01)
\end{equation}
which after multiplication of both sides by $2 \hat{\sigma}^4$ leads to:
\begin{equation}
    T \hat{\sigma}^2 = \sum_{t=1}^T \left(y_t - \mu_{y,t} \right)^2 , 
    (\#eq:MLEFullLogLikScale02)
\end{equation}
or
\begin{equation}
    \hat{\sigma}^2 = \frac{1}{T}\sum_{t=1}^T \left(y_t - \mu_{y,t} \right)^2 .
    (\#eq:MLEFullLogLikScale)
\end{equation}
The value \@ref(eq:MLEFullLogLikScale) is in fact a [Mean Squared Error](#errorMeasures) (MSE) of the model. If we calculate the value of $\hat{\sigma}^2$ using the formula \@ref(eq:MLEFullLogLikScale), we will maximise the likelihood with respect to the scale parameter. In fact, we can insert \@ref(eq:MLEFullLogLikScale) in \@ref(eq:MLEFullLogLik) in order to obtain the so called concentrated (or profile) log-likelihood for the normal distribution:
\begin{equation}
    \ell^* (\boldsymbol{\theta}, \hat{\sigma}^2 | \mathbf{y}) = -\frac{T}{2}\left( \log(2 \pi e) + \log \hat{\sigma}^2 \right) .
    (\#eq:MLEFullLogLikConcentrated)
\end{equation}
This function is useful because it simplifies some calculations and also demonstrates the condition, for which the likelihood is maximised: the first part on the right hand side of the formula does not depend on the parameters of the model, it is only the $\log \hat{\sigma}^2$ that does. So, the maximum of the concentrated log-likelihood \@ref(eq:MLEFullLogLikConcentrated) is obtained, when $\hat{\sigma}^2$ is minimised, implying the minimisation of MSE, which is the mechanism behind the "Ordinary Least Squares" (OLS)
) estimation method. By doing this, we have just demonstrated that if we assume normality in the model, then the estimates of its parameters obtained via the maximisation of the likelihood coincide with the values obtained from OLS. So, why bother with MLE, when we have OLS?

First, the finding above holds for normal distribution only. If we assume a different [distribution](#distributions), we would get different estimates of parameters. In some cases, it might not be possible or reasonable to use OLS, but MLE would be a plausible option (for example, logistic, Poisson and any other non-standard model).

Second, the MLE of parameters have good statistical properties: they are [consistent](#estimatesPropertiesConsistency) and [efficient](#estimatesPropertiesEfficiency). These properties hold almost universally for many likelihoods under very mild conditions. Note that the MLE of parameters are not necessarily [unbiased](#estimatesPropertiesBias), but after estimating the model, one can de-bias some of them (for example, calculate the standard deviation of the error via devision of the sum of squared errors by the number of degrees of freedom $T-k$ instead of $T$).

Third, likelihood can be used for the model assessment, even when the standard statistics, such as $R^2$ or F-test are not available. We do not discuss these aspects in this textbook.

Finally, it permits the [model selection](#modelSelection) via information criteria. In general, this is not possible to do unless you assume a distribution and maximise the respective likelihood. In some statistical literature, you can notice that information criteria are calculated for the models estimated via OLS, but what the authors of such resources do not tell you is that there is still an assumption of normality behind this (see the link between OLS and MLE of Normal distribution above).

Note that the likelihood approach assumes that all parameters of the model are estimated, including location, scale, shape, shift etc of distribution. So typically it has more parameters to estimate than, for example, the OLS does. This is discussed in some detail later in the [next section](#statisticsNumberOfParameters).


## Calculating number of parameters in models {#statisticsNumberOfParameters}
When performing model selection and calculating different statistics, it is important to know how many parameters were estimated in the model. While this might seems trivial there are a number of edge cases and wrinkles that are seldom discussed in detail.

When it comes to inference based on regression models, the general idea is to calculate the number of **all the independent estimated parameters** $k$. This typically includes all initial components and all coefficients of the model together with the scale, shape and shift parameters of the assumed distribution (e.g. variance in the Normal distribution).

```{example}
In a simple regression model: $y_t = \beta_0 + \beta_1 x_t + \epsilon_t$ - assuming Normal distribution for $\epsilon_t$, using the MLE will result in the estimation of $k=3$: the two parameters of the model ($\beta_0$ and $\beta_1$) and the variance of the error term $\sigma^2$.
```

If likelihood is not used, then the number of parameters might be different. For example, if we estimate the model via the minimisation of MSE (similar to OLS), then the number of all estimated parameters does not include the variance anymore - it is obtained as a by product of the estimation. This is because the likelihood needs to have all the parameters of distribution in order to be maximised, but with MSE, we just minimise the mean of squared errors, and the variance of the distribution is obtained automatically. While the values of parameters might be the same, the logic is slightly different.

```{example}
This means that for the same simple linear regression, estimated using OLS, the number of parameters is equal to 2: estimates of $\beta_0$ and $\beta_1$.
```

In addition, all the restrictions on the parameters can reduce the number of estimated parameters, when they get to the boundary values.

```{example}
If we know that the parameter $\beta_1$ lies between 0 and 1, and in the estimation process it gets to the value of 1 (due to how the optimiser works), it can be considered as a restriction $\beta_1=1$. So, when estimated via the minimum of MSE with this restriction, this would imply that $k=1$.
```

In general, if a parameter is provided in the model, then it does not count towards the number of all estimated parameters. So, setting $b_1=1$ acts in the same fashion.

Finally, if a parameter is just a function of another one, then it does not count towards the $k$ as well.

```{example}
If we know that in the same simple linear regression $\beta_1 = \frac{\beta_0}{\sigma^2}$, then the number of all the estimated parameter via the maximum likelihood is 2: $\beta_0$ and $\sigma^2$.
```

We will come back to the number of parameters later in this textbook, when we discuss specific models.

A final note: typically, the standard maximum likelihood estimators for the scale, shape and shift parameters are biased in small samples and do not coincide with the OLS estimators. For example, in case of Normal distribuiton, OLS estimate of variance has $T-k$ in the denominator, while the likelihood one has just $T$. This needs to be taken into account, when the variance is used in forecasting.




## Typical assumptions of statistical models {#assumptions}
In order for a statistical model to work adequately and not to fail on data, several assumptions about it, when it is *applied* to the data, should hold. If they do not, then the model might lead to biased or inefficient estimates of parameters and forecasts. Here we briefly discuss the main of them, united in three big groups:

1. Model is correctly specified;
2. Residuals are independent and identicaly distributed (i.i.d.);
3. The explanatory variables are not correlated with anything but the response variable;

In Section \@ref(diagnostics) we also discuss how to diagnose the constructed models and fix the issues in cases, when the assumptions are violated.

### Model is correctly specified {#assumptionsCorrectModel}
This implies that:

1. We have not omitted important variables in the model (underfitting the data);
2. We do not have redundant variables in the model (overfitting the data);
3. The necessary transformation of the variables are applied;
4. We do not have outliers in the model.

(1): if there are some important variables that we did not include in the model, then the estimates of the parameters might be *biased* and in some cases quite seriously (e.g. positive sign instead of the negative one). This also means that the point forecasts from the model might be *biased* as well (systematic under or over forecasting).

(2): if there are redundant variables that are not needed in the model, then the estimates of parameters and point forecasts might be *unbiased*, but *inefficient.* This implies that the variance of parameters can be lower than needed and the prediction intervals can be narrower than needed.

(3): this means that, for example, instead of using a multiplicative model, we apply an additive one. The estimates of parameters and the point forecasts might be *biased* in this case as well: the model will produce linear trajectory of the forecast, when a non-linear one is needed.

(4): in a way, this is similar to (1), the presence of outliers might mean that we have missed some important information, meaning that the estimates of parameters and forecasts would be *biased* as well. There can be other reasons for outliers as well. For example, we might be using a wrong distributional assumptions. If so, this would imply that the prediction intervals from the model are narrower than needed.


### Residuals are i.i.d. {#assumptionsResidualsAreIID}
There are five assumptions in this group:

1. There is no autocorrelation in the residuals;
2. The residuals are homoscedastic;
3. The expectation of residuals is zero, no matter what;
4. The variable follows the specified distribution;
5. More generally speaking, distribution of residuals does not change over time.

(1): we expect that the model captures all the important aspects, so if the residuals are autocorrelated, then something is neglected by the applied model. Typically, this leads to *inefficient* estimates of parameters and in some cases they can also become *biased*. As a result, the point forecasts can be less accurate than expected and the prediction intervals might be wrong (wider or narrower than needed).

(2): if this is violated, then we say that there is a **heteroscedasticity** in the model. This means that with a change of variable, the variance of the residuals changes as well. If the model neglects this, then typically the estimates of parameters become *inefficient* and prediction intervals are wrong: they are wider than needed in some cases and narrower than needed in the other ones.

(3): while in sample, this holds automatically in many cases (e.g. when using Least Squares method for regression model estimation), this assumption might be violated in the holdout sample. In this case the point forecasts would be *biased*, because they typically do not take the non-zero mean of forecast error into account, and the prediction interval might be off as well, because of the wrong estimation of the scale of distribution (e.g. variance is higher than needed). This assumption also implies that the expectation of residuals is zero even conditional on the explanatory variables in the model. If it is not, then this might mean that there is still some important information omitted in the applied model.

Note that some models in ADAM framework assume that the expectation of residuals is equal to one instead of zero (e.g. multiplicative error models). The idea of the assumption stays the same, it is only the value that changes.

(4): in some cases we are interested in using methods that imply specific distributional assumptions about the model and its residuals. For example, it is assumed in the classical linear model that the error term follows Normal distribution. Estimating this model using MLE with the probability density function of Normal distribution or via minimisation of [Mean Squared Error](#errorMeasures) (MSE) would give *efficient* and *consistent* estimates of parameters. If the assumption of normality does not hold, then the estimates might be *inefficient* and in some cases *inconsistent*. When it comes to forecasting, the main issue in the wrong distributional assumption appears, when prediction intervals are needed: they might rely on a wrong distribution and be narrower or wider than needed. Finally, if we deal with the wrong distribution, then the model selection mechanism might be flawed and would lead to the selection of an inappropriate model.

(5): this assumption aligns with (4), but in this specific context implies that all the parameters of distribution stay the same and the shape of distribution does not change. If the former is violated then we might have one of the issues discussed above. If the latter is violated then we might produce *biased* forecasts and underestimate / overestimate the uncertainty about the future.


### The explanatory variables are not correlated with anything but the response variable {#assumptionsXreg}
There are two cases here as well:

1. No multicollinearity;
2. No endogeneity;

(1): the effect of **multicollinearity** implies that the variables included in the model are linearly dependent from each other. In this case, it becomes difficult to distinguish the effect of one variables from the other one. As a result, the estimates of parameters become *inefficient* and might become *biased* in some sever cases. In case of forecasting, the effect is not as straight forward, and in some cases might not damage the point forecasts, but can lead to prediction intervals of an incorrect width. It is important to note that this is in a way an assumption about the estimation of the model rather than the model itself: it is unreasonable to assume that explanatory variables are independent - reality is more complicated than we would want it to be, so inevitably some variables will be correlated. The main issue of multicollinearity comes to the difficulties in the model estimation in a sample. If we had all the data in the world, then the issue would not exist.

(2): **endogeneity** applies to the situation, when the dependent variable $y_t$ influences the explanatory variable $x_t$ in the model on the same observation. The relation in this case becomes bi-directional, meaning that the basic model is not appropriate in this situation any more. The parameters and forecasts will typically be *biased*, and a different estimation method is needed or maybe a different model would need to be constructed in order to fix this.

In many cases, in our discussions in this textbook, we assume that all of these assumptions hold. In some of the cases, we will say explicitly, which are violated and what needs to be done in those situations. In Section \@ref(diagnostics) we will discuss how these assumptions can be checked and how the issues caused by their violation can be fixed.


<!-- ## ARDL model -->

<!-- ## Scale model -->
