# Explanatory variables in ADAM {#ADAMX}
In real life, the need for explanatory variables arises when some external factors influence the response variable, which cannot be ignored and impact the final forecasts and their accuracy. Examples of such variables in the demand forecasting context include price changes, promotional activities, temperature etc. In some cases, the changes in these factors would not substantially impact the demand, but this does not apply universally to the problem. If we omit this information from the model, this will be damaging for both point forecasts and prediction intervals [see discussion in Chapter 12 of @SvetunkovSBA].

While the inclusion of explanatory variables in the context of ARIMA models is a relatively well-studied topic [for example, this was discussed by @Box1976], in the case of ETS, there is only a Chapter 9 in @Hyndman2008b and a handful of papers. @Koehler2012 discuss the mechanism detection and approximation of outliers via an ETSX model (ETS with explanatory variables). The authors show that if an outlier appears at the end of the series, it will seriously impact the final forecast. However, if it appears either in the middle or at the beginning of the series, the impact on the final forecast is typically negligible. This is relevant to our discussion because there is a direct link between dealing with outliers in @Koehler2012 and including explanatory variables in ETSX in terms of how the model is formulated in these two situations. @Kourentzes2015 used ETSX successfully for promotional modelling, demonstrating that it outperforms the conventional ETS in terms of point forecasts accuracy in cases when promotions happen.

The state-space model \@ref(eq:ETSADAMStateSpace) can be easily extended by including additional components and explanatory variables. This chapter discusses the main aspects of ADAM with explanatory variables, how it is formulated, and how the more advanced models can be built upon it. Furthermore, the parameters for these additional components can either be fixed (static) or change over time (dynamic). We discuss both in the following sections. We also show that the stability and forecastability conditions, discussed in Section \@ref(stabilityConditionAdditiveError) for the pure additive ETS model, will be different in the case of the ETSX model and that the classical definitions should be updated to cater for the introduction of the explanatory variables. We also briefly discuss the inclusion of categorical variables in the ETSX model and show that the seasonal ETS models can be considered special cases of ADAM ETSX in some situations.

Furthermore, we will use the terms "**deterministic**" and "**stochastic**" explanatory variables to denote the situations when the values of these variables are known in advance or can be controlled by us. An example of the former would be the price of a product or a promotion that we decide to have. An example of the latter would be the temperature, which we cannot control and thus can be considered as a stochastic variable.

As a final note, we will carry out the discussion of the topic on the example of ADAM ETSX, keeping in mind that the same principles will hold for ADAM ARIMAX because the two are formulated in the same way. The more general dynamic model (encompassing ETS and/or ARIMA) with explanatory variables is called "ADAMX" in this and further chapters.


## ADAMX: Model formulation {#ADAMXFormulation}
As discussed previously, there are two types of errors in ADAM:

1. Additive discussed in @Hyndman2008b in Chapter \@ref(ADAMETSIntroduction) in case of ETS and Chapter \@ref(ADAMARIMA) for ARIMA,
2. Multiplicative covered in Chapter \@ref(ADAMETSPureMultiplicativeChapter) for ETS and in Subsection \@ref(ADAMARIMAPureMultiplicative).

The inclusion of explanatory variables in ADAMX is determined by the type of the error, so that in case of (1) the measurement equation of the model is:
\begin{equation}
  {y}_{t} = a_{0,t} + a_{1,t} x_{1,t} + a_{2,t} x_{2,t} + \dots + a_{n,t} x_{n,t} + \epsilon_t ,
  (\#eq:ETSXADAMStateSpacePureAdditiveMeasurement)
\end{equation}
where $a_{0,t}$ is the point value based on all ETS components (for example, $a_{0,t}=l_{t-1}$ in case of ETS(A,N,N)), $x_{i,t}$ is the $i$-th explanatory variable, $a_{i,t}$ is its parameter and $n$ is the number of explanatory variables. We will call the estimated parameters of such model $\hat{a}_{i,t}$. In the simple case, the transition equation for such model would imply that the parameters $a_{i,t}$ do not change over time:
\begin{equation}
    a_{i,t} = a_{i,t-1} \text{ for all } i = 1, \dots, n
  (\#eq:ETSXADAMStateSpacePureAdditiveTransition)
\end{equation}
Various complex mechanisms for the states update can be proposed instead of \@ref(eq:ETSXADAMStateSpacePureAdditiveTransition), but we do not discuss them at this point. Typically, the initial values of parameters would be estimated at the optimisation stage, either based on likelihood or some other loss function, so the index $t$ can be dropped, substituting $a_{i,t}=a_{i}$ for all $i=1,\dots,n$.

When it comes to the multiplicative error model, it should be formulated differently. The most straight forward would be to formulate the model in logarithms in order to linearise it:
\begin{equation}
  \log {y}_{t} = \log a_{0,t} + a_{1,t} x_{1,t} + a_{2,t} x_{2,t} + \dots + a_{n,t} x_{n,t} + \log(1+ \epsilon_t).
  (\#eq:ETSXADAMStateSpacePureMultiplicativeMeasurement)
\end{equation}

::: remark
If log-log model is required, all that needs to be done, is to substitute $x_{i,t}$ with $\log x_{i,t}$.
:::

The compact form of the ADAMX model implies that the explanatory variables $x_{i,t}$ are included in the measurement vector $\mathbf{w}_{t}$, making it change over time. The parameters are then moved to the state vector, and a diagonal matrix is added to the existing transition matrix. Finally, the persistence vector for the parameters of explanatory variables should contain zeroes. The state space model, in that case, can be represented as:
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \mathbf{w}'_t \mathbf{v}_{t-\mathbf{l}} + \epsilon_t \\
    & \mathbf{v}_t = \mathbf{F} \mathbf{v}_{t-\mathbf{l}} + \mathbf{g} \epsilon_t
  \end{aligned}
  (\#eq:ETSXADAMStateSpacePureAdditiveFull)
\end{equation}
for the pure additive and 
\begin{equation}
  \begin{aligned}
    {y}_{t} = & \exp\left(\mathbf{w}'_t \log \mathbf{v}_{t-\mathbf{l}} + \log(1 + \epsilon_t)\right) \\
    \log \mathbf{v}_t = & \mathbf{F} \log \mathbf{v}_{t-\mathbf{l}} + \log(\mathbf{1}_k + \mathbf{g} \epsilon_t)
  \end{aligned}. 
  (\#eq:ETSXADAMStateSpacePureMultiplicativeFull)
\end{equation}
for the pure multiplicative models. So, the only thing that changes in these models is the time varying measurement vector $\mathbf{w}'_t$ instead of the fixed one. For example, in case of ETSX(A,Ad,A) we will have:
\begin{equation}
  \begin{aligned}
    \mathbf{F} =
    \begin{pmatrix} 1 & \phi & 0 & 0 & \dots & 0 \\
                    0 & \phi & 0 & 0 & \dots & 0 \\
                    0 & 0 & 1 & 0 & \dots & 0 \\
                    0 & 0 & 0 & 1 & \dots & 0 \\
                    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & 0 & 0 & \dots & 1
    \end{pmatrix},
    & \mathbf{w}_t = \begin{pmatrix} 1 \\ \phi \\ 1 \\ x_{1,t} \\ \vdots \\x_{n,t} \end{pmatrix},
    & \mathbf{g} = \begin{pmatrix} \alpha \\ \beta \\ \gamma \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \\
    & \mathbf{v}_{t} = \begin{pmatrix} l_t \\ b_t \\ s_t \\ a_{1,t} \\ \vdots \\ a_{n,t} \end{pmatrix},
    & \mathbf{l} = \begin{pmatrix} 1 \\ 1 \\ m \\ 1 \\ \vdots \\ 1 \end{pmatrix}
  \end{aligned},
  (\#eq:ETSXADAMAAAMatrices)
\end{equation}
which is equivalent to the combination of equations \@ref(eq:ETSXADAMStateSpacePureAdditiveMeasurement) and \@ref(eq:ETSXADAMStateSpacePureAdditiveTransition):
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + \phi b_{t-1} + s_{t-m} + a_{1,t} x_{1,t} + \dots + a_{n,t} x_{n,t} + \epsilon_t \\
    & l_t = l_{t-1} + \phi b_{t-1} + \alpha \epsilon_t \\
    & b_t = \phi b_{t-1} + \beta \epsilon_t \\
    & s_t = s_{t-m} + \gamma \epsilon_t \\
    & a_{1,t} = a_{1,t-1} \\
    & \vdots \\
    & a_{n,t} = a_{n,t-1}
  \end{aligned}.
  (\#eq:ETSXADAMAAA)
\end{equation}
Alternatively, the state, measurement and persistence vectors and transition matrix can be split each into two parts, separating the ETS and X parts in the state space equations:
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \mathbf{w}' \mathbf{v}_{t-\mathbf{l}} + \mathbf{x}'_{t} \mathbf{a}_{t-1} + \epsilon_t \\
    & \mathbf{v}_{1,t} = \mathbf{F} \mathbf{v}_{t-\mathbf{l}} + \mathbf{g} \epsilon_t \\
    & \mathbf{a}_{t} = \mathbf{a}_{t-1}
  \end{aligned} ,
  (\#eq:ETSXADAMStateSpacePureAdditiveFullAlternative)
\end{equation}
where $\mathbf{w}$, $\mathbf{F}$, $\mathbf{g}$ and $\mathbf{v}_{t}$ contain the elements of the conventional components of ADAM and $\mathbf{a}_{t}$ is the vector of parameters for the explanatory variables.

When all the smoothing parameters of the ETS part of the model are equal to zero, the ETSX reverts to a deterministic model, directly related to the multiple linear regression. For example, in case of ETSX(A,N,N) with $\alpha=0$ we get:
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + a_{1,t} x_{1,t} + \dots + a_{n,t} x_{n,t} + \epsilon_t \\
    & l_t = l_{t-1} \\
    & a_{1,t} = a_{1,t-1} \\
    & \vdots \\
    & a_{n,t} = a_{n,t-1}
  \end{aligned},
  (\#eq:ETSXADAMANNDeterministic)
\end{equation}
where $l_t=a_0$ is the intercept of the model. \@ref(eq:ETSXADAMANNDeterministic) can be rewritten in the conventional way, dropping the transition part of the state space model:
\begin{equation}
    y_{t} = a_0 + a_{1} x_{1,t} + \dots + a_{n} x_{n,t} + \epsilon_t .
  (\#eq:linearRegression)
\end{equation}
In the case of models with trend and/or seasonal, the model becomes equivalent to the regression with deterministic trend and/or seasonality. This means that, in general, ADAMX implies that we are dealing with a regression with time-varying intercept, where the principles of this variability are defined by the ADAM components (e.g. intercept can vary seasonally). Similar properties are obtained with the multiplicative error model. The main difference is that the impact of explanatory variables on the response variable will vary with the intercept changes. The model, in this case, combines the strengths of the multiplicative regression and the dynamic model, where the variability of the response variable changes with the change of the baseline model (ADAM ETS and/or ADAM ARIMA in this case).


## Conditional expectation and variance of ADAMX {#ADAMXConventionalConditionalMoments}
### ADAMX with deterministic explanatory variables
ETS models have a severe limitation, which will be discussed in chapter \@ref(ADAMUncertainty): they assume that the model's parameters are known, i.e. there is no variability in them and that the in-sample estimates are fixed no matter how the sample size changes. This limitation also impacts the ETSX part. While in the case of point forecasts, this is not important, this affects the conditional variance and prediction intervals. As a result, the conditional mean and variance of the conventional ADAMX assume that the parameters $a_0, \dots a_n$ are also known, leading to the following formulae in the case of the pure additive model, based on what was discussed in the section \@ref(pureAdditiveExpectationAndVariance):
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & \sum_{i=1}^d \left(\mathbf{w}_{m_i,t}' \mathbf{F}_{m_i}^{\lceil\frac{h}{m_i}\rceil-1} \right) \mathbf{v}_{t} \\
    \text{V}(y_{t+h}|t) = & \left( \sum_{i=1}^d \left(\mathbf{w}_{m_i,t}' \sum_{j=1}^{\lceil\frac{h}{m_i}\rceil-1} \mathbf{F}_{m_i}^{j-1} \mathbf{g}_{m_i} \mathbf{g}'_{m_i} (\mathbf{F}_{m_i}')^{j-1} \mathbf{w}_{m_i,t} \right) + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSXADAMStateSpaceANNRecursionMeanAndVarianceGeneral)
\end{equation}
the main difference from the moments of the conventional model (from Section \@ref(pureAdditiveExpectationAndVariance)) being is the index $t$ in the measurement vector $\mathbf{w}_t$. As an example, here how the two statistics will look in case of ETSX(A,N,N):
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & l_{t} + \sum_{i=1}^n a_i x_{i,t+h} \\
    \text{V}(y_{t+h}|t) = & \left((h-1) \alpha^2 + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSXADAMStateSpaceANNRecursionMeanAndVariance)
\end{equation}
where the variance ignores the potential variability rising from the explanatory variables because of the ETS limitations. This assumes that the future values of explanatory variables $x_{i,t}$ are known (the variable is deterministic). As a result, the prediction and confidence intervals for the ADAMX model would typically be narrower than expected and would only be adequate in cases of large samples, where the law of large numbers would start working [Section 4.2 of @SvetunkovSBA], reducing the variance of parameters (this is assuming that the typical assumptions of the model from Section \@ref(assumptions) hold).


### ADAMX with stochastic explanatory variables {#ADAMXConventionalConditionalMomentsRandom}
Note that the ADAMX works well in cases when the future values of $x_{i,t+h}$ are known. It is a realistic assumption when we control the explanatory variables (e.g. prices and promotions for our product). But when the variables are out of our control, they need to be forecasted somehow. In this case we are assuming that each $x_{i,t}$ is a stochastic variable with some dynamic conditional one step ahead expectation $\mu_{x_{i,t}}$ and a one step ahead variance $\sigma^2_{x_{i,1}}$. **Note** that in this case, we treat the available explanatory variables as models on their own, not just as values given to us from above. This assumption of randomness will change the conditional moments of the model. Here is what we will have in the case of ETSX(A,N,N) (given that the typical assumptions from Section \@ref(assumptions) hold):
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & l_{t} + \sum_{i=1}^n a_i \mu_{x_{i,t+h}} \\
    \text{V}(y_{t+h}|t) = & \left((h-1) \alpha^2 + 1 \right) \sigma^2 + \sum_{i=1}^n a^2_i \sigma^2_{x_{i,h}} + 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i a_j \sigma_{x_{i,h},x_{j,h}}
  \end{aligned},
  (\#eq:ETSXADAMStateSpaceANNRecursionMeanAndVarianceRandomness)
\end{equation}
where $\sigma^2_{x_{i,h}}$ is the conditional variance of $x_{i}$ h steps ahead, $\sigma_{x_{i,h},x_{j,h}}$ is the h steps ahead covariance between the explanatory variables $x_{i,h}$ and $x_{j,h}$, both conditional on the information available at the observation $t$. similarly, if we are interested in one step ahead point forecast from the model, it should take the randomness of explanatory variables into account and become:
\begin{equation}
  \begin{aligned}
    \mu_{y,t |t-1} = &  \left. \mathrm{E}\left(l_{t-1} + \sum_{i=1}^n a_i x_{i,t} + \epsilon_{t} \right| t-1 \right) = \\
                = & l_{t-1} + \sum_{i=1}^n a_i \mu_{x_{i,t}}
  \end{aligned}.
  (\#eq:ADAMETSXANNStepAhead)
\end{equation}
So, in the case of ADAMX with random explanatory variables, the model should be constructed based on the expectations of those variables, not the random values themselves. This explains, for example, why @Athanasopoulos2011 found that some models with predicted explanatory variables work better than the model with the variables themselves. This means that, when estimating the model, such as ETS(A,N,N), the following should be constructed:
\begin{equation}
  \begin{aligned}
    & \hat{y}_{t} = \hat{l}_{t-1} + \sum_{i=1}^n \hat{a}_{i,t} \hat{x}_{i,t} \\
    & e_t = y_t -\hat{y}_{t} \\
    & \hat{l}_{t} = \hat{l}_{t-1} + \hat{\alpha} e_t \\
    & \hat{a}_{i,t} = \hat{a}_{i,t-1} \text{ for each } i \in \{1, \dots, n\}
  \end{aligned},
  (\#eq:ADAMETSXANNConstructed)
\end{equation}
where $\hat{x}_{i,t}$ is the in-sample conditional one step ahead mean for the explanatory variable $x_i$.

Finally, as discussed previously, the conditional moments for the pure multiplicative and mixed models do not generally have closed forms, implying that the simulations need to be carried out. The situation becomes more challenging in the case of random explanatory variables because that randomness needs to be introduced in the model itself and propagated throughout the time series. This is not a trivial task, which we will discuss in Section \@ref(adamRefitted).


## Dynamic X in ADAMX {#ADAMXDynamic}
::: remark
The model discussed in this section assumes particular dynamics of parameters, aligning with what the conventional ETS assumes: parameters are correlated with the states of the model. It does not treat parameters as independent as, for example, MSOE state space models do, which makes this model restrictive in its application. But this type of model works well with categorical variables, as I show later in this section.
:::

As discussed in Section \@ref(ADAMXFormulation), the parameters of the explanatory variables in ADAMX can be assumed to be constant over time or can be assumed to vary according to some mechanism. The most reasonable one in the SSOE framework relies on the same error for different components of the model because this mechanism aligns with the model itself. @Osman2015 proposed one of such mechanisms, relying on the differences of the data. The primary motivation of their approach was to make the dynamic ADAMX model stable, which is a challenging task. However, this mechanism relies on the assumption of non-stationarity of the explanatory variables, which does not always make sense (for example, it is unreasonable in the case of promotional data). An alternative approach discussed in this section is the one initially proposed by @Svetunkov1985 based on the stochastic approximation mechanism and further developed in @Svetunkov2014Textbook.

We start with the following linear regression model:
\begin{equation}
    y_{t} = a_{0,t-1} + a_{1,t-1} x_{1,t} + \dots + a_{n,t-1} x_{n,t} + \epsilon_t ,
  (\#eq:linearRegressionDynamic)
\end{equation}
where all parameters vary over time and $a_{0,t}$ represents the value from the conventional additive error ETS model. The updating mechanism for the parameters is straight forward and relies on the ratio of the error term and the respective explanatory variables:
\begin{equation}
    a_{i,t} = a_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta_i \frac{\epsilon_t}{x_{i,t}} \text{ for each } i \in \{1, \dots, n\}, \text{ if } x_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. ,
  (\#eq:linearRegressionDynamicUpdates)
\end{equation}
where $\delta_i$ is the smoothing parameter of the $i$-th explanatory variable. The same model can be represented in the state space form, based on the equations, similar to \@ref(eq:ETSXADAMStateSpacePureAdditiveFull):
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \mathbf{w}'_t \mathbf{v}_{t-\mathbf{l}} + \epsilon_t \\
    & \mathbf{v}_t = \mathbf{F} \mathbf{v}_{t-\mathbf{l}} + \mathbf{z}_t \mathbf{g} \epsilon_t
  \end{aligned}
  (\#eq:ADAMETSXPureAdditiveDynamicFull)
\end{equation}
where $\mathbf{z}_t = \mathrm{diag}\left(\mathbf{w}_t\right)^{-1}=\mathbf{I}_{k+n} \odot (\mathbf{w}_t \mathbf{1}_{k+n})$ is the diagonal matrix consisting of inverses of explanatory variables, $\mathbf{I}_{k+n}$ is the identity matrix for $k$ ADAM components and $n$ explanatory variables and $\odot$ is Hadamard product for element-wise multiplication. This is the inverse of the diagonal matrix based on the measurement vector, for which those values that cannot be inverted (due to division by zero) are substitute by zeroes in order to reflect the condition in \@ref(eq:linearRegressionDynamicUpdates). In addition to what \@ref(eq:ETSXADAMStateSpacePureAdditiveFull) contained, we add smoothing parameters $\delta_i$ in the persistence vector $\mathbf{g}$ for each of the explanatory variables.

If the error term is multiplicative, then the model changes to:
\begin{equation}
  \begin{aligned}
    & y_{t} = \exp \left(a_{0,t-1} + a_{1,t-1} x_{1,t} + \dots + a_{n,t-1} x_{n,t} + \log(1+ \epsilon_t) \right) \\
    & a_{i,t} = a_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta_i \frac{\log(1+\epsilon_t)}{x_{i,t}} \text{ for each } i \in \{1, \dots, n\}, \text{ if } x_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right.
  \end{aligned} .
  (\#eq:linearRegressionDynamicMultiplicative)
\end{equation}
The formulation \@ref(eq:linearRegressionDynamicMultiplicative) differs from the conventional pure multiplicative ETS model because the smoothing parameter $\delta_i$ is not included inside the error term $1+\epsilon_t$, which simplifies some derivations and makes the model easier to work with. Mixed ETS models can also have explanatory variables, but I suggest aligning the type of explanatory variable model with the error term.

Note that if it is suspected that the explanatory variables exhibit non-stationarity and are not cointegrated with the response variable, then their differences can be used instead of $x_{i,t}$ in \@ref(eq:ADAMETSXPureAdditiveDynamicFull) and \@ref(eq:linearRegressionDynamicMultiplicative). In this case, the model would coincide with the one proposed by @Osman2015. The decision of taking the differences for the different parts of the model should be made based on each specific situation. Here is an example of the ETSX(A,N,N) model with differenced explanatory variables:
\begin{equation}
  \begin{aligned}
    & y_{t} = a_{0,t-1} + a_{1,t-1} \Delta x_{1,t} + \dots + a_{n,t-1} \Delta x_{n,t} + \epsilon_t , \\
    & a_{i,t} = a_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta_i \frac{\epsilon_t}{\Delta x_{i,t}} \text{ for each } i \in \{1, \dots, n\}, \text{ if } \Delta x_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. ,
  \end{aligned}
  (\#eq:linearRegressionDynamicDifferences)
\end{equation}
where $\Delta x_{i,t} = x_{i,t} -x_{i,t-1}$ is the differences of the $i$-th exogenous variable.

Finally, to distinguish the ADAMX with static parameters from the ADAMX with dynamic ones, we will use the letters "S" and "D" in the names of models. So, the model \@ref(eq:ETSXADAMANNDeterministic) can be called ETSX(A,N,N){S}, while the model \@ref(eq:linearRegressionDynamicMultiplicative), assuming that $a_{0,t-1}=l_{t-1}$, would be called ETSX(M,N,N){D}. We use curly brackets to split the ETS states from the type of X. Furthermore, given that the model with static regressors is assumed in many contexts to be the default one, the ETSX(\*,\*,\*){S} model can also be denoted as just ETSX(\*,\*,\*).


### Recursion for dynamic ADAMX {#ADAMXDynamicMoments}
Similar to how it was discussed in Section \@ref(ADAMXConventionalConditionalMomentsRandom), we can have two cases in the dynamic model: (1) deterministic explanatory variables, (2) stochastic explanatory variables. For illustrative purposes, we will use a non-seasonal model for which the lag vector $\mathbf{l}$ contains ones only, keeping in mind that other pure additive models can be easily used instead. The cases of non-additive ETS models are not discussed in this part in detail -- the moments for these models need to be calculated based on simulations. So, as discussed previously, the model can be written in the following general way, assuming that all elements of $\mathbf{l}$ are equal to one:
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \mathbf{w}'_t \mathbf{v}_{t-1} + \epsilon_t \\
    & \mathbf{v}_t = \mathbf{F} \mathbf{v}_{t-1} + \mathbf{z}_t \mathbf{g} \epsilon_t
  \end{aligned} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonal)
\end{equation}
Based on this model, we can get the recursive relation for $h$ steps ahead, similar to how it was done in Section \@ref(adamETSPureAdditiveRecursive):
\begin{equation}
  \begin{aligned}
    & {y}_{t+h} = \mathbf{w}'_{t+h} \mathbf{v}_{t+h-1} + \epsilon_{t+h} \\
    & \mathbf{v}_{t+h-1} = \mathbf{F} \mathbf{v}_{t+h-2} + \mathbf{z}_{t+h-1} \mathbf{g} \epsilon_{t+h-1}
  \end{aligned} ,
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursive01)
\end{equation}
where the second equation can be represented based on the values available on observation $t$:
\begin{equation}
  \mathbf{v}_{t+h-1} = \mathbf{F}^{h-1} \mathbf{v}_{t} + \sum_{j=1}^{h-1} \mathbf{F}^{h-1-j} \mathbf{z}_{t+j} \mathbf{g} \epsilon_{t+j} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursive02)
\end{equation}
Substituting the equation \@ref(eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursive02) in the measurement equation of \@ref(eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursive01) leads to the final recursion:
\begin{equation}
    {y}_{t+h} = \mathbf{w}'_{t+h} \mathbf{F}^{h-1} \mathbf{v}_{t} + \mathbf{w}'_{t+h} \sum_{j=1}^{h-1} \mathbf{F}^{h-1-j} \mathbf{z}_{t+j} \mathbf{g} \epsilon_{t+j} + \epsilon_{t+h} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursiveFinal)
\end{equation}

### Conditional moments for deterministic explanatory variables in ADAMX{D}
Based on this recursion, we can calculate the conditional mean and variance for the model. First, we assume that the explanatory variables are controlled by an analyst, and are known for $j=1, \dots, h$:
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = & \text{E}(y_{t+h}|t) = \mathbf{w}'_{t+h} \mathbf{F}^{h-1} \mathbf{v}_{t} \\
    & \text{V}(y_{t+h}|t) = \left(\mathbf{w}'_{t+h} \sum_{j=1}^{h-1} \mathbf{F}^{h-1-j} \mathbf{z}_{t+j} \mathbf{g} \right)^2 \sigma^2 + \sigma^2
  \end{aligned} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalMomentsNonRandom)
\end{equation}
The formulae for conditional moments in this case look similar to the ones from the pure additive ETS model in Section \@ref(pureAdditiveExpectationAndVariance) with only difference being the interaction with time varying measurument vector.

### Conditional mean for stochastic explanatory variables in ADAMX{D}
In the case of stochastic explanatory variables, the conditional expectation is straightforward and is similar to the one in the static ADAMX model:
\begin{equation}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = \boldsymbol{\mu}'_{w,t+h} \mathbf{F}^{h-1} \mathbf{v}_{t} ,
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalMomentsRandom)
\end{equation}
where $\boldsymbol{\mu}'_{w,t+h}$ is the vector of conditional h steps ahead expectations for each element in the $\mathbf{w}_{t+h}$. In the case of ETS components, the vector would contain ones. However, when it comes to conditional variance, it is more complicated because it introduces complex interactions between variances of different variables and the error term. As a result, it would be easier to get the correct variance based on simulations, assuming that the explanatory variables and the error term change according to some assumed models.


## Stability and forecastability conditions of ADAMX
It can be shown that any static ADAMX is not stable (as it was defined for pure additive ETS models in Section \@ref(stabilityConditionAdditiveError)), meaning that the weights of such model do not decline to zero over time. To see this, we can draw an analogy with a deterministic model, discussed in the context of pure additive ETS (from Section \@ref(stabilityConditionAdditiveError)). For example, we have already discussed that when $\alpha=0$, the ETS(A,N,N) model becomes equivalent to the global level, loses the stability condition, but still can be forecastable. It becomes a simple but still useful and efficient model:
\begin{equation}
  y_{t} = a_0 + \epsilon_t .
  (\#eq:ADAMETSGlobalLevelSimple)
\end{equation}
Similarly, the X part of ADAMX will always be unstable, but can be useful. For example, with $\alpha=0$, ETSX(A,N,N) reverts back to the linear regression:
\begin{equation}
  y_{t} = a_0 + a_{1} x_{1,t} + \dots + a_n x_{n,t} + \epsilon_t .
  (\#eq:ADAMETSXRegression)
\end{equation}
This does not make the model inappropriate and only means that the weights do not decline over time. According to the conventional approach to ETS, if the dynamic part of the model is stable, but the overall model does not pass stability check just because of the X part, then the whole model will be considered unstable and potentially dangerous to use. But this is absurd. Following the same logic, we would need to avoid regression models in forecasting because they are not stable. Furthermore, there are no issues constructing ARIMAX models, but @Hyndman2008b claim that there are some with ETSX, which does not make sense if we recall the connection between ETS and ARIMA (discussed in Section \@ref(ARIMAandETS)). This only means that the stability/forecastability conditions should be checked for the dynamic part of the model (ETS or ARIMA) separately, ignoring the X part. Technically, this implies creating a separate transition matrix, persistence and measurement vectors and calculating the discount matrix for the ETS / ARIMA part to check already discussed stability and forecastability conditions (Section \@ref(stabilityConditionAdditiveError)).

When it comes to the dynamic ADAMX, the situation changes because now the smoothing parameters for the model coefficients determine how weights decline over time. It can be shown based on \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04) that the values of the state vector on the observation $t$ can be calculated via the recursion (here we provide a formula for the non-seasonal case, keeping in mind that in case of the seasonal one, the derivation and the main message will be similar):
\begin{equation}
  \mathbf{v}_{t} = \prod_{j=1}^{t-1}\mathbf{D}_{t-j} \mathbf{v}_{0} + \sum_{j=0}^{t-1} \prod_{i=0}^{j} \mathbf{D}_{t-i} y_{t-j},
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalBackRecursion)
\end{equation}
where $\mathbf{D}_t=\mathbf{F} -\mathbf{z}_t \mathbf{g} \mathbf{w}_{t}'$ is the time varying discount matrix. The main issue in the case of dynamic ADAMX is that the stability condition varies over time together with the values of explanatory variables in $\mathbf{z}_t$. So, it is not possible to derive the stability condition for the general case. In order to make sure that the model is stable, we need for all eigenvalues of each $\mathbf{D}_{j}$ for all $j=\{1,\dots,t\}$ to lie in the unit circle.

Alternatively, we can introduce a new condition. We say that the model is **stable on average** if the eigenvalues of $\mathbf{\bar{D}}=\frac{1}{t}\sum_{j=1}^t\mathbf{D}_j$ all lie in the unit circle. This way, some of the observations might have a higher impact on the final value, but they will be cancelled out by those with much lower weights in the product in \@ref(eq:ADAMETSXPureAdditiveDynamicNonSeasonalBackRecursion). This condition can be checked during the model estimation, similar to how the conventional stability condition is checked.

As for the **forecastability** condition, for the ADAMX{D} it should be (based on the same logic as in Section \@ref(stabilityConditionAdditiveError)):
\begin{equation}
  \lim\limits_{t\rightarrow\infty}\left(\mathbf{w}'_{t}\prod_{j=1}^{t-1}\mathbf{D}_{t-j} \mathbf{v}_{0}\right) = \text{const} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalBackForecastability)
\end{equation}
However, this condition will always be violated for the ADAMX models, just because the explanatory variables in $\mathbf{w}_{t}$ have their own variability and typically do not converge to a stable value with the increase of the sample size. So, if a forecastability condition needs to be checked for either ADAMX{D} or ADAMX{S}, we recommend checking it separately for the dynamic part of the model.


## Dealing with categorical variables in ADAMX {#ETSXDynamicCategories}
When dealing with categorical variables in a regression context, they are typically expanded to a set of dummy variables [see Chapter 10 of @SvetunkovSBA]. So, for example, a variable "promotions" that can be "light", "medium" and "heavy" for different observations $t$ would be expanded to three dummy variables, `promoLight`, `promoMedium` and `promoHeavy`, each one of which is equal to 1, when the respective promotion type happens and equal to zero otherwise. When including these variables in the model, we would typically drop one of them (which is sometimes called pivot variable) and have a model with two dummy variables of a type:
\begin{equation}
  y_t = a_0 + a_1 x_{1,t} + \dots + a_n x_{n,t} + d_1 promoLight_t + d_2 promoMedium_t + \epsilon_t,
  (\#eq:RegressionWithDummies)
\end{equation}
where $d_i$ is the parameter for the $i$-th dummy variable. The same procedure can be done in the context of ADAMX, and the principles will be exactly the same for ADAMX{S}. However, when it comes to the dynamic model, the parameters have time indeces, and there can be different ways of formulating the model. Here is the first one:
\begin{equation}
  \begin{aligned}
    & y_{t} = a_{0,t-1} + a_{1,t-1} x_{1,t} + \dots + a_{n,t-1} x_{n,t} + d_1 promoLight_t + d_2 promoMedium_t + \epsilon_t \\
    & a_{i,t} = a_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta_i \frac{\log(1+\epsilon_t)}{x_{i,t}} \text{ for each } i \in \{1, \dots, n\}, \text{ if } x_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. \\
    & d_{1,t} = d_{1,t-1} + \left \lbrace \begin{aligned}
                  &\delta_{n+1} \epsilon_t, \text{ if } promoLight_t\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. \\
    & d_{2,t} = d_{2,t-1} + \left \lbrace \begin{aligned}
                  &\delta_{n+2} \epsilon_t, \text{ if } promoMedium_t\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right.
  \end{aligned} .
  (\#eq:ETSXDynamicDummies01)
\end{equation}
Here we assume that each specific category of the variable promotion changes over time on its own with its own smoothing parameters $\delta_{n+1}$ and $\delta_{n+2}$. Alternatively, we can assume that they have the same smoothing parameters, implying that the changes of the parameters are similar throughout different categories of the variable:
\begin{equation}
  \begin{aligned}
    & d_{1,t} = d_{1,t-1} + \left \lbrace \begin{aligned}
                  &\delta_{n+1} \epsilon_t, \text{ if } promoLight_t\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. \\
    & d_{2,t} = d_{2,t-1} + \left \lbrace \begin{aligned}
                  &\delta_{n+1} \epsilon_t, \text{ if } promoMedium_t\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right.
  \end{aligned} .
  (\#eq:ETSXDynamicDummies02)
\end{equation}
The rationale for such restriction is that we might expect the adaptation mechanism to apply to the promo variable as a whole, not to its specific values. This case also becomes useful in connecting the ETSX and the conventional seasonal ETS model. Let's assume that we deal with quarterly data with no trend, and we have a categorical variable `quarterOfYear`, which can be `First`, `Second`, `Third` and `Fourth`, depending on the specific observation. For convenience, I will call the parameters for the dummy variables, created from this categorical variable $s_{1,t}, s_{2,t}, s_{3,t} \text{ and } s_{4,t}$. Based on \@ref(eq:ETSXDynamicDummies02), the model can then be formulated as:
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + s_{1,t} quarterOfYear_{1,t} + s_{2,t} quarterOfYear_{2,t} \\
            & + s_{3,t} quarterOfYear_{3,t} + s_{4,t} quarterOfYear_{4,t} + \epsilon_t \\
    & l_t = l_{t-1} + \alpha \epsilon_t \\
    & s_{i,t} = s_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta \epsilon_t \text{ for each } i \in \{1, \dots, 4\}, \text{ if } quarterOfYear_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right.
  \end{aligned} .
  (\#eq:ETSXDynamicDummiesSeasonal)
\end{equation}
We intentionally added all four dummy variables in \@ref(eq:ETSXDynamicDummiesSeasonal) to separate the seasonal effect from the level component. While in regression and ETSX{S} contexts, this does not make much sense, in the ETSX{D} we avoid the trap of dummy variables due to the dynamic update of parameters. Having done that, we have just formulated the conventional ETS(A,N,A) model using a set of dummy variables and one smoothing parameter, the difference being that the latter relies on the lag of component:
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + s_{t-4} + \epsilon_t \\
    & l_t = l_{t-1} + \alpha \epsilon_t \\
    & s_t = s_{t-4} + \gamma \epsilon_t \\
  \end{aligned} .
  (\#eq:ADAMETSANAQuarterly)
\end{equation}
So, this comparison shows on one hand that the mechanism of ADAMX{D} is natural for the ADAM, and on the other hand that using the same smoothing parameters for categorical variables can be a reasonable idea, especially in cases when we do not have grounds to assume that each category of the variable should evolve independently.


## Examples of application {#ETSXRExample}
For this example, we will use the data of Road Casualties in Great Britain 1969â€“84, `Seatbelts` dataset in `datasets` package for R, which contains several variables (the description is provided in the documentation for the data and can be accessed via `?Seatbelts` command). The variable of interest, in this case, is `drivers`, and the dataset contains more variables than needed, so we will restrict the data with `drivers`, `kms` (distance driven), `PetrolPrice` and `law` -- the latter three seem to influence the number of injured/killed drivers in principle:
```{r}
SeatbeltsData <- Seatbelts[,c("drivers","kms","PetrolPrice","law")]
```
The dynamics of these variables over time is shown in Figure \@ref(fig:Seatbelts).

```{r Seatbelts, fig.cap="The time series dynamics of variables from Seatbelts dataset.", echo=FALSE}
plot(SeatbeltsData)
```

Apparently, the `drivers` variable exhibits seasonality but does not seem to have a trend. The type of seasonality is challenging to determine, but we will assume that it is multiplicative for now. A simple ETS(M,N,M) model applied to the data will produce the following (we will withhold the last 12 observations for the forecast evaluation, Figure \@ref(fig:SeatbeltsForecast)):

```{r SeatbeltsForecast, fig.cap="The actual values for drivers and a forecast from ETS(M,N,M) model."}
adamETSMNMSeat <- adam(SeatbeltsData[,"drivers"], "MNM",
                       h=12, holdout=TRUE)
plot(forecast(adamETSMNMSeat, h=12, interval="prediction"))
```

This simple model already does a fine job fitting the data and producing forecasts. However, the forecast is biased and is lower than needed because of the sudden drop in the level of series, which can only be explained by the introduction of the new law in the UK in 1983, making the seatbelts compulsory for drivers. Due to the sudden drop, the smoothing parameter for the level of series is higher than needed, leading to wider intervals and less accurate forecasts. Here is the output of the model:
```{r}
adamETSMNMSeat
```

In order to further explore the data we will produce the scatterplots and boxplots between the variables using `spread()` function from `greybox` package (Figure \@ref(fig:driversSpread)):

```{r driversSpread, fig.cap="The relation between variables from Seatbelts dataset"}
spread(SeatbeltsData)
```

Figure \@ref(fig:driversSpread) shows a negative relation between `kms` and `drivers`: the higher the distance driven, the lower the total of car drivers killed or seriously injured. A similar relation is observed between the PetrolPrice and drivers (when the prices are high, people tend to drive less, thus causing fewer incidents). Interestingly, the increase of both variables causes the variance of the response variable to decrease (heteroscedasticity effect). Using a multiplicative error model and including the variables in logarithms, in this case, might address this potential issue. Note that we do not need to take the logarithm of `drivers`, as we already use the model with multiplicative error. Finally, the legislation of a new law seems to have caused a decrease in the number of causalities. To have a better model in terms of explanatory and predictive power, we should include all three variables. This is how we can do that using `adam()`:

```{r}
adamETSXMNMSeat <- adam(SeatbeltsData, "MNM", h=12, holdout=TRUE,
                        formula=drivers~log(kms)+log(PetrolPrice)+law)
```

The parameter `formula` in general is not compulsory. It can either be substituted by `formula=drivers~.` or dropped completely -- the function would fit the model of the first variable in the matrix from everything else. We need it in our case because we introduce log-transformations of some explanatory variables.

```{r SeatbeltsForecastX, fig.cap="The actual values for drivers and a forecast from ETSX(M,N,M) model."}
plot(forecast(adamETSXMNMSeat, h=12, interval="prediction"))
```

Figure \@ref(fig:SeatbeltsForecastX) shows the forecast from the second model, which is slightly more accurate. More importantly, the prediction interval is narrower than in the simple ETS(M,N,M) because now the model takes the external information into account. Here is the summary of the second model:

```{r}
adamETSXMNMSeat
```

Note that the smoothing parameter $\alpha$ has reduced from `r round(adamETSMNMSeat$persistence["alpha"],2)` to `r round(adamETSXMNMSeat$persistence["alpha"],2)`. This led to the reduction in error measures. For example, based on MASE, we can conclude that the model with explanatory variables is more precise than the simple univariate ETS(M,N,M). Still, we could try introducing the update of the parameters for the explanatory variables to see how it works (it might be unnecessary for this data):
```{r SeatbeltsForecastXD, fig.cap="The actual values for drivers and a forecast from ETSX(M,N,M){D} model."}
adamETSXMNMDSeat <- adam(SeatbeltsData, "MNM", h=12, holdout=TRUE,
                         formula=drivers~log(kms)+log(PetrolPrice)+law,
                         regressors="adapt")
```

In this specific case, the difference between the ETSX and ETSX{D} models is infinitesimal in terms of the accuracy of final forecasts and prediction intervals. Here is the output of the model:
```{r}
adamETSXMNMDSeat
```

We can spot that the error measures of the dynamic model are a bit higher than the ones from the static one (e.g., compare MASE and RMSSE of models). In addition, the information criteria are slightly lower for the static model. So based on all of this, we should probably use the static one for forecasting and analytical purposes. To see the effect of the explanatory variables on the number of incidents with drivers, we can look at the parameters for those variables:

```{r, warning=FALSE}
adamETSXMNMSeat$initial$xreg
```

Based on that, we can conclude that the introduction of the law reduced on average the number of incidents by approximately `r abs(round(adamETSXMNMSeat$initial$xreg[3],2))*100`%, while the increase of the petrol price by 1% leads on average to decrease in the number of incidents by `r abs(round(adamETSXMNMSeat$initial$xreg[2],2))`%. Finally, the distance negatively impacts the incidents as well, reducing it on average by `r abs(round(adamETSXMNMSeat$initial$xreg[1],1))`% for each 1% increase in the distance. This is the standard interpretation of parameters, which we can use based on the estimated model [see, for example, discussion in Section 8.3 of @SvetunkovSBA]. We will discuss how to do the analysis using ADAM in Chapter \@ref(ADAMUncertainty), introducing the standard errors and confidence intervals for the parameters.

Finally, `adam()` has some shortcuts when a matrix of variables is provided with no formula, assuming that the necessary expansion has already been done. This leads to the decrease in computational time of the function and becomes especially useful when working on large samples of data. Here is an example with ETSX(M,N,N):
```{r eval=FALSE}
# Create matrix for the model
SeatbeltsDataExpanded <-
  ts(model.frame(drivers~log(kms)+log(PetrolPrice)+law,
                 SeatbeltsData),
     start=start(SeatbeltsData), frequency=frequency(SeatbeltsData))
# Fix the names of variables
colnames(SeatbeltsDataExpanded) <-
  make.names(colnames(SeatbeltsDataExpanded))
# Apply the model
adamETSXMNMExpandedSeat <- adam(SeatbeltsDataExpanded, "MNM",
                                lags=12, h=12, holdout=TRUE)
```
