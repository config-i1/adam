# Explanatory variables in ADAM {#ADAMX}
In real life, the need for explanatory variables arises when there are some external factors influencing the response variable, which cannot be ignored and impact the final forecasts. Examples of such variables in demand forecasting context include price changes, promotional activities, temperature etc. In some cases the changes in these factors would not have a substantial impact on the demand, but in the others they would be essential for improving the accuracy. If we omit this information from the model, then this will be damaging for both point forecasts and prediction intervals (see discussion in Section 12 of @SvetunkovSBA).

While inclusion of explanatory variables in context of ARIMA models is relatively well studied topic, in case of ETS, there is only a handful of papers. One of such papers is @Koehler2012, which discusses the mechanism of outliers detection and approximation of outliers via an ETSX model (ETS with explanatory variables). The authors show that if an outlier appears at the end of series, then it will have a serious impact on the final forecast. However, if it appears either in the middle or in the beginning of series, the impact on the final forecast is typically negligible. This is relevant to our discussion, because there is a direct link between dealing with outliers in @Koehler2012 and including explanatory variables in ETSX in terms of how the model is formulated in these two cases.

In our situation, the state space model \@ref(eq:ETSADAMStateSpace) allows easily extending it by including additional components and explanatory variables. In this chapter, we discuss the main aspects of ADAM with explanatory variables, how it is formulated and how the more advanced models can be built upon it. Furthermore, the parameters for these additional components can either be fixed (static) or change over time (dynamic). We discuss both in the next sections. We also show that the stability and forecastability conditions, discussed in Section \@ref(stabilityConditionAdditiveError) for the pure additive ETS model, will be different in case of ETSX model and that the classical definitions should be amended in order to cater for the introduction of the explanatory variables. We also discuss briefly the inclusion of categorical variables in the ETSX model and show how the seasonal ETS models can be considered as special cases of ADAM ETSX in some situations.

As a final note, we will carry out the discussion of the topic on the example of ADAM ETSX, keeping in mind that the same principles will hold for ADAM ARIMAX as well, because the two are formulated in the same way. The more general dynamic model (encompassing ETS and / or ARIMA) with explanatory variables is called "ADAMX" in this and further chapters.


## ADAMX: Model formulation
As discussed previously, there are fundamentally two types of ETS models:

1. Additive error model [which was discussed in @Hyndman2008b in Chapter 9],
2. Multiplicative error model.

The inclusion of explanatory variables in ADAMX is determined by the type of the error, so that in case of (1) the measurement equation of the model is:
\begin{equation}
  {y}_{t} = a_{0,t} + a_{1,t} x_{1,t} + a_{2,t} x_{2,t} + \dots + a_{n,t} x_{n,t} + \epsilon_t ,
  (\#eq:ETSXADAMStateSpacePureAdditiveMeasurement)
\end{equation}
where $a_{0,t}$ is the point value based on all ETS components (for example, $a_{0,t}=l_{t-1}$ in case of ETS(A,N,N)), $x_{i,t}$ is the $j$-th explanatory variable, $a_{i,t}$ is the parameter for that component and $n$ is the number of explanatory variables. We will call the estimated parameters of such model $\hat{a}_{i,t}$. In the simple case, the transition equation for such model would imply that the parameters $a_{i,t}$ do not change over time:
\begin{equation}
  \begin{aligned}
    &a_{1,t} = a_{1,t-1} \\
    &a_{2,t} = a_{2,t-1} \\
    &\vdots \\
    &a_{n,t} = a_{n,t-1}
  \end{aligned} .
  (\#eq:ETSXADAMStateSpacePureAdditiveTransition)
\end{equation}
Complex mechanisms for the states update can be proposed instead of \@ref(eq:ETSXADAMStateSpacePureAdditiveTransition), but we do not discuss them at this point. Typically, the initial values of parameters would be estimated at the optimisation stage, either based on likelihood or some other loss function, so the index $t$ can be dropped, substituting $a_{1,t}=a_{1}$.

When it comes to the mulitplicative error, the multiplication should be used instead of addition. However, it is easier to formulate the model in logarithms in order to linearise it:
\begin{equation}
  \log {y}_{t} = \log a_{0,t} + a_{1,t} x_{1,t} + a_{2,t} x_{2,t} + \dots + a_{n,t} x_{n,t} + \log(1+ \epsilon_t).
  (\#eq:ETSXADAMStateSpacePureAdditiveMeasurement)
\end{equation}
Note that if log-log model is required, all that needs to be done, is that $x_{i,t}$ should be substituted by $\log x_{j,t}$.

One of the other ways to formulate the ADAMX model is to move the explanatory variables $x_{i,t}$ in the measurement vector $\mathbf{w}_{t}$, making it change over time, to move the parameters in the state vector, add diagonal matrix to the existing transition matrix and set values of the persistence vector for the parameters of explanatory variables to zero. The [general state space model](#ADAMETSIntroduction) does not change in that case, but the pure ones can be specifically represented as:
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \mathbf{w}'_t \mathbf{v}_{t-\boldsymbol{l}} + \epsilon_t \\
    & \mathbf{v}_t = \mathbf{F} \mathbf{v}_{t-\boldsymbol{l}} + \mathbf{g} \epsilon_t
  \end{aligned}
  (\#eq:ETSXADAMStateSpacePureAdditiveFull)
\end{equation}
and 
\begin{equation}
  \begin{aligned}
    {y}_{t} = & \exp\left(\mathbf{w}'_t \log \mathbf{v}_{t-\boldsymbol{l}} + \log(1 + \epsilon_t)\right) \\
    \log \mathbf{v}_t = & \mathbf{F} \log \mathbf{v}_{t-\boldsymbol{l}} + \log(\mathbf{1}_k + \mathbf{g} \epsilon_t)
  \end{aligned}. 
  (\#eq:ETSXADAMStateSpacePureMultiplicativeFull)
\end{equation}
So, the only thing that changes in these models is the time varying measurement vector $\mathbf{w}'_t$ instead of the fixed one. For example, in case of ETSX(A,A,A) we will have:
\begin{equation}
  \begin{aligned}
    \mathbf{F} =
    \begin{pmatrix} 1 & 1 & 0 & 0 & \dots & 0 \\
                    0 & 1 & 0 & 0 & \dots & 0 \\
                    0 & 0 & 1 & 0 & \dots & 0 \\
                    0 & 0 & 0 & 1 & \dots & 0 \\
                    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & 0 & 0 & \dots & 1
    \end{pmatrix},
    & \mathbf{w}_t = \begin{pmatrix} 1 \\ 1 \\ 1 \\ x_{1,t} \\ \vdots \\x_{n,t} \end{pmatrix},
    & \mathbf{g} = \begin{pmatrix} \alpha \\ \beta \\ \gamma \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \\
    & \mathbf{v}_{t} = \begin{pmatrix} l_t \\ b_t \\ s_t \\ a_{1,t} \\ \vdots \\ a_{n,t} \end{pmatrix},
    & \boldsymbol{l} = \begin{pmatrix} 1 \\ 1 \\ m \\ 1 \\ \vdots \\ 1 \end{pmatrix}
  \end{aligned},
  (\#eq:ETSXADAMAAAMatrices)
\end{equation}
which is equivalent to the combination of equations \@ref(eq:ETSXADAMStateSpacePureAdditiveMeasurement) and \@ref(eq:ETSXADAMStateSpacePureAdditiveTransition), giving us:
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + b_{t-1} + s_{t-m} + a_{1,t} x_{1,t} + \dots + a_{n,t} x_{n,t} + \epsilon_t \\
    & l_t = l_{t-1} + b_{t-1} + \alpha \epsilon_t \\
    & b_t = b_{t-1} + \beta \epsilon_t \\
    & s_t = s_{t-m} + \gamma \epsilon_t \\
    & a_{1,t} = a_{1,t-1} \\
    & \vdots \\
    & a_{n,t} = a_{n,t-1}
  \end{aligned}.
  (\#eq:ETSXADAMAAA)
\end{equation}
Alternatively, the state, measurement and persistence vectors and transition matrix can be split into two, separatign the ETS and X parts in the state space equations.

When all the smoothing parameters of the ETS part of the model are equal to zero, the ETSX reverts to a deterministic model, directly related to the multiple linear regression. For example, in case of ETSX(A,N,N) with $\alpha=0$ we get:
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + a_{1,t} x_{1,t} + \dots + a_{n,t} x_{n,t} + \epsilon_t \\
    & l_t = l_{t-1} \\
    & a_{1,t} = a_{1,t-1} \\
    & \vdots \\
    & a_{n,t} = a_{n,t-1}
  \end{aligned},
  (\#eq:ETSXADAMANNDeterministic)
\end{equation}
where $l_t=a_0$ is the intercept of the model. \@ref(eq:ETSXADAMANNDeterministic) can be rewritten in the conventional way, dropping the transition part of the state space model:
\begin{equation}
    y_{t} = a_0 + a_{1} x_{1,t} + \dots + a_{n} x_{n,t} + \epsilon_t .
  (\#eq:linearRegression)
\end{equation}
So, in general ADAMX implies that we are dealing with regression with time varying intercept, where the principles of this variability are defined by the ADAM components and smoothing parameters (e.g. intercept can vary seasonally). Similar properties are obtained with the multiplicative error model, with the main difference that the impact of explanatory variables on the response variable will vary with the changes of the intercept.


## Conditional expectation and variance of ADAMX {#ADAMXConventionalConditionalMoments}
### The ADAMX with known explanatory variables
ETS models have a serious limitation, which we will discuss in one of the latter chapters: they assume that the parameters of the model are known, i.e. there is no variability in them and that the in-sample values are fixed no matter what. This limitation also impacts the ETSX part. While in case of point forecasts this is not an issue, this impacts the conditional variance and prediction intervals. As a result, the conditional mean and variance of the conventional ADAMX assume that the parameters $a_0, \dots a_n$ are also known, leading to the following formulae in case of pure additive model, based on what was discussed in [the section on pure additive models](#pureAdditiveExpectationAndVariance):
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & \sum_{i=1}^d \left(\mathbf{w}_{m_i,t}' \mathbf{F}_{m_i}^{\lceil\frac{h}{m_i}\rceil-1} \right) \mathbf{v}_{t} \\
    \text{V}(y_{t+h}|t) = & \left( \sum_{i=1}^d \left(\mathbf{w}_{m_i,t}' \sum_{j=1}^{\lceil\frac{h}{m_i}\rceil-1} \mathbf{F}_{m_i}^{j-1} \mathbf{g}_{m_i} \mathbf{g}'_{m_i} (\mathbf{F}_{m_i}')^{j-1} \mathbf{w}_{m_i,t} \right) + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSXADAMStateSpaceANNRecursionMeanAndVarianceGeneral)
\end{equation}
the main difference from [the conventional model](#pureAdditiveExpectationAndVariance) being is the index $t$ in the measurement vector. As an example, here how the two statistics will look in case of ETSX(A,N,N):
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & l_{t} + \sum_{i=1}^n a_i x_{i,t+h} \\
    \text{V}(y_{t+h}|t) = & \left((h-1) \alpha^2 + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSXADAMStateSpaceANNRecursionMeanAndVariance)
\end{equation}
where the variance ignores the potential variability rising from the explanatory variables because of the ETS limitations. As a result, the prediction and confidence intervals for the ADAMX model would typically be narrower than needed and would only be adequate in cases of large samples, where [law of large numbers](#LLNandCLT) would start working, reducing the variance of parameters (this is assuming that the [typical assumptions](#assumptions) of the model hold).


### ADAMX with random explanatory variables {#ADAMXConventionalConditionalMomentsRandom}
Note that the ADAMX works well in cases, when the future values of $x_{i,t+h}$ are known, which is not always the case. It is a realistic assumption, when we have control over the explanatory variables (e.g. prices and promotions for our product). But in the case, when the variables are out of our control, they need to be forecasted somehow. In this case we are assuming that each $x_{i,t}$ is a random variable with some dynamic conditional one step ahead expectation $\mu_{x_{i,t}}$ and a one step ahead variance $\sigma^2_{x_{i,1}}$. **Note** that in this case we treat the available explanatory variables as models on their own, not just as values given to us from above. This assumption of randomness will change the conditional moments of the model. Here what we will have in case of ETSX(A,N,N) (given that the [typical assumptions](#assumptions) hold):
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & l_{t} + \sum_{i=1}^n a_i \mu_{x_{i,t+h}} \\
    \text{V}(y_{t+h}|t) = & \left((h-1) \alpha^2 + 1 \right) \sigma^2 + \sum_{i=1}^n a^2_i \sigma^2_{x_{i,h}} + 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i a_j \sigma_{x_{i,h},x_{j,h}}
  \end{aligned},
  (\#eq:ETSXADAMStateSpaceANNRecursionMeanAndVarianceRandomness)
\end{equation}
where $\sigma^2_{x_{i,h}}$ is the variance of $x_{i}$ h steps ahead, $\sigma_{x_{i,h},x_{j,h}}$ is the h steps ahead covariance between the explanatory variables $x_{i,h}$ and $x_{j,h}$, both conditional on the information available at the observation $t$. similarly, if we are interested in one step ahead point forecast from the model, it should take the randomness of explanatory variables into account and become:
\begin{equation}
  \begin{aligned}
    \mu_{y,t} = &  \left. \mathrm{E}\left(l_{t-1} + \sum_{i=1}^n a_i x_{i,t} + \epsilon_{t} \right| t-1 \right) = \\
                = & l_{t-1} + \sum_{i=1}^n a_i \mu_{x_{i,t}}
  \end{aligned}.
  (\#eq:ADAMETSXANNStepAhead)
\end{equation}
So, in case of ADAMX with random explanatory variables, the model should be constructed based on the expectations of those variables, not the random values themselves. This does not appear in the context of the classical linear regression, because it does not rely on the one step ahead recursion. But this explains, for example, why @Athanasopoulos2011 found that some models with predicted explanatory variables works better than the model with the variables themselves. This becomes important, when estimating the model, such as ETS(A,N,N), when the following is constructed:
\begin{equation}
  \begin{aligned}
    & \hat{y}_{t} = \hat{l}_{t-1} + \sum_{i=1}^n \hat{a}_{i,t} \hat{x}_{i,t} \\
    & e_t = y_t - \hat{y}_{t} \\
    & \hat{l}_{t} = \hat{l}_{t-1} + \hat{\alpha} e_t \\
    & \hat{a}_{i,t} = \hat{a}_{i,t-1} \text{ for each } i \in \{1, \dots, n\}
  \end{aligned},
  (\#eq:ADAMETSXANNStepAhead)
\end{equation}
where $\hat{x}_{i,t}$ is the in-sample conditional one step ahead mean for the explanatory variable $x_i$.

Summarising this section, the adequate ADAMX model needs to be able to work in at least two regimes: (1) assuming that the explanatory variable is known, (2) assuming that the explanatory variable is random.

Finally, as discussed previously, the conditional moments for the pure multiplicative and mixed models do not have closed forms in general, implying that the simulations need to be carried out. The situation becomes more challenging in case of random explanatory variables, because that randomness needs to be introduced in the model itself and propagated throught the time series. This is not a trivial task, which we will discuss later in this textbook.


## Dynamic X in ADAMX {#ADAMXDynamic}
**Note**: the model discussed in this section assumes a very specific dynamics of parameters (that they are correlated with other states of the model), aligning with what the conventional ETS assumes. It does not treat parameters as independent as the MSOE state space models do. But this type of model works well with categorical variables as I show later in this section.

As discussed [earlier in this chapter](#ADAMX), the parameters of the explanatory variables in ADAMX can be assumed to stay constant over time or can be assumed to vary according to some mechanism. The most reasonable mechanism in SSOE framework is the one relying on the same error for different components of the model. @Osman2015 proposed one of such mechanisms, relying on the differences of the data.  The main motivation of the research was to make the dynamic ADAMX model stable, which is a challenging task. However, this mechanism relies on the assumption of non-stationarity of the explanatory variables, which does not always make sense (for example, it is not reasonable in case of promotional data). An alternative approach that we will discuss in this section, is the one originally proposed by @Svetunkov1985 based on stochastic approximation mechanism and further developed in @Svetunkov2014Textbook.

In this method, we consider the following regression model:
\begin{equation}
    y_{t} = a_{0,t-1} + a_{1,t-1} x_{1,t} + \dots + a_{n,t-1} x_{n,t} + \epsilon_t ,
  (\#eq:linearRegressionDynamic)
\end{equation}
where all parameters vary over time and $a_{0,t}$ represents the value from the conventional pure additive ETS model. The updating mechanism for the parameters is straight forward and relies on the ratio of the error term and the respective explanatory variables:
\begin{equation}
    a_{i,t} = a_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta_i \frac{\epsilon_t}{x_{i,t}} \text{ for each } i \in \{1, \dots, n\}, \text{ if } x_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. ,
  (\#eq:linearRegressionDynamicUpdates)
\end{equation}
where $\delta_i$ is the smoothing parameter of the $i$-th explanatory variable. The same model can be represented in the state space form, based on the equations, similar to \@ref(eq:ETSXADAMStateSpacePureAdditiveFull):
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \mathbf{w}'_t \mathbf{v}_{t-\boldsymbol{l}} + \epsilon_t \\
    & \mathbf{v}_t = \mathbf{F} \mathbf{v}_{t-\boldsymbol{l}} + \mathrm{diag}\left(\mathbf{w}_t\right)^{-1} \mathbf{g} \epsilon_t
  \end{aligned}
  (\#eq:ADAMETSXPureAdditiveDynamicFull)
\end{equation}
where $\mathrm{diag}\left(\mathbf{w}_t\right)^{-1}=\mathbf{I}_{k+n} \odot (\mathbf{w}_t \mathbf{1}_{k+n})$ (where $\mathbf{I}_{k+n}$ is the identity matrix for $k$ ADAM components and $n$ explanatory variables and $\odot$ is Hadamard product for element-wise multiplication). This is the inverse of the diagonal matrix based on the measurement vector, for which those values that cannot be inverted (due to division by zero) are substitute by zeroes in order to reflect the condition in \@ref(eq:linearRegressionDynamicUpdates). In addition to what \@ref(eq:ETSXADAMStateSpacePureAdditiveFull) contained, we add smoothing parameters $\delta_i$ in the persistence vector $\mathbf{g}$ for each of the explanatory variables.

If the error term is multiplicative, then the model changes to:
\begin{equation}
  \begin{aligned}
    & y_{t} = \exp \left(a_{0,t-1} + a_{1,t-1} x_{1,t} + \dots + a_{n,t-1} x_{n,t} + \log(1+ \epsilon_t) \right) \\
    & a_{i,t} = a_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta_i \frac{\log(1+\epsilon_t)}{x_{i,t}} \text{ for each } i \in \{1, \dots, n\}, \text{ if } x_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right.
  \end{aligned} .
  (\#eq:linearRegressionDynamicMultiplicative)
\end{equation}
The formulation \@ref(eq:linearRegressionDynamicMultiplicative) differs from the conventional pure multiplicative ETS model because the smoothing parameter $\delta_i$ is not included inside the error term $1+\epsilon_t$, which simplifies some derivations and makes model easier to work with. Mixed ETS models can also have explanatory variables, but we suggest to align the type of explanatory variables model with the error term.

Finally, in order to distinguish the ADAMX with static parameters from the ADAMX with dynamic ones, we will use the letters "S" and "D" in the names of models. So, the model \@ref(eq:ETSXADAMANNDeterministic) can be called ETSX(A,N,N){S}, while the model \@ref(eq:linearRegressionDynamicMultiplicative), assuming that $a_{0,t-1}=l_{t-1}$, would be called ETSX(M,N,N){D}. We use curly brackets in order to split the ETS states from the type of X. Furthermore, given that the model with static regressors is assumed in many contexts to be the default one, the ETSX(\*,\*,\*){S} model can also be denoted just ETSX(\*,\*,\*).


### Conditional moments of dynamic ADAMX {#ADAMXDynamicMoments}
Similar to how it was discussed in [the previous section](#ETSXConventionalConditionalMoments), we can have two cases in the dynamic model: (1) when explanatory variables are assumed to be known, (2) when explanatory variables are assumed to be random. For illustrative purposes, we will use a non-seasonal model for which the lag vector contains ones only, keeping in mind that other pure additive models can be easily used instead. The cases of other ETS models are not discussed in this part in detail - the moments for these models need to be calculated based on simulations. So, as discussed previously, the model can be written in the following general way, assuming that all $\boldsymbol{l}=1$:
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \mathbf{w}'_t \mathbf{v}_{t-1} + \epsilon_t \\
    & \mathbf{v}_t = \mathbf{F} \mathbf{v}_{t-1} + \mathrm{diag}\left(\mathbf{w}_t\right)^{-1} \mathbf{g} \epsilon_t
  \end{aligned} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonal)
\end{equation}
Based on this model, we can get the recursive relation for $h$ steps ahead, similar to how it was done in [one of the previos sections](#adamETSPureAdditiveRecursive):
\begin{equation}
  \begin{aligned}
    & {y}_{t+h} = \mathbf{w}'_{t+h} \mathbf{v}_{t+h-1} + \epsilon_{t+h} \\
    & \mathbf{v}_{t+h-1} = \mathbf{F} \mathbf{v}_{t+h-2} + \mathrm{diag}\left(\mathbf{w}_{t+h-1}\right)^{-1} \mathbf{g} \epsilon_{t+h-1}
  \end{aligned} ,
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursive01)
\end{equation}
where the second equation can be represented based on the values available on observation $t$:
\begin{equation}
  \mathbf{v}_{t+h-1} = \mathbf{F}^{h-1} \mathbf{v}_{t} + \sum_{j=1}^{h-1} \mathbf{F}^{h-1-j} \mathrm{diag}\left(\mathbf{w}_{t+j}\right)^{-1} \mathbf{g} \epsilon_{t+j} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursive02)
\end{equation}
Substituting the equation \@ref(eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursive02) in the measurement equation of \@ref(eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursive01) leads to the final recursion:
\begin{equation}
    {y}_{t+h} = \mathbf{w}'_{t+h} \mathbf{F}^{h-1} \mathbf{v}_{t} + \mathbf{w}'_{t+h} \sum_{j=1}^{h-1} \mathbf{F}^{h-1-j} \mathrm{diag}\left(\mathbf{w}_{t+j}\right)^{-1} \mathbf{g} \epsilon_{t+j} + \epsilon_{t+h} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursiveFinal)
\end{equation}

### Known explanatory variables
Based on this recursion, we can calculate the conditional mean and variance for the model. First, we assume that the explanatory variables are controlled by an analyst, so that they are not random:
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = & \text{E}(y_{t+h}|t) = \mathbf{w}'_{t+h} \mathbf{F}^{h-1} \mathbf{v}_{t} \\
    & \text{V}(y_{t+h}|t) = \left(\mathbf{w}'_{t+h} \sum_{j=1}^{h-1} \mathbf{F}^{h-1-j} \mathrm{diag}\left(\mathbf{w}_{t+j}\right)^{-1} \mathbf{g} \right)^2 \sigma^2 + \sigma^2
  \end{aligned} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalMomentsNonRandom)
\end{equation}
The formulae for conditional moments in this case look similar to the ones from [the pure additive ETS](#pureAdditiveExpectationAndVariance) model with only difference being the interaction with time varying measurument vector.

### Random explanatory variables
In the case of random explanatory variables, the conditional expectation is straightforward and is similar to the one in the static ADAMX model:
\begin{equation}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = \boldsymbol{\mu}'_{w,t+h} \mathbf{F}^{h-1} \mathbf{v}_{t} ,
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalMomentsRandom)
\end{equation}
where $\boldsymbol{\mu}'_{w,t+h}$ is the vector of conditional h steps ahead expectations for each element in the $\mathbf{w}_{t+h}$. In case of ETS components, the vector would contain ones. However, when it comes to the conditional variance, it is more complicated, because it introduces complex interactions between variances of different variables and error term. As a result, it would be easier to get the correct variance based on simulations, assuming that the explanatory variables and the error term change according to some assumed distributions.


## Stability and forecastability conditions of ADAMX
It can be shown that any ADAMX{S} is not stable (as it was defined for [pure additive ETS models](#stabilityConditionAdditiveError)), meaning that the weights of such model do not decline exponentially to zero. This becomes apparent, when we compare the explanatory part of any ADAMX with a deterministic model, discussed in the context of [pure additive models](#stabilityConditionAdditiveError). For example, we have already discussed that when $\alpha=0$ in ETS(A,N,N), then the model becomes equivalent to the global level, looses the stability condition, but still can be forecastable. Similarly, the X part of ADAMX{S} will always be unstable, but can be forecastable. However, this is unreasonable from the model building point of view: if for example, the ETS part of the model is stable but the model does not pass stability check just because of the X part, then the check itself is incorrect. Furthremore, there are no issues constructing ARIMAX models, but @Hyndman2008b claim that there are issues with stability of ETSX. This only means that the stability / forecastability conditions should be checked for the dynamic part of the model (ETS or ARIMA) separately, ignoring the X part. Technically, this implies creating separate transition matrix, persistence and measurement vectors and calculating the discount matrix for the ETS part in order to check already discussed [stability and forecastability](#stabilityConditionAdditiveError) conditions.

When it comes to the dynamic ADAMX, then the situation changes, because now the smoothing parameters for the coefficients of the model determine, how weights decline over time. It can be shown based on \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04) that the values of the state vector on the observation $t$ can be calculated via the recursion (here we provide formula for the non-seasonal case, keeping in mind that in case of the seasonal one, the derivation and the main message will be similar):
\begin{equation}
  \mathbf{v}_{t} = \prod_{j=1}^{t-1}\mathbf{D}_{t-j} \mathbf{v}_{0} + \sum_{j=0}^{t-1} \prod_{i=0}^{j} \mathbf{D}_{t-i} y_{t-j},
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalBackRecursion)
\end{equation}
where $\mathbf{D}_t=\mathbf{F} - \mathrm{diag}\left(\mathbf{w}_{t}\right)^{-1} \mathbf{g} \mathbf{w}_{t}'$ is the time varying discount matrix. The main issue in the case of dynamic ADAMX is that the stability condition varies over time together with the values of explanatory variables. So, it is not possible to derive it for the general case. In order to make sure that the model is stable, we need for all eigenvalues of each $\mathbf{D}_{j}$ for all $j=\{1,\dots,t\}$ to lie in the unit circle.

Alternatively, we can introduce a new condition. We say that the model is **stable on average** if the eigenvalues of $\mathbf{\bar{D}}=\frac{1}{t}\sum_{j=1}^t\mathbf{D}_t$ all lie in the unit circle. This way, some of observations might have a higher impact on the final value, but they will be canceled out by those that have much lower weights. This condition can be checked during the model estimation, similar to how the conventional stability condition is checked.

As for the **forecastability** condition, for the ADAMX{D} it should be (based on and the same logic as [in section 6.4](#stabilityConditionAdditiveError)):
\begin{equation}
  \lim\limits_{t\rightarrow\infty}\left(\mathbf{w}'_{t}\prod_{j=1}^{t-1}\mathbf{D}_{t-j} \mathbf{v}_{0}\right) = \text{const} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalBackForecastability)
\end{equation}
However, for the reasons discussed earlier in this subsection, this condition will always be violated for the ADAMX models, just because the explanatory variables in $\mathbf{w}_{t}$ have their own variability and typically do not converge to a stable value with the increase of the sample size. So, if a forecastability condition needs to be checked for either ADAMX{D} or ADAMX{S}, we recommend checking it separately for the dynamic part of the model.


## Dealing with categorical variables in ADAMX {#ETSXDynamicCategories}
When dealing with categorical variables in regression context, they are typically expanded to a set of dummy variables (see Section \@ref(dummyVariables)). So, for example, having a variable promotions that can be "light", "medium" and "heavy" for different observations $t$, we would expand it to three dummy variables, `promolight`, `promomedium` and `promoheavy`, each one of which is equal to 1, when the respective promotion type is used and equal to zero otherwise. As discussed earlier, when including these variables in the model, we would drop one of them (which is sometimes called pivot variable) and have a model with two dummy variables of a type:
\begin{equation}
  y_t = a_0 + a_1 x_{1,t} + \dots + a_n x_{n,t} + d_1 promolight_t + d_2 promomedium_t + \epsilon_t,
  (\#eq:RegressionWithDummies)
\end{equation}
where $d_i$ is the parameter for the $i$-th dummy variable. The same procedure can be done in the context of ADAMX, and the principles will be exactly the same for ADAMX{S}. However, when it comes to the dynamic model, the parameters have time indeces, and there can be different ways of formulating the model. Here is the first one:
\begin{equation}
  \begin{aligned}
    & y_{t} = a_{0,t-1} + a_{1,t-1} x_{1,t} + \dots + a_{n,t-1} x_{n,t} + d_1 promolight_t + d_2 promomedium_t + \epsilon_t \\
    & a_{i,t} = a_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta_i \frac{\log(1+\epsilon_t)}{x_{i,t}} \text{ for each } i \in \{1, \dots, n\}, \text{ if } x_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. \\
    & d_{1,t} = d_{1,t-1} + \left \lbrace \begin{aligned}
                  &\delta_{n+1} \epsilon_t, \text{ if } promolight_t\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. \\
    & d_{2,t} = d_{2,t-1} + \left \lbrace \begin{aligned}
                  &\delta_{n+2} \epsilon_t, \text{ if } promomedium_t\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right.
  \end{aligned} .
  (\#eq:ETSXDynamicDummies01)
\end{equation}
Here we assume that each specific category of the variable promotion changes over time on its own with their own smoothing parameters $\delta_{n+1}$ and $\delta_{n+1}$. Alternatively, we can assume that they have the same smoothing parameters, implying that the changes of the parameters are similar throughout different categories of the variable:
\begin{equation}
  \begin{aligned}
    & d_{1,t} = d_{1,t-1} + \left \lbrace \begin{aligned}
                  &\delta_{n+1} \epsilon_t, \text{ if } promolight_t\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. \\
    & d_{2,t} = d_{2,t-1} + \left \lbrace \begin{aligned}
                  &\delta_{n+1} \epsilon_t, \text{ if } promomedium_t\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right.
  \end{aligned} .
  (\#eq:ETSXDynamicDummies02)
\end{equation}
This case becomes useful in making the connection between the ETSX and the conventional seasonal ETS model. Let's assume that we deal with quarterly data with no trend and we have a categorical variable `quarterOfYear`, which can be `First`, `Second`, `Third` and `Fourth`, depending on the specific observation. For convenience, I will call the parameters for the dummy variables, created from this categorical variable $s_{1,t}, s_{2,t}, s_{3,t} \text{ and } s_{4,t}$. Based on \@ref(eq:ETSXDynamicDummies02), the model can then be formulated as:
\begin{equation}
  \begin{aligned}
    & y_{t} = & l_{t-1} + s_{1,t} quarterOfYear_{1,t} + s_{2,t} quarterOfYear_{2,t} \\
            & + s_{3,t} quarterOfYear_{3,t} + s_{4,t} quarterOfYear_{4,t} + \epsilon_t \\
    & l_t = & l_{t-1} + \alpha \epsilon_t \\
    & s_{i,t} = & s_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta \epsilon_t \text{ for each } i \in \{1, \dots, 4\}, \text{ if } quarterOfYear_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right.
  \end{aligned} .
  (\#eq:ETSXDynamicDummiesSeasonal)
\end{equation}
We intentionally added all 4 dummy variables here, so that they separate seasonal effect from the level component. While in the regression and ETSX{S} contexts, this does not make much sense, in the ETSX{D} we avoid the trap of dummy variables due to the dynamic update of parameters. Having done that, we have just formulated the conventional ETS(A,N,A) model using a set of dummy variables, the difference being that the latter relies on the lag of component:
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + s_{t-4} + \epsilon_t \\
    & l_t = l_{t-1} + \alpha \epsilon_t \\
    & s_t = s_{t-4} + \gamma \epsilon_t \\
  \end{aligned} .
  (\#eq:ADAMETSANAQuarterly)
\end{equation}
So, this comparison shows on one hand that the mechanism of ADAMX{D} is natural for the ADAM model, and on the other hand that using the same smoothing parameters for categorical variables can be a reasonable idea, especially in cases, when we do not have grounds to assume that each category of variable should evolve over time independently.


## Examples of application {#ETSXRExample}
For this example, we will use the data of Road Casualties in Great Britain 1969–84, `Seatbelts` dataset in `datasets` package for R, which contains several variables, the description for which is provided in the documentation for the data (can be accessed via `?Seatbelts` command). The variable of interest in this case is `drivers`, and the dataset contains more variables than needed, so we will restrict the data with `drivers`, `kms` (distance driven), `PetrolPrice` and `law` - the latter three seem to influence the number of injured / killed drivers in principle:
```{r}
SeatbeltsData <- Seatbelts[,c("drivers","kms","PetrolPrice","law")]
```
The dynamics of these variables over time is shown on figure \@ref(fig:Seatbelts)

```{r Seatbelts, fig.cap="The time series dynamics of variables from Seatbelts dataset."}
plot(SeatbeltsData)
```

It is apparent that the `drivers` variable exhibits seasonality, but does not seem to have a trend. The type of seasonality is difficult to determine, but we will assume that it is multiplicative. So a simple ETS(M,N,M) model applies to the data will produce the following (we will withhold the last 12 observations for the forecast evaluation):

```{r}
adamModelETSMNM <- adam(SeatbeltsData[,"drivers"],"MNM",h=12,holdout=TRUE)
plot(forecast(adamModelETSMNM,h=12,interval="prediction"))
```

This simple model already does a fine job in fitting and forecasting the data, although the forecast is biased and is lower than needed because of the sudden drop in the level of series, which can only be explained by the introduction of the new law in the UK in 1983, making the seatbelts compulsory for drivers. Due to the sudden drop, the smoothing parameter for the level of series is higher than needed, leading to wider intervals and less accurate forecasts, here is the output of the model:
```{r}
adamModelETSMNM
```

In order to further explore the data we will produce the scatterplots and boxplots between the variables using `spread()` function from `greybox` package:
```{r driversSpread, fig.cap="The relation between variables from Seatbelts dataset"}
spread(SeatbeltsData)
```

The plot on Figure \@ref(fig:driversSpread) shows that there is a negative relation between `kms` and `drivers`: the higher the distance driven, the lower the total of car drivers killed or seriously injuried. A similar relation is observed between the PetrolPrice and drivers (when the prices are high, people tend to drive less, thus causing less incidents). Interestingly, the increase of both variables causes the variance of the response variable to decrease (heteroscedasticity effect). Using multiplicative error model and including the variables in logarithms in this case might address this potential issue. Note that we do not need to take the logarithm of `drivers`, as we already use the model with multiplicative error. Finally, the legislation of a new law seems to have caused the decrease in the number of causalities. In order to have a better model in terms of explanatory and predictive power, we should include all three variables in the model. This is how we can make it in ADAM:
```{r}
adamModelETSXMNM <- adam(SeatbeltsData,"MNM",h=12,holdout=TRUE,
                         formula=drivers~log(kms)+log(PetrolPrice)+law)
plot(forecast(adamModelETSXMNM,h=12,interval="prediction"))
```

The parameter `formula` in general is not compulsory and can either be substituted by `formula=drivers~.` or dropped completely - the function would fit the model of the first variable in the matrix from everything else. We need it in our case, because we introduce log-transformations of some of explanatory variables. The forecast from the second model is slightly more accurate and, what is even more important, the prediction interval is narrower, because now the model takes the external information into account. Here is the summary of the second model:

```{r}
adamModelETSXMNM
```

The model with explanatory variables is already more precise than the simple univariate ETS(M,N,M) (e.g. MASE on the holdout is lower), but we could try introducing the update of the parameters for the explanatory variables, just to see how it works (it might be unnecessary for this data):
```{r}
adamModelETSXMNMD <- adam(SeatbeltsData,"MNM",h=12,holdout=TRUE,
                          formula=drivers~log(kms)+log(PetrolPrice)+law,regressors="adapt")
plot(forecast(adamModelETSXMNMD,h=12,interval="prediction"))
```

In this specific case, the difference between the ETSX and ETSX{D} models is infinitesimal in terms of the accuracy of final forecasts and prediction intervals. Here is the output of the model:
```{r}
adamModelETSXMNMD
```

We can spot that the error measures of the dynamic model are a bit higher than the ones from the static one (e.g., compare MASE and RMSSE of models). In addition, the information criteria are slightly lower for the static model, so based on all of this, we should probably use the static one for the forecasting and anlytical purposes. In order to see the effect of the explanatory variables on the number of incidents with drivers, we can look at the parameters for those variables:

```{r, warning=FALSE}
adamModelETSXMNM$initial$xreg
```

Based on that, we can point out that the introduction of the law reduced on average the number of incidents by approximately `r abs(round(adamModelETSXMNM$initial$xreg[3],2))*100`%, while the increase of the petrol price by 1% leads on average to decrease in the number of incidents by `r abs(round(adamModelETSXMNM$initial$xreg[2],2))`%. Finally, the distance has a negative impact on incidents as well, reducing it on average by `r abs(round(adamModelETSXMNM$initial$xreg[1],1))`% for each 1% increase in the distance. All of this is the standard interpretation of parameters, which we can use based on the estimated model. We will discuss how to do analysis using ADAM in future chapters, introducing the standard errors and confidence intervals for the parameters.

Finally, `adam()` has some shortcuts in cases, when a matrix of variables is provided with no formula, assuming that the necessary expansion has already been done. This leads to the decrease in computational time of the function and becomes especially useful when working on large samples of data. Here is an example with ETSX(M,N,N):
```{r eval=FALSE}
SeatbeltsDataExpanded <- ts(model.frame(drivers~log(kms)+log(PetrolPrice)+law,
                                               SeatbeltsData),
                            start=start(SeatbeltsData), frequency=frequency(SeatbeltsData))
colnames(SeatbeltsDataExpanded) <- make.names(colnames(SeatbeltsDataExpanded))
adamModelETSXMNMExpanded <- adam(SeatbeltsDataExpanded,"MNM",lags=12,h=12,holdout=TRUE)
```
