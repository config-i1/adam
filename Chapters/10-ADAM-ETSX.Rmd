# Explanatory variables in ADAM {#ADAMX}
In real life, the need for explanatory variables arises when there are some external factors that have relation with the response variable and impact the final forecasts and their accuracy. Examples of such variables in the demand forecasting context include price changes, promotional activities, temperature etc. In some cases, the changes in these factors would not substantially influence the demand, but this does not apply universally to the problem. If we omit this information from the model, this will be damaging for both point forecasts and prediction intervals [see discussion in Chapter 15 of @SvetunkovSBA].

While the inclusion of explanatory variables in the context of ARIMA models is a relatively well-studied topic [for example, this was discussed by @Box1976], in the case of ETS, there is only a Chapter 9 in @Hyndman2008b and a handful of papers. @Koehler2012 discuss the mechanism of detection and approximation of outliers via an ETSX model (ETS with explanatory variables). The authors show that if an outlier appears at the end of the series, it will seriously impact the final forecast and needs to be modelled correctly. However, if it appears either in the middle or at the beginning of the series, the impact on the final forecast is typically negligible.
<!-- This is relevant to our discussion because there is a direct link between dealing with outliers in @Koehler2012 and including explanatory variables in ETSX in terms of how the model is formulated in these two situations. -->
@Kourentzes2015 used ETSX successfully for promotional modelling, demonstrating that it outperforms the conventional ETS in terms of point forecasts accuracy in cases when promotions happen. So, the inclusion of explanatory variables in dynamic models is not just a nice feature, but in some situations is a necessity, which helps improving the forecasting accuracy.

In ADAM, the state-space model \@ref(eq:ETSADAMStateSpace) can be easily extended by including additional components and explanatory variables. This chapter discusses the main aspects of ADAM with explanatory variables, how it is formulated, and how the more advanced models can be built upon it. Furthermore, the parameters for these additional components can either be fixed (static) or change over time (dynamic). We discuss both in the following sections. We also show that the stability and forecastability conditions, discussed in Section \@ref(stabilityConditionAdditiveError) for the pure additive ETS model, will be different in the case of the ETSX model and that the classical definitions should be updated to cater for the introduction of the explanatory variables. We also briefly discuss the inclusion of categorical variables in the ETSX model and show that the seasonal ETS models can be considered as special cases of ADAM ETSX with dummy variables.

Furthermore, we will use the term "**deterministic**" explanatory variable to denote the situations when the values of variables are known in advance or can be controlled by us. An example is the price of a product or a promotion that we decide to have. On contrary, we will use term "**stochastic**" explanatory variable for the cases, when its future value is not known and is beyond our control. An example of this variable is the temperature, which we cannot control and do not know for sure in advance. Usage of deterministic variables in dynamic models might differ from the usage of the stochastic ones.

As a final note, we will carry out the discussion of the topic in this Chapter on the example of ADAM ETSX, keeping in mind that the same principles will hold for ADAM ARIMAX because the two are formulated in the same framework. We will call the more general dynamic model (encompassing ETS and/or ARIMA) with explanatory variables "ADAMX" in this and further chapters.


## ADAMX: Model formulation {#ADAMXFormulation}
As discussed previously, there are two types of error terms in ADAM:

1. Additive discussed in Chapter \@ref(ADAMETSIntroduction) in case of ETS and Chapter \@ref(ADAMARIMA) for ARIMA,
2. Multiplicative covered in Chapter \@ref(ADAMETSPureMultiplicativeChapter) for ETS and in Subsection \@ref(ADAMARIMAPureMultiplicative) for ARIMA.

The inclusion of explanatory variables in ADAMX is determined by the type of the error, so that in case of (1) the measurement equation of the model is:
\begin{equation}
  {y}_{t} = a_{0,t-1} + a_{1,t-1} x_{1,t} + a_{2,t-1} x_{2,t} + \dots + a_{n,t-1} x_{n,t} + \epsilon_t ,
  (\#eq:ETSXADAMStateSpacePureAdditiveMeasurement)
\end{equation}
where $a_{0,t-1}$ is the point value based on all ETS components (for example, $a_{0,t-1}=l_{t-1}$ in case of ETS(A,N,N)), $x_{i,t}$ is the $i$-th explanatory variable, $a_{i,t-1}$ is its parameter and $n$ is the number of explanatory variables. We will denote the estimated parameters of such model as "$\hat{a}_{i,t-1}$". In the simple case, the transition equation for such model would imply that the parameters $a_{i,t}$ do not change over time and are equal to some fixed value:
\begin{equation}
    a_{i,t} = a_{i,t-1} = \dots = a_{i,0} \text{ for all } i = 1, \dots, n
  (\#eq:ETSXADAMStateSpacePureAdditiveTransition)
\end{equation}
Various complex mechanisms for the states update can be proposed instead of \@ref(eq:ETSXADAMStateSpacePureAdditiveTransition), but we do not discuss them at this point. Typically, the initial values of parameters would be estimated at the optimisation stage, either based on likelihood or some other loss function, so the index $t$ can be dropped, substituting $a_{i,t}=a_{i}$ for all $i=1,\dots,n$.

When it comes to the multiplicative error model, it should be formulated differently. The most straight forward would be to formulate the model in logarithms in order to linearise it:
\begin{equation}
  \log {y}_{t} = \log a_{0,t-1} + a_{1,t-1} x_{1,t} + a_{2,t-1} x_{2,t} + \dots + a_{n,t-1} x_{n,t} + \log(1+ \epsilon_t).
  (\#eq:ETSXADAMStateSpacePureMultiplicativeMeasurement)
\end{equation}

::: remark
If log-log model is required, all that needs to be done, is to substitute $x_{i,t}$ with $\log x_{i,t}$.
:::

The model \@ref(eq:ETSXADAMStateSpacePureMultiplicativeMeasurement) aligns with both pure multiplicative ETS and ARIMA, discussed respectively in Chapter \@ref(ADAMETSPureMultiplicativeChapter) and in Subsection \@ref(ADAMARIMAPureMultiplicative).

The compact form of the ADAMX model implies that the explanatory variables $x_{i,t}$ are included in the measurement vector $\mathbf{w}_{t}$, making it change over time. The parameters are then moved to the state vector, and a diagonal matrix is added to the existing transition matrix to reflect the updating mechanism \@ref(eq:ETSXADAMStateSpacePureAdditiveTransition). Finally, the persistence vector for the parameters of explanatory variables should contain zeroes, because for now we assume that the parameters do not change over time. The pure additive state space model, in that case, can be represented as:
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \mathbf{w}'_t \mathbf{v}_{t-\boldsymbol{l}} + \epsilon_t \\
    & \mathbf{v}_t = \mathbf{F} \mathbf{v}_{t-\boldsymbol{l}} + \mathbf{g} \epsilon_t
  \end{aligned} ,
  (\#eq:ETSXADAMStateSpacePureAdditiveFull)
\end{equation}
while the pure multiplicative models is:
\begin{equation}
  \begin{aligned}
    {y}_{t} = & \exp\left(\mathbf{w}'_t \log \mathbf{v}_{t-\boldsymbol{l}} + \log(1 + \epsilon_t)\right) \\
    \log \mathbf{v}_t = & \mathbf{F} \log \mathbf{v}_{t-\boldsymbol{l}} + \log(\mathbf{1}_k + \mathbf{g} \epsilon_t)
  \end{aligned}. 
  (\#eq:ETSXADAMStateSpacePureMultiplicativeFull)
\end{equation}
So, the only thing that changes in these models in comparison with the conventional ones in Chapter \@ref(ADAMETSIntroduction) and \@ref(ADAMETSPureMultiplicativeChapter) is the time varying measurement vector $\mathbf{w}'_t$ instead of the fixed one. For example, in case of ETSX(A,Ad,A) we will have:
\begin{equation}
  \begin{aligned}
    \mathbf{F} =
    \begin{pmatrix} 1 & \phi & 0 & 0 & \dots & 0 \\
                    0 & \phi & 0 & 0 & \dots & 0 \\
                    0 & 0 & 1 & 0 & \dots & 0 \\
                    0 & 0 & 0 & 1 & \dots & 0 \\
                    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & 0 & 0 & \dots & 1
    \end{pmatrix},
    & \mathbf{w}_t = \begin{pmatrix} 1 \\ \phi \\ 1 \\ x_{1,t} \\ \vdots \\x_{n,t} \end{pmatrix},
    & \mathbf{g} = \begin{pmatrix} \alpha \\ \beta \\ \gamma \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \\
    & \mathbf{v}_{t} = \begin{pmatrix} l_t \\ b_t \\ s_t \\ a_{1,t} \\ \vdots \\ a_{n,t} \end{pmatrix},
    & \boldsymbol{l} = \begin{pmatrix} 1 \\ 1 \\ m \\ 1 \\ \vdots \\ 1 \end{pmatrix}
  \end{aligned},
  (\#eq:ETSXADAMAAAMatrices)
\end{equation}
which is equivalent to the set of equations:
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + \phi b_{t-1} + s_{t-m} + a_{1,t-1} x_{1,t} + \dots + a_{n,t-1} x_{n,t} + \epsilon_t \\
    & l_t = l_{t-1} + \phi b_{t-1} + \alpha \epsilon_t \\
    & b_t = \phi b_{t-1} + \beta \epsilon_t \\
    & s_t = s_{t-m} + \gamma \epsilon_t \\
    & a_{1,t} = a_{1,t-1} \\
    & \vdots \\
    & a_{n,t} = a_{n,t-1}
  \end{aligned}.
  (\#eq:ETSXADAMAAA)
\end{equation}
Alternatively, the state, measurement and persistence vectors and transition matrix can be split each into two parts, separating the ETS and X parts in the state space equations:
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \mathbf{w}' \mathbf{v}_{t-\boldsymbol{l}} + \mathbf{x}'_{t} \mathbf{a}_{t-1} + \epsilon_t \\
    & \mathbf{v}_{1,t} = \mathbf{F} \mathbf{v}_{t-\boldsymbol{l}} + \mathbf{g} \epsilon_t \\
    & \mathbf{a}_{t} = \mathbf{a}_{t-1}
  \end{aligned} ,
  (\#eq:ETSXADAMStateSpacePureAdditiveFullAlternative)
\end{equation}
where $\mathbf{w}$, $\mathbf{F}$, $\mathbf{g}$ and $\mathbf{v}_{t}$ contain the elements of the conventional components of ADAM and $\mathbf{a}_{t}$ is the vector of parameters for the explanatory variables.

When all the smoothing parameters of the ETS part of the model are equal to zero, the ETSX reverts to a deterministic model, becoming just a multiple linear regression. For example, in case of ETSX(A,N,N) with $\alpha=0$ we get:
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + a_{1,t-1} x_{1,t} + \dots + a_{n,t-1} x_{n,t} + \epsilon_t \\
    & l_t = l_{t-1} \\
    & a_{1,t} = a_{1,t-1} \\
    & \vdots \\
    & a_{n,t} = a_{n,t-1}
  \end{aligned},
  (\#eq:ETSXADAMANNDeterministic)
\end{equation}
where $l_t=a_0$ is the intercept of the model. \@ref(eq:ETSXADAMANNDeterministic) can be rewritten then in the conventional way, dropping the transition part of the state space model:
\begin{equation}
    y_{t} = a_0 + a_{1} x_{1,t} + \dots + a_{n} x_{n,t} + \epsilon_t .
  (\#eq:linearRegression)
\end{equation}
In the case of models with trend and/or seasonal components, the model becomes equivalent to the regression with deterministic trend and/or seasonality. This means that, in general, ADAMX implies that we are dealing with a regression with time-varying intercept, where the principles of this variability are defined by the ADAM components (e.g. intercept can vary seasonally). Similar properties are obtained with the multiplicative error model. The main difference is that the specific impact of explanatory variables on the response variable will vary with the intercept changes. The model, in this case, combines the strengths of the multiplicative regression and the dynamic model, where the variability of the response variable changes with the change of the baseline model (ADAM ETS and/or ADAM ARIMA in this case).


## Conditional expectation and variance of ADAMX {#ADAMXConventionalConditionalMoments}
### ADAMX with deterministic explanatory variables
The conventional ETS and ARIMA models have a severe limitation, which will be discussed in Chapter \@ref(ADAMUncertainty): it assume that the model's parameters are known, i.e. there is no variability in them and that the in-sample estimates are fixed no matter how the sample size changes. While in the case of point forecasts, this is not important, this affects the conditional variance and prediction intervals, which appear too narrow in many studies [e.g. @Athanasopoulos2011]. In case of regression, this limitation is lifted: the uncertainty of parameters in it translates to the final uncertainty in confidence/prediction interval. ADAMX, having both dynamic (ETS/ARIMA) and static (regression) parts, has the limitation of the former, which can be resolved only with more complicated approaches (Section \@ref(adamRefitted)). As a result, the conditional mean and variance of the conventional ADAMX assume that the parameters $a_0, \dots a_n$ are known, leading to the following formulae in the case of the pure additive model, based on what was discussed in the Section \@ref(pureAdditiveExpectationAndVariance):
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & \sum_{i=1}^d \left(\mathbf{w}_{m_i,t}' \mathbf{F}_{m_i}^{\lceil\frac{h}{m_i}\rceil-1} \right) \mathbf{v}_{t} \\
    \text{V}(y_{t+h}|t) = & \left( \sum_{i=1}^d \left(\mathbf{w}_{m_i,t}' \sum_{j=1}^{\lceil\frac{h}{m_i}\rceil-1} \mathbf{F}_{m_i}^{j-1} \mathbf{g}_{m_i} \mathbf{g}'_{m_i} (\mathbf{F}_{m_i}')^{j-1} \mathbf{w}_{m_i,t} \right) + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSXADAMStateSpaceANNRecursionMeanAndVarianceGeneral)
\end{equation}
the main difference from the moments of the conventional model (from Section \@ref(pureAdditiveExpectationAndVariance)) being is the index $t$ in the measurement vector $\mathbf{w}_t$. As an example, here how the two statistics will look in case of ETSX(A,N,N):
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & l_{t} + \sum_{i=1}^n a_i x_{i,t+h} \\
    \text{V}(y_{t+h}|t) = & \left((h-1) \alpha^2 + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSXADAMStateSpaceANNRecursionMeanAndVariance)
\end{equation}
where the variance ignores the potential variability rising from the explanatory variables because of the limitations discussed above. The formulae assume that the future values of explanatory variables $x_{i,t}$ are known (the variable is deterministic). As a result, the prediction and confidence intervals for the ADAMX would typically be narrower than expected and would only be adequate in cases of large samples, where the Law of Large Numbers would start working [Section 6.1 of @SvetunkovSBA], reducing the variance of parameters (this is assuming that the typical assumptions of the model from Subsection \@ref(assumptions) hold).


### ADAMX with stochastic explanatory variables {#ADAMXConventionalConditionalMomentsRandom}
Note that the ADAMX works well in cases when the future values of $x_{i,t+h}$ are known. It is a realistic assumption when we control the explanatory variables (e.g. prices and promotions for our product). But when the variables are out of our control, they need to be forecasted somehow. In this case we are assuming that each $x_{i,t}$ is a stochastic variable with some dynamic conditional one step ahead expectation $\mu_{x_{i,t}}$ and a one step ahead variance $\sigma^2_{x_{i,1}}$.

::: remark
In this case, we treat the available explanatory variables as models on their own, not just as values given to us from above. This assumption of randomness will change the conditional moments of the model.
:::

Here is what we will have for the moments of the model in the case of ETSX(A,N,N) (given that the typical assumptions from Subsection \@ref(assumptions) hold):
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & l_{t} + \sum_{i=1}^n a_i \mu_{x_{i,t+h}} \\
    \text{V}(y_{t+h}|t) = & \left((h-1) \alpha^2 + 1 \right) \sigma^2 + \sum_{i=1}^n a^2_i \sigma^2_{x_{i,h}} + 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i a_j \sigma_{x_{i,h},x_{j,h}}
  \end{aligned},
  (\#eq:ETSXADAMStateSpaceANNRecursionMeanAndVarianceRandomness)
\end{equation}
where $\sigma^2_{x_{i,h}}$ is the conditional variance of $x_{i}$ h steps ahead, $\sigma_{x_{i,h},x_{j,h}}$ is the h steps ahead covariance between the explanatory variables $x_{i,t+h}$ and $x_{j,t+h}$, both conditional on the information available at the observation $t$. Similarly, if we are interested in one step ahead point forecast from the model, it should take the randomness of explanatory variables into account and become:
\begin{equation}
  \begin{aligned}
    \mu_{y,t |t-1} = &  \left. \mathrm{E}\left(l_{t-1} + \sum_{i=1}^n a_i x_{i,t} + \epsilon_{t} \right| t-1 \right) = \\
                = & l_{t-1} + \sum_{i=1}^n a_i \mu_{x_{i,t}}
  \end{aligned}.
  (\#eq:ADAMETSXANNStepAhead)
\end{equation}
So, in the case of ADAMX with random explanatory variables, the model should be constructed based on the expectations of those variables, not the random values themselves. This explains, for example, why @Athanasopoulos2011 found that some models with predicted explanatory variables work better than the models with the original variables. This means that when estimating the model, such as ETS(A,N,N), the following should be constructed:
\begin{equation}
  \begin{aligned}
    & \hat{y}_{t} = \hat{l}_{t-1} + \sum_{i=1}^n \hat{a}_{i,t-1} \hat{x}_{i,t} \\
    & e_t = y_t -\hat{y}_{t} \\
    & \hat{l}_{t} = \hat{l}_{t-1} + \hat{\alpha} e_t \\
    & \hat{a}_{i,t} = \hat{a}_{i,t-1} \text{ for each } i \in \{1, \dots, n\}
  \end{aligned},
  (\#eq:ADAMETSXANNConstructed)
\end{equation}
where $\hat{x}_{i,t}$ is the in-sample conditional one step ahead mean for the explanatory variable $x_i$.

Finally, as discussed previously, the conditional moments for the pure multiplicative and mixed models do not generally have closed forms, implying that the simulations need to be carried out. The situation becomes more challenging in the case of random explanatory variables because that randomness needs to be introduced in the model itself and propagated throughout the time series. This is not a trivial task, which has not been resolved yet.


## Dynamic X in ADAMX {#ADAMXDynamic}
::: remark
The model discussed in this section assumes particular dynamics of parameters, aligning with what the conventional ETS assumes: regression parameters are correlated with the states of the model. It does not treat parameters as independent as, for example, MSOE state space models do, which makes this model restrictive in its application. But this type of model works well with categorical variables, as I show later in this section.
:::

As discussed in Section \@ref(ADAMXFormulation), the parameters of the explanatory variables in ADAMX can be assumed to be constant over time or can be assumed to vary according to some mechanism. The most reasonable one in the SSOE framework relies on the same error for different components of the model because this mechanism aligns with the model itself. @Osman2015 proposed one of such mechanisms, relying on the differences of the data. The primary motivation of their approach was to make the dynamic ETSX model stable, which is a challenging task. However, this mechanism relies on the assumption of non-stationarity of the explanatory variables, which does not always make sense (for example, it is unreasonable in the case of promotional data). An alternative approach discussed in this section is the one initially proposed by @Svetunkov1985, based on the stochastic approximation mechanism and further developed in @Svetunkov2014Textbook.

We start with the following linear regression model:
\begin{equation}
    y_{t} = a_{0,t-1} + a_{1,t-1} x_{1,t} + \dots + a_{n,t-1} x_{n,t} + \epsilon_t ,
  (\#eq:linearRegressionDynamic)
\end{equation}
where all parameters vary over time and $a_{0,t}$ represents the value from the conventional additive error ETS model (e.g. level of series, i.e. $a_{0,t}=l_t$). The updating mechanism for the parameters in this case is straight forward and relies on the ratio of the error term and the respective explanatory variables:
\begin{equation}
    a_{i,t} = a_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta_i \frac{\epsilon_t}{x_{i,t}} \text{ for each } i \in \{1, \dots, n\}, \text{ if } x_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. ,
  (\#eq:linearRegressionDynamicUpdates)
\end{equation}
where $\delta_i$ is the smoothing parameter of the $i$-th explanatory variable. The same model can be represented in the state space form, based on the equations, similar to \@ref(eq:ETSXADAMStateSpacePureAdditiveFull):
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \mathbf{w}'_t \mathbf{v}_{t-\boldsymbol{l}} + \epsilon_t \\
    & \mathbf{v}_t = \mathbf{F} \mathbf{v}_{t-\boldsymbol{l}} + \mathbf{z}_t \mathbf{g} \epsilon_t
  \end{aligned}
  (\#eq:ADAMETSXPureAdditiveDynamicFull)
\end{equation}
where $\mathbf{z}_t = \mathrm{diag}\left(\mathbf{w}_t\right)^{-1}= \left(\mathbf{I}_{k+n} \odot (\mathbf{w}_t \mathbf{1}^{\prime}_{k+n})\right)^{-1}$ is the diagonal matrix consisting of inverses of explanatory variables, $\mathbf{I}_{k+n}$ is the identity matrix for $k$ components and $n$ explanatory variables and $\odot$ is Hadamard product for element-wise multiplication. This is the inverse of the diagonal matrix based on the measurement vector, for which those values that cannot be inverted (due to division by zero) are substitute by zeroes in order to reflect the condition in \@ref(eq:linearRegressionDynamicUpdates). In addition to what \@ref(eq:ETSXADAMStateSpacePureAdditiveFull) contained, we add smoothing parameters $\delta_i$ in the persistence vector $\mathbf{g}$ for each of the explanatory variables.

If the error term is multiplicative, then the model changes to:
\begin{equation}
  \begin{aligned}
    & y_{t} = \exp \left(a_{0,t-1} + a_{1,t-1} x_{1,t} + \dots + a_{n,t-1} x_{n,t} + \log(1+ \epsilon_t) \right) \\
    & a_{i,t} = a_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta_i \frac{\log(1+\epsilon_t)}{x_{i,t}} \text{ for each } i \in \{1, \dots, n\}, \text{ if } x_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right.
  \end{aligned} .
  (\#eq:linearRegressionDynamicMultiplicative)
\end{equation}
The formulation \@ref(eq:linearRegressionDynamicMultiplicative) differs from the conventional pure multiplicative ETS model because the smoothing parameter $\delta_i$ is not included inside the error term $1+\epsilon_t$, which simplifies some derivations and makes the model easier to work with (it has some similarities to logARIMA from Subsection \@ref(ADAMARIMAPureMultiplicative)).

Note that if it is suspected that the explanatory variables exhibit non-stationarity and are not cointegrated with the response variable, then their differences can be used instead of $x_{i,t}$ in \@ref(eq:ADAMETSXPureAdditiveDynamicFull) and \@ref(eq:linearRegressionDynamicMultiplicative). In this case, the additive model would coincide with the one proposed by @Osman2015. However, the decision of taking the differences for the different parts of the model should be made based on each specific situation, not across the whole set of variables. Here is an example of the ETSX(A,N,N) model with differenced explanatory variables:
\begin{equation}
  \begin{aligned}
    & y_{t} = a_{0,t-1} + a_{1,t-1} \Delta x_{1,t} + \dots + a_{n,t-1} \Delta x_{n,t} + \epsilon_t , \\
    & a_{i,t} = a_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta_i \frac{\epsilon_t}{\Delta x_{i,t}} \text{ for each } i \in \{1, \dots, n\}, \text{ if } \Delta x_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. ,
  \end{aligned}
  (\#eq:linearRegressionDynamicDifferences)
\end{equation}
where $\Delta x_{i,t} = x_{i,t} -x_{i,t-1}$ is the differences of the $i$-th exogenous variable.

Finally, to distinguish the ADAMX with static parameters from the one with dynamic ones, we will use the letters "S" and "D" in the names of models. So, the model \@ref(eq:ETSXADAMANNDeterministic) can be called ETSX(A,N,N){S}, while the model \@ref(eq:linearRegressionDynamicMultiplicative), assuming that $a_{0,t-1}=l_{t-1}$, would be called ETSX(M,N,N){D}. We use curly brackets to split the ETS states from the type of X. Furthermore, given that the model with static regressors is assumed in many contexts to be the default one, the ETSX(\*,\*,\*){S} model can also be denoted as just ETSX(\*,\*,\*).


### Recursion for dynamic ADAMX {#ADAMXDynamicMoments}
Similarly to how it was discussed in Subsection \@ref(ADAMXConventionalConditionalMomentsRandom), we can have two cases in the dynamic model: (1) deterministic explanatory variables, (2) stochastic explanatory variables. For illustrative purposes, we will use a non-seasonal model for which the lag vector $\boldsymbol{l}$ contains ones only, keeping in mind that other pure additive models can be easily used instead with slight changes in formulae. The cases of non-additive ETS models are complicated and are not discussed in this monograph. So, as discussed previously, the model can be written in the following general way:
\begin{equation}
  \begin{aligned}
    & {y}_{t} = \mathbf{w}'_t \mathbf{v}_{t-1} + \epsilon_t \\
    & \mathbf{v}_t = \mathbf{F} \mathbf{v}_{t-1} + \mathbf{z}_t \mathbf{g} \epsilon_t
  \end{aligned} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonal)
\end{equation}
Based on this model, we can get the recursive relation for $h$ steps ahead, similar to how it was done in Section \@ref(adamETSPureAdditiveRecursive):
\begin{equation}
  \begin{aligned}
    & {y}_{t+h} = \mathbf{w}'_{t+h} \mathbf{v}_{t+h-1} + \epsilon_{t+h} \\
    & \mathbf{v}_{t+h-1} = \mathbf{F} \mathbf{v}_{t+h-2} + \mathbf{z}_{t+h-1} \mathbf{g} \epsilon_{t+h-1}
  \end{aligned} ,
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursive01)
\end{equation}
where the second equation can be expressed via matrices and vectors using the values available on observations from $t$ to $t+h-1$:
\begin{equation}
  \mathbf{v}_{t+h-1} = \mathbf{F}^{h-1} \mathbf{v}_{t} + \sum_{j=1}^{h-1} \mathbf{F}^{h-1-j} \mathbf{z}_{t+j} \mathbf{g} \epsilon_{t+j} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursive02)
\end{equation}
Substituting the equation \@ref(eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursive02) in the measurement equation of \@ref(eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursive01) leads to the final recursion:
\begin{equation}
    {y}_{t+h} = \mathbf{w}'_{t+h} \mathbf{F}^{h-1} \mathbf{v}_{t} + \mathbf{w}'_{t+h} \sum_{j=1}^{h-1} \mathbf{F}^{h-1-j} \mathbf{z}_{t+j} \mathbf{g} \epsilon_{t+j} + \epsilon_{t+h} ,
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursiveFinal)
\end{equation}
which can be used for the derivation of moments of ADAMX{D}.


### Conditional moments for deterministic explanatory variables in ADAMX{D}
Based on the recursion \@ref(eq:ADAMETSXPureAdditiveDynamicNonSeasonalRecursiveFinal), we can calculate the conditional mean and variance for the model. First, we assume that the explanatory variables are controlled by an analyst, and are known for all $j=1, \dots, h$, which leads to:
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = & \text{E}(y_{t+h}|t) = \mathbf{w}'_{t+h} \mathbf{F}^{h-1} \mathbf{v}_{t} \\
    & \text{V}(y_{t+h}|t) = \left( \left(\mathbf{w}'_{t+h} \sum_{j=1}^{h-1} \mathbf{F}^{h-1-j} \mathbf{z}_{t+j} \mathbf{g} \right)^2 + 1 \right) \sigma^2
  \end{aligned} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalMomentsNonRandom)
\end{equation}
The formulae for conditional moments in this case look similar to the ones from the pure additive ETS model in Section \@ref(pureAdditiveExpectationAndVariance) with only difference being that the element $\mathbf{z}_{t+j} \mathbf{g}$ is in general not equal to zero for the parameters of the explanatory variables.

### Conditional mean for stochastic explanatory variables in ADAMX{D}
In the case of stochastic explanatory variables, the conditional expectation is straightforward and is similar to the one in the static ADAMX model:
\begin{equation}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = \boldsymbol{\mu}'_{w,t+h} \mathbf{F}^{h-1} \mathbf{v}_{t} ,
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalMomentsRandom)
\end{equation}
where $\boldsymbol{\mu}'_{w,t+h}$ is the vector of conditional h steps ahead expectations for each element in the $\mathbf{w}_{t+h}$. In the case of ETS components, the vector would contain ones. However, when it comes to conditional variance, it becomes more complicated because it introduces complex interactions between variances of different variables and the error term of the model. As a result, it would be easier to get the correct variance based on simulations, assuming that the explanatory variables and the error term change according to some assumed models instead of deriving analytical expressions.


## Stability and forecastability conditions of ADAMX
It can be shown that any **static** ADAMX is not *stable* (as it was defined for pure additive ETS models in Section \@ref(stabilityConditionAdditiveError)), meaning that the weights of such model do not decline to zero over time. To see this, we can draw an analogy with a deterministic model, discussed in the context of pure additive ETS (from Section \@ref(stabilityConditionAdditiveError)). For example, we have already discussed that when $\alpha=0$, the ETS(A,N,N) model becomes equivalent to the global level, loses the stability condition, but still can be forecastable. It becomes a simple but still useful and efficient model:
\begin{equation}
  y_{t} = a_0 + \epsilon_t .
  (\#eq:ADAMETSGlobalLevelSimple)
\end{equation}
Similarly, the X part of ADAMX will always be unstable, but can be useful. For example, with $\alpha=0$, ETSX(A,N,N) reverts back to the linear regression:
\begin{equation}
  y_{t} = a_0 + a_{1} x_{1,t} + \dots + a_n x_{n,t} + \epsilon_t ,
  (\#eq:ADAMETSXRegression)
\end{equation}
which is not stable, because its weights do not decline over time, and the very first observation (a set of initial states with models parameters) impacts the final forecast. This does not make the model inappropriate in any way, but according to the conventional approach to ETS, if the dynamic part of the model is stable, but the overall model does not pass stability check just because of the X part, then the whole model will be considered unstable and potentially dangerous to use. This is absurd. Following the same logic, we would need to avoid regression models in forecasting because they are not stable. Furthermore, there are no issues constructing ARIMAX models, but @Osman2015 argue that there are some with ETSX, which does not make sense if we recall the connection between ETS and ARIMA (discussed in Section \@ref(ARIMAandETS)). This only means that the stability/forecastability conditions should be checked for the dynamic part of the model (ETS or ARIMA) separately, ignoring the X part. Technically, this implies creating a separate transition matrix, persistence and measurement vectors and calculating the discount matrix for the ETS / ARIMA part to check already discussed stability and forecastability conditions (Section \@ref(stabilityConditionAdditiveError)).

When it comes to the **dynamic** ADAMX, the situation changes because now the smoothing parameters for the model coefficients determine how weights decline over time. It can be shown based on \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04) that the values of the state vector on the observation $t$ can be calculated via the recursion (here we provide a formula for the non-seasonal case, keeping in mind that in case of the seasonal one, the derivation and the main message will be similar):
\begin{equation}
  \mathbf{v}_{t} = \prod_{j=1}^{t-1}\mathbf{D}_{t-j} \mathbf{v}_{0} + \sum_{j=0}^{t-1} \prod_{i=0}^{j} \mathbf{D}_{t-i} y_{t-j},
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalBackRecursion)
\end{equation}
where $\mathbf{D}_t=\mathbf{F} -\mathbf{z}_t \mathbf{g} \mathbf{w}_{t}'$ is the time varying discount matrix. The main issue in the case of dynamic ADAMX is that the stability condition varies over time together with the values of explanatory variables in $\mathbf{z}_t$. So, it is not possible to derive the stability condition for the general case. In order to make sure that the model is stable, we need for all eigenvalues of each $\mathbf{D}_{j}$ for all $j=\{1,\dots,t\}$ to lie in the unit circle.

Alternatively, we can introduce a new condition. We say that the model is **stable on average** if the eigenvalues of the geometric mean $\mathbf{\bar{D}}=\sqrt[t]{\prod_{j=1}^t\mathbf{D}_j}$ all lie in the unit circle. This way, some of the observations might have a higher impact on the final value, but they will be cancelled out by those with much lower weights in the product in \@ref(eq:ADAMETSXPureAdditiveDynamicNonSeasonalBackRecursion). This condition can be checked during the model estimation, similar to how the conventional stability condition is checked.

As for the **forecastability** condition, for the ADAMX{D} it should be (based on the same logic as in Section \@ref(stabilityConditionAdditiveError)):
\begin{equation}
  \lim\limits_{t\rightarrow\infty}\left(\mathbf{w}'_{t}\prod_{j=1}^{t-1}\mathbf{D}_{t-j} \mathbf{v}_{0}\right) = \text{const} .
  (\#eq:ADAMETSXPureAdditiveDynamicNonSeasonalBackForecastability)
\end{equation}
However, this condition will always be violated for the ADAMX models, just because the explanatory variables in $\mathbf{w}_{t}$ have their own variability and typically do not converge to a stable value with the increase of the sample size. So, if a forecastability condition needs to be checked for either ADAMX{D} or ADAMX{S}, it should be checked separately for the dynamic part of the model, dropping the X part.


## Dealing with categorical variables in ADAMX {#ETSXDynamicCategories}
When dealing with categorical variables in a regression context, they are typically expanded to a set of dummy variables. So, for example, a variable "promotions" that can be "light", "medium" and "heavy" for different observations $t$ would be expanded to three dummy variables, `promoLight`, `promoMedium` and `promoHeavy`, each one of which is equal to 1, when the respective promotion type happens and equal to zero otherwise. When including these variables in the model, we would typically drop one of them (it is sometimes called pivot variable) and have a model with two dummy variables of a type:
\begin{equation}
  y_t = a_0 + a_1 x_{1,t} + \dots + a_n x_{n,t} + d_1 promoLight_t + d_2 promoMedium_t + \epsilon_t,
  (\#eq:RegressionWithDummies)
\end{equation}
where $d_i$ is the parameter for the $i$-th dummy variable. The same procedure can be done in the context of ADAMX. The logic will be exactly the same for ADAMX{S}, but when it comes to the dynamic model, the parameters will have time indeces, and there can be different ways of formulating the model. Here is the first one:
\begin{equation}
  \begin{aligned}
    & y_{t} = a_{0,t-1} + a_{1,t-1} x_{1,t} + \dots + a_{n,t-1} x_{n,t} + d_{1,t-1} promoLight_t + d_{2,t-1} promoMedium_t + \epsilon_t \\
    & a_{i,t} = a_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta_i \frac{\log(1+\epsilon_t)}{x_{i,t}} \text{ for each } i \in \{1, \dots, n\}, \text{ if } x_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. \\
    & d_{1,t} = d_{1,t-1} + \left \lbrace \begin{aligned}
                  &\delta_{n+1} \epsilon_t, \text{ if } promoLight_t\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. \\
    & d_{2,t} = d_{2,t-1} + \left \lbrace \begin{aligned}
                  &\delta_{n+2} \epsilon_t, \text{ if } promoMedium_t\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right.
  \end{aligned} .
  (\#eq:ETSXDynamicDummies01)
\end{equation}
Here we assume that each specific category of the variable promotion changes over time on its own with its own smoothing parameters $\delta_{n+1}$ and $\delta_{n+2}$. Alternatively, we can assume that they have the same smoothing parameters, implying that the changes of the parameters are similar throughout different categories of the variable:
\begin{equation}
  \begin{aligned}
    & d_{1,t} = d_{1,t-1} + \left \lbrace \begin{aligned}
                  &\delta_{n+1} \epsilon_t, \text{ if } promoLight_t\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right. \\
    & d_{2,t} = d_{2,t-1} + \left \lbrace \begin{aligned}
                  &\delta_{n+1} \epsilon_t, \text{ if } promoMedium_t\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right.
  \end{aligned} .
  (\#eq:ETSXDynamicDummies02)
\end{equation}
The rationale for such restriction is that we might expect the adaptation mechanism to apply to the promo variable as a whole, not to its specific values. Indeed, in the example above, the variable of interest is `promo`, not the `promoLight` or `promoMedium`. Doing that will reduce the number of parameters and might simplify the estimation.

The mechanism \@ref(eq:ETSXDynamicDummies02) also becomes useful in connecting the ETSX and the conventional seasonal ETS model. Consider an example with quarterly data with no trend, and a categorical variable `quarterOfYear`, which can be `First`, `Second`, `Third` and `Fourth`, depending on the specific observation. For convenience, I will call the parameters for the dummy variables, created from this categorical variable as $s_{1,t}, s_{2,t}, s_{3,t} \text{ and } s_{4,t}$. Based on \@ref(eq:ETSXDynamicDummies02), the model can then be formulated as:
\begin{equation}
  \begin{aligned}
    y_{t} = & l_{t-1} + s_{1,t} quarterOfYear_{1,t} + s_{2,t} quarterOfYear_{2,t} + \\
            & s_{3,t} quarterOfYear_{3,t} + s_{4,t} quarterOfYear_{4,t} + \epsilon_t \\
    l_t = & l_{t-1} + \alpha \epsilon_t \\
    s_{i,t} = & s_{i,t-1} + \left \lbrace \begin{aligned}
                  &\delta \epsilon_t \text{ for each } i \in \{1, \dots, 4\}, \text{ if } quarterOfYear_{i,t}\neq 0 \\
                  &0 \text{ otherwise }
            \end{aligned} \right.
  \end{aligned} .
  (\#eq:ETSXDynamicDummiesSeasonal)
\end{equation}
We intentionally added all four dummy variables in \@ref(eq:ETSXDynamicDummiesSeasonal) to separate the seasonal effect from the level component. While in regression and ETSX{S} contexts, this does not make much sense, in the ETSX{D} we avoid the trap of dummy variables due to the dynamic update of parameters. Having done that, we have just formulated the conventional ETS(A,N,A) model using a set of dummy variables and one smoothing parameter, the difference being that the latter relies on the lag of component:
\begin{equation}
  \begin{aligned}
    & y_{t} = l_{t-1} + s_{t-4} + \epsilon_t \\
    & l_t = l_{t-1} + \alpha \epsilon_t \\
    & s_t = s_{t-4} + \gamma \epsilon_t \\
  \end{aligned} .
  (\#eq:ADAMETSANAQuarterly)
\end{equation}
So, this comparison shows on one hand that the mechanism of ADAMX{D} is natural for the ADAM, and on the other hand that using the same smoothing parameter for different values of a categorical variable can be a reasonable idea, especially in cases when we can assume that all categories of the variable should evolve together.


## Examples of application {#ETSXRExample}
For the example in this Section, we will use the data of Road Casualties in Great Britain 1969–84, `Seatbelts` dataset in `datasets` package for R, which contains several variables (the description is provided in the documentation for the data and can be accessed via `?Seatbelts` command). The variable of interest, in this case is `drivers`, and the dataset contains more variables than needed, so we will restrict the data with `drivers`, `kms` (distance driven), `PetrolPrice` and `law` -- the latter three seem to influence the number of injured/killed drivers in principle:
```{r}
SeatbeltsData <- Seatbelts[,c("drivers","kms","PetrolPrice","law")]
```
The dynamics of these variables over time is shown in Figure \@ref(fig:Seatbelts).

```{r Seatbelts, fig.cap="The time series dynamics of variables from Seatbelts dataset.", echo=FALSE}
plot(SeatbeltsData)
```

Apparently, the `drivers` variable exhibits seasonality but does not seem to have a trend. The type of seasonality is challenging to determine, but we will assume that it is multiplicative for now. A simple ETS(M,N,M) model applied to the data will produce the following (we will withhold the last 12 observations for the forecast evaluation, Figure \@ref(fig:SeatbeltsForecast)):

```{r SeatbeltsForecast, fig.cap="The actual values for drivers and a forecast from ETS(M,N,M) model."}
adamETSMNMSeat <- adam(SeatbeltsData[,"drivers"], "MNM",
                       h=12, holdout=TRUE)
forecast(adamETSMNMSeat, h=12, interval="prediction") |>
    plot()
```

This simple model already does a fine job fitting the data and producing forecasts. However, the forecast is biased and is consistently lower than needed because of the sudden drop in the level of series, which can only be explained by the introduction of the new law in the UK in 1983, making the seatbelts compulsory for drivers. Due to the sudden drop, the smoothing parameter for the level of series is higher than needed, leading to wider intervals and less accurate forecasts. Here is the output of the model:
```{r}
adamETSMNMSeat
```

In order to further explore the data we will produce the scatterplots and boxplots between the variables using `spread()` function from `greybox` package (Figure \@ref(fig:driversSpread)):

```{r driversSpread, fig.cap="The relation between variables from Seatbelts dataset"}
spread(SeatbeltsData)
```

Figure \@ref(fig:driversSpread) shows a negative relation between `kms` and `drivers`: the higher the distance driven, the lower the total of car drivers killed or seriously injured. A similar relation is observed between the PetrolPrice and drivers (when the prices are high, people tend to drive less, thus causing fewer incidents). Interestingly, the increase of both variables causes the variance of the response variable to decrease (heteroscedasticity effect). Using a multiplicative error model and including the variables in logarithms, in this case, might address this potential issue. Note that we do not need to take the logarithm of `drivers`, as we already use the model with multiplicative error. Finally, the legislation of a new law seems to have caused a decrease in the number of causalities. To have a better model in terms of explanatory and predictive power, we should include all three variables. This is how we can do that using `adam()`:

```{r}
adamETSXMNMSeat <- adam(SeatbeltsData, "MNM", h=12, holdout=TRUE,
                        formula=drivers~log(kms)+log(PetrolPrice)+law)
```

The parameter `formula` in general is not compulsory. It can either be substituted by `formula=drivers~.` or dropped completely -- in the latter case, the function would fit the model of the first variable in the matrix from everything else. We need the formula in our case because we introduce log-transformations of some explanatory variables.

```{r SeatbeltsForecastX, fig.cap="The actual values for drivers and a forecast from ETSX(M,N,M) model."}
forecast(adamETSXMNMSeat, h=12, interval="prediction") |>
    plot()
```

Figure \@ref(fig:SeatbeltsForecastX) shows the forecast from the second model, which is slightly more accurate. More importantly, the prediction interval is narrower than in the simple ETS(M,N,M) because now the model takes the external information into account. Here is the summary of the second model:

```{r}
adamETSXMNMSeat
```

Note that the smoothing parameter $\alpha$ has reduced from `r round(adamETSMNMSeat$persistence["alpha"],2)` to `r round(adamETSXMNMSeat$persistence["alpha"],2)`. This led to the reduction in error measures. For example, based on RMSE, we can conclude that the model with explanatory variables is more precise than the simple univariate ETS(M,N,M). Still, we could try introducing the update of the parameters for the explanatory variables to see how it works (it might be unnecessary for this data):
```{r SeatbeltsForecastXD, fig.cap="The actual values for drivers and a forecast from ETSX(M,N,M){D} model."}
adamETSXMNMDSeat <- adam(SeatbeltsData, "MNM", h=12, holdout=TRUE,
                         formula=drivers~log(kms)+log(PetrolPrice)+law,
                         regressors="adapt", maxeval=10000)
```

::: remark
Given the complexity of the estimation task, the default number of iterations needed for the optimiser to find the minimum of the loss function might not be sufficient. This is why I introduced `maxeval=10000` in the code above, increasing the number of maximum iterations to 10,000. In order to see how the optimiser worked out, you can add `print_level=41` in the code above.
:::

In our specific case, the difference between the ETSX and ETSX{D} models is infinitesimal in terms of the accuracy of final forecasts and prediction intervals. Here is the output of the model:
```{r}
adamETSXMNMDSeat
```

We can spot that the error measures of the dynamic model are slightly lower than the ones from the static one (e.g., compare RMSE of models). The information criteria are slightly lower as well. So based on all of this, we should use the dynamic model for forecasting and analytical purposes. In order to see the set of smoothing parameters for the explanatory variables in this model, we can use the command:

```{r}
round(adamETSXMNMDSeat$persistence,4)
```

And see how the states/parameters of the model change over time (Figure \@ref(fig:adamETSXMNMDSeatStates)):
```{r adamETSXMNMDSeatStates, fig.cap="Dynamic of states of the ETSX(M,N,M){D} model."}
plot(adamETSXMNMDSeat, 12, main="")
```

As we see in Figure \@ref(fig:adamETSXMNMDSeatStates), the effects of variables change over time. This mainly applies to `PetrolPrice` variable, the smoothing parameter for which is the highest among all $\delta_i$ parameters.

To see the initial effects of the explanatory variables on the number of incidents with drivers, we can look at the parameters for those variables:

```{r, warning=FALSE}
adamETSXMNMDSeat$initial$xreg
```

Based on that, we can conclude that the introduction of the law reduced on average the number of incidents by approximately `r abs(round(adamETSXMNMSeat$initial$xreg[3],2))*100`%, while the increase of the petrol price by 1% leads on average to decrease in the number of incidents by `r abs(round(adamETSXMNMSeat$initial$xreg[2],2))`%. Finally, the distance negatively impacts the incidents as well, reducing it on average by `r abs(round(adamETSXMNMSeat$initial$xreg[1],1))`% for each 1% increase in the distance. This is the standard interpretation of parameters, which we can use based on the estimated model [see, for example, discussion in Section 11.3 of @SvetunkovSBA]. We will discuss how to do further analysis using ADAM in Chapter \@ref(ADAMUncertainty), introducing the standard errors and confidence intervals for the parameters.

Finally, `adam()` has some shortcuts when a matrix of variables (not a data frame!) is provided with no formula, assuming that the necessary expansion has already been done. This leads to the decrease in computational time of the function and becomes especially useful when working on large samples of data. Here is an example with ETSX(M,N,N):
```{r eval=FALSE}
# Create matrix for the model
SeatbeltsDataExpanded <-
  ts(model.frame(drivers~log(kms)+log(PetrolPrice)+law,
                 SeatbeltsData),
     start=start(SeatbeltsData), frequency=frequency(SeatbeltsData))
# Fix the names of variables
colnames(SeatbeltsDataExpanded) <-
  make.names(colnames(SeatbeltsDataExpanded))
# Apply the model
adamETSXMNMExpandedSeat <- adam(SeatbeltsDataExpanded, "MNM",
                                lags=12, h=12, holdout=TRUE)
```
