# Handling uncertainty in ADAM {#ADAMUncertainty}
So far, when we discussed forecasts from ADAM models, we have assumed that the smoothing parameters and initial values are known, even though we have acknowledged in Chapter \@ref(ADAMETSEstimation) that they are estimated. This is the conventional assumption of ETS models from @Hyndman2008b (it also applies to ARIMA models). However, in reality, the parameters are never known and are always estimated in-sample. This means that the estimates of parameters will inevitably change with the change of sample size. This uncertainty will impact the model fit, the point forecasts and prediction intervals. To overcome this issue, @Bergmeir2016 proposed bagging -- the procedure that decomposes time series using STL [@Cleveland1990], then recreates many time series by bootstrapping the remainder then fits best ETS models to each of the newly created time series and combines the forecasts from the models. This way [as was explained by @Petropoulos2018], the parameters of models will differ from one generated time series to another. Thus the final forecasts will handle the uncertainty about the parameters. In addition, this approach also covers the model uncertainty element, which was discussed in Section \@ref(ADAMCombinations). The main issue with the approach is that it is computationally expensive and assumes that STL decomposition is appropriate for time series and that the residuals from this decomposition are independent.

In this chapter, we focus on a discussion of uncertainty about the estimates of parameters of ADAM models, starting from dealing with confidence intervals for them and ending with propagating the parameters uncertainty to the states and fitted values of the model. We also discuss a method that allows capturing this uncertainty and use it for fitted values and forecasts of the model.


## Covariance matrix of parameters {#ADAMUncertaintyVCOV}
One of the basic conventional statistical ways of capturing uncertainty about estimates of parameters is via the calculation of the covariance matrix of parameters. The covariance matrix that is typically calculated in regression context is based on the assumption of normality and can be derived based on the maximum likelihood estimate of parameters. It relies on the "Fisher Information", which in turn is calculated as a negative expectation of Hessian of parameters (the matrix of second derivatives of the likelihood function with respect to all the estimates of parameters). The idea of Hessian is to capture the curvatures of the likelihood function in its optimal point to understand what impact each of the parameters has on it. If we calculate the Hessian matrix and have Fisher Information, then using Cramer-Rao bound [@WikipediaCramerRaoBound], the true variance of parameters will be greater or equal to the inverse of Fisher Information:
\begin{equation}
    \mathrm{V}(\hat{\theta_j}) \geq \frac{1}{\mathrm{FI}(\hat{\theta_j})} ,
    (\#eq:FICovariance)
\end{equation}
where $\theta_j$ is the parameter under consideration. The property \@ref(eq:FICovariance) can then be used for the calculation of the covariance matrix of parameters. While in case of regression this calculation has an analytical solution, in case of ETS and ARIMA, this can only be done via numeric methods, because the models rely on recursive relations.

In R, an efficient calculation of Hessian can be done via the `hessian()` function from the `pracma` package. In `smooth` there is a method `vcov()` that does all the calculations, estimating the negative Hessian inside the `adam()` and then inverts the result. Here is an example of how this works:
```{r}
adamModel <- adam(BJsales, h=10, holdout=TRUE)
adamModelVcov <- vcov(adamModel)
round(adamModelVcov, 3)
```
The precision of the estimate will depend on the closeness of the likelihood function to its maximum in the estimated parameters. If the likelihood was not properly maximised and the function stopped prematurely, then the covariance matrix might be incorrect and contain errors. Out of curiosity, we could calculate the correlation matrix of the estimated parameters:
```{r}
round(adamModelVcov /
        sqrt(diag(adamModelVcov) %*%
               t(diag(adamModelVcov))), 3)
```
This matrix does not provide helpful analytical information but demonstrates that the estimates of the initial level and trend of the `r adamModel$model` model applied to this data are negatively correlated. The values from the covariance matrix can then be used for various purposes, including calculation of confidence intervals of parameters, construction of confidence interval for the fitted value and point forecasts and finally, the construction of more adequate prediction intervals.

In some cases, the `vcov()` method would complain that the Fisher Information cannot be inverted. This typically means that the `adam()` failed to reach the maximum of the likelihood function. Re-estimating the model might resolve the problem.

Note that this method only works when `loss="likelihood"` or when the loss is aligned with the assumed distribution (e.g. `loss="MSE"` and `distribution="dnorm"`). In all the other cases, other approaches (such as bootstrap) would need to be used to estimate the covariance matrix of parameters.


### Bootstrapped covariance matrix
An alternative way of constructing the matrix is via the bootstrap. The one implemented in `smooth` is based on the `coefboostrap()` method from the `greybox` package, which implements the modified case resampling. It is less efficient than the Fisher Information method in terms of computational time and works only for larger samples. The algorithm implemented in the function uses a continuous sub-sample of the original data, starting from the initial point $t=1$ (if backcasting is used, as discussed in Section \@ref(ADAMInitialisationOptAndBack), then the starting point will be allowed to vary). This sub-sample is then used to re-estimate `adam()` to get the empirical estimates of parameters. The procedure is repeated `nsim` times, which for `adam()` is by default equal to 100. This approach is far from ideal and will typically lead to the underestimated variance of initials because of the sample size restrictions. Still, it does not break the data structure and allows obtaining results relatively fast without imposing any additional assumptions on the model and the data. I recommend using it in case of the initialisation via backcasting.

Here is an example of how the function works on the data above -- it is possible to speed up the process by doing parallel calculations:
```{r}
adamModelBoot <- coefbootstrap(adamModel, parallel=TRUE)
adamModelBoot
```
The covariance matrix can then be extracted from the result via `adamModelBoot$vcov`. The same procedure is used in `vcov()` method if `bootstrap=TRUE`:
```{r}
round(vcov(adamModel, bootstrap=TRUE, parallel=TRUE), 3)
```


## Confidence intervals for parameters
As discussed in Section 5.1 of @SvetunkovSBA, if several vital assumptions (discussed in Section \@ref(diagnostics)) are satisfied and CLT holds, then the distribution of estimates of parameters will follow the Normal one, which will allow us to construct confidence intervals for them. In the case of ETS and ARIMA models in the ADAM framework, the estimated parameters include smoothing, dampening and ARMA parameters together with the initial states' values. In the case of explanatory variables, the pool of parameters is increased by the coefficients for those variables and their smoothing parameters (if the dynamic model from Section \@ref(ADAMXDynamic) is used). And in the case of the intermittent state space model, the parameters will also include the elements of the occurrence part of the model. The CLT should work if:

1. Estimates of parameters are consistent (e.g. MSE or Likelihood is used in estimation, see Section \@ref(ADAMETSEstimation)),
2. the parameters do not lie near the bounds,
3. the model is correctly specified and
4. moments of the distribution of error term are finite.

In the case of ETS and ARIMA, the parameters are bounded, and the estimates might lie near the bounds. This means that the distribution of estimates might not be normal. However, given that the bounds of the parameters are typically fixed and are forced by the optimiser, the estimates of parameters will follow rectified normal distribution [@WikipediaRectifiedNormal]. This is important because knowing the distribution, we can derive the confidence intervals for the parameters. We would need to use t-statistics because we estimate the standard errors of parameters. The confidence intervals will be constructed in a conventional way in this case, using the formula [see Section 5.1 of @SvetunkovSBA]:
\begin{equation}
    \theta_j \in (\hat{\theta_j} + t_{\alpha/2}(df) s_{\theta_j}, \hat{\theta_j} + t_{1-\alpha/2}(df) s_{\theta_j}), 
    (\#eq:confidenceInterval)
\end{equation}
where $t_{\alpha/2}(df)$ is Student's t-statistics for $df=T-k$ degrees of freedom ($T$ is the sample size and $k$ is the number of estimated parameters) and $\alpha$ is the significance level. Then, after constructing the intervals, we can cut their values with the bounds of parameters, thus imposing rectified distribution (t distribution in this case). An example would be the ETS(A,N,N) model, for which the smoothing parameter is typically restricted by (0, 1) region and thus the confidence interval should not go beyond these bounds as well.

To construct the interval, we need to know the standard errors of parameters. Luckily, they can be calculated as square roots of the diagonal of the covariance matrix of parameters (discussed in Section \@ref(ADAMUncertaintyVCOV)):
```{r}
sqrt(diag(adamModelVcov))
```
Based on these values and the formula \@ref(eq:confidenceInterval), we can produce confidence intervals for parameters of any ADAM model, which is done in R using the `confint()` method. For example, here are the intervals for the significance level of 1%:
```{r}
confint(adamModel, level=0.99)
```
In the output above, the distributions for $\alpha$, $\beta$ and $\phi$ are rectified: $\alpha$ and $\phi$ are restricted with one from above, while $\beta$ is restricted with zero from below. To have the bigger picture, we can produce the summary of the model, which will include the table above:
```{r}
summary(adamModel, level=0.99)
```
The output above shows the estimates of parameters and their 99% confidence intervals. For example, based on this output, we can conclude that the uncertainty about the initial trend estimate is considerable, and in the "true model", it could be either positive or negative (or even close to zero). At the same time, the "true" parameter of the initial level will lie in 99% of the cases between 194.56 and 208.06. Just as a reminder, Figure \@ref(fig:adamModelBJAAdN) shows the model fit and point forecasts for the estimated ETS model on this data.

```{r adamModelBJAAdN, fig.cap="Model fit and point forecasts of ETS(A,Ad,N) on Box-Jenkins Sales data.", echo=FALSE}
plot(adamModel,7)
```

As another example, we can have a similar summary for ARIMA models in ADAM:

```{r}
adamModelARIMA <- adam(BJsales, "NNN", h=10, holdout=TRUE,
                       order=list(ar=3,i=2,ma=3,select=TRUE))
summary(adamModelARIMA)
```

From the summary above, we can see that the parameter $\theta_2$ is close to zero, and the interval around it is wide. So, we can expect that it might change the sign if the sample size increases or become even closer to zero. Given that the model above was estimated with the optimisation of initial states, we also see the values for the ARIMA states and their confidence intervals in the summary above. If we used `initial="backcasting"`, the summary would not include them.

This estimate of uncertainty via confidence intervals might also be helpful to see what can happen with the estimates of parameters if the sample size increases: will they change substantially or not. If they do, then the decisions made on Monday based on the available data might differ considerably from the decisions made on Tuesday. So, in the ideal world, we would want to have as narrow confidence intervals as possible.


## Conditional variance with uncertain parameters {#conditionalVarianceUncertainty}
We now consider two special cases with pure additive state space models:

(1) When the values of the initial state vector are unknown;
(2) When the model parameters (e.g. smoothing or AR/MA parameters) are estimated on a sample of data.

We discuss analytical formulae for the conditional variances for these cases. This variance can then be used to construct the confidence interval of the fitted line and/or for the confidence/prediction interval for the holdout period. We do not cover the more realistic case when both initials and parameters are estimated because there is no closed analytical form for this due to potential correlations between the estimates of parameters.

### Estimated initial state
First, we need to recall the recursive relations discussed in Section \@ref(stabilityConditionAdditiveError), specifically formula \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion04). Just to simplify all the derivations in this section, we consider the non-seasonal case, in which all elements of $\mathbf{l}$ are equal to one. This can be ETS(A,N,N), ETS(A,A,N), ETS(A,Ad,N) or several ARIMA models. The more general case is more complicated but is derivable using the same principles as discussed below. The recursive relation from the first observation till the end of the sample can be written as:
\begin{equation}
    \hat{\mathbf{v}}_{t} = \mathbf{D}^{t} \hat{\mathbf{v}}_{0} + \sum_{j=0}^{t-1} \mathbf{D}^{j} \hat{\mathbf{g}} y_{t-j} ,
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal01)
\end{equation}
where $\mathbf{D}=\mathbf{F} -\mathbf{g}\mathbf{w}^\prime$. The formula \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal01) shows that the most recent value of the state vector depends on the initial value $\hat{\mathbf{v}}_{0}$ and on the linear combination of actual values. **Note** that we assume in this part that the matrix $\mathbf{D}$ is known, i.e. the smoothing parameters are not estimated. Although this is an unrealistic assumption, it helps in showing how the variance of initial state would influence the conditional variance of actual values at the end of sample. If we now take the variance of state vector conditional on the previous actual values $y_{t-j}$ for all $j=\{0, \dots, t-1 \}$, then we will have (due to independence of two terms in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal01)):
\begin{equation}
    \mathrm{V}(\hat{\mathbf{v}}_{t} | y_1, y_2, \dots y_t) = \mathrm{V}\left( \mathbf{D}^{t} \hat{\mathbf{g}} \hat{\mathbf{v}}_{0} \right) + \mathrm{V}\left(\sum_{j=0}^{t-1} \mathbf{D}^{j} y_{t-j} | y_1, y_2, \dots y_t \right) .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal02)
\end{equation}
We condition the variance on actual values because they are given to us, and we want to see how different initial states would lead to the changes in the model fit given these values and thus how the uncertainty will propagate from $j=1$ to $j=t$. In the formula \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal02), the right-hand side is equal to zero because all actual values are known, and $\mathbf{D}$ does not have any uncertainty due to the assumption above. This leads to the following covariance matrix of states on observation $t$:
\begin{equation}
    \mathrm{V}(\hat{\mathbf{v}}_{t} | y_1, y_2, \dots y_t) = \mathbf{D}^{t} \mathrm{V}\left( \hat{\mathbf{v}}_{0} \right) \left(\mathbf{D}^{t}\right)^\prime .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal03)
\end{equation}
Inserting the values of matrix $\mathbf{D}$ in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal03), we can then get the variance of state vector. For example, for ETS(A,N,N), the conditional variance of the level on observation $t$ is:
\begin{equation}
    \mathrm{V}(\hat{l}_{t} | y_1, y_2, \dots y_t) = (1-\alpha)^{t} \mathrm{V}\left( \hat{l}_{0} \right) (1-\alpha)^{t} .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionANN)
\end{equation}
As the formula above shows, if the smoothing parameter lies between zero and one, then the uncertainty of the initial level will not have a big impact on the uncertainty on observation $t$. The closer $\alpha$ is to zero, the more impact the variance of the initial level will have on the variance of the final level. If we use admissible bounds (see Section \@ref(ETSParametersBounds)), then the smoothing parameter might lie in the region (1, 2), and thus the impact of the variance of the initial state will increase with the increase of the sample size $t$.

Now that we have the variance of the state, we can also calculate the variance of the fitted values (or one step ahead in-sample forecast). In the pure additive model, the fitted values are calculated as:
\begin{equation}
    \hat{y}_t = \mu_{t|t-1} = \mathbf{w}^\prime \hat{\mathbf{v}}_{t-1}.
  (\#eq:ETSADAMStateSpacePureAdditiveFitted)
\end{equation}
The variance conditional on all actual observations will then be:
\begin{equation}
    \mathrm{V}(\hat{y}_t | y_1, y_2, \dots y_t) = \mathrm{V}\left( \mathbf{w}^\prime \hat{\mathbf{v}}_{t-1} \right) ,
  (\#eq:ETSADAMStateSpacePureAdditiveFittedVariance01)
\end{equation}
which after inserting \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal03) in \@ref(eq:ETSADAMStateSpacePureAdditiveFittedVariance01) leads to:
\begin{equation}
    \mathrm{V}(\hat{y}_t | y_1, y_2, \dots y_t) = \mathbf{w}^\prime \mathbf{D}^{t-1} \mathrm{V}\left( \hat{\mathbf{v}}_{0} \right) \left(\mathbf{D}^{t-1}\right)^\prime \mathbf{w} .
  (\#eq:ETSADAMStateSpacePureAdditiveFittedVariance02)
\end{equation}
This variance can then be used to calculate the confidence interval for the fitted values, assuming that the estimates of initials state follow a normal distribution (due to CLT).

Finally, the variance of initial states will also impact the conditional h steps ahead variance of the model. This can be seen from the recursion \@ref(eq:ETSADAMStateSpacePureAdditiveRecursion05), which in case of non-seasonal models simplifies to:
\begin{equation}
    y_{t+h} = \mathbf{w}^\prime \mathbf{F}^{h-1} \hat{\mathbf{v}}_{t} + \mathbf{w}^\prime \sum_{j=1}^{h-1} \mathbf{F}^{j-1} \mathbf{g} e_{t+h-j} + e_{t+h} .
  (\#eq:ETSADAMStateSpacePureAdditiveForecastVariance01)
\end{equation}
Taking the variance of $y_{t+h}$ conditional on the all the information until the observation $t$ (all actual values) leads to:
\begin{equation}
    \begin{aligned}
    \mathrm{V}( y_{t+h} | y_1, y_2, \dots y_t) = & \mathbf{w}^\prime \mathbf{F}^{h-1} \mathbf{D}^{t-1} \mathrm{V}\left( \hat{\mathbf{v}}_{0} \right) \left(\mathbf{D}^{t-1}\right)^\prime (\mathbf{F}^\prime)^{h-1} \mathbf{w} + \\
                                                 & \left( \left(\mathbf{w}^\prime \sum_{j=1}^{h-1} \mathbf{F}^{j-1} \mathbf{g} \mathbf{g}^\prime (\mathbf{F}^\prime)^{j-1} \mathbf{w} \right) + 1 \right) \sigma^2 .
    \end{aligned}
  (\#eq:ETSADAMStateSpacePureAdditiveForecastVariance02)
\end{equation}
This formula can then be used for the construction of prediction intervals of the model, for example using formula \@ref(eq:ETSADAMStateSpacePureAdditivePredictionInterval). The topic of construction of prediction intervals will be discussed later in Section \@ref(ADAMForecastingPI).

As a final note, it is also possible to derive the variances for the seasonal models. The only thing that would change in this situation is that the matrices $\mathbf{F}$, $\mathbf{w}$ and $\mathbf{g}$ will need to be split into submatrices, similar to how it was done in Section \@ref(adamETSPureAdditiveRecursive).


### Estimated parameters of ADAM model
Now we discuss the case when the initial states are either known or not estimated directly. This, for example, corresponds to the situation with backcasted initials. Continuing our non-seasonal model example, we can use the following recursion (similar to \@ref(eq:ETSADAMStateSpacePureAdditiveForecastVariance01)), keeping in mind that now the value of the initial state vector $\mathbf{v}_0$ is known:
\begin{equation}
    \mathbf{v}_{t+h-1} = \hat{\mathbf{F}}^{h-1} \mathbf{v}_{t} + \sum_{j=1}^{h-1} \hat{\mathbf{F}}^{j-1} \hat{\mathbf{g}} e_{t+h-j} .
  (\#eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal04)
\end{equation}
The conditional variance of the state, given the values on observation $t$ in \@ref(eq:ETSADAMStateSpacePureAdditiveRecursionNonSeasonal04) in general does not have a closed-form because of the exponentiation of the transition matrix $\hat{\mathbf{F}}$. However, in a special case, when the matrix does not contain the parameters (e.g. non-damped trend ETS models or ARIMA without AR terms), there is an analytical solution to the variance. In this case, $\mathbf{F}$ is provided rather than being estimated, which simplifies the inference:
\begin{equation}
    \mathrm{V}(\mathbf{v}_{t+h-1} | t) = \mathrm{V}\left(\sum_{j=1}^{h-1} \mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j}\right)
  (\#eq:ETSADAMStateSpacePureAdditiveVariance01)
\end{equation}

The variance of the sum in \@ref(eq:ETSADAMStateSpacePureAdditiveVariance01) can be expanded as:
\begin{equation}
    \mathrm{V} \left(\sum_{j=1}^{h-1} \mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j} \right) = \sum_{j=1}^{h-1} \mathrm{V} \left(\mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j}\right) + 2 \sum_{j=2}^{h-1} \sum_{i=1}^{j-1} \mathrm{cov}(\mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j},\mathbf{F}^{i} \hat{\mathbf{g}} e_{t+h-i}).
  (\#eq:ETSADAMStateSpacePureAdditiveVariance02)
\end{equation}
Each variance in left-hand side of \@ref(eq:ETSADAMStateSpacePureAdditiveVariance02) can be expressed via:
\begin{equation}
    \mathrm{V} \left(\mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j}\right) = \mathbf{F}^{j-1} \left( \mathrm{V} (\hat{\mathbf{g}}) \mathrm{V}(e_{t+h-j}) + \mathrm{V} (\hat{\mathbf{g}}) \mathrm{E}(e_{t+h-j})^2 + \mathrm{E} (\hat{\mathbf{g}}) \mathrm{E} (\hat{\mathbf{g}})^\prime \mathrm{V}(e_{t+h-j})\right) (\mathbf{F}^{j-1})^\prime.
  (\#eq:ETSADAMStateSpacePureAdditiveVariance03)
\end{equation}
Given that the expectation of error term is assumed to be zero, and substituting $\mathrm{V}(e_{t+h-j})=\sigma^2$, this simplifies to:
\begin{equation}
    \mathrm{V} \left(\mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j}\right) = \mathbf{F}^{j-1} \left( \mathrm{V} (\hat{\mathbf{g}}) + \mathrm{E} (\hat{\mathbf{g}}) \mathrm{E} (\hat{\mathbf{g}})^\prime \right) (\mathbf{F}^{j})^\prime \sigma^2.
  (\#eq:ETSADAMStateSpacePureAdditiveVariance04)
\end{equation}
As for the covariances in \@ref(eq:ETSADAMStateSpacePureAdditiveVariance02), after the expansion it can be shown that each of them is equal to:
\begin{equation}
    \begin{aligned}
    \mathrm{cov}(\mathbf{F}^{j-1} \hat{\mathbf{g}} e_{t+h-j},\mathbf{F}^{i} \hat{\mathbf{g}} e_{t+h-i}) = & \mathrm{V}(\mathbf{F}^{j-1} \hat{\mathbf{g}}) \mathrm{cov}(e_{t+h-i},e_{t+h-j}) \\
     & + \left(\mathbf{F}^{j-1} \hat{\mathbf{g}}\right)^2 \mathrm{cov}(e_{t+h-i},e_{t+h-j}) \\
    & + \mathrm{E}(e_{t+h-i}) \mathrm{E}(e_{t+h-j}) \mathrm{V}(\mathbf{F}^{j-1} \hat{\mathbf{g}}).
    \end{aligned}
  (\#eq:ETSADAMStateSpacePureAdditiveVariance05)
\end{equation}
Given the assumptions of the model, the autocovariances of error terms should all be equal to zero, and the expectation of the error term should be equal to zero as well, which means that all the value in \@ref(eq:ETSADAMStateSpacePureAdditiveVariance05) will be equal to zero as well. Based on this, the conditional variance of states is equal to:
\begin{equation}
    \mathrm{V}(\mathbf{v}_{t+h-1}|t) = \sum_{j=1}^{h-1} \mathbf{F}^{j-1} \left( \mathrm{V} (\hat{\mathbf{g}}) + \mathrm{E} (\hat{\mathbf{g}}) \mathrm{E} (\hat{\mathbf{g}})^\prime \right) (\mathbf{F}^{j})^\prime \sigma^2
  (\#eq:ETSADAMStateSpacePureAdditiveVariance06)
\end{equation}
As discussed in Section \@ref(pureAdditiveExpectationAndVariance), the conditional variance of the actual value $h$ steps ahead is:
\begin{equation}
    \mathrm{V}(y_{t+h}|t) = \mathbf{w}^\prime \mathrm{V}(\mathbf{v}_{t+h-1}|t) \mathbf{w} + \sigma^2
  (\#eq:ETSADAMStateSpacePureAdditiveVariance07)
\end{equation}
Inserting \@ref(eq:ETSADAMStateSpacePureAdditiveVariance06) in \@ref(eq:ETSADAMStateSpacePureAdditiveVariance07), we get the final conditional h steps ahead variance of the model:
\begin{equation}
    \sigma^2_h = \mathrm{V}(y_{t+h}|t) = \left(\mathbf{w}^\prime \sum_{j=1}^{h-1} \mathbf{F}^{j-1} \left( \mathrm{V} (\hat{\mathbf{g}}) + \mathrm{E} (\hat{\mathbf{g}}) \mathrm{E} (\hat{\mathbf{g}})^\prime \right) (\mathbf{F}^{j})^\prime \mathbf{w} + 1 \right)\sigma^2,
  (\#eq:ETSADAMStateSpacePureAdditiveVarianceFinal)
\end{equation}
which looks similar to the one in formula \@ref(eq:ETSADAMStateSpaceANNRecursionMeanAndVariance) from Section \@ref(pureAdditiveExpectationAndVariance), but now has the covariance of persistence vector in it.

Unfortunately, the conditional variances for the other models are more complicated due to the introduction of convolutions of parameters. Furthermore, the formula \@ref(eq:ETSADAMStateSpacePureAdditiveVarianceFinal) only focuses on the conditional variance given the known $\mathbf{v}_t$ but does not take into account the uncertainty of it for the fitted values in-sample. Given the complexity of the problem, in the next section, we introduce a technique that allows correctly propagating the uncertainty of parameters and initial values to the forecasts of any ADAM model.


## Multi-scenarios for ADAM states {#adamRefitted}
As discussed in Section \@ref(conditionalVarianceUncertainty), it is difficult to capture the impact of the uncertainty about the parameters on the states of the model and, as a result, difficult to take it into account on the forecasting stage. Furthermore, so far, we have only discussed pure additive models, for which it is at least possible to do some derivations. When it comes to models with multiplicative components, it becomes nearly impossible to demonstrate how the uncertainty propagates over time. To overcome these limitations, we develop a simulation-based approach that relies on the selected model form.

The idea of the approach is to get the covariance matrix of the parameters of the selected model (see Section \@ref(ADAMUncertaintyVCOV)) and then generate $n$ sets of parameters randomly from a rectified multivariate normal distribution using the matrix and the values of estimated parameters. After that, the model is applied to the data with each generated parameter combination to get the states, fitted values, and residuals. This way, we propagate the uncertainty about the parameters from the first observation to the last. The final states can also be used to produce point forecasts and prediction intervals based on each set of parameters. These scenarios allow creating more adequate prediction intervals from the model and/or confidence intervals for the fitted values, states and conditional expectations. All of this is done without additional assumptions (as it is done in bagging), relying entirely on the model. However, the approach is computationally expensive, as it requires fitting all the $n$ models to the data, although no estimation is needed. If the uncertainty about the model needs to be taken into account, then the combination of models can be used, as described in Section \@ref(ADAMCombinations).

`smooth` package has the method `reapply()` that implements this approach for `adam()` models. This works with ADAM ETS, ARIMA, regression and any combination of the three. Here is an example in R with $n=1000$:

```{r adamETSRefitted, fig.cap="Refitted ADAM ETS(M,M,M) model on AirPassengers data."}
adamModelETS <- adam(AirPassengers, "MMM", h=10, holdout=TRUE)
adamModelETSRefitted <- reapply(adamModelETS, nsim=1000)
plot(adamModelETSRefitted)
```

Figure \@ref(fig:adamETSRefitted) demonstrates how the approach works on the example of `AirPassengers` data with ETS(M,M,M) model. The grey areas around the fitted line show quantiles from the fitted values, forming confidence intervals of width 95%, 80%, 60%, 40% and 20%. They show how the fitted value would vary if the parameters would differ from the estimated ones. Note that there was a warning about the covariance matrix of parameters, which typically arises if the optimal value of the loss function was not reached. If this happens, I would recommend tuning the optimiser (see Section \@ref(ADAMInitialisation)). For example, we could try more iterations via setting the `maxeval` parameter to a higher value or re-estimating the model, providing the estimates of parameters in `B`. If these fail and the bounds from the `reapply()` are too wide, it might make sense to consider backcasting for the initialisation.

The `adamModelETSRefitted` object contains several variables, including:

- `adamModelETSRefitted$states` -- the array of states of dimensions $k \times (T+m) \times n$, where $m$ is the maximum lag of the model, $k$ is the number of components and $T$ is the sample size;
- `adamModelETSRefitted$refitted` -- distribution of fitted values of dimensions $T \times n$;
- `adamModelETSRefitted$transition` -- the array of transition matrices of the size $k \times k \times n$;
- `adamModelETSRefitted$measurement` -- the array of measurement matrices of the size $(T+m) \times k \times n$;
- `adamModelETSRefitted$persistence` -- the persistence matrix of the size $k \times n$;

The last three will contain the random parameters (smoothing, damping and AR/MA parameters), which is why they are provided together with the other values.

As mentioned earlier, ADAM ARIMA also supports this approach. Here is an example on artificial, non-seasonal data (see Figure \@ref(fig:adamARIMARefitted)):

```{r eval=FALSE}
y <- rnorm(200,100,10)
adamModelARIMA1 <- adam(y, "NNN", h=10, holdout=TRUE,
                        orders=c(0,1,1))
adamModelARIMARefitted1 <- reapply(adamModelARIMA1)
plot(adamModelARIMARefitted1)
```

```{r adamARIMARefitted, fig.cap="Refitted ADAM ARIMA(0,1,1) model on artificial data.", echo=FALSE}
load("data/adamRefitted.Rdata")
plot(adamModelARIMARefitted1)
```

Note that the more complicated the fitted model is, the more difficult it is to optimise it, and thus the more difficult it is to get accurate estimates of the covariance matrix of parameters. This might result in highly uncertain states and thus fitted values. The safer approach, in this case, is using bootstrap for the estimation of the covariance matrix, but this is more computationally expensive and would only work on longer time series. See example in R (and Figure \@ref(fig:adamARIMARefitted200)):

```{r eval=FALSE}
adamModelARIMA2 <- adam(y, "NNN", h=10, holdout=TRUE,
                       orders=c(0,1,1))
adamModelARIMARefitted2 <- reapply(adamModelARIMA2, bootstrap=TRUE,
                                   nsim=1000, parallel=TRUE)
plot(adamModelARIMARefitted2)
```

```{r adamARIMARefitted200, fig.cap="Refitted ADAM ARIMA(0,1,1) model on artificial data, bootstrapped covariance matrix.", echo=FALSE}
plot(adamModelARIMARefitted2)
```

The approach described in this section is still a work in progress. While it works in theory, there are computational difficulties with calculating the Hessian matrix. If the covariance matrix is not estimated accurately, it might contain high variances, leading to the higher than needed uncertainty of the model. This will result in unreasonable confidence bounds and lead to extremely wide prediction intervals.
