# From time series components to ETS {#tsDecomposition}
Before we turn to state space framework, ETS, ARIMA and other models we need to discuss time series decomposition and the ETS taxonomy. These topics lie at the heart of ETS models and are essential for the understanding of the further material.

In this chapter we start with a discussion of time series components, then move to the idea of decomposing time series into distinct components and then to the conventional ETS taxonomy, as formulated by @Hyndman2008b, demonstrating its connection with the previous topics.

## Time series components {#tsComponents}
The main idea behind many forecasting techniques is that any time series can contain several unobservable components, such as:

1. **Level** of the series - the average value for specific period of time,
2. **Growth** of the series - the average increase or decrease of the value over a period of time,
3. **Seasonality** - a pattern that repeats itself with a fixed periodicity.
4. **Error** - unexplainable white noise.

Level is the basic component that is present in any time series. In the simplest form (without variability), when plotted on its own without other components, it will look like a straight line, shown, for example, in Figure \@ref(fig:levelExample).

```{r levelExample, fig.cap="Level of time series without any variability."}
level <- rep(100,40)
plot(ts(level, frequency=4),
     type="l", xlab="Time", ylab="Sales", ylim=c(80,160))
```

If the time series exhibits growth, the level will change depending on the observation. For example, if the growth is positive and constant, we can update the level in Figure \@ref(fig:levelExample) to have a straight line with a non-zero slope as shown in Figure \@ref(fig:trendExample).

```{r trendExample, fig.cap="Time series with a positive trend and no variability."}
growth <- c(1:40)
plot(ts(level+growth, frequency=4),
     type="l", xlab="Time", ylab="Sales", ylim=c(80,160))
```

The seasonal pattern will introduce some similarities from one period to another. This pattern does not have to literally be seasonal, like beer sales being higher in Summer than they are in Winter (season of year). Any pattern with a fixed periodicity works: the number of hospital visitors is higher on Mondays than on Saturdays or Sundays because people tend to stay at home over the weekend (day of week seasonality), and sales are higher during daytime than they are at night (hour of the day seasonality). Adding a deterministic seasonal component to the example above will result in fluctuations around the straight line as shown in Figure \@ref(fig:seasonalExample).

```{r seasonalExample, fig.cap="Time series with a positive trend, seasonal pattern and no variability."}
seasonal <- rep(c(10,15,-20,-5),10)
plot(ts(level+growth+seasonal, frequency=4),
     type="l", xlab="Time", ylab="Sales", ylim=c(80,160))
```

Finally, we can introduce the random error to the plots above to have more realistic time series as shown in Figure \@ref(fig:allExample).

```{r allExample, fig.cap="Time series with random errors.", echo=FALSE}
par(mfcol=c(3,1),mar=c(2,2,2,1))
error <- rnorm(40,0,5)
plot(ts(level+error, frequency=4),
     type="l", xlab="Time", ylab="Sales", ylim=c(80,160), main="Level time series")
plot(ts(level+growth+error, frequency=4),
     type="l", xlab="Time", ylab="Sales", ylim=c(80,160), main="Time series with growth")
plot(ts(level+growth+seasonal+error, frequency=4),
     type="l", xlab="Time", ylab="Sales", ylim=c(80,160), main="Trend-seasonal time series")
```

The plots in Figure \@ref(fig:allExample) show artificial time series with the components discussed above. The level, growth and seasonal components in those plots are **deterministic**, they are fixed and do not evolve over time (growth is positive and equal to 1 from year to year). However, in real life, typically these components will have a more complex dynamics, changing over time and thus demonstrating their **stochastic** nature. For example, in case of stochastic seasonality, the seasonal shape might change and instead of having peaks in sales in January the data would exhibit peaks in May due to the change in consumers' behaviour.

Note that each textbook and paper might use slightly different names to refer to the components discussed above. For example, in classical decomposition [@Persons1919] it is assumed that (1) and (2) jointly represent a "trend" component so a model will contain error, trend and seasonality. There are modifications of this decomposition, which also contain cyclical component(s).

When it comes to ETS, the growth component (2) is called "trend", so the model consists of the four components: level, trend, seasonal and error term. We will use the ETS formulation in this textbook. According to this formulation the components can interact with each other in one of two ways: additively or multiplicatively. The pure additive model in this case can be summarised as:
\begin{equation}
    y_t = l_{t-1} + b_{t-1} + s_{t-m} + \epsilon_t ,
    (\#eq:PureAdditive)
\end{equation}
where $l_{t-1}$ is the level, $b_{t-1}$ is the trend, $s_{t-m}$ is the seasonal component with periodicity $m$ (e.g. 12 for months of year data, implying that something is repeated every 12 months) - all these components are produced on the previous observations and are used on the current one. Finally, $\epsilon_t$ is the error term, which follows some distribution and has zero mean. The pure additive models were plotted in Figure \@ref(fig:allExample). Similarly, the pure multiplicative model is:
\begin{equation}
    y_t = l_{t-1} b_{t-1} s_{t-m} \varepsilon_t ,
    (\#eq:PureMultiplicative)
\end{equation}
where $\varepsilon_t$ is the error term that has mean of one. The interpretation of the model \@ref(eq:PureAdditive) is that the different components add up to each other, so, for example, the sales increase over time by the value $b_{t-1}$, each January they typically change by the amount $s_{t-m}$, and that there is still some randomness in the model. The pure additive models can be applied to data that can have positive, negative and zero values. In case of the multiplicative model \@ref(eq:PureMultiplicative), the interpretation is different, showing by how many times the sales change over time and from one season to another. The sales in this case will change every January by $(s_{t-m}-1)$% from the baseline. The model \@ref(eq:PureMultiplicative) only work on data with strictly positive values (data with purely negative values are also possible but rare in practice).

It is also possible to define mixed models in which, for example, the trend is additive but the other components are multiplicative:
\begin{equation}
    y_t = (l_{t-1} + b_{t-1}) s_{t-m} \varepsilon_t
    (\#eq:MixedAdditiveTrend)
\end{equation}
These models work well in practice when the data has large values far from zero. In other cases, however, they might break and produce strange results (e.g. negative values on positive data) so the conventional decomposition techniques only consider the pure models.


## Classical Seasonal Decomposition {#ClassicalDecomposition}
### How to do?
One of the classical textbook methods for decomposing the time series into unobservable components is called "Classical Seasonal Decomposition" [@Persons1919]. It assumes either a pure additive or pure multiplicative model, is done using centred moving averages and is focused on approximation, not on forecasting. The idea of the method can be summarised in the following steps:

1. Decide, which of the models to use based on the type of seasonality in the data: additive \@ref(eq:PureAdditive) or multiplicative \@ref(eq:PureMultiplicative)
2. Smooth the data using a centred moving average (CMA) of order equal to the periodicity of the data $m$. If $m$ is the an number then the formula is:
\begin{equation}
    d_t = \frac{1}{m}\sum_{i=-(m-1)/2}^{(m-1)/2} y_{t+i},
    (\#eq:CMAOdd)
\end{equation}
which means that, for example, the value on Thursday is the average of values from Monday to Sunday. If $m$ is an even number then a different weighting scheme is typically used, involving the inclusion of additional an value:
\begin{equation}
    d_t = \frac{1}{m}\left(\frac{1}{2}\left(y_{t+(m-1)/2}+y_{t-(m-1)/2}\right) + \sum_{i=-(m-2)/2}^{(m-2)/2} y_{t+i}\right),
    (\#eq:CMAEven)
\end{equation}
which means that we use half of the December of the previous year and half of the December of the current year in order to calculate the centred moving average in June. The values $d_t$ are placed in the middle of the window going through the series (e.g. on Thursday the average will contain values from Monday to Sunday).

The resulting series is deseasonalised. When we average e.g. sales in a year we automatically remove the potential seasonality, which can be observed individually in each month. A drawback of using CMA is that we inevitably lose $\frac{m}{2}$ observations at the beginning and the end of the series.

In R, the `ma()` function from the `forecast` package implements CMA.

3. De-trend the data:
- For the additive decomposition this is done using: ${y^\prime}_t = y_t - d_t$;
- For the multiplicative decomposition, it is: ${y^\prime}_t = \frac{y_t}{d_t}$;
4. If the data is seasonal, then the average value for each period is calculated based on the de-trended series. e.g. we produce average seasonal indices for each January, February, etc. This will give us the set of seasonal indices $s_t$;
5. Calculate the residuals based on what you assume in the model:
- additive seasonality: $e_t = y_t - d_t - s_t$;
- multiplicative seasonality: $e_t = \frac{y_t}{d_t s_t}$;
- no seasonality: $e_t = {y^\prime}_t$.

Note that the functions in R typically allow you to select between additive and multiplicative seasonality. There is no option for "none" and so even if the data is not seasonal you will nonetheless get values for $s_t$ in the output. Also, notice that the classical decomposition assumes that there is a deseasonalised series $d_t$ but does not make any further split of this variable into level $l_t$ and trend $b_t$.

### A couple of examples
An example of the classical decomposition in R is the `decompose()` function from `stats` package. Here is an example with pure multiplicative model and `AirPassengers` data:
```{r decomposeAirPassengers}
ourDecomposition <- decompose(AirPassengers,
                              type="multiplicative")
plot(ourDecomposition)
```

We can see that the function has smoothed the original series and produced the seasonal indices. Note that the trend component has gaps at the beginning and at the end. This is because the method relies on CMA (see above). Note also that the error term still contains some seasonal elements, which is a downside of such a simple decomposition procedure. However, the lack of precision in this method is compensated by the simplicity and speed of calculation. Note again that the trend component in `decompose()` function is in fact $d_t = l_{t}+b_{t}$.

Here is an example of decomposition of the **non-seasonal data** (we assume pure additive model in this example):
```{r decomposeRandomNoise}
y <- ts(c(1:100)+rnorm(100,0,10),frequency=12)
ourDecomposition <- decompose(y, type="additive")
plot(ourDecomposition)
```

As you can see, the original data is not seasonal but the decomposition assumes that it is and proceeds with the default approach returning a seasonal component. You get what you ask for.

### Other techniques
There are other techniques that decompose series into error, trend and seasonal components but make different assumptions about each component. The general procedure, however, always remains the same: (1) smooth the original series, (2) extract the seasonal components, (3) smooth them out. The methods differ in the smoother they use (LOESS, e.g., uses a bisquare function instead of CMA) and in some cases multiple rounds of smoothing are performed to make sure that the components are split correctly.

There are many functions in R that implement seasonal decomposition. Here is a small selection:

- `decomp()` from the `tsutils` package does classical decomposition and fills in the tail and head of the smoothed trend with forecasts from exponential smoothing;
- `stl()` from the `stats` package uses a different approach - seasonal decomposition via LOESS. It is an iterative algorithm that smoothes the states and allows them to evolve over time. So, for example, the seasonal component in STL can change;
- `mstl()` from the `forecast` package does the STL for data with several seasonalities;
- `msdecompose()` from the `smooth` package does a classical decomposition for multiple seasonal series.

### "Why bother?"
"Why decompose?" you may wonder at this point. Understanding the idea behind decompositions and how to perform them helps in understanding ETS, which relies on it. From a practical point of view it can be useful if you want to see if there is a trend in the data and whether the residuals contain outliers or not. It will _not_ show you if the data is seasonal as the seasonality is _assumed_ in the decomposition (I stress this because many students think otherwise). Additionally, when seasonality cannot be added to the model under consideration decomposing the series, predicting the trend and then reseasonalising can be a viable solution. Finally, the values from the decomposition can be used as starting points for the estimation of components in ETS or other dynamic models relying on the error-trend-seasonality.


## Simple forecasting methods {#simpleForecastingMethods}
Now that we understand that time series might contain different components and that there are approaches for their decomposition, we can introduce several simple forecasting methods that can be used in practice at least as benchmarks. Their usage aligns with the idea of forecasting principles, discussed in Section \@ref(forecastingPrinciples).

### Naïve {#Naive}
Naïve is one of the simplest forecasting methods. According to each, the one step ahead forecast is equal to the most recent actual value:
\begin{equation}
    \hat{y}_t = y_{t-1} .
    (\#eq:Naive)
\end{equation}
Using this approach might sound naïve indeed, but there are cases, where it is very hard to outperform the method. Consider an example with temperature forecasting. If we want to know what the temperature outside will be in 5 minutes, then Naïve would be typically very accurate: the temperature in 5 minutes will be the same as it is right now. The statistical model underlying Naïve is called "Random Walk" and is written as:
\begin{equation}
    y_t = y_{t-1} + \epsilon_t.
    (\#eq:RandomWalk)
\end{equation}
The variability of $\epsilon_t$ will impact the speed of change of the data: the higher it is, the more rapid the values will change. Graphically Random Walk and Naïve can be represented in Figure \@ref(fig:naiveExample). In the example below we use simple moving average (discussed later in Section \@ref(SMA)) of order 1 to generate the data from Random Walk and then produce forecasts using Naïve.

```{r naiveExample, fig.cap="A Random Walk example."}
y <- sim.sma(1, 120)
testModel <- sma(y$data, 1,
                 h=10, holdout=TRUE)
plot(testModel, 7, main="")
```

As it can be seen from the plot in Figure \@ref(fig:naiveExample), Naïve lags behind the actual time series by one observation, because of how it is constructed via equation \@ref(eq:Naive). The point forecast corresponds to the straight line, parallel to x-axis. Given that the data was generated from Random Walk, the point forecast shown in Figure \@ref(fig:naiveExample) is the best possible forecast for the time series, even though it exhibits rapid changes in the level.

Note that if the time series exhibits level shifts or other types of unexpected changes in dynamics, Naïve will update rapidly and reach the new level instantaneously. However, because it only has a memory of one (last observation), it will not filter out the noise in the data, but rather copy it into the future. So, it has a limited usefulness in practice. However, being the simplest possible forecasting method, it is considered as one of the basic forecasting benchmarks. If your model cannot beat it, then it is not worth using.


### Global Mean {#GlobalMean}
While Naïve considered only one observation (the most recent one), global mean (aka "global average") relies on all the observations in the data:
\begin{equation}
    \hat{y}_t = \bar{y} = \frac{1}{T} \sum_{t=1}^T y_{t} ,
    (\#eq:GlobalMean)
\end{equation}
where $T$ is the sample size. The model underlying this forecasting method is called "global level" and is written as:
\begin{equation}
    y_t = \mu + \epsilon_t,
    (\#eq:GlobalLevel)
\end{equation}
so that the $\bar{y}$ is an estimate of the fixed expectation $\mu$. Graphically, this is represented with a straight line going through the series as shown in Figure \@ref(fig:globalMeanExample).

```{r globalMeanExample, fig.cap="A global level example."}
y <- rnorm(120, 100, 10)
testModel <- es(y, "ANN", persistence=0,
                 h=10, holdout=TRUE)
plot(testModel, 7, main="")
```

The series shown in Figure \@ref(fig:globalMeanExample) is generated from the global level model and the point forecast corresponds to the forecast from the global mean method. Note that the method assumes that the weights between the in sample observation are equal, i.e. the first observation has exactly the same weight of $\frac{1}{T}$ as the last one. If the series exhibits some changes in level over time, global mean will not be suitable, because it will produce the averaged out forecast, considering values for parts before and after the change.


### Simple Moving Average {#SMA}
Naïve and Global Mean can be considered as opposite points in the spectrum of possible level time series (although, there are series beyond Naïve, see for example ARIMA(0,1,1) with $\theta_1>0$, discussed in Section \@ref(ARIMA)). The series between them exhibit slow changes in level and can be modelled using different forecasting approaches. One of those is Simple Moving Average (SMA), which uses the mechanism of the mean for a small part of time series. It relies on the formula:
\begin{equation}
    \hat{y}_t = \frac{1}{m}\sum_{j=1}^{m} y_{t-j},
    (\#eq:SMA)
\end{equation}
which implies going through time series with something like a "window" of $m$ observations and using their average for forecasting. The order $m$ determines the length of the memory of the method: if it is equal to 1, then \@ref(eq:SMA) turns into Naïve, while in case of $m=T$ it transforms into Global Mean. The order $m$ is typically decided by a forecaster, keeping in mind that the lower $m$ corresponds to the shorter memory method, while the higher one corresponds to the longer one.

@Svetunkov2017 have shown that SMA has an underlying non-stationary AR(m) model with $\phi_j=\frac{1}{m}$ for all $j=1, \dots, m$. While the conventional approach to forecasting from SMA is to produce the straight line, equal to the last obtained observation, @Svetunkov2017 demonstrate that in general the point forecast does not correspond to the straight line.

```{r SMAExample, fig.cap="Examples of SMA time series and several SMA models of different orders applied to it."}
y <- sim.sma(10,120)
par(mfcol=c(2,2), mar=c(2,2,2,1))
# SMA(1)
testModel <- sma(y, order=1,
                 h=10, holdout=TRUE)
plot(testModel, 7, main=testModel$model)
# SMA(10)
testModel <- sma(y, order=10,
                 h=10, holdout=TRUE)
plot(testModel, 7, main=testModel$model)
# SMA(20)
testModel <- sma(y, order=20,
                 h=10, holdout=TRUE)
plot(testModel, 7, main=testModel$model)
# SMA(110)
testModel <- sma(y, order=110,
                 h=10, holdout=TRUE)
plot(testModel, 7, main=testModel$model)
```

Figure \@ref(fig:SMAExample) demonstrates the time series generated from SMA(10) and several SMA models applied to it. We can see that the higher orders of SMA lead to smoother fitted lines and calmer point forecasts. On the other hand, the SMA of very high order, such as SMA(110) does not follow the changes in time series efficiently, ignoring the potential changes in level. Given the difficulty with the selection of the order $m$, @Svetunkov2017 proposed using information criteria for the order selection of SMA in practice.

Finally, an attentive reader has already spotted that the formula for SMA corresponds to the formula of CMA of an odd order from equation \@ref(eq:CMAOdd). They are indeed similar, but they have a different purpose: CMA is needed in order to smooth out the series, the calculated values are inserted in the middle of the average, while SMA is used for forecasting, and the point forecasts are inserted at the end of the average.


### Random Walk with drift {#RWWithDrift}
So far we have discussed the methods used for level time series. But as discussed in the Section \@ref(tsComponents), there are other components in time series as well. In case of the series with a trend, Naïve, Global Mean and SMA will be inappropriate, because they would be missing the important component. The simplest model that can be used in this case is called "Random Walk with drift", which is formulated as:
\begin{equation}
    y_t = y_{t-1} + a_0 + \epsilon_t,
    (\#eq:RandomWalkWithDrift)
\end{equation}
where $a_0$ is a constant term, the introduction of which leads to increasing or decreasing trajectories, depending on the value of $a_0$. The point forecast from this model is calculated as:
\begin{equation}
    \hat{y}_{t+h} = y_{t} + a_0 h,
    (\#eq:RandomWalkWithDriftForecast)
\end{equation}
implying that the forecast from the model is a straight line with the slope parameter $a_0$. Figure \@ref(fig:RWDriftExample) shows how the data generated from Random Walk with drift and $a_0=10$ looks like. This model is discussed in some detail in Section \@ref(ARIMA).

```{r RWDriftExample, fig.cap="Random Walk with drift data and the model applied to that data."}
y <- sim.ssarima(orders=list(i=1), lags=1, obs=120,
                 constant=10)
testModel <- msarima(y, orders=list(i=1), constant=TRUE,
                     h=10, holdout=TRUE)
plot(testModel, 7, main="")
```

The data in Figure \@ref(fig:RWDriftExample) demonstrates positive trend (because $a_0>0$) and a randomness from one observation to another. The model is useful as a benchmark and as a special case for several other models, because it is simple and requires the estimation of only one parameter.


### Seasonal Naïve {#NaiveSeasonal}
Finally, in case of seasonal data, there is a simple forecasting method that can be considered as a good benchmark in many situations. Similar to Naïve, Seasonal Naïve relies only on one observation, but instead of taking the most recent value, it uses the value from the same period a season ago. For example, for producing a forecast for January 1984, we would use the value from the January 1983. Mathematically this is written as:
\begin{equation}
    \hat{y}_t = y_{t-m} ,
    (\#eq:NaiveSeasonal)
\end{equation}
where $m$ is the seasonal frequency. This method has an underlying model, Seasonal Random Walk:
\begin{equation}
    \hat{y}_t = y_{t-m} + \epsilon_t.
    (\#eq:RWSeasonal)
\end{equation}
Similar to Naïve, the higher is the variability of the error term $\epsilon_t$ in \@ref(eq:RWSeasonal), the faster will the data exhibit changes. Seasonal Naïve does not require estimation of any parameters and thus is considered as one of the popular benchmarks in case of seasonal data. Figure \@ref(fig:NaiveSeasonalExample) demonstrates how the data generated from seasonal Random Walk looks like and how the point forecast from the seasonal Naïve applied to this data performs.

```{r NaiveSeasonalExample, fig.cap="Seasonal Random Walk and Seasonal Naïve."}
y <- sim.ssarima(orders=list(i=1), lags=4,
                 obs=120, sd=50)
testModel <- msarima(y, orders=list(i=1), lags=4,
                     h=10, holdout=TRUE)
plot(testModel, 7, main="")
```

Similarly to the previous methods, if Seasonal Naïve cannot be outperformed by other approaches under consideration, then it is not worth spending time on those approaches.


## ETS taxonomy {#ETSTaxonomy}
Building on the idea of time series components (from Section \@ref(tsComponents)) we can move to the ETS taxonomy. ETS stands for "Error-Trend-Seasonality" and defines how specifically the components interact with each other. Based on the type of error, trend and seasonality, @Pegels1969 proposed a taxonomy, which was then developed further by @Hyndman2002 and refined by @Hyndman2008b. According to this taxonomy, error, trend and seasonality can be:

1. Error: "Additive" (A), or "Multiplicative" (M);
2. Trend: "None" (N), or "Additive" (A), or "Additive damped" (Ad), or "Multiplicative" (M), or "Multiplicative damped" (Md);
3. Seasonality: "None" (N), or "Additive" (A), or "Multiplicative" (M).

According to this taxonomy, the model \@ref(eq:PureAdditive) is denoted as ETS(A,A,A) while the model \@ref(eq:PureMultiplicative) is denoted as ETS(M,M,M), and \@ref(eq:MixedAdditiveTrend) is ETS(M,A,M).

The components in ETS taxonomy have clear interpretations. Furthermore, ETS supports 30 models with different types of error, trend and seasonality. Figure \@ref(fig:ETSTaxonomyAdditive) shows examples of different time series with deterministic (they do not change over time) level, trend, seasonality and with additive error term.

```{r ETSTaxonomyAdditive, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Time series corresponding to the additive error ETS models"}
modelsList <- c("ANN","AAN","AAdN","AMN","AMdN","ANA","AAA","AAdA","AMA","AMdA","ANM","AAM","AAdM","AMM","AMdM",
                "MNN","MAN","MAdN","MMN","MMdN","MNA","MAA","MAdA","MMA","MMdA","MNM","MAM","MAdM","MMM","MMdM")
level <- 500
trend <- c(100,1.05)
seasonality <- list((c(1.3,1.1,0.9,0.75)-1)*2*level,c(1.3,1.1,0.9,0.75))
generatedData <- vector("list", length(modelsList))
scale <- 0.05
for(i in 1:length(modelsList)){
  initial <- switch(substr(modelsList[i],2,2),
                    "A"=c(level,trend[1]),
                    "M"=c(level,trend[2]),
                    level*2);
  initialSeason <- switch(substr(modelsList[i],nchar(modelsList[i]),nchar(modelsList[i])),
                    "A"=seasonality[[1]],
                    "M"=seasonality[[2]],
                    NULL);
  if(nchar(modelsList[i])==4){
    phi <- 0.95;
  }
  else{
    phi <- 1;
  }
  sdValue <- switch(substr(modelsList[i],1,1),
                    "A"=sqrt(level^2*(exp(scale^2)-1)*exp(scale)),
                    "M"=scale)
  meanValue <- switch(substr(modelsList[i],1,1),
                      "A"=0,
                      "M"=1)
  generatedData[[i]] <- smooth::sim.es(modelsList[i], obs=36, frequency=4, persistence=0, phi=phi,
                                       initial=initial, initialSeason=initialSeason, mean=meanValue, sd=sdValue)$data
}

# Prepare the canvas
par(mfcol=c(5,3),mar=c(2,3,2,1))
# The matrix that corresponds to the i
ylimValues <- matrix(c(1:15),5,3)
for(i in 1:15){
  if(i>15){
    ylimRow <- which(ylimValues == i-15,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]+15]))
  }
  else{
    ylimRow <- which(ylimValues == i,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]]))
  }
  plot(generatedData[[i]], main=paste0("ETS(",modelsList[i],")"),ylim=ylim,ylab="")
}
```

Things to note from the plots in Figure \@ref(fig:ETSTaxonomyAdditive):

1. When seasonality is multiplicative its amplitude increases with the increase of level of the data while with additive seasonality the amplitude is constant. Compare, for example, ETS(A,A,A) with ETS(A,A,M): the distance between the highest and the lowest points for the former in the first year is roughly the same as in the last year. In the case of ETS(A,A,M) the distance increases with the increase in the level of series.
2. When the trend is multiplicative data exhibits exponential growth / decay result.
3. The damped trend models slow down both additive and multiplicative trends.
4. It is practically impossible to distinguish additive and multiplicative seasonality if the level series does not change because the amplitude of seasonality will be constant in both cases (compare ETS(A,N,A) and ETS(A,N,M)).

Figure \@ref(fig:ETSTaxonomyMultiplicative) demonstrates a similar plot for the multiplicative error models.

```{r ETSTaxonomyMultiplicative, echo=FALSE, warning=FALSE, fig.cap="Time series corresponding to the multiplicative error ETS models"}
# Prepare the canvas
par(mfcol=c(5,3),mar=c(2,3,2,1))
# The matrix that corresponds to the i
ylimValues <- matrix(c(1:15),5,3)
for(i in 16:30){
  if(i>15){
    ylimRow <- which(ylimValues == i-15,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]+15]))
  }
  else{
    ylimRow <- which(ylimValues == i,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]]))
  }
  plot(generatedData[[i]], main=paste0("ETS(",modelsList[i],")"),ylim=ylim,ylab="")
}
```

The plots in Figure \@ref(fig:ETSTaxonomyMultiplicative) show roughly the same idea as the additive case, the main difference being that the variance of the error increases with the increase of the level of the data - this becomes clearer on ETS(M,A,N) and ETS(M,M,N) data. This property is called heteroscedasticity in statistics and @Hyndman2008b argue that the main benefit of the multiplicative error models is being able to capture this feature.

In the next chapters we will discuss the most important members of the ETS family. Not all the models in this taxonomy are particularly sensible and some are typically ignored entirely. Although ADAM implements the entire taxonomy we will discuss potential issues and what to expect from them.


## Mathematical models in the ETS taxonomy {#ETSTaxonomyMaths}
I hope that it becomes clearer to the reader how the ETS framework is built upon the idea of time series decomposition (from Section \@ref(tsComponents)). By introducing different components and defining their types and by adding the equations for their update, we can construct models that would work better on the time series at hands. The equations discussed in Section \@ref(tsComponents) represent so called "measurement" or "observation" equations of the ETS models. But we should also take into account the potential change in components over time. The "transition" or "state" equation is supposed to reflect this change: they explain, how the level, trend or seasonal components evolve over time.

As discussed in Section \@ref(ETSTaxonomy), given different types of components and their interactions, we end up with 30 models in the taxonomy. Tables \@ref(tab:ETSAdditiveError) and \@ref(tab:ETSMultiplicativeError) summarise mathematically all 30 ETS models shown graphically on Figures \@ref(fig:ETSTaxonomyAdditive) and \@ref(fig:ETSTaxonomyMultiplicative), presenting formulae for measurement and transition equations.

```{r ETSAdditiveError, echo=FALSE}
# T="N"
etsAdditiveTable <- c("$\\begin{aligned} &y_{t} = l_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\alpha \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\alpha \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1}}
    \\end{aligned}$",
# T="A"
    "$\\begin{aligned} &y_{t} = l_{t-1} + b_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned}
      &y_{t} = l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1}) s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + b_{t-1}}
    \\end{aligned}$",
# T="Ad"
    "$\\begin{aligned} &y_{t} = l_{t-1} + \\phi b_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = \\phi b_{t-1} + \\beta \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = \\phi b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + \\phi b_{t-1}}
    \\end{aligned}$",
# T="M"
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}}
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}}
    \\end{aligned}$",
# T="Md"
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}}
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}}
    \\end{aligned}$")
etsAdditiveTable <- matrix(etsAdditiveTable, 5, 3, byrow=TRUE,
                           dimnames=list(c("**No trend**","**Additive trend**","**Additive damped trend**",
                                           "**Multiplicative trend**","**Multiplicative damped trend**"),
                                         c("N","A","M")))
kableTable <- kableExtra::kable(etsAdditiveTable, escape=FALSE, caption="Additive error ETS models",
                                col.names=c("Nonseasonal","Additive seasonality","Multiplicative seasonality"))
kable_styling(kableTable, font_size=12, protect_latex=TRUE)
```


```{r ETSMultiplicativeError, echo=FALSE}
# T="N"
etsMultiplicativeTable <- c("$\\begin{aligned} &y_{t} = l_{t-1}(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1}(1 + \\alpha \\epsilon_t)
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} s_{t-m}(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\
      &s_t = s_{t-m}(1 + \\gamma \\epsilon_t)
    \\end{aligned}$",
# T="A"
    "$\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1})(1 + \\epsilon_t) \\\\
      &l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\
      &l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} + \\beta (l_{t-1} + b_{t-1}) \\epsilon_t \\\\
      &s_t = s_{t-m} (1 + \\gamma \\epsilon_t)
    \\end{aligned}$",
# T="Ad"
    "$\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1})(1 + \\epsilon_t) \\\\
      &l_t = (l_{t-1} + \\phi b_{t-1})(1 + \\alpha \\epsilon_t) \\\\
      &b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &b_t = \\phi b_{t-1} + \\beta \\mu_{y,t} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} (1 + \\alpha \\epsilon_t) \\\\
      &b_t = \\phi b_{t-1} + \\beta (l_{t-1} + \\phi b_{t-1}) \\epsilon_t \\\\
      &s_t = s_{t-m}(1 + \\gamma \\epsilon_t)
    \\end{aligned}$",
# T="M"
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} (1 + \\beta \\epsilon_t)
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1} s_{t-m} (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\\\
      &s_t = s_{t-m} (1 + \\gamma \\epsilon_t)
    \\end{aligned}$",
# T="Md"
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi (1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1}^\\phi (1 + \\beta \\epsilon_t)
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = (l_{t-1} b_{t-1}^\\phi + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\mu_{y,t} \\epsilon_t \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\mu_{y,t}}{l_{t-1}} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t} \\epsilon_t
    \\end{aligned}$",
    "$\\begin{aligned} &y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi \\left(1 + \\alpha \\epsilon_t\\right) \\\\
      &b_t = b_{t-1}^\\phi \\left(1 + \\beta \\epsilon_t\\right) \\\\
      &s_t = s_{t-m} \\left(1 + \\gamma \\epsilon_t\\right)
    \\end{aligned}$")
etsMultiplicativeTable <- matrix(etsMultiplicativeTable, 5, 3, byrow=TRUE,
                           dimnames=list(c("**No trend**","**Additive trend**","**Additive damped trend**",
                                           "**Multiplicative trend**","**Multiplicative damped trend**"),
                                         c("N","A","M")))
kableTable <- kableExtra::kable(etsMultiplicativeTable, escape=FALSE, caption="Multiplicative error ETS models",
                                col.names=c("Nonseasonal","Additive seasonality","Multiplicative seasonality"))
kable_styling(kableTable, font_size=12, protect_latex=TRUE)
```

 The formulae summarised in Table \@ref(tab:ETSModelsForecasts) 


From statistical point of view, formulae in Tables \@ref(tab:ETSAdditiveError) and \@ref(tab:ETSMultiplicativeError) correspond to the "true models" [see Section 1.2 of @SvetunkovSBA], they explain the models underlying potential data, but when it comes to their construction and estimation, the $\epsilon_t$ is substituted by the estimated $e_t$ (which is calculated differently depending on the error type), and time series components and smoothing parameters are also substituted by their estimated analogues (e.g. $\hat{\alpha}$ instead of $\alpha$). However, if the values of parameters of these models were known, then it would be possible to produce point forecasts and conditional h steps ahead expectations from these models. Table \@ref(tab:ETSModelsForecasts) summarises:

- Conditional one step ahead expectation $\mu_{y,t} = \mu_{y,t|t-1}$;
- Multiple steps ahead point forecast $\hat{y}_{t+h}$;
- Conditional multiple steps ahead expectation $\mu_{y,t+h|t}$;

In case of the additive error models, the point forecasts correspond to the expectations only when the expectation of the error term is zero, i.e. $\text{E}(\epsilon_t)=0$, while in case of the multiplicative one the condition is changed to $\text{E}(1+\epsilon_t)=1$.

::: remark
**Not all the point forecasts of ETS models correspond to conditional expectations**. This issue applies to the models with multiplicative trend and / or multiplicative seasonality. This is because ETS model assumes that different states are correlated (they have the same source of error) and as a result multiple steps ahead values (when h>1) of states introduce products of error terms. So, the conditional expectations in these cases might not have analytical forms ("n.c.f." in Table \@ref(tab:ETSModelsForecasts) stands for "no closed form"), and when working with these models, simulations might be required. This does not apply to the one step ahead forecasts, for which all the classical formulae work.
:::

```{r ETSModelsForecasts, echo=FALSE}
# T="N"
etsAdditiveTable <- c("$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{aligned}$",
# T="A"
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} + b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} + h b_t \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} + b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + h b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = (l_{t-1} + b_{t-1}) s_{t-m} \\\\
      &\\hat{y}_{t+h} = \\left(l_{t} + h b_{t-1}\\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{aligned}$",
# T="Ad"
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_t \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} \\\\
      &\\hat{y}_{t+h} = \\left(l_{t} + \\sum_{j=1}^h \\phi^j b_t \\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{aligned}$",
# T="M"
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} b_t^h \\\\
      &\\mu_{y,t+h|t} \\text{ - n.c.f. for } h>1
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^h + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - n.c.f. for } h>1
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} b_{t-1} s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^h s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - n.c.f. for } h>1
    \\end{aligned}$",
# T="Md"
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi \\\\
      &\\hat{y}_{t+h} = l_{t} b_t^{\\sum_{j=1}^h \\phi^j} \\\\
      &\\mu_{y,t+h|t} \\text{ - n.c.f. for } h>1
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - n.c.f. for } h>1
    \\end{aligned}$",
    "$\\begin{aligned}
      &\\mu_{y,t} = l_{t-1} b_{t-1}^\\phi s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - n.c.f. for } h>1
    \\end{aligned}$")
etsAdditiveTable <- matrix(etsAdditiveTable, 5, 3, byrow=TRUE,
                           dimnames=list(c("**No trend**","**Additive trend**","**Additive damped trend**",
                                           "**Multiplicative trend**","**Multiplicative damped trend**"),
                                         c("N","A","M")))
kableTable <- kableExtra::kable(etsAdditiveTable, escape=FALSE, caption="Point forecasts and expectations of ETS models. n.c.f. stands for \"No Closed Form\".",
                                col.names=c("Nonseasonal","Additive seasonality","Multiplicative seasonality"))
kable_styling(kableTable, font_size=12, protect_latex=TRUE)
```

The multiplicative error models have the same one step ahead expectations and the same point forecasts as the additive error ones, but due to the multiplication by the error term, the multiple steps ahead conditional expectations between the two models might differ, specifically for the multiplicative trend and multiplicative seasonal models. These values do not have closed forms and can only be obtained via simulations.

Although there are 30 potential ETS models, not all of them are stable. So, Rob Hyndman has reduced the pool of models under consideration in the `ets()` function of `forecast` package to the following 19: ANN, AAN, AAdN, ANA, AAA, AAdA, MNN, MAN, MAdN, MNA, MAA, MAdA, MNM, MAM, MAdM, MMN, MMdN, MMM, MMdM. In addition, the multiplicative trend models and are unstable in cases of data with outliers, so they are switched off in the `ets()` function by default, which reduces the pool of models further to the first 15.
